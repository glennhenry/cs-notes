"use strict";(self.webpackChunkcs_notes=self.webpackChunkcs_notes||[]).push([[9049],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>h});var r=t(67294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,r,i=function(e,n){if(null==e)return{};var t,r,i={},a=Object.keys(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=r.createContext({}),c=function(e){var n=r.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},p=function(e){var n=c(e.components);return r.createElement(s.Provider,{value:n},e.children)},m="mdxType",g={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},d=r.forwardRef((function(e,n){var t=e.components,i=e.mdxType,a=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),m=c(t),d=i,h=m["".concat(s,".").concat(d)]||m[d]||g[d]||a;return t?r.createElement(h,o(o({ref:n},p),{},{components:t})):r.createElement(h,o({ref:n},p))}));function h(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var a=t.length,o=new Array(a);o[0]=d;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[m]="string"==typeof e?e:i,o[1]=l;for(var c=2;c<a;c++)o[c]=t[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}d.displayName="MDXCreateElement"},19327:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>g,frontMatter:()=>a,metadata:()=>l,toc:()=>c});var r=t(87462),i=(t(67294),t(3905));const a={slug:"/deep-learning/reinforcement-learning/imitation-learning",id:"imitation-learning",title:"Imitation Learning",description:"Imitation Learning"},o=void 0,l={unversionedId:"deep-learning/reinforcement-learning/imitation-learning/imitation-learning",id:"deep-learning/reinforcement-learning/imitation-learning/imitation-learning",title:"Imitation Learning",description:"Imitation Learning",source:"@site/docs/deep-learning/16-reinforcement-learning/09-imitation-learning/imitation-learning.md",sourceDirName:"deep-learning/16-reinforcement-learning/09-imitation-learning",slug:"/deep-learning/reinforcement-learning/imitation-learning",permalink:"/cs-notes/deep-learning/reinforcement-learning/imitation-learning",draft:!1,editUrl:"https://github.com/glennhenry/cs-notes/tree/main/docs/deep-learning/16-reinforcement-learning/09-imitation-learning/imitation-learning.md",tags:[],version:"current",lastUpdatedBy:"glennhenry",lastUpdatedAt:1698320570,formattedLastUpdatedAt:"Oct 26, 2023",frontMatter:{slug:"/deep-learning/reinforcement-learning/imitation-learning",id:"imitation-learning",title:"Imitation Learning",description:"Imitation Learning"},sidebar:"sidebar",previous:{title:"Policy-Gradient",permalink:"/cs-notes/deep-learning/reinforcement-learning/policy-gradient"},next:{title:"Multi-Agent",permalink:"/cs-notes/deep-learning/reinforcement-learning/multi-agent"}},s={},c=[{value:"Behavioral Cloning",id:"behavioral-cloning",level:3},{value:"Inverse Reinforcement Learning (IRL)",id:"inverse-reinforcement-learning-irl",level:3}],p={toc:c},m="wrapper";function g(e){let{components:n,...a}=e;return(0,i.kt)(m,(0,r.Z)({},p,a,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Main Source :")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},(0,i.kt)("a",{parentName:"strong",href:"https://smartlabai.medium.com/a-brief-overview-of-imitation-learning-8a8a75c44a9c"},"A brief overview of Imitation Learning by SmartLab AI - Medium")))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Imitation Learning")," is technique where an agent learns a policy by imitating the behavior of an expert. An expert refers to a knowledgeable or skilled agent that already knows how to perform a task. An expert can be another reinforcement learning agent, or a human that provides demonstrations for the learning agent. Instead of doing trial and error exploration, the agent can leverages the knowledge of the expert to accelerate the learning process, this is useful when it is hard to define the desired behavior or reward function."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Imitation learning",src:t(73368).Z,width:"850",height:"452"}),(0,i.kt)("br",{parentName:"p"}),"\n","Source : ",(0,i.kt)("a",{parentName:"p",href:"https://www.researchgate.net/figure/The-framework-of-Reinforcement-Learning-Imitation-Learning-and-their-integration-The_fig4_322094035"},"https://www.researchgate.net/figure/The-framework-of-Reinforcement-Learning-Imitation-Learning-and-their-integration-The_fig4_322094035")),(0,i.kt)("h3",{id:"behavioral-cloning"},"Behavioral Cloning"),(0,i.kt)("p",null,"Behavioral cloning is a direct imitation learning technique, it is a simple technique where the agent directly imitating the behavior of an expert. Behavioral cloning is a supervised learning technique where the data is the expert behavior in state-action pair."),(0,i.kt)("p",null,"The agent can use standard supervised learning architecture such as classifier or regressor, the model will aim to minimize the difference between predicted actions and the expert actions using a loss function."),(0,i.kt)("p",null,"Behavioral cloning can be beneficial if a well-defined expert demonstration is available, however, this assume the demonstration is optimal or near-optimal so that the agent doesn't replicate the expert's mistakes."),(0,i.kt)("h3",{id:"inverse-reinforcement-learning-irl"},"Inverse Reinforcement Learning (IRL)"),(0,i.kt)("p",null,"In behavioral cloning, the agent imitate the expert behavior. In other word, the agent try to learn its state-action map. On the other hand, ",(0,i.kt)("strong",{parentName:"p"},"Inverse Reinforcement Learning (IRL)")," instead learns the reward function and find the optimal policy that maximizes the reward function."),(0,i.kt)("p",null,"Same as behavioral cloning, IRL takes state-action pair from the expert, it also assume they are optimal or near optimal behavior. The goal is to infer the reward function that best explains the observed expert behavior. Once the reward function is inferred, the next step is to find the optimal policy. We will then compare the learned policy with the expert's policy."),(0,i.kt)("p",null,"This process is repeated until the learned policy is close enough to the expert's policy. Overall, IRL is valuable in scenarios where the reward function is hard to define."),(0,i.kt)("p",null,"An IRL problem can be divided into two, the first case is when we know the environment ",(0,i.kt)("strong",{parentName:"p"},"(model-based)"),", and the other case is when we do not have explicit model of environment ",(0,i.kt)("strong",{parentName:"p"},"(model-free)"),". The model-free case relies on trial and error approach, it can potentially have large state-action spaces."))}g.isMDXComponent=!0},73368:(e,n,t)=>{t.d(n,{Z:()=>r});const r=t.p+"assets/images/imitation-learning-655ba3e83a93fe194bdf0d4380251e73.png"}}]);