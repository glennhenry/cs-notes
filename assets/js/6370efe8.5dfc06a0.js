"use strict";(self.webpackChunkcs_notes=self.webpackChunkcs_notes||[]).push([[161],{28453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var t=r(96540);const s={},i=t.createContext(s);function o(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(i.Provider,{value:n},e.children)}},73355:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/gpt-architecture-e3c388290a39a563bb39bafcb5219dda.png"},87004:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"deep-learning/transformers/gpt/gpt","title":"GPT","description":"GPT","source":"@site/docs/deep-learning/14-transformers/04-gpt/gpt.md","sourceDirName":"deep-learning/14-transformers/04-gpt","slug":"/deep-learning/transformers/gpt","permalink":"/cs-notes/deep-learning/transformers/gpt","draft":false,"unlisted":false,"editUrl":"https://github.com/glennhenry/cs-notes/tree/main/docs/deep-learning/14-transformers/04-gpt/gpt.md","tags":[],"version":"current","lastUpdatedBy":"glennhenry","lastUpdatedAt":1723455088000,"frontMatter":{"slug":"/deep-learning/transformers/gpt","id":"gpt","title":"GPT","description":"GPT"},"sidebar":"sidebar","previous":{"title":"BERT","permalink":"/cs-notes/deep-learning/transformers/bert"},"next":{"title":"BART","permalink":"/cs-notes/deep-learning/transformers/bart"}}');var s=r(74848),i=r(28453);const o={slug:"/deep-learning/transformers/gpt",id:"gpt",title:"GPT",description:"GPT"},a=void 0,d={},l=[{value:"Architecture",id:"architecture",level:3},{value:"GPT Version",id:"gpt-version",level:3}];function c(e){const n={a:"a",br:"br",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Main Source:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"https://youtu.be/3IweGfgytgY?si=Fed0FkK55LAH5SAL",children:"GPT - Explained \u2014 CodeEmporium"})})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/deep-learning/transformers/transformers-architecture",children:"Transformers Architecture"})})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Generative Pre-trained Transformer (GPT)"})," is language model (computational model trained to understand and generate human language) based on the ",(0,s.jsx)(n.a,{href:"/deep-learning/transformers/transformers-architecture",children:"transformers architecture"}),". GPT is a decoder-only model, meaning it can only generates sequence based on the previous sequence. In other word, GPT predict next word in a sequence given the context provided by the preceding words. So, GPT require an initial word or prompt as input to provide the context to generate text."]}),"\n",(0,s.jsx)(n.p,{children:"The key features of GPT are:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Autoregressive"}),": Also known as ",(0,s.jsx)(n.strong,{children:"causal-attention"})," which is similar to the properties of decoder of the standard transformers architecture, it generates next word based on preceding word in sequential manner. GPT is often called ",(0,s.jsx)(n.strong,{children:"causal language model"})," because it predict the next word in causal manner."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pre-trained & Fine Tuned"}),": GPT model is pre-trained on a large amount of text dataset to helps the model learn the general representation of the langauge. The model will then be fine tuned on specific tasks to adapt them on specific application."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"architecture",children:"Architecture"}),"\n",(0,s.jsx)(n.p,{children:"GPT consist of 12 transformers decoder stacked on top of each other. Each decoder layer consider the previously generated words and uses self-attention mechanisms to capture the relationships between the words."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.img,{alt:"GPT architecture",src:r(73355).A+"",width:"1058",height:"437"}),(0,s.jsx)(n.br,{}),"\n","Source: ",(0,s.jsx)(n.a,{href:"https://paperswithcode.com/method/gpt",children:"https://paperswithcode.com/method/gpt"})]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Input"}),": The input of GPT which is a decoder is its own previous output or the initial prompt. The input is tokenized or turned into numerical representation and then transformed into vector (embedding). Alongside the input embedding, GPT also incorporates positional encoding to capture the position of the tokens in the sequence."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Decoder Layer"}),": A single decoder layer includes masked multi self attention, add & layer normalization, feed forward network, and another add & layer normalization."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Output"}),": The output from previous layer is passed into linear layer and a softmax activation function, the probability produced represents the likelihood of each word in the vocabulary being the next word in the generated text. The output processing can be customized into the task requirement."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"gpt-version",children:"GPT Version"}),"\n",(0,s.jsx)(n.p,{children:"GPT has introduced improvements and advancements over its predecessors:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"GPT-1"}),": The original version of GPT released in June 2018, consist of 12 transformer decoder layers and approximately 117 million parameters."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"GPT-2"}),": GPT-2 was released in February 2019, ranging from 117 million to 1.5 billion parameters and have up to 48 decoder layers."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"GPT-3"}),": Released in June 2020, it introduced significant leap in scale and performance. Has 125 million to 175 billion parameters and 96 decoder layers."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"GPT-3.5"}),": Released in March 2022 and used for ",(0,s.jsx)(n.a,{href:"https://chat.openai.com/",children:"ChatGPT"})," which is the fine tuned model from GPT-3.5 series released in November 2022."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"GPT-4"}),": The latest model released in March 2023, it was pre-trained on a combination of public data and fine tuned with reinforcement learning for a better feedback for human."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);