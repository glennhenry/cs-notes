"use strict";(self.webpackChunkcs_notes=self.webpackChunkcs_notes||[]).push([[4411],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>h});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),c=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(n),m=a,h=u["".concat(l,".").concat(m)]||u[m]||d[m]||o;return n?r.createElement(h,i(i({ref:t},p),{},{components:n})):r.createElement(h,i({ref:t},p))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:a,i[1]=s;for(var c=2;c<o;c++)i[c]=n[c];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},9485:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var r=n(87462),a=(n(67294),n(3905));const o={slug:"/deep-learning/u-net",id:"u-net",title:"U-Net",description:"U-Net"},i=void 0,s={unversionedId:"deep-learning/u-net/u-net",id:"deep-learning/u-net/u-net",title:"U-Net",description:"U-Net",source:"@site/docs/deep-learning/06-u-net/u-net.md",sourceDirName:"deep-learning/06-u-net",slug:"/deep-learning/u-net",permalink:"/cs-notes/deep-learning/u-net",draft:!1,editUrl:"https://github.com/glennhenry/cs-notes/tree/main/docs/deep-learning/06-u-net/u-net.md",tags:[],version:"current",lastUpdatedBy:"glennhenry",lastUpdatedAt:1697188152,formattedLastUpdatedAt:"Oct 13, 2023",frontMatter:{slug:"/deep-learning/u-net",id:"u-net",title:"U-Net",description:"U-Net"},sidebar:"sidebar",previous:{title:"ResNet",permalink:"/cs-notes/deep-learning/resnet"},next:{title:"Siamese Network",permalink:"/cs-notes/deep-learning/siamese-network"}},l={},c=[{value:"Architecture",id:"architecture",level:3},{value:"Output &amp; Learning",id:"output--learning",level:3}],p={toc:c},u="wrapper";function d(e){let{components:t,...o}=e;return(0,a.kt)(u,(0,r.Z)({},p,o,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Main Source :")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},(0,a.kt)("a",{parentName:"strong",href:"https://youtu.be/NhdzGfB1q74?si=BiC8L1cs-YRLn9T4"},"The U-Net (actually) explained in 10 minutes - rupert ai")))),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"U-Net")," is a U-shaped ",(0,a.kt)("a",{parentName:"p",href:"/deep-learning/cnn"},"convolutional neural network")," architecture that follows the encoder-decoder network with a ",(0,a.kt)("a",{parentName:"p",href:"/deep-learning/resnet#residual-connection"},"skip connection"),"."),(0,a.kt)("p",null,"An encoder-decoder network consist of an encoder and a decoder. The encoder is a component responsible for processing input data and extracting relevant features. The decoder reconstructs or generates the desired output based on the encoded information produced by encoder."),(0,a.kt)("h3",{id:"architecture"},"Architecture"),(0,a.kt)("p",null,'As the name suggests, the architecture has a "U" shape and is symmetric. It begins by extracting the features of input image, downsampling, and then the reverse process.'),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"U-Net architecture",src:n(28394).Z,width:"750",height:"505"}),(0,a.kt)("br",{parentName:"p"}),"\n","Source : ",(0,a.kt)("a",{parentName:"p",href:"https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5"},"https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5")),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Input")," : U-Net typically used for image segmentation tasks, the input of U-Net is an image that requires segmentation."),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Encoder")," : The encoder, which is the left part of the architecture, extracts the features of the image, it consists of a series of convolutional layers with ReLU activation function followed by pooling or down-sampling operations."),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Skip Connection")," : Each encoder's layers level is included with skip connection that connects directly to decoder, allowing the features to flow without being sampled in the encoder's layer."),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Decoder")," : The last level of encoder's layer produced the highly abstract and global information about the input image. The decoder takes it and perform the reverse process of encoder including upsampling followed by convolutional layers.")),(0,a.kt)("p",null,"The skip connection combined with decoder makes U-Net suitable for tasks requiring image segmentation, particularly in domains like biomedicine. The decoder is able to combine high-level features from the output of encoder with the information from multiple level of encoder's layer accessed using skip connection."),(0,a.kt)("h3",{id:"output--learning"},"Output & Learning"),(0,a.kt)("p",null,"The output of U-Net is a segmentation mask or a probability map that indicates the presence or absence of different classes or regions in the input image. The initial output will obviously be messed up."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"U-Net output",src:n(99704).Z,width:"1124",height:"293"}),(0,a.kt)("br",{parentName:"p"}),"\n","Source : ",(0,a.kt)("a",{parentName:"p",href:"https://catchzeng.medium.com/the-easiest-way-to-train-a-u-net-image-segmentation-model-using-tensorflow-and-labelme-fe130de45a19"},"https://catchzeng.medium.com/the-easiest-way-to-train-a-u-net-image-segmentation-model-using-tensorflow-and-labelme-fe130de45a19")),(0,a.kt)("p",null,"The loss is calculated by comparing the output of U-Net with the ground truth segmentation masks. This mean we need labeled segmentation masks for each input we have. The loss will then backpropagated through the network and the gradients with respect to network's parameters will be adjusted."))}d.isMDXComponent=!0},28394:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/u-net-architecture-53f7db64426e1c6028985f6d2905ef78.png"},99704:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/u-net-output-41243821876b691ef9a0a6427d66235f.png"}}]);