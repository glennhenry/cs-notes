"use strict";(self.webpackChunkcs_notes=self.webpackChunkcs_notes||[]).push([[8613],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>u});var r=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=r.createContext({}),m=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=m(e.components);return r.createElement(s.Provider,{value:t},e.children)},d="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},h=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),d=m(n),h=o,u=d["".concat(s,".").concat(h)]||d[h]||p[h]||a;return n?r.createElement(u,i(i({ref:t},c),{},{components:n})):r.createElement(u,i({ref:t},c))}));function u(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,i=new Array(a);i[0]=h;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[d]="string"==typeof e?e:o,i[1]=l;for(var m=2;m<a;m++)i[m]=n[m];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}h.displayName="MDXCreateElement"},9721:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>l,toc:()=>m});var r=n(87462),o=(n(67294),n(3905));const a={slug:"/deep-learning/reinforcement-learning/monte-carlo-method",id:"monte-carlo-method",title:"Monte Carlo Method",description:"Monte Carlo Method"},i=void 0,l={unversionedId:"deep-learning/reinforcement-learning/monte-carlo-method/monte-carlo-method",id:"deep-learning/reinforcement-learning/monte-carlo-method/monte-carlo-method",title:"Monte Carlo Method",description:"Monte Carlo Method",source:"@site/docs/deep-learning/16-reinforcement-learning/04-monte-carlo-method/monte-carlo-method.md",sourceDirName:"deep-learning/16-reinforcement-learning/04-monte-carlo-method",slug:"/deep-learning/reinforcement-learning/monte-carlo-method",permalink:"/cs-notes/deep-learning/reinforcement-learning/monte-carlo-method",draft:!1,editUrl:"https://github.com/glennhenry/cs-notes/tree/main/docs/deep-learning/16-reinforcement-learning/04-monte-carlo-method/monte-carlo-method.md",tags:[],version:"current",lastUpdatedBy:"glennhenry",lastUpdatedAt:1698144329,formattedLastUpdatedAt:"Oct 24, 2023",frontMatter:{slug:"/deep-learning/reinforcement-learning/monte-carlo-method",id:"monte-carlo-method",title:"Monte Carlo Method",description:"Monte Carlo Method"},sidebar:"sidebar",previous:{title:"Markov Decision Process",permalink:"/cs-notes/deep-learning/reinforcement-learning/markov-decision-process"},next:{title:"Temporal Difference",permalink:"/cs-notes/deep-learning/reinforcement-learning/temporal-difference"}},s={},m=[{value:"Algorithm",id:"algorithm",level:3},{value:"Example",id:"example",level:4}],c={toc:m},d="wrapper";function p(e){let{components:t,...a}=e;return(0,o.kt)(d,(0,r.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Main Source :")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},(0,o.kt)("a",{parentName:"strong",href:"https://medium.com/data-science-in-your-pocket/monte-carlo-for-reinforcement-learning-with-example-1754439dd628"},"Monte Carlo for Reinforcement Learning with example by Mehul Gupta - Medium")))),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Monte Carlo Method")," is a technique to estimate a ",(0,o.kt)("a",{parentName:"p",href:"/deep-learning/reinforcement-learning/reinforcement-learning-fundamental#value-function"},"value")," of a state or state-action pairs based on the experiences gained through interactions with the environment. Monte Carlo method is a ",(0,o.kt)("a",{parentName:"p",href:"/deep-learning/reinforcement-learning/reinforcement-learning-fundamental#model-based--model-free"},"model-free")," method, meaning we don't need information or prior knowledge about the environment such as the transition probabilities and the rewards associated with each state and action."),(0,o.kt)("p",null,"The question is :"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"How does Monte Carlo method interact with the environment without knowing it?")),(0,o.kt)("p",null,"Monte Carlo method works by simulating the environment, it simulates the environment based on current knowledge and observations. Initially, we start with a not perfect simulation, we will then define the initial state and action. The agent begins its interaction, the observed interaction will be used to update the simulation. We will also do the main purpose of this method, which is to estimate the value or policy based on the agent experience or outcomes. The value is estimated based on the average rewards the agent received, while the policy is updated to maximize the estimated value."),(0,o.kt)("p",null,"The process is done in a single ",(0,o.kt)("strong",{parentName:"p"},"episode"),", which is a predetermined length sequence of interactions between an agent and its environment. This will be repeated for multiple times, as we gain more information about the environment, we can also estimate the value more accurately."),(0,o.kt)("p",null,"The simulated environment allows the agent to interact with the environment without having real access to it. Overall, it's a very nice method to use if we have limited knowledge about the environment."),(0,o.kt)("h3",{id:"algorithm"},"Algorithm"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Monte Carlo method algorithm",src:n(76632).Z,width:"845",height:"468"}),(0,o.kt)("br",{parentName:"p"}),"\n","Source : ",(0,o.kt)("a",{parentName:"p",href:"https://youtu.be/o8XGKkIA1gE?si=NpJ_6VxZwT06ZHta"},"https://youtu.be/o8XGKkIA1gE?si=NpJ_6VxZwT06ZHta")),(0,o.kt)("h4",{id:"example"},"Example"),(0,o.kt)("p",null,"For example, consider a maze problem. In this case, we don't know the information about the environment including its state, action, and rewards. We don't know where we at, we don't know if we should move in definite direction (e.g. left, right, up, or down). The goal of the problem may also be unknown, it is purely driven by rewards."),(0,o.kt)("p",null,"The agent starts exploring the maze randomly in any direction. During the visits, we also record the states it observed to improve the simulation and the rewards it received. For each state it visited, update the value estimate by averaging the returns obtained after visiting that states. Based on the value estimation, we will also update the policy. This process is done in a single episode and will be repeated for multiple times."))}p.isMDXComponent=!0},76632:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/monte-carlo-method-algorithm-28606f9b6e9f60b6a84e7e4b7cad642e.png"}}]);