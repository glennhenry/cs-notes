"use strict";(self.webpackChunkcs_notes=self.webpackChunkcs_notes||[]).push([[6540],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>m});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=r.createContext({}),s=function(e){var t=r.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},d=function(e){var t=s(e.components);return r.createElement(c.Provider,{value:t},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},h=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,c=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=s(n),h=a,m=u["".concat(c,".").concat(h)]||u[h]||p[h]||o;return n?r.createElement(m,i(i({ref:t},d),{},{components:n})):r.createElement(m,i({ref:t},d))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=h;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l[u]="string"==typeof e?e:a,i[1]=l;for(var s=2;s<o;s++)i[s]=n[s];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}h.displayName="MDXCreateElement"},77983:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>p,frontMatter:()=>o,metadata:()=>l,toc:()=>s});var r=n(87462),a=(n(67294),n(3905));const o={slug:"/deep-learning/autoencoder",id:"autoencoder",title:"Autoencoder",description:"Autoencoder"},i=void 0,l={unversionedId:"deep-learning/autoencoder/autoencoder",id:"deep-learning/autoencoder/autoencoder",title:"Autoencoder",description:"Autoencoder",source:"@site/docs/deep-learning/10-autoencoder/autoencoder.md",sourceDirName:"deep-learning/10-autoencoder",slug:"/deep-learning/autoencoder",permalink:"/cs-notes/deep-learning/autoencoder",draft:!1,editUrl:"https://github.com/glennhenry/cs-notes/tree/main/docs/deep-learning/10-autoencoder/autoencoder.md",tags:[],version:"current",lastUpdatedBy:"glennhenry",lastUpdatedAt:1696245437,formattedLastUpdatedAt:"Oct 2, 2023",frontMatter:{slug:"/deep-learning/autoencoder",id:"autoencoder",title:"Autoencoder",description:"Autoencoder"},sidebar:"sidebar",previous:{title:"GRU",permalink:"/cs-notes/deep-learning/gru"},next:{title:"Variational Autoencoder",permalink:"/cs-notes/deep-learning/variational-autoencoder"}},c={},s=[{value:"Encoder &amp; Decoder",id:"encoder--decoder",level:3},{value:"Encoder",id:"encoder",level:4},{value:"Decoder",id:"decoder",level:4}],d={toc:s},u="wrapper";function p(e){let{components:t,...o}=e;return(0,a.kt)(u,(0,r.Z)({},d,o,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Main Source :")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},(0,a.kt)("a",{parentName:"strong",href:"https://youtu.be/1icvxbAoPWc?si=h_INIqnLio0jnUw6"},"Encoder Decoder Network - Computerphile"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},(0,a.kt)("a",{parentName:"strong",href:"https://youtu.be/Rdpbnd0pCiI?si=CxZ0egCEEmxiIAIo"},"What is an Autoencoder? | Two Minute Papers #86")))),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Autoencoder")," is a type of ",(0,a.kt)("a",{parentName:"p",href:"/deep-learning/neural-network"},"neural network")," used for unsupervised learning, where the data doesn't have correct label or output, this makes the model learns the pattern in the data without explicit guidance."),(0,a.kt)("p",null,"Autoencoder is used to reconstruct input data, given a data, it will be reconstructed for specific purpose such as :"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Dimensionality Reduction")," : Dimensionality reduction is the process of compressing high-dimensional data into a lower-dimensional representation while retaining its important characteristics. For example, in the case of image data, a higher-resolution image can be downscaled into a lower-resolution image without the loss of information."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Data Denoising")," : When a data is nosied, meaning it has unwanted information, we can reconstruct it to get the clean version of the input data."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Anomaly Detection")," : In autoencoder, the model know how to reconstruct a data. When an unusual pattern or anomalies exist in the data, the model should be able to identify it.")),(0,a.kt)("h3",{id:"encoder--decoder"},"Encoder & Decoder"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Autoencoder architecture",src:n(42454).Z,width:"578",height:"279"}),(0,a.kt)("br",{parentName:"p"}),"\n","Source : ",(0,a.kt)("a",{parentName:"p",href:"https://medium.com/@birla.deepak26/autoencoders-76bb49ae6a8f"},"https://medium.com/@birla.deepak26/autoencoders-76bb49ae6a8f")),(0,a.kt)("p",null,"Autoencoder network consist of two components :"),(0,a.kt)("h4",{id:"encoder"},"Encoder"),(0,a.kt)("p",null,"Encoder is responsible for taking the input data and transforming it into a lower-dimensional representation, this process is also called ",(0,a.kt)("strong",{parentName:"p"},"encoding")," or ",(0,a.kt)("strong",{parentName:"p"},"latent space representation"),"."),(0,a.kt)("p",null,"The encoder consists of several layers of neural network such as ",(0,a.kt)("a",{parentName:"p",href:"/deep-learning/cnn"},"convolutional neural network")," that gradually reduce the dimensionality of the input data and capture the essential charateristics. The networks has model's parameters such as weight and bias which will be adjusted later in the learning process. ",(0,a.kt)("a",{parentName:"p",href:"/deep-learning/deep-learning-foundation#activation-function"},"Activation function")," will also be included inside to introduce non-linearity."),(0,a.kt)("p",null,"The result of the layers will be flattened and fed into a fully-connected layer, similar to traditional CNN. The output of fully-connected layer is connected to another layer called ",(0,a.kt)("strong",{parentName:"p"},"bottleneck"),", this is where data is represented in latent space."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Encoding, downscaling the data, in this case an image data",src:n(52034).Z,width:"470",height:"207"}),(0,a.kt)("br",{parentName:"p"}),"\n","Source : ",(0,a.kt)("a",{parentName:"p",href:"https://www.gabormelli.com/RKB/Convolutional_Autoencoder"},"https://www.gabormelli.com/RKB/Convolutional_Autoencoder")),(0,a.kt)("h4",{id:"decoder"},"Decoder"),(0,a.kt)("p",null,"The bottleneck layer which has the lower-dimensional representation of the data serve as the input for the decoder. Decoder is the reverse process of encoder, it has direct connection with it, meaning they will have the identical layers."),(0,a.kt)("p",null,"From the bottleneck layer, the data is fed into a fully-connected layer and will be reshaped into the same size before it was flattened. Continuing the process, the decoder will have the same convolution layers and if pooling layers were used in the encoder, the reverse process will be done."),(0,a.kt)("p",null,"The final layer of decoder is responsible for outputting or reconstructing the input data. Keep in mind that because autoencoder is an unsupervised learning, loss calculation will be different with the traditional CNN. The loss will be calculated by comparing the original input data with the reconstructed data. The model will measure how different is the reconstructed data and will adjust its parameters to be able to reconstruct a more similar data."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Decoder, which is symmetric to encoder",src:n(44182).Z,width:"850",height:"311"}),(0,a.kt)("br",{parentName:"p"}),"\n","Source : ",(0,a.kt)("a",{parentName:"p",href:"https://analyticsindiamag.com/how-to-implement-convolutional-autoencoder-in-pytorch-with-cuda/"},"https://analyticsindiamag.com/how-to-implement-convolutional-autoencoder-in-pytorch-with-cuda/")))}p.isMDXComponent=!0},42454:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/autoencoder-73786b25b317bd5fca128f87b1f02408.png"},44182:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/decoder-8bc56c1b6a3c0eef370fa56b09a74f36.png"},52034:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/encoding-5d24e16a41fda34edeb4a70bbd834e2e.png"}}]);