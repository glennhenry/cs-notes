"use strict";(self.webpackChunkcs_notes=self.webpackChunkcs_notes||[]).push([[1780],{3905:(e,t,n)=>{n.d(t,{Zo:()=>s,kt:()=>u});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var p=r.createContext({}),c=function(e){var t=r.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},s=function(e){var t=c(e.components);return r.createElement(p.Provider,{value:t},e.children)},d="mdxType",g={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,p=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),d=c(n),m=a,u=d["".concat(p,".").concat(m)]||d[m]||g[m]||i;return n?r.createElement(u,o(o({ref:t},s),{},{components:n})):r.createElement(u,o({ref:t},s))}));function u(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=m;var l={};for(var p in t)hasOwnProperty.call(t,p)&&(l[p]=t[p]);l.originalType=e,l[d]="string"==typeof e?e:a,o[1]=l;for(var c=2;c<i;c++)o[c]=n[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},89555:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>o,default:()=>g,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var r=n(87462),a=(n(67294),n(3905));const i={slug:"/deep-learning/reinforcement-learning/policy-gradient",id:"policy-gradient",title:"Policy-Gradient",description:"Policy-Gradient"},o=void 0,l={unversionedId:"deep-learning/reinforcement-learning/policy-gradient/policy-gradient",id:"deep-learning/reinforcement-learning/policy-gradient/policy-gradient",title:"Policy-Gradient",description:"Policy-Gradient",source:"@site/docs/deep-learning/16-reinforcement-learning/08-policy-gradient/policy-gradient.md",sourceDirName:"deep-learning/16-reinforcement-learning/08-policy-gradient",slug:"/deep-learning/reinforcement-learning/policy-gradient",permalink:"/cs-notes/deep-learning/reinforcement-learning/policy-gradient",draft:!1,editUrl:"https://github.com/glennhenry/cs-notes/tree/main/docs/deep-learning/16-reinforcement-learning/08-policy-gradient/policy-gradient.md",tags:[],version:"current",lastUpdatedBy:"glennhenry",lastUpdatedAt:1698320570,formattedLastUpdatedAt:"Oct 26, 2023",frontMatter:{slug:"/deep-learning/reinforcement-learning/policy-gradient",id:"policy-gradient",title:"Policy-Gradient",description:"Policy-Gradient"},sidebar:"sidebar",previous:{title:"Q-Learning",permalink:"/cs-notes/deep-learning/reinforcement-learning/q-learning"},next:{title:"Imitation Learning",permalink:"/cs-notes/deep-learning/reinforcement-learning/imitation-learning"}},p={},c=[{value:"Vanilla Policy Gradient (VPG)",id:"vanilla-policy-gradient-vpg",level:3}],s={toc:c},d="wrapper";function g(e){let{components:t,...i}=e;return(0,a.kt)(d,(0,r.Z)({},s,i,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Main Source :")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},(0,a.kt)("a",{parentName:"strong",href:"https://spinningup.openai.com/en/latest/algorithms/vpg.html"},"Vanilla Policy Gradient - OpenAI Spinning Up Docs")))),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Policy-Gradient")," is a class of reinforcement learning algorithm that find optimal policy directly without estimating the value function first. The policy is adjusted by using the notion of gradient, which provides information on how the policy should be updated."),(0,a.kt)("p",null,"The policy will be parameterized, meaning we will model it with a parameters or value that can be adjusted or learned. We will then take the gradient of the rewards we earned with respect to the policy parameters. The gradient tells us how our current policy affects the rewards, we can then update the policy to maximize the rewards."),(0,a.kt)("h3",{id:"vanilla-policy-gradient-vpg"},"Vanilla Policy Gradient (VPG)"),(0,a.kt)("p",null,"VPG, also known as ",(0,a.kt)("strong",{parentName:"p"},"REINFORCE"),", is a very simple policy gradient method."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"VPG formula",src:n(98678).Z,width:"692",height:"401"}),(0,a.kt)("br",{parentName:"p"}),"\n","Source : ",(0,a.kt)("a",{parentName:"p",href:"https://spinningup.openai.com/en/latest/algorithms/vpg.html"},"https://spinningup.openai.com/en/latest/algorithms/vpg.html")),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Collect Trajectory")," : ",(0,a.kt)("a",{parentName:"p",href:"/deep-learning/reinforcement-learning/reinforcement-learning-fundamental#trajectory"},"Trajectory"),", or the sequence states, actions, and rewards the agent made during the interaction with environment using the current policy.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Compute Reward & Advantage Function")," : Compute the return for each state and action encountered in the trajectory, also compute the ",(0,a.kt)("a",{parentName:"p",href:"/deep-learning/reinforcement-learning/reinforcement-learning-fundamental#advantage-function"},"advantage function"),".")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Compute Policy Gradient")," : The policy function outputs a probability distribution over actions in a given state. By updating the policy parameters, it means we are updating how will it produce the distribution. The agent take action by sampling from that distribution, we have option to sample it in a stochastic or greedy (select highest reward) manner."),(0,a.kt)("p",{parentName:"li"},"The computation of policy gradient involve taking the gradient of the logarithm of the policy's probability distribution with respect to the policy parameters, which is scaled by the advantage function to encourage actions that are better than the expected return and discourage actions that are worse. The result will be summed up for each time step.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Update Policy Parameters")," : The policy parameters will be updated with the ",(0,a.kt)("a",{parentName:"p",href:"/machine-learning/linear-regression#gradient-descent"},"gradient descent algorithm"),".")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Value Function Update")," : The value function, which estimates the expected return, can be fitted using regression techniques such as ",(0,a.kt)("a",{parentName:"p",href:"/machine-learning/linear-regression"},"linear regression"),". This involves minimizing the difference between the predicted values and the observed returns by using the mean squared error loss function and the gradient descent algorithm."))))}g.isMDXComponent=!0},98678:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/vpg-f8106665288ef76c0baf65a9437b67d9.png"}}]);