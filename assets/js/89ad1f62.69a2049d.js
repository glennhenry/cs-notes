"use strict";(self.webpackChunkcs_notes=self.webpackChunkcs_notes||[]).push([[2196],{1579:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>i,metadata:()=>t,toc:()=>o});const t=JSON.parse('{"id":"deep-learning/reinforcement-learning/temporal-difference/temporal-difference","title":"Temporal Difference","description":"Temporal Difference","source":"@site/docs/deep-learning/16-reinforcement-learning/05-temporal-difference/temporal-difference.md","sourceDirName":"deep-learning/16-reinforcement-learning/05-temporal-difference","slug":"/deep-learning/reinforcement-learning/temporal-difference","permalink":"/cs-notes/deep-learning/reinforcement-learning/temporal-difference","draft":false,"unlisted":false,"editUrl":"https://github.com/glennhenry/cs-notes/tree/main/docs/deep-learning/16-reinforcement-learning/05-temporal-difference/temporal-difference.md","tags":[],"version":"current","lastUpdatedBy":"glennhenry","lastUpdatedAt":1723455088000,"frontMatter":{"slug":"/deep-learning/reinforcement-learning/temporal-difference","id":"temporal-difference","title":"Temporal Difference","description":"Temporal Difference"},"sidebar":"sidebar","previous":{"title":"Monte Carlo Method","permalink":"/cs-notes/deep-learning/reinforcement-learning/monte-carlo-method"},"next":{"title":"SARSA","permalink":"/cs-notes/deep-learning/reinforcement-learning/sarsa"}}');var s=r(74848),a=r(28453);const i={slug:"/deep-learning/reinforcement-learning/temporal-difference",id:"temporal-difference",title:"Temporal Difference",description:"Temporal Difference"},l=void 0,c={},o=[{value:"TD Error",id:"td-error",level:3}];function m(e){const n={a:"a",annotation:"annotation",br:"br",h3:"h3",img:"img",li:"li",math:"math",mi:"mi",mrow:"mrow",msub:"msub",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Main Source:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"https://youtu.be/L64E_NTZJ_0?si=4qVOFTxGu6789Xth",children:"Temporal Difference Learning - Reinforcement Learning Chapter 6 \u2014 Connor Shorten"})})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Temporal Difference (TD)"})," is ",(0,s.jsx)(n.a,{href:"/deep-learning/reinforcement-learning/reinforcement-learning-fundamental#model-based--model-free",children:"model-free"})," method to estimate value of a state or state-action pair in an iterative manner by comparing the current estimate with a newly observed estimate."]}),"\n",(0,s.jsxs)(n.p,{children:["TD can be thought as the combination of ",(0,s.jsx)(n.a,{href:"/deep-learning/reinforcement-learning/markov-decision-process",children:"MDP"})," and ",(0,s.jsx)(n.a,{href:"/deep-learning/reinforcement-learning/monte-carlo-method",children:"Monte Carlo method"}),", it updates value in iterative manner without the complete knowledge of the environment."]}),"\n",(0,s.jsx)(n.h3,{id:"td-error",children:"TD Error"}),"\n",(0,s.jsxs)(n.p,{children:["The key idea of TD is by predicting the value and see how wrong is the prediction and use that knowledge to make better prediction. In essence, it is similar to the traditional machine learning algorithm: ",(0,s.jsx)(n.a,{href:"/machine-learning/linear-regression",children:"linear regression"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["The how wrong our prediction is called ",(0,s.jsx)(n.strong,{children:"TD error"}),", it is the difference between our current prediction or estimate for the value with the value we are getting in the current state. The value we are getting in the current state also depends on the value of the next state (similar to MDP)."]}),"\n",(0,s.jsx)(n.p,{children:"Here is the formula for TD error:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.img,{alt:"TD error formula",src:r(23371).A+"",width:"529",height:"331"}),(0,s.jsx)(n.br,{}),"\n","Source: ",(0,s.jsx)(n.a,{href:"https://www.slideserve.com/menefer/reinforcement-learning-part-2",children:"https://www.slideserve.com/menefer/reinforcement-learning-part-2"})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.span,{className:"katex",children:[(0,s.jsx)(n.span,{className:"katex-mathml",children:(0,s.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(n.semantics,{children:[(0,s.jsx)(n.mrow,{children:(0,s.jsxs)(n.msub,{children:[(0,s.jsx)(n.mi,{children:"r"}),(0,s.jsx)(n.mi,{children:"t"})]})}),(0,s.jsx)(n.annotation,{encoding:"application/x-tex",children:"r_t"})]})})}),(0,s.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(n.span,{className:"base",children:[(0,s.jsx)(n.span,{className:"strut",style:{height:"0.5806em",verticalAlign:"-0.15em"}}),(0,s.jsxs)(n.span,{className:"mord",children:[(0,s.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"r"}),(0,s.jsx)(n.span,{className:"msupsub",children:(0,s.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,s.jsxs)(n.span,{className:"vlist-r",children:[(0,s.jsx)(n.span,{className:"vlist",style:{height:"0.2806em"},children:(0,s.jsxs)(n.span,{style:{top:"-2.55em",marginLeft:"-0.0278em",marginRight:"0.05em"},children:[(0,s.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,s.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,s.jsx)(n.span,{className:"mord mathnormal mtight",children:"t"})})]})}),(0,s.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,s.jsx)(n.span,{className:"vlist-r",children:(0,s.jsx)(n.span,{className:"vlist",style:{height:"0.15em"},children:(0,s.jsx)(n.span,{})})})]})})]})]})})]}),": The immediate reward received after taking an action in the current state.",(0,s.jsx)(n.br,{}),"\n",(0,s.jsxs)(n.span,{className:"katex",children:[(0,s.jsx)(n.span,{className:"katex-mathml",children:(0,s.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(n.semantics,{children:[(0,s.jsx)(n.mrow,{children:(0,s.jsx)(n.mi,{children:"\u03b3"})}),(0,s.jsx)(n.annotation,{encoding:"application/x-tex",children:"\\gamma"})]})})}),(0,s.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(n.span,{className:"base",children:[(0,s.jsx)(n.span,{className:"strut",style:{height:"0.625em",verticalAlign:"-0.1944em"}}),(0,s.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.05556em"},children:"\u03b3"})]})})]}),": Discount factor.",(0,s.jsx)(n.br,{}),"\n",(0,s.jsxs)(n.span,{className:"katex",children:[(0,s.jsx)(n.span,{className:"katex-mathml",children:(0,s.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(n.semantics,{children:[(0,s.jsx)(n.mrow,{children:(0,s.jsx)(n.mi,{children:"\u03b1"})}),(0,s.jsx)(n.annotation,{encoding:"application/x-tex",children:"\\alpha"})]})})}),(0,s.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(n.span,{className:"base",children:[(0,s.jsx)(n.span,{className:"strut",style:{height:"0.4306em"}}),(0,s.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.0037em"},children:"\u03b1"})]})})]}),": Learning rate."]}),"\n",(0,s.jsxs)(n.p,{children:["After calculating the error, we will then update the value with the formula on the right. The formula says that the new value will be the current value plus the error multiplied by some constant called ",(0,s.jsx)(n.strong,{children:"learning rate"}),", basically it controls how big do we want to update the value. Value can increase or decrease depending on the TD error, which can be positive or negative. The update process is very similar to ",(0,s.jsx)(n.a,{href:"/machine-learning/linear-regression#gradient-descent",children:"gradient descent"}),"."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},23371:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/td-error-a42edfac6d7ea67f0fd24979dd275d27.png"},28453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>l});var t=r(96540);const s={},a=t.createContext(s);function i(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);