"use strict";(self.webpackChunkcs_notes=self.webpackChunkcs_notes||[]).push([[2611],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>d});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),c=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},h="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),h=c(n),u=a,d=h["".concat(l,".").concat(u)]||h[u]||m[u]||i;return n?r.createElement(d,o(o({ref:t},p),{},{components:n})):r.createElement(d,o({ref:t},p))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[h]="string"==typeof e?e:a,o[1]=s;for(var c=2;c<i;c++)o[c]=n[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}u.displayName="MDXCreateElement"},36020:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var r=n(87462),a=(n(67294),n(3905));const i={slug:"/deep-learning/transformers/transformers-architecture",id:"transformers-architecture",title:"Transformers Architecture",description:"Transformers Architecture"},o=void 0,s={unversionedId:"deep-learning/transformers/transformers-architecture/transformers-architecture",id:"deep-learning/transformers/transformers-architecture/transformers-architecture",title:"Transformers Architecture",description:"Transformers Architecture",source:"@site/docs/deep-learning/13-transformers/02-transformers-architecture/transformers-architecture.md",sourceDirName:"deep-learning/13-transformers/02-transformers-architecture",slug:"/deep-learning/transformers/transformers-architecture",permalink:"/cs-notes/deep-learning/transformers/transformers-architecture",draft:!1,editUrl:"https://github.com/glennhenry/cs-notes/tree/main/docs/deep-learning/13-transformers/02-transformers-architecture/transformers-architecture.md",tags:[],version:"current",lastUpdatedBy:"glennhenry",lastUpdatedAt:1696676294,formattedLastUpdatedAt:"Oct 7, 2023",frontMatter:{slug:"/deep-learning/transformers/transformers-architecture",id:"transformers-architecture",title:"Transformers Architecture",description:"Transformers Architecture"},sidebar:"sidebar",previous:{title:"Attention Mechanism",permalink:"/cs-notes/deep-learning/transformers/attention-mechanism"},next:{title:"Transformers for NLP",permalink:"/cs-notes/deep-learning/transformers/transformers-for-nlp"}},l={},c=[{value:"Transformers Architecture",id:"transformers-architecture",level:2},{value:"Encoder",id:"encoder",level:3},{value:"Decoder",id:"decoder",level:3},{value:"Learning Process",id:"learning-process",level:3}],p={toc:c},h="wrapper";function m(e){let{components:t,...i}=e;return(0,a.kt)(h,(0,r.Z)({},p,i,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Main Source :")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},(0,a.kt)("a",{parentName:"strong",href:"https://youtu.be/z1xs9jdZnuY?si=_rWDHRCle8k-x8SG"},"Transformer (Attention is all you need) - Minsuk Heo \ud5c8\ubbfc\uc11d")))),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Transformers")," is a type of deep learning architecture that revolutionize the NLP field. Transformers specifically uses the ",(0,a.kt)("a",{parentName:"p",href:"/deep-learning/transformers/attention-mechanism"},"attention mechanism")," as the key component. While ",(0,a.kt)("a",{parentName:"p",href:"/deep-learning/transformers/attention-mechanism#rnn-with-attention"},"RNN with attention")," also use the attention mechanism, transformers is a standalone architecture that doesn't need traditional sequential model like ",(0,a.kt)("a",{parentName:"p",href:"/deep-learning/rnn"},"RNN"),"."),(0,a.kt)("p",null,"RNN processes information in sequence, each step has to wait for the previous step to complete, this will makes computation slow. We also need to keep track of its hidden state, leading to increase in memory requirement. The removal of RNN will indeed allow us to get better performance. The motivation behind transformers is to address the performance issue we are facing in RNN. The question is, how can we preserve the attention mechanism while removing the RNN?"),(0,a.kt)("h2",{id:"transformers-architecture"},"Transformers Architecture"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"The main architecture of transformers",src:n(45557).Z,width:"406",height:"594"}),(0,a.kt)("br",{parentName:"p"}),"\n","Source : ",(0,a.kt)("a",{parentName:"p",href:"https://machinelearningmastery.com/the-transformer-model/"},"https://machinelearningmastery.com/the-transformer-model/")),(0,a.kt)("p",null,"Transformers follow the encoder and decoder architecture, they contain the other working principle of transformers."),(0,a.kt)("h3",{id:"encoder"},"Encoder"),(0,a.kt)("p",null,"The encoder is responsible for taking the input sequence, it consist of multiple layer that works together. Here is the walkthrough of input processing in encoder :"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Input Embedding")," : The input sequence is first transformed into a dense vector (often called as ",(0,a.kt)("strong",{parentName:"p"},"token"),") that captures its semantic meaning.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Positional Encoding")," : Since transformers doesn't include RNN that process input sequentially, ",(0,a.kt)("strong",{parentName:"p"},"positional encoding")," is a technique to capture the relative position of tokens within a sequence."),(0,a.kt)("p",{parentName:"li"},"Positional encoding is done by adding a sinusoidal function (sin and cos) of different frequency to the token vector. Basically the sinusoidal function will model the position information of each element in the sequence using different variation of frequency. There are few reason why sinusoidal function is used, one of the important is the ",(0,a.kt)("strong",{parentName:"p"},"periodic")," nature offers an unlimited encoding regardless of our input length. Other reason are its smooth transition and non-linear function."),(0,a.kt)("p",{parentName:"li"},(0,a.kt)("img",{alt:"Positional encoding",src:n(81133).Z,width:"589",height:"357"}),(0,a.kt)("br",{parentName:"p"}),"\n","Source : ",(0,a.kt)("a",{parentName:"p",href:"https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model"},"https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model")))),(0,a.kt)("p",null,"The first and second is the input pre-processing, the third and so on is the actual processing inside an encoder layer."),(0,a.kt)("ol",{start:3},(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Self-Attention")," : Transformers uses a mechanism called ",(0,a.kt)("strong",{parentName:"p"},"self-attention"),". The standard attention mechanism produces a context vector (vector that contains important information about the sequence we are processing) by considering the input sequence and the current output we are generating. In other word, it needs two sequence which is the input and current output to generate another output."),(0,a.kt)("p",{parentName:"li"},"On the other hand, self-attention only need a single sequence. Self-attention weigh the relevance or importance of each element by comparing it with other element in the sequence itself. This will make different element have different relevance in the sequence."),(0,a.kt)("p",{parentName:"li"},"What makes it superior than the standard attention mechanism, it offers us ",(0,a.kt)("strong",{parentName:"p"},"parallelization"),". Remember that in ",(0,a.kt)("a",{parentName:"p",href:"/deep-learning/transformers/attention-mechanism#attention"},"RNN with attention")," on each output step, the attention vector is different, they are computed based on current output step. Self-attention that uses single sequence allows us to compute attention scores for all elements simultaneously, leading to better performance."),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Self-attention process")," :",(0,a.kt)("br",{parentName:"li"}),"The self-attention is implemented by matrix multiplication. The token embedding are fed into the first layer in encoder. The self-attention mechanism is applied, it is done by calculating three types of vector called ",(0,a.kt)("strong",{parentName:"li"},"query, key, and value vectors"),". The tokens embedding are combined in a matrix, it will be multiplied by three types of matrices that correspond to each vector mentioned. The matrix are ",(0,a.kt)("strong",{parentName:"li"},"Wq, Wk, and Wv"),", they are basically weights in form of matrix, which mean they are learnable.")),(0,a.kt)("p",{parentName:"li"},"These three vector will be used to calculate the attention weight which is the value of importance of an element in sequence."),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"The query vector represent specific position of token in the input sequence that we want to compute the attention weights for."),(0,a.kt)("li",{parentName:"ul"},"Key vector represent the other token in the sequence that is being compared to the query vector. In contrast, we will compare all the element with each other."),(0,a.kt)("li",{parentName:"ul"},"Value vector contain the actual information or the features of each token in the sequence.")),(0,a.kt)("p",{parentName:"li"},"The query and key vector (transposed) will be multiplied together, producing something called ",(0,a.kt)("strong",{parentName:"p"},"attention scores"),". They will be divided by the square root of the dimensionality of the key vectors, to prevent large number."),(0,a.kt)("p",{parentName:"li"},(0,a.kt)("a",{parentName:"p",href:"/deep-learning/neural-network#softmax-activation-function"},"Softmax activation function")," will be applied to the previous result, resulting normalized value (they sum up to 1). The normalized value will be multiplied with the value vector, which is the actual token. The result of it is what we call ",(0,a.kt)("strong",{parentName:"p"},"attention weights"),". The output (called ",(0,a.kt)("strong",{parentName:"p"},"attention layer output"),") will be the sum of all attention weights and this will be done for all element in sequence."),(0,a.kt)("p",{parentName:"li"},(0,a.kt)("img",{alt:"Self-attention matrix multiplication",src:n(85170).Z,width:"907",height:"241"}),(0,a.kt)("br",{parentName:"p"}),"\n","Source : ",(0,a.kt)("a",{parentName:"p",href:"https://theaisummer.com/transformer/"},"https://theaisummer.com/transformer/")),(0,a.kt)("p",{parentName:"li"},(0,a.kt)("img",{alt:"Another visualization of self-attention",src:n(94141).Z,width:"759",height:"293"}),(0,a.kt)("br",{parentName:"p"}),"\n","Source : ",(0,a.kt)("a",{parentName:"p",href:"https://youtu.be/z1xs9jdZnuY?si=czJyixA7IV3DxG7k&t=475"},"https://youtu.be/z1xs9jdZnuY?si=czJyixA7IV3DxG7k&t=475"))),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Multi-Head Attention")," : Multi-head attention is a technique that extend self-attention mechanism for optimizing it using parallel computation."),(0,a.kt)("p",{parentName:"li"},"The set of query, key, and value vector we have obtained is grouped in something called ",(0,a.kt)("strong",{parentName:"p"},"attention head"),". They first goes into a linear layer to be projected into different vector spaces, basically projecting them mean we are looking through these vector from different perspective. This will allows the model to capture different representation of the input."),(0,a.kt)("p",{parentName:"li"},"So, to calculate attention weights in parallel, we will compute all the attention head simultaneously. The same calculation that includes multiplication between query and key vector, softmax normalization, multiplication with the value vector is done."),(0,a.kt)("p",{parentName:"li"},"Each result of attention head will be concatenated together and will be passed into a linear layer again, producing the final output of multi-head attention layer."),(0,a.kt)("p",{parentName:"li"},(0,a.kt)("img",{alt:"Multi-head attention",src:n(23811).Z,width:"351",height:"452"}),(0,a.kt)("br",{parentName:"p"}),"\n","Source : ",(0,a.kt)("a",{parentName:"p",href:"https://paperswithcode.com/method/multi-head-attention"},"https://paperswithcode.com/method/multi-head-attention"))),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Residual Connection and Layer Normalization (Add & Norm)")," : We did alot of calculation, during the backpropagation process, we may lose some information including the positional encoding we did in the earlier step. Transformers uses the ",(0,a.kt)("a",{parentName:"p",href:"/deep-learning/resnet#residual-connection"},"residual connection concept")," to help prevent the vanishing gradient issue. This layer also include a normalization process to normalize the output of attention layer, stabilizing the training process.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Feed-Forward Networks (FFN)")," : The attention output is passed through a feed-forward network within the encoder layer. The FFN that includes activation function like ReLU introduces non-linearity. The output of the FFN is then passed through another residual connection and layer normalization."))),(0,a.kt)("p",null,"The step from 3 to 6 represent the process of a single encoder layer. In conclusion, a single layer of encoder processes and transform the input sequence to capture relevant information in the sequence."),(0,a.kt)("p",null,"Transformers architecture may includes multiple encoder layer, they have identical architecture but they don't share weights."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"The encoder layer",src:n(87063).Z,width:"214",height:"393"}),(0,a.kt)("br",{parentName:"p"}),"\n","Source : ",(0,a.kt)("a",{parentName:"p",href:"https://machinelearningmastery.com/the-transformer-model/"},"https://machinelearningmastery.com/the-transformer-model/")," (with modification)"),(0,a.kt)("h3",{id:"decoder"},"Decoder"),(0,a.kt)("p",null,"The decoder is responsible for generating the output sequence, it consist of similar layer with encoder like multi-head attention, add & norm, and feed-forward network. The decoder takes input from encoder layer (the relevant information) and from the previous output as well."),(0,a.kt)("p",null,"The decoder first process the previous output and then it will be combined with the output from encoder."),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Output Embedding")," : The previously generated output embedded or will be turned into a vector representation (token), similar to input embedding in encoder.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Positional Encoding")," : The similar encoding process to capture the relative position of each token in the sequence.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Masked Multi-Head Attention")," : In the normal multi-head attention, we calculated the attention weights using matrix multiplication which include multiplying every element with each other. Multiplying with every element includes accessing its query, key, vector, meaning we have information about them. However, we don't want this to happen in decoder, we don't want to generate output with the information from future. This is implemented by changing some of the value in the matrix during the matrix multiplication to a very large negative number."),(0,a.kt)("p",{parentName:"li"},(0,a.kt)("img",{alt:"Masked multi-head attention",src:n(6337).Z,width:"784",height:"242"}),(0,a.kt)("br",{parentName:"p"}),"\n","Source : ",(0,a.kt)("a",{parentName:"p",href:"https://tigris-data-science.tistory.com/entry/%EC%B0%A8%EA%B7%BC%EC%B0%A8%EA%B7%BC-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-Transformer4-Masked-Multi-Head-Attention%EA%B3%BC-Decoder"},"https://tigris-data-science.tistory.com/entry/%EC%B0%A8%EA%B7%BC%EC%B0%A8%EA%B7%BC-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-Transformer4-Masked-Multi-Head-Attention%EA%B3%BC-Decoder"))),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Add & Norm")," : It then goes to the add & norm layer again which consist of residual connection and normalization layer.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Multi-head Attention")," : In this step, the multi-head attention will be done again, the input will be the combination of the output from encoder and previous result from decoder. It then goes to add & norm layer again.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Feed-Forward Network + Add & Norm")," : The next component in the decoder layer is the feed-forward network. Same as the encoder, it consist of fully connected layer with non-linear activation function. Next, it will be normalized again in the add & norm layer.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Output")," : Finally, the output from previous layer will go into a linear layer, followed with softmax activation function to produces the probability of each token. The model select the token with highest probability and use it as the input for next decoder step."))),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Transformers output",src:n(92277).Z,width:"479",height:"291"}),(0,a.kt)("br",{parentName:"p"}),"\n","Source : ",(0,a.kt)("a",{parentName:"p",href:"https://www.linkedin.com/pulse/intro-transformer-architecture-jithin-s-l"},"https://www.linkedin.com/pulse/intro-transformer-architecture-jithin-s-l")),(0,a.kt)("h3",{id:"learning-process"},"Learning Process"),(0,a.kt)("p",null,"After output is generated, the prediction or whatever the output is will be compared with the actual label of the input. For example :"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"In machine translation, the labels would be the target sequences or translations corresponding to the source sequences."),(0,a.kt)("li",{parentName:"ul"},"In text classification, the labels represent different categories of the input text.")),(0,a.kt)("p",null,"After loss is calculated, the similar learning process will be done, including the backpropagation process through all the layer of transformers model from the decoder output until the encoder input."))}m.isMDXComponent=!0},87063:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/encoder-layer-6a205a39c14cf4505b729a703469f5f0.png"},6337:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/masked-multi-head-attention-46ddaaea0aa6d6fd268a3e2ba570cc84.png"},23811:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/multi-head-attention-3944081d5838af9adaaecb5c7beb0abb.png"},81133:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/positional-encoding-45f699c2543499baa834ada806243631.png"},94141:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/self-attention-2-f32b43e6f7b09d16f9cac0f2e824bd1b.png"},85170:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/self-attention-a556567684fdea41975866e764f76c8e.png"},45557:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/transformers-architecture-9911e83a7fc25ec5fea90217f8d20267.png"},92277:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/transformers-output-eda0dc134579527e86a626998bcc2abd.png"}}]);