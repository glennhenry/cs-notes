"use strict";(self.webpackChunkcs_notes=self.webpackChunkcs_notes||[]).push([[8062],{27601:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/example-e0b68d7cc5ce61de9f74183f75f1117a.png"},28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var i=t(96540);const r={},o=i.createContext(r);function a(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(o.Provider,{value:n},e.children)}},32712:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/multi-agent-16fe76bc9f3a4fccca9850ddf76438f5.png"},95611:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"deep-learning/reinforcement-learning/multi-agent/multi-agent","title":"Multi-Agent","description":"Multi-Agent","source":"@site/docs/deep-learning/16-reinforcement-learning/10-multi-agent/multi-agent.md","sourceDirName":"deep-learning/16-reinforcement-learning/10-multi-agent","slug":"/deep-learning/reinforcement-learning/multi-agent","permalink":"/cs-notes/deep-learning/reinforcement-learning/multi-agent","draft":false,"unlisted":false,"editUrl":"https://github.com/glennhenry/cs-notes/tree/main/docs/deep-learning/16-reinforcement-learning/10-multi-agent/multi-agent.md","tags":[],"version":"current","lastUpdatedBy":"glennhenry","lastUpdatedAt":1723455088000,"frontMatter":{"slug":"/deep-learning/reinforcement-learning/multi-agent","id":"multi-agent","title":"Multi-Agent","description":"Multi-Agent"},"sidebar":"sidebar","previous":{"title":"Imitation Learning","permalink":"/cs-notes/deep-learning/reinforcement-learning/imitation-learning"},"next":{"title":"Software Engineering","permalink":"/cs-notes/software-engineering"}}');var r=t(74848),o=t(28453);const a={slug:"/deep-learning/reinforcement-learning/multi-agent",id:"multi-agent",title:"Multi-Agent",description:"Multi-Agent"},s=void 0,l={},c=[{value:"Example",id:"example",level:3}];function h(e){const n={a:"a",br:"br",h3:"h3",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Main Source:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://youtu.be/qgb0gyrpiGk?si=X7_dix618_wHpfqs",children:"Introduction to Multi-Agent Reinforcement Learning \u2014 MATLAB"})})}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multi-Agent"})," is a reinforcement learning settings where there are multiple agents that interact with each other in the environment simultaneously."]}),"\n",(0,r.jsx)(n.p,{children:"Here's an overview how multi-agent RL operates:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Environment"}),": The environment consist of multiple agent that receives observations which can include information about the state of the environment, the actions, state, and reward of other agents. Observations can be either global (shared among all agents), or local (specific to each agent)."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Policy & Action"}),": Each agent has its own policy, which can deterministic or stochastic, it determines the agent's behavior in the environment. The actions of each agent can affect the state of the environment and potentially impact the observations and rewards of other agents."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Rewards"}),": Agents receive rewards from the environment based on their actions and the state of the environment. Rewards can be individual rewards specific to each agent, shared rewards that reflect the collective performance of all agents, or a combination of both, depending on the problem."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Collaboration"}),": Depending on the RL problem, the agents can either collaborate, compete, or have mixed interactions with each other."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.img,{alt:"Multi-agent",src:t(32712).A+"",width:"565",height:"330"}),(0,r.jsx)(n.br,{}),"\n","Source: ",(0,r.jsx)(n.a,{href:"https://towardsdatascience.com/multi-agent-deep-reinforcement-learning-in-15-lines-of-code-using-pettingzoo-e0b963c0820b",children:"https://towardsdatascience.com/multi-agent-deep-reinforcement-learning-in-15-lines-of-code-using-pettingzoo-e0b963c0820b"})]}),"\n",(0,r.jsx)(n.h3,{id:"example",children:"Example"}),"\n",(0,r.jsx)(n.p,{children:"Let's consider an example, a tasks where several robot works together to move object scattered around to a desired place. The objective is to move all the object efficiently. The goal is to teach the robot to not collide with each other while efficiently moving objects to their desired locations in the presence of obstacles."}),"\n",(0,r.jsx)(n.p,{children:"The action is the movement of the agent (e.g. left, right, up, down) and also manipulating object (e.g. dropping or taking an object). The positive reward is awarded for successful object delivery, collision avoidance, and transportation efficiency, while the negative is the opposite of these."}),"\n",(0,r.jsx)(n.p,{children:"The agent should have proper coordination in order to efficiently move scattered object. A mechanism like sharing task to each agent can be useful, this will enable them to communicate their intentions to handle specific objects. The agents should prioritize handling objects that are closer to their own positions and allow other agents to handle objects that are further away."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Example of multi-agent problem",src:t(27601).A+"",width:"408",height:"408"})})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}}}]);