"use strict";(self.webpackChunkcs_notes=self.webpackChunkcs_notes||[]).push([[4461],{3905:(a,e,t)=>{t.d(e,{Zo:()=>o,kt:()=>k});var n=t(67294);function s(a,e,t){return e in a?Object.defineProperty(a,e,{value:t,enumerable:!0,configurable:!0,writable:!0}):a[e]=t,a}function m(a,e){var t=Object.keys(a);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(a);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(a,e).enumerable}))),t.push.apply(t,n)}return t}function r(a){for(var e=1;e<arguments.length;e++){var t=null!=arguments[e]?arguments[e]:{};e%2?m(Object(t),!0).forEach((function(e){s(a,e,t[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(a,Object.getOwnPropertyDescriptors(t)):m(Object(t)).forEach((function(e){Object.defineProperty(a,e,Object.getOwnPropertyDescriptor(t,e))}))}return a}function p(a,e){if(null==a)return{};var t,n,s=function(a,e){if(null==a)return{};var t,n,s={},m=Object.keys(a);for(n=0;n<m.length;n++)t=m[n],e.indexOf(t)>=0||(s[t]=a[t]);return s}(a,e);if(Object.getOwnPropertySymbols){var m=Object.getOwnPropertySymbols(a);for(n=0;n<m.length;n++)t=m[n],e.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(a,t)&&(s[t]=a[t])}return s}var i=n.createContext({}),l=function(a){var e=n.useContext(i),t=e;return a&&(t="function"==typeof a?a(e):r(r({},e),a)),t},o=function(a){var e=l(a.components);return n.createElement(i.Provider,{value:e},a.children)},c="mdxType",N={inlineCode:"code",wrapper:function(a){var e=a.children;return n.createElement(n.Fragment,{},e)}},h=n.forwardRef((function(a,e){var t=a.components,s=a.mdxType,m=a.originalType,i=a.parentName,o=p(a,["components","mdxType","originalType","parentName"]),c=l(t),h=s,k=c["".concat(i,".").concat(h)]||c[h]||N[h]||m;return t?n.createElement(k,r(r({ref:e},o),{},{components:t})):n.createElement(k,r({ref:e},o))}));function k(a,e){var t=arguments,s=e&&e.mdxType;if("string"==typeof a||s){var m=t.length,r=new Array(m);r[0]=h;var p={};for(var i in e)hasOwnProperty.call(e,i)&&(p[i]=e[i]);p.originalType=a,p[c]="string"==typeof a?a:s,r[1]=p;for(var l=2;l<m;l++)r[l]=t[l];return n.createElement.apply(null,r)}return n.createElement.apply(null,t)}h.displayName="MDXCreateElement"},36422:(a,e,t)=>{t.r(e),t.d(e,{assets:()=>i,contentTitle:()=>r,default:()=>N,frontMatter:()=>m,metadata:()=>p,toc:()=>l});var n=t(87462),s=(t(67294),t(3905));const m={slug:"/deep-learning/deep-learning-foundation",id:"deep-learning-foundation",title:"Deep Learning Foundation",description:"Deep Learning Foundation"},r=void 0,p={unversionedId:"deep-learning/deep-learning-foundation/deep-learning-foundation",id:"deep-learning/deep-learning-foundation/deep-learning-foundation",title:"Deep Learning Foundation",description:"Deep Learning Foundation",source:"@site/docs/deep-learning/01-deep-learning-foundation/deep-learning-foundation.md",sourceDirName:"deep-learning/01-deep-learning-foundation",slug:"/deep-learning/deep-learning-foundation",permalink:"/cs-notes/deep-learning/deep-learning-foundation",draft:!1,editUrl:"https://github.com/glennhenry/cs-notes/tree/main/docs/deep-learning/01-deep-learning-foundation/deep-learning-foundation.md",tags:[],version:"current",lastUpdatedBy:"glennhenry",lastUpdatedAt:1694602267,formattedLastUpdatedAt:"Sep 13, 2023",frontMatter:{slug:"/deep-learning/deep-learning-foundation",id:"deep-learning-foundation",title:"Deep Learning Foundation",description:"Deep Learning Foundation"},sidebar:"sidebar",previous:{title:"Deep Learning",permalink:"/cs-notes/deep-learning"},next:{title:"Neural Network",permalink:"/cs-notes/deep-learning/neural-network"}},i={},l=[{value:"Loss Function",id:"loss-function",level:3},{value:"Entropy",id:"entropy",level:3},{value:"Cross Entropy",id:"cross-entropy",level:4},{value:"Cross Entropy Loss",id:"cross-entropy-loss",level:4},{value:"Binary Cross Entropy Loss",id:"binary-cross-entropy-loss",level:4},{value:"Stochastic Gradient Descent",id:"stochastic-gradient-descent",level:3},{value:"Adam",id:"adam",level:4},{value:"Activation Function",id:"activation-function",level:3},{value:"Terminology",id:"terminology",level:3}],o={toc:l},c="wrapper";function N(a){let{components:e,...m}=a;return(0,s.kt)(c,(0,n.Z)({},o,m,{components:e,mdxType:"MDXLayout"}),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Main Source :")),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},(0,s.kt)("a",{parentName:"strong",href:"https://youtu.be/rBKWVHhFqGU?si=K8imDow_FIeEfvXq"},"Mengenal Cross Entropy Loss - Anak AI"))),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},(0,s.kt)("a",{parentName:"strong",href:"https://youtu.be/hBBOjCiFcuo?si=DZdZmAXYcw_M49zC"},"Deep learning lesson 3 - fastai"))),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},(0,s.kt)("a",{parentName:"strong",href:"https://youtu.be/UmathvAKj80?si=_OhMXYlZYrCc0xIp"},"The Unreasonable Effectiveness of Stochastic Gradient Descent (in 3 minutes) - Visually Explained"))),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},(0,s.kt)("a",{parentName:"strong",href:"https://youtu.be/VyWAvY2CF9c?si=254TjhySXqspex_B"},"Deep Learning Crash Course for Beginners - freeCodeCamp")))),(0,s.kt)("h3",{id:"loss-function"},"Loss Function"),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Loss function")," is a function that measure how well does a machine learning model performs. Loss function is typically calculated from mathematical function that takes actual or true output and the predicted output from the model. For example, a simple loss function in machine learning is the Mean Squared Error (MSE) function, commonly used in ",(0,s.kt)("a",{parentName:"p",href:"/machine-learning/linear-regression"},"linear regression"),"."),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"MSE formula",src:t(62423).Z,width:"300",height:"168"}),(0,s.kt)("br",{parentName:"p"}),"\n","Source : ",(0,s.kt)("a",{parentName:"p",href:"https://suboptimal.wiki/explanation/mse/"},"https://suboptimal.wiki/explanation/mse/")),(0,s.kt)("p",null,"The model predict what may be the y value for specific x value. It sums all the error or the difference between actual and predicted value and then square it and get the average. In the case of MSE, the larger means the worse performance and the lower means the better."),(0,s.kt)("p",null,"The point of machine learning is we keep measuring the performance of our model and adjust our model to make it performs better. The less result we get from the loss function (or more depending on the loss function itself) reflect of how our model performs. We need to optimize the loss function, there are many way to optimize it, such as the ",(0,s.kt)("a",{parentName:"p",href:"/machine-learning/linear-regression#gradient-descent"},"gradient descent algorithm")),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Machine learning flow",src:t(44628).Z,width:"653",height:"232"})),(0,s.kt)("h3",{id:"entropy"},"Entropy"),(0,s.kt)("p",null,"In machine learning, entropy is a measure of uncertainty or randomness in a set of data. It is often used as a criterion to quantify the impurity or disorder within a group of samples."),(0,s.kt)("p",null,"The formula for entropy is (base of the log can vary) :"),(0,s.kt)("p",null,(0,s.kt)("span",{parentName:"p",className:"math math-inline"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mi",{parentName:"mrow"},"E"),(0,s.kt)("mo",{parentName:"mrow"},"="),(0,s.kt)("mo",{parentName:"mrow"},"\u2212"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"\u03a3"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"p"),(0,s.kt)("mi",{parentName:"msub"},"i")),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mrow",{parentName:"msub"},(0,s.kt)("mi",{parentName:"mrow"},"log"),(0,s.kt)("mo",{parentName:"mrow"},"\u2061")),(0,s.kt)("mn",{parentName:"msub"},"2")),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"p"),(0,s.kt)("mi",{parentName:"msub"},"i")),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")")),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"E = - \\Sigma (p_{i} \\log_{2} (p_{i}))")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6833em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.05764em"}},"E"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mrel"},"="),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},"\u2212"),(0,s.kt)("span",{parentName:"span",className:"mord"},"\u03a3"),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"p"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3117em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"i"))))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mop"},(0,s.kt)("span",{parentName:"span",className:"mop"},"lo",(0,s.kt)("span",{parentName:"span",style:{marginRight:"0.01389em"}},"g")),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.207em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.4559em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"2"))))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.2441em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"p"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3117em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"i"))))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mclose"},"))")))))),(0,s.kt)("p",null,"Entropy is calculated from a set of data or event, each of it has a probability of occuring which is the ",(0,s.kt)("span",{parentName:"p",className:"math math-inline"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"p"),(0,s.kt)("mi",{parentName:"msub"},"i"))),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"p_i")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"0.625em",verticalAlign:"-0.1944em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"p"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3117em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"i")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"}))))))))))," or the probability of event ",(0,s.kt)("span",{parentName:"p",className:"math math-inline"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mi",{parentName:"mrow"},"i")),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"i")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6595em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"i"))))),"."),(0,s.kt)("p",null,"A higher entropy means the data is uncertain, it's the opposite when entropy is low."),(0,s.kt)("p",null,"For example, an equally likely probabilities of coin flip has high entropy. If probabilities of getting head and tail is same, it's hard to predict what happen next. If the head has an odds of 0.9 and the tail has an odds of 0.1, then entropy will be lower."),(0,s.kt)("p",null,"Entropy can be thought as calculating the disorder of probability distribution of the event. Probability distribution describes the probability of all different outcomes in an event."),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Entropy distribution of an event",src:t(72268).Z,width:"220",height:"229"}),(0,s.kt)("br",{parentName:"p"}),"\n","Source : ",(0,s.kt)("a",{parentName:"p",href:"https://twitter.com/page_eco/status/1631267143890407426"},"https://twitter.com/page_eco/status/1631267143890407426")),(0,s.kt)("h4",{id:"cross-entropy"},"Cross Entropy"),(0,s.kt)("p",null,"Cross entropy has a similar concept with entropy, the difference is cross entropy calculate the disorder of 2 probability distribution of an event instead."),(0,s.kt)("p",null,"The formula for cross entropy is :"),(0,s.kt)("p",null,(0,s.kt)("span",{parentName:"p",className:"math math-inline"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mi",{parentName:"mrow"},"H"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mi",{parentName:"mrow"},"P"),(0,s.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,s.kt)("mi",{parentName:"mrow"},"Q"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")"),(0,s.kt)("mo",{parentName:"mrow"},"="),(0,s.kt)("mo",{parentName:"mrow"},"\u2212"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"\u03a3"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mi",{parentName:"mrow"},"P"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mi",{parentName:"mrow"},"x"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")"),(0,s.kt)("mi",{parentName:"mrow"},"log"),(0,s.kt)("mo",{parentName:"mrow"},"\u2061"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mi",{parentName:"mrow"},"Q"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mi",{parentName:"mrow"},"x"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")")),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"H(P, Q) = - \\Sigma (P(x) \\log (Q(x)))")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.08125em"}},"H"),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.13889em"}},"P"),(0,s.kt)("span",{parentName:"span",className:"mpunct"},","),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"Q"),(0,s.kt)("span",{parentName:"span",className:"mclose"},")"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mrel"},"="),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},"\u2212"),(0,s.kt)("span",{parentName:"span",className:"mord"},"\u03a3"),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.13889em"}},"P"),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"x"),(0,s.kt)("span",{parentName:"span",className:"mclose"},")"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mop"},"lo",(0,s.kt)("span",{parentName:"span",style:{marginRight:"0.01389em"}},"g")),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"Q"),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"x"),(0,s.kt)("span",{parentName:"span",className:"mclose"},")))")))))),(0,s.kt)("p",null,"Machine learning is typically used for prediction, the prediction output can be probabilities. Cross entropy is used in the context of machine learning, the probability distribution included is the actual probability ",(0,s.kt)("span",{parentName:"p",className:"math math-inline"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mi",{parentName:"mrow"},"P"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mi",{parentName:"mrow"},"x"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")")),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"P(x)")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.13889em"}},"P"),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"x"),(0,s.kt)("span",{parentName:"span",className:"mclose"},")")))))," and the predicted probabilities by the machine learning model ",(0,s.kt)("span",{parentName:"p",className:"math math-inline"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mi",{parentName:"mrow"},"Q"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mi",{parentName:"mrow"},"x"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")")),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"Q(x)")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"Q"),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"x"),(0,s.kt)("span",{parentName:"span",className:"mclose"},")"))))),"."),(0,s.kt)("h4",{id:"cross-entropy-loss"},"Cross Entropy Loss"),(0,s.kt)("p",null,"Loss function in machine learning, measure how well a model performs in a training. Knowing how well it performs make us able to train the model to improve it."),(0,s.kt)("p",null,"The cross entropy function explained before can be used to calculate a loss function, typically for classification tasks that outputs probabilities. It still use the same formula, however, the notation for actual probability is typically denoted as ",(0,s.kt)("span",{parentName:"p",className:"math math-inline"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mi",{parentName:"mrow"},"y")),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"y")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"0.625em",verticalAlign:"-0.1944em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"y")))))," and the prediction is denoted as ",(0,s.kt)("span",{parentName:"p",className:"math math-inline"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mover",{parentName:"mrow",accent:"true"},(0,s.kt)("mi",{parentName:"mover"},"y"),(0,s.kt)("mo",{parentName:"mover"},"^"))),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"\\hat{y}")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"0.8889em",verticalAlign:"-0.1944em"}}),(0,s.kt)("span",{parentName:"span",className:"mord accent"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.6944em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-3em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"y")),(0,s.kt)("span",{parentName:"span",style:{top:"-3em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,s.kt)("span",{parentName:"span",className:"accent-body",style:{left:"-0.1944em"}},(0,s.kt)("span",{parentName:"span",className:"mord"},"^")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.1944em"}},(0,s.kt)("span",{parentName:"span"}))))))))),"."),(0,s.kt)("h4",{id:"binary-cross-entropy-loss"},"Binary Cross Entropy Loss"),(0,s.kt)("p",null,"Binary cross entropy loss is another form of cross entropy loss which is used for binary classifcation, or a classification that only has 2 output. The formula is below :"),(0,s.kt)("p",null,(0,s.kt)("span",{parentName:"p",className:"math math-inline"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mi",{parentName:"mrow"},"L"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mi",{parentName:"mrow"},"y"),(0,s.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,s.kt)("mover",{parentName:"mrow",accent:"true"},(0,s.kt)("mi",{parentName:"mover"},"y"),(0,s.kt)("mo",{parentName:"mover"},"^")),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")"),(0,s.kt)("mo",{parentName:"mrow"},"="),(0,s.kt)("mo",{parentName:"mrow"},"\u2212"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mi",{parentName:"mrow"},"y"),(0,s.kt)("mi",{parentName:"mrow"},"log"),(0,s.kt)("mo",{parentName:"mrow"},"\u2061"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mover",{parentName:"mrow",accent:"true"},(0,s.kt)("mi",{parentName:"mover"},"y"),(0,s.kt)("mo",{parentName:"mover"},"^")),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")"),(0,s.kt)("mo",{parentName:"mrow"},"+"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mn",{parentName:"mrow"},"1"),(0,s.kt)("mo",{parentName:"mrow"},"\u2212"),(0,s.kt)("mi",{parentName:"mrow"},"y"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")"),(0,s.kt)("mi",{parentName:"mrow"},"log"),(0,s.kt)("mo",{parentName:"mrow"},"\u2061"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mn",{parentName:"mrow"},"1"),(0,s.kt)("mo",{parentName:"mrow"},"\u2212"),(0,s.kt)("mover",{parentName:"mrow",accent:"true"},(0,s.kt)("mi",{parentName:"mover"},"y"),(0,s.kt)("mo",{parentName:"mover"},"^")),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")")),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"L(y, \\hat{y}) = - (y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}))")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"L"),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"y"),(0,s.kt)("span",{parentName:"span",className:"mpunct"},","),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mord accent"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.6944em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-3em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"y")),(0,s.kt)("span",{parentName:"span",style:{top:"-3em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,s.kt)("span",{parentName:"span",className:"accent-body",style:{left:"-0.1944em"}},(0,s.kt)("span",{parentName:"span",className:"mord"},"^")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.1944em"}},(0,s.kt)("span",{parentName:"span"}))))),(0,s.kt)("span",{parentName:"span",className:"mclose"},")"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mrel"},"="),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},"\u2212"),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"y"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mop"},"lo",(0,s.kt)("span",{parentName:"span",style:{marginRight:"0.01389em"}},"g")),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord accent"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.6944em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-3em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"y")),(0,s.kt)("span",{parentName:"span",style:{top:"-3em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,s.kt)("span",{parentName:"span",className:"accent-body",style:{left:"-0.1944em"}},(0,s.kt)("span",{parentName:"span",className:"mord"},"^")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.1944em"}},(0,s.kt)("span",{parentName:"span"}))))),(0,s.kt)("span",{parentName:"span",className:"mclose"},")"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}}),(0,s.kt)("span",{parentName:"span",className:"mbin"},"+"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord"},"1"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}}),(0,s.kt)("span",{parentName:"span",className:"mbin"},"\u2212"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"y"),(0,s.kt)("span",{parentName:"span",className:"mclose"},")"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mop"},"lo",(0,s.kt)("span",{parentName:"span",style:{marginRight:"0.01389em"}},"g")),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord"},"1"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}}),(0,s.kt)("span",{parentName:"span",className:"mbin"},"\u2212"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,s.kt)("span",{parentName:"span",className:"mord accent"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.6944em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-3em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"y")),(0,s.kt)("span",{parentName:"span",style:{top:"-3em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,s.kt)("span",{parentName:"span",className:"accent-body",style:{left:"-0.1944em"}},(0,s.kt)("span",{parentName:"span",className:"mord"},"^")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.1944em"}},(0,s.kt)("span",{parentName:"span"}))))),(0,s.kt)("span",{parentName:"span",className:"mclose"},"))")))))),(0,s.kt)("h3",{id:"stochastic-gradient-descent"},"Stochastic Gradient Descent"),(0,s.kt)("p",null,'In traditional gradient descent, model\'s parameter (e.g. the slope and y-intercept in linear regression) are updated every iteration, this can be slow for large datasets. Traditional gradient descent "walks" slowly, it may reach a bad local minima or even stuck at saddle point.'),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Stochastic Gradient Descent (SGD)")," is a variant of ",(0,s.kt)("a",{parentName:"p",href:"/machine-learning/linear-regression#gradient-descent"},"gradient descent")," which is suited for larger datasets. The idea of SGD is, instead of considering all dataset to calculate the gradient and update the parameters, SGD randomly selects a single data point (or a small batch of data points) at each iteration and calculate that particular gradient and use it to update the model's parameters."),(0,s.kt)("p",null,"By not considering all the data, SGD may not be stable. However, with the faster computation, we can update more and eventually catch up with traditional gradient descent."),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"SGD comparison with traditional gradient descent",src:t(73397).Z,width:"735",height:"532"}),(0,s.kt)("br",{parentName:"p"}),"\n","Source : ",(0,s.kt)("a",{parentName:"p",href:"https://youtu.be/UmathvAKj80?si=jHExCTVk7diEA6_6&t=92"},"https://youtu.be/UmathvAKj80?si=jHExCTVk7diEA6_6&t=92")),(0,s.kt)("p",null,"With randomness, SGD able to escape saddle point or bad local minima, this is because it allows the algorithm to explore different directions and not get stuck in a single negative curvature."),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Escaping saddle point",src:t(38853).Z,width:"554",height:"353"}),(0,s.kt)("br",{parentName:"p"}),"\n","Source : ",(0,s.kt)("a",{parentName:"p",href:"https://youtu.be/UmathvAKj80?si=Tpo1K5_hXo94UGJh&t=107"},"https://youtu.be/UmathvAKj80?si=Tpo1K5_hXo94UGJh&t=107")),(0,s.kt)("h4",{id:"adam"},"Adam"),(0,s.kt)("p",null,"Adaptive Moment Estimation (Adam) is an upgrade to SGD. In high-level, Adam is able to adapt the model learning to different case we are facing. Adam have several parameters, these parameters will be adjusted based on past gradients."),(0,s.kt)("p",null,"Adam uses several adaptation technique including :"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Adaptive Learning Rate")," : SGD has a fixed learning rate which is set before model training begins. Adam is able to adjust the learning rate for each parameter based on their past gradients."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Momentum")," : Momentum is a configuration that helps us goes into minimum region like saddle point in the loss function faster by making the model adjust the parameter in the successful direction."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Bias Correction")," : During the early training, we typically set the parameters of our model (weight and bias) to some specific value. This may make our model biased toward that specific value, in simple term, Adam adjust our estimation based on the number of iterations or epochs.")),(0,s.kt)("h3",{id:"activation-function"},"Activation Function"),(0,s.kt)("p",null,"While predicting in machine learning, we often find that the relationship between dependent and independent variable is non-linear. Sometimes, it can't be easily approximated using line in linear regression. Remember that the point is to predict by fitting a line, so we must construct a complex function that fits the data."),(0,s.kt)("p",null,"We can construct any complex function that captures non-linear relationship by summing up several function or lines. However, we may not achieve a desired function just by summing up all the line."),(0,s.kt)("p",null,"This is where an ",(0,s.kt)("strong",{parentName:"p"},"activation function"),' comes, an activation function is used to filter or "decide" which line should we sum, this way we control the sum and make any function we want. This is when we use activation function to introduce non-linearity and constructing complex function.'),(0,s.kt)("p",null,"There are many activation function, for example ",(0,s.kt)("strong",{parentName:"p"},"Rectified Linear Unit (ReLU)")," is a simple linear activation function that defined as : ",(0,s.kt)("span",{parentName:"p",className:"math math-inline"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mtext",{parentName:"mrow"},"ReLU(x)"),(0,s.kt)("mo",{parentName:"mrow"},"="),(0,s.kt)("mtext",{parentName:"mrow"},"max(0,\xa0x)")),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"\\text{ReLU(x)} = \\text{max(0, x)}")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,s.kt)("span",{parentName:"span",className:"mord text"},(0,s.kt)("span",{parentName:"span",className:"mord"},"ReLU(x)")),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mrel"},"="),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,s.kt)("span",{parentName:"span",className:"mord text"},(0,s.kt)("span",{parentName:"span",className:"mord"},"max(0,\xa0x)")))))),", in other word, it filters any negative value."),(0,s.kt)("p",null,"In image below, we summed 2 relu function, it now looks like an elbow like graph which obviously can't be constructed by linear function. However, if we sum all the linear function and only takes what we need, we may be able to construct it."),(0,s.kt)("p",null,"The relu line is defined with the ",(0,s.kt)("span",{parentName:"p",className:"math math-inline"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mi",{parentName:"mrow"},"m"),(0,s.kt)("mi",{parentName:"mrow"},"x"),(0,s.kt)("mo",{parentName:"mrow"},"+"),(0,s.kt)("mi",{parentName:"mrow"},"b")),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"mx + b")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6667em",verticalAlign:"-0.0833em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"m"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"x"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}}),(0,s.kt)("span",{parentName:"span",className:"mbin"},"+"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6944em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"b"))))),", we can sum as many relu as we want, and in a more complex problem, the equation may also involve more variable."),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"2 relu summed",src:t(36207).Z,width:"619",height:"354"}),(0,s.kt)("br",{parentName:"p"}),"\n","Source : ",(0,s.kt)("a",{parentName:"p",href:"https://youtu.be/hBBOjCiFcuo?si=B8PDlJRj4QTa99k-&t=2721"},"https://youtu.be/hBBOjCiFcuo?si=B8PDlJRj4QTa99k-&t=2721")),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Activation function example",src:t(36590).Z,width:"558",height:"282"}),(0,s.kt)("br",{parentName:"p"}),"\n","Source : ",(0,s.kt)("a",{parentName:"p",href:"https://medium.com/@shrutijadon/survey-on-activation-functions-for-deep-learning-9689331ba092"},"https://medium.com/@shrutijadon/survey-on-activation-functions-for-deep-learning-9689331ba092")),(0,s.kt)("h3",{id:"terminology"},"Terminology"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Hyperparameter")," : Settings or configuration choices that are set before the learning process begins. They are not learned from the data but are determined by the practitioner or researcher. For example, learning rate is a hyperparameter that controls rate at which a model adjusts its parameters during training (e.g. used in gradient descent). After we trained the model and evaluate it, we may want to adjust the hyperparameter to explore if there is a more optimal result, this is called ",(0,s.kt)("strong",{parentName:"p"},"hyperparameter tuning"),".")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Epoch")," : Sometimes, training data is used more than one times while training model. The number of how many iteration pass through the entire dataset is called epoch. For example, when we say 5 epoch, it means that we are training the model with the same dataset for 5 times. Epoch is considered as hyperparameter as it is set before learning process.")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Batch")," : Batch is the number of samples or data points that is processed together in a single learning process. If we have 1000 data and we used batch of 32, it means during each iteration, we will process 32 at each time.")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Iteration")," : Iteration is the number of time we want to repeat each batch. It is different with epoch, epoch is the iteration of whole dataset, while this is just an iteration of a batch.")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Metric")," : Metric is a measure of the quality of model's predictions. Metric is different with loss function, loss function is used to train while metric is used to evaluate the prediction. By evaluating the models, we can compare with different model or configuration. For example, a simple metric is accuracy, it is defined as the number of correct predictions divided by the total number of predictions.")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Data Split")," : Data is typically split into 3 :"),(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Training Data")," : Training data is the part of data used to train the machine learning model, the model will learn from this data by adjusting its parameters (weight and bias) based on the actual data."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Test Data")," : Test data is the data used to evaluate performance of the model after the training is done."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Validation Data")," : Validation data is a subset of training data used to validate or assess model's performance, this way we can adjusts the hyperparamater if required."))),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Fine Tuning")," : Fine tuning is the process of training a pre-trained model, typically with a smaller dataset or new task. The purpose of fine-tuning is to leverage the knowledge and learned representations from the pre-trained model and adapt it to the new task or dataset.")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Overfitting")," : Overfitting, also known as high variance, is a situation where a model learns too much from the training data, making it becomes too specialized and performs poorly on new, unseen data. This is the same when you practice alot on a math problem without studying the concept and only memorizing it, turns out the actual exam asked different questions. For a machine learning model, the concept is the pattern of the data, it is important to capture the broader patterns rather than a specific example.")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Underfitting")," : Underfitting, also known as high bias, is where the model is too simple or didn't capture the underlying patterns in the data. It fails to learn the relevant relationships or make a very general assumptions for the training data."),(0,s.kt)("p",{parentName:"li"},(0,s.kt)("img",{alt:"Overfit and underfit",src:t(13806).Z,width:"826",height:"287"}),(0,s.kt)("br",{parentName:"p"}),"\n","Source : ",(0,s.kt)("a",{parentName:"p",href:"https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76"},"https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76"))),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Optimizer")," : An optimizer is an algorithm or method used to optimize the loss function. Example of optimizer are ",(0,s.kt)("a",{parentName:"p",href:"/machine-learning/linear-regression#gradient-descent"},"gradient descent"),", ",(0,s.kt)("a",{parentName:"p",href:"/deep-learning/deep-learning-foundation#stochastic-gradient-descent"},"stochastic gradient descent"),", Adam, and etc.")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Regularization")," : In simple term, regularization is a technique to make model prediction simpler (prevent overfitting) by forcing some feature coefficients to be 0. In other word, we exclude some variable contribution to our prediction. The features we are excluding are the one that has low influence toward the overall prediction. This way we can focus on the more important and influential features.")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Data Augmentation")," : Data augmentation is the process of increasing the diversity and variability of the training data, to make our model more generalized to unseen data. This technique is typically used when our data is limited or in an image classification task. For example, while training a dog or cat classifier, with the same image data, we can do some transformation such as rotating the image, skew it, flip, scale, or adjust its color in order to create more variety of data to reduce overfitting."))))}N.isMDXComponent=!0},36590:(a,e,t)=>{t.d(e,{Z:()=>n});const n=t.p+"assets/images/activation-function-259005443ab63a55a88e82aafe85c67d.png"},72268:(a,e,t)=>{t.d(e,{Z:()=>n});const n=t.p+"assets/images/entropy-distribution-2fcefccfa560f47344acd94d43a805e6.png"},38853:(a,e,t)=>{t.d(e,{Z:()=>n});const n=t.p+"assets/images/escape-saddle-point-23557dade512500474a272a32f2a0abf.png"},44628:(a,e,t)=>{t.d(e,{Z:()=>n});const n=t.p+"assets/images/machine-learning-flow-cbcfb640e9a093a5bcb9a85644ddf507.png"},62423:(a,e,t)=>{t.d(e,{Z:()=>n});const n="data:image/png;base64,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"},36207:(a,e,t)=>{t.d(e,{Z:()=>n});const n=t.p+"assets/images/relu-sum-8fadd676df42a53fbcc46803278b3295.png"},73397:(a,e,t)=>{t.d(e,{Z:()=>n});const n=t.p+"assets/images/stochastic-gradient-descent-b4f4e05e666a7216ca4825529e469657.png"},13806:(a,e,t)=>{t.d(e,{Z:()=>n});const n=t.p+"assets/images/underfit-overfit-6abff391da2e545f14d7fe27f635b139.png"}}]);