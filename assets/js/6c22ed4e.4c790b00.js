"use strict";(self.webpackChunkcs_notes=self.webpackChunkcs_notes||[]).push([[6175],{3905:(e,t,r)=>{r.d(t,{Zo:()=>l,kt:()=>u});var n=r(67294);function a(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function i(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function o(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?i(Object(r),!0).forEach((function(t){a(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},i=Object.keys(e);for(n=0;n<i.length;n++)r=i[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)r=i[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var c=n.createContext({}),p=function(e){var t=n.useContext(c),r=t;return e&&(r="function"==typeof e?e(t):o(o({},t),e)),r},l=function(e){var t=p(e.components);return n.createElement(c.Provider,{value:t},e.children)},m="mdxType",f={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var r=e.components,a=e.mdxType,i=e.originalType,c=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),m=p(r),d=a,u=m["".concat(c,".").concat(d)]||m[d]||f[d]||i;return r?n.createElement(u,o(o({ref:t},l),{},{components:r})):n.createElement(u,o({ref:t},l))}));function u(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=r.length,o=new Array(i);o[0]=d;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[m]="string"==typeof e?e:a,o[1]=s;for(var p=2;p<i;p++)o[p]=r[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,r)}d.displayName="MDXCreateElement"},21564:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>f,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var n=r(87462),a=(r(67294),r(3905));const i={slug:"/deep-learning/transformers/vision-transformers",id:"vision-transformers",title:"Vision Transformers",description:"Vision Transformers"},o=void 0,s={unversionedId:"deep-learning/transformers/vision-transformers/vision-transformers",id:"deep-learning/transformers/vision-transformers/vision-transformers",title:"Vision Transformers",description:"Vision Transformers",source:"@site/docs/deep-learning/14-transformers/07-vision-transformers/vision-transformers.md",sourceDirName:"deep-learning/14-transformers/07-vision-transformers",slug:"/deep-learning/transformers/vision-transformers",permalink:"/cs-notes/deep-learning/transformers/vision-transformers",draft:!1,editUrl:"https://github.com/glennhenry/cs-notes/tree/main/docs/deep-learning/14-transformers/07-vision-transformers/vision-transformers.md",tags:[],version:"current",lastUpdatedBy:"glennhenry",lastUpdatedAt:1697109148,formattedLastUpdatedAt:"Oct 12, 2023",frontMatter:{slug:"/deep-learning/transformers/vision-transformers",id:"vision-transformers",title:"Vision Transformers",description:"Vision Transformers"},sidebar:"sidebar",previous:{title:"BART",permalink:"/cs-notes/deep-learning/transformers/bart"},next:{title:"Diffusion Model",permalink:"/cs-notes/deep-learning/diffusion/diffusion-model"}},c={},p=[{value:"Architecture",id:"architecture",level:3}],l={toc:p},m="wrapper";function f(e){let{components:t,...i}=e;return(0,a.kt)(m,(0,n.Z)({},l,i,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Vision Transformers (ViT)")," is a ",(0,a.kt)("a",{parentName:"p",href:"/deep-learning/transformers/transformers-architecture"},"transformer-like architecture")," used for computer vision tasks, it is known as the competitor of ",(0,a.kt)("a",{parentName:"p",href:"/deep-learning/cnn"},"CNN"),". Leveraging transformers architecture, ViT treats 2-dimensional images as a flat sequence and then use the self-attention mechanism to capture the relationship within an image."),(0,a.kt)("h3",{id:"architecture"},"Architecture"),(0,a.kt)("p",null,"The original ViT architecture is an encoder only model. At the end of encoder's output, a feed-forward network is used to classify the image."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"ViT architecture",src:r(76589).Z,width:"789",height:"511"}),(0,a.kt)("br",{parentName:"p"}),"\n","Source : ",(0,a.kt)("a",{parentName:"p",href:"https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b"},"https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b")),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Input")," : The input of ViT is indeed an image, the image will be divided into smaller non-overlapping region called ",(0,a.kt)("strong",{parentName:"p"},"patch"),". The number of patches depend on the image size and the chosen patch size. For example, if the image size is 224x224 pixels and the patch size is 16x16 pixels, then it will result in 196 patches."),(0,a.kt)("p",{parentName:"li"},"The patch will be turned into vector representation or linearly projected into an embedding space (embedding process). In addition, positional embedding that encode the relative position of the patch in the input image is also added."),(0,a.kt)("p",{parentName:"li"},(0,a.kt)("img",{alt:"Image patches",src:r(18077).Z,width:"764",height:"415"}),(0,a.kt)("br",{parentName:"p"}),"\n","Source : ",(0,a.kt)("a",{parentName:"p",href:"https://gowrishankar.info/blog/transformers-everywhere-patch-encoding-technique-for-vision-transformersvit-explained/"},"https://gowrishankar.info/blog/transformers-everywhere-patch-encoding-technique-for-vision-transformersvit-explained/"))),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Transformers or Encoder Block")," : The embedded patches are fed into the transformers encoder block, consisting multi-head self-attention, fully connected layer, and add & norm layer. The ViT model stack several encoder block together. Transformers architecture make it possible to capture the global information about the image, compared to CNN that captures only the local patterns.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},(0,a.kt)("strong",{parentName:"p"},"Output")," : The output is a sequence of patch embedding that captured the global representation of the image. Depending on the task, the output processing may be different. In a classification tasks, the output goes to a fully connected layer to produce probability distribution over different classes or labels."))))}f.isMDXComponent=!0},18077:(e,t,r)=>{r.d(t,{Z:()=>n});const n=r.p+"assets/images/image-patch-aa9a36be88d793f4eca03deead8bfe9a.png"},76589:(e,t,r)=>{r.d(t,{Z:()=>n});const n=r.p+"assets/images/vit-architecture-939d8123c2a75b3a9c975eb0cfa5c809.png"}}]);