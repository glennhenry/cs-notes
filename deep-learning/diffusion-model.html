<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-deep-learning/diffusion-model/diffusion-model" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">Diffusion Model | CS Notes</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://glennhenry.github.io/cs-notes/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://glennhenry.github.io/cs-notes/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://glennhenry.github.io/cs-notes/deep-learning/diffusion-model"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Diffusion Model | CS Notes"><meta data-rh="true" name="description" content="Diffusion Model"><meta data-rh="true" property="og:description" content="Diffusion Model"><link data-rh="true" rel="icon" href="/cs-notes/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://glennhenry.github.io/cs-notes/deep-learning/diffusion-model"><link data-rh="true" rel="alternate" href="https://glennhenry.github.io/cs-notes/deep-learning/diffusion-model" hreflang="en"><link data-rh="true" rel="alternate" href="https://glennhenry.github.io/cs-notes/deep-learning/diffusion-model" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/cs-notes/assets/css/styles.dec0385d.css">
<link rel="preload" href="/cs-notes/assets/js/runtime~main.30bb93ae.js" as="script">
<link rel="preload" href="/cs-notes/assets/js/main.73f11487.js" as="script">
</head>
<body class="navigation-with-keyboard" data-theme="light">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbar--dark"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/cs-notes/"><div class="navbar__logo"><img src="/cs-notes/img/logo.svg" alt="Docusaurus Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/cs-notes/img/logo.svg" alt="Docusaurus Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Notes</b></a><a href="https://github.com/glennhenry/cs-notes" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Docusaurus<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS darkNavbarColorModeToggle_X3D1" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="dsla-search-wrapper"><div class="dsla-search-field" data-tags="default,docs-default-current"></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/">Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/digital-signal-processing">Digital Signal Processing</a><button aria-label="Toggle the collapsible sidebar category &#x27;Digital Signal Processing&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/computer-and-programming-fundamentals">Computer &amp; Programming Fundamentals</a><button aria-label="Toggle the collapsible sidebar category &#x27;Computer &amp; Programming Fundamentals&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/digital-media-processing">Digital Media Processing</a><button aria-label="Toggle the collapsible sidebar category &#x27;Digital Media Processing&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/computer-networking">Computer Networking</a><button aria-label="Toggle the collapsible sidebar category &#x27;Computer Networking&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/data-structures-and-algorithms">Data Structures &amp; Algorithms</a><button aria-label="Toggle the collapsible sidebar category &#x27;Data Structures &amp; Algorithms&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/computer-organization-and-architecture">Computer Organization &amp; Architecture</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/operating-system">Operating System</a><button aria-label="Toggle the collapsible sidebar category &#x27;Operating System&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/theory-of-computation-and-automata">Theory of Computation &amp; Automata</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/compilers">Compilers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/programming-language-theory">Programming Language Theory</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/system-programming">System Programming</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/database-system">Database System</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/computer-graphics">Computer Graphics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Computer Graphics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/frontend-web-development">Frontend Web Development</a><button aria-label="Toggle the collapsible sidebar category &#x27;Frontend Web Development&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/backend-development">Backend Development</a><button aria-label="Toggle the collapsible sidebar category &#x27;Backend Development&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/computer-security">Computer Security</a><button aria-label="Toggle the collapsible sidebar category &#x27;Computer Security&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/machine-learning">Machine Learning</a><button aria-label="Toggle the collapsible sidebar category &#x27;Machine Learning&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/cs-notes/deep-learning">Deep Learning</a><button aria-label="Toggle the collapsible sidebar category &#x27;Deep Learning&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/deep-learning-foundation">Deep Learning Foundation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/deep-learning-tasks">Deep Learning Tasks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/neural-network">Neural Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/cnn">CNN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/resnet">ResNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/u-net">U-Net</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/siamese-network">Siamese Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/rnn">RNN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/lstm">LSTM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/gru">GRU</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/autoencoder">Autoencoder</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/variational-autoencoder">Variational Autoencoder</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/gan">GAN</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/cs-notes/deep-learning/transformers/attention-mechanism">Transformers</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/cs-notes/deep-learning/diffusion-model">Diffusion Model</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/cs-notes/deep-learning/reinforcement-learning/reinforcement-learning-fundamental">Reinforcement Learning</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/software-engineering">Software Engineering</a><button aria-label="Toggle the collapsible sidebar category &#x27;Software Engineering&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/cloud-computing">Cloud Computing</a><button aria-label="Toggle the collapsible sidebar category &#x27;Cloud Computing&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/extras">Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/cs-notes/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/cs-notes/deep-learning"><span itemprop="name">Deep Learning</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Diffusion Model</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Diffusion Model</h1></header><p><strong>Main Source :</strong></p><ul><li><strong><a href="https://en.wikipedia.org/wiki/Diffusion_model" target="_blank" rel="noopener noreferrer">Wikipedia Diffusion model</a></strong></li><li><strong><a href="https://youtu.be/fbLgFrlTnGU?si=tR6le4piBvVpeR_9" target="_blank" rel="noopener noreferrer">What are Diffusion Models? - Ari Seff</a></strong></li><li><strong><a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/" target="_blank" rel="noopener noreferrer">Introduction to Diffusion Models for Machine Learning - AssemblyAI</a></strong></li><li><strong><a href="https://youtu.be/aUqbWBQTKaA?si=BONjB3Ul4iGOmy-1" target="_blank" rel="noopener noreferrer">[Lab Seminar] DDIM: Denoising Diffusion Implicit Model (ICLR, 2021) - DSAIL SKKU</a></strong></li><li>Good article : <strong><a href="https://theaisummer.com/diffusion-models/" target="_blank" rel="noopener noreferrer">How diffusion models work: the math from scratch by Sergios Karagiannakos,Nikolas Adaloglou</a></strong></li><li><strong><a href="https://stable-diffusion-art.com/samplers/" target="_blank" rel="noopener noreferrer">Stable Diffusion Samplers: A Comprehensive Guide</a></strong></li><li><strong><a href="https://youtu.be/J87hffSMB60?si=yPxmuVSLbgcxPJTN" target="_blank" rel="noopener noreferrer">How does Stable Diffusion work? – Latent Diffusion Models EXPLAINED - AI Coffee Break with Letitia</a></strong></li><li>LDM / stable diffusion paper : <strong><a href="https://arxiv.org/abs/2112.10752" target="_blank" rel="noopener noreferrer">High-Resolution Image Synthesis with Latent Diffusion Models</a></strong></li></ul><p><strong>Diffusion Model</strong> is a class of generative model, meaning it generates new data by learning the underlying distribution of a given dataset and uses this knowledge to generate new data samples. Diffusion model is typically used for tasks including image generation, image denoising, generating high-resolution image, and etc.</p><p>Diffusion model is inspired by the concept of <a href="https://en.wikipedia.org/wiki/Diffusion" target="_blank" rel="noopener noreferrer">diffusion</a> in physics, which is a stochastic phenomenon where particles spread out from an area of high concentration to an area of low concentration, eventually leading to a uniform concentration. The underlying principles and equations of diffusion in physics provide a mathematical foundation that is adapted and applied to model to generates data.</p><p>The idea of diffusion model is, we model the target image (the target image we want to generate) as a data distribution (called target distribution), and then the model will aim to transform a simple base distribution, typically a Gaussian distribution, into the target distribution through an iterative diffusion process.</p><video width="720" height="360" controls=""><source src="https://learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models_flower_inference_2.mp4" type="video/mp4"></video><p>Source : <a href="https://learnopencv.com/denoising-diffusion-probabilistic-models/" target="_blank" rel="noopener noreferrer">https://learnopencv.com/denoising-diffusion-probabilistic-models/</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="denoising-diffusion-model">Denoising Diffusion Model<a href="#denoising-diffusion-model" class="hash-link" aria-label="Direct link to Denoising Diffusion Model" title="Direct link to Denoising Diffusion Model">​</a></h2><p>There are many variation of diffusion model, each with their own concept, <strong>Denoising Diffusion Model</strong> is the type of diffusion model that uses the diffusion concept.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="denoising-diffusion-probabilistic-model-ddpm">Denoising Diffusion Probabilistic Model (DDPM)<a href="#denoising-diffusion-probabilistic-model-ddpm" class="hash-link" aria-label="Direct link to Denoising Diffusion Probabilistic Model (DDPM)" title="Direct link to Denoising Diffusion Probabilistic Model (DDPM)">​</a></h3><p>The type of denoising diffusion model that learns the underlying <strong>probability distribution</strong> of a dataset and generate new samples from that distribution.</p><p>The overall process of denoising diffusion model consist of two steps, the forward process that gradually add noise to the image, and the reverse process that tries to reverse the process or remove the noise to generate clean samples.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="forward-process">Forward Process<a href="#forward-process" class="hash-link" aria-label="Direct link to Forward Process" title="Direct link to Forward Process">​</a></h4><p>In DDPM, the noising process is modeled using a <a href="/cs-notes/deep-learning/reinforcement-learning/markov-models#markov-chain"><strong>Markov chain</strong></a>. Markov chain is a mathematical model that assumes the future state of a system only depends on current step. In other word, the current state of a system depends only on the previous state. The key idea behind using a Markov chain in diffusion models is to describe the evolution of the system&#x27;s state as an iterative stochastic process.</p><p>The forward process begins with adding noise to the image in gradual manner, the process will be divided into discrete time steps. The noise, we are adding is modeled using a <strong>Gaussian (normal) distribution</strong>. A Gaussian distribution is characterized by its mean (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">μ</span></span></span></span></span>) and variance (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>), which determine the central tendency and spread of the distribution, respectively.</p><p><img loading="lazy" alt="Gaussian distribution noises" src="/cs-notes/assets/images/gaussian-distribution-noise-a0d40f8ab3d16bcd3d56e322cff74168.png" width="666" height="245" class="img_ev3q"><br>
<!-- -->Source : <a href="https://analyticsindiamag.com/a-guide-to-different-types-of-noises-and-image-denoising-methods/" target="_blank" rel="noopener noreferrer">https://analyticsindiamag.com/a-guide-to-different-types-of-noises-and-image-denoising-methods/</a></p><p>Utilizing the Markov chain with diffusion model, the distribution of noise at some time step <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span> only depends on previous time step <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6984em;vertical-align:-0.0833em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>. Following the calculation in Markov chain, current step distribution will be calculated by the product of each previously conditional step.</p><p><img loading="lazy" alt="Forward process notation" src="/cs-notes/assets/images/forward-process-notation-f53c42c883304c68a3667a76fc7b8573.png" width="369" height="249" class="img_ev3q"><br>
<!-- -->Source : <a href="https://youtu.be/fbLgFrlTnGU?si=tR6le4piBvVpeR_9&amp;t=109" target="_blank" rel="noopener noreferrer">https://youtu.be/fbLgFrlTnGU?si=tR6le4piBvVpeR_9&amp;t=109</a></p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>The forward process is fixed, meaning it doesn&#x27;t have learnable parameters.</p></div></div><p>In the context of diffusion model, the type of Gaussian distribution used is the <strong>diagonal Gaussian distribution</strong>. The variance, denoted as <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span>, varies at each time step and is constrained to be within the range of 0 and 1. The lower variance implies that the diffusion or transformation of the distribution occurs more gradually and with smaller perturbations, which may help us on the reverse process.</p><p><img loading="lazy" alt="Gaussian distribution" src="/cs-notes/assets/images/gaussian-distribution-9569cf4b790dadd74f4101bcfe2bae0b.png" width="490" height="103" class="img_ev3q"><br>
<!-- -->Source : <a href="https://youtu.be/fbLgFrlTnGU?si=mN1d8DKDP9vYJjQ0&amp;t=129" target="_blank" rel="noopener noreferrer">https://youtu.be/fbLgFrlTnGU?si=mN1d8DKDP9vYJjQ0&amp;t=129</a></p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>The beta parameters is used to control or adjust the noise added in forward process or removed in the reverse process, this technique is also called <strong>noise scheduling</strong>.</p></div></div><p>As we iteratively perform the forward diffusion process, the noise gradually converges towards a Gaussian distribution. Mathematically speaking, the noise can be approximated as a <strong>multivariate Gaussian distribution with a mean vector of zero and an identity covariance matrix</strong>.</p><p><img loading="lazy" alt="Getting closer to identity matrix" src="/cs-notes/assets/images/approach-identity-matrix-2336ab708059a75b31a755e0dad9c4f5.png" width="471" height="192" class="img_ev3q"><br>
<!-- -->Source : <a href="https://youtu.be/fbLgFrlTnGU?si=C8gzMh3VVKSUXqBg&amp;t=163" target="_blank" rel="noopener noreferrer">https://youtu.be/fbLgFrlTnGU?si=C8gzMh3VVKSUXqBg&amp;t=163</a></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="reverse-process">Reverse Process<a href="#reverse-process" class="hash-link" aria-label="Direct link to Reverse Process" title="Direct link to Reverse Process">​</a></h4><p>The reverse process will also be modeled using Markov chain, the noise will be assumed as a <strong>unimodal diagonal Gaussian distribution</strong> (the formula above in the image below), which takes current state (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>) and current time step (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>) as input.</p><p>In the reverse process of a diffusion model, it involves inferring the previous step given the current step. The calculation is done by multiplying the product of conditional distributions at each time step in reverse order (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_{t - 1} | x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>) with the Gaussian noise distribution, denoted as <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(x_T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>, which was generated during the forward process (We assume the forward diffusion approaches Gaussian distribution).</p><p><img loading="lazy" alt="Reverse process notation" src="/cs-notes/assets/images/reverse-process-notation-fa196f341c9afbfd2b8202827e0b3ec0.png" width="462" height="340" class="img_ev3q"><br>
<!-- -->Source : <a href="https://youtu.be/fbLgFrlTnGU?si=v6ixlxk-gmWiHtDW&amp;t=279" target="_blank" rel="noopener noreferrer">https://youtu.be/fbLgFrlTnGU?si=v6ixlxk-gmWiHtDW&amp;t=279</a></p><p>The inference process is where the process is made learnable or adjustable. In the implementation of reverse process, there are two parameters, the mean (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">μ</span></span></span></span></span>) and the variance (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Σ</span></span></span></span></span>) of the Gaussian distribution. The variance is made fixed and only the mean is made learnable, for training stabilization purposes. In essence, the model will dynamically learn and adapt the optimal parameters to effectively reverse the diffusion process.</p><p><img loading="lazy" alt="Reverse process implementation" src="/cs-notes/assets/images/reverse-process-implementation-c4884f9c757dba8b0d036e36237361c9.png" width="524" height="110" class="img_ev3q"><br>
<!-- -->Source : <a href="https://youtu.be/fbLgFrlTnGU?si=o0xlAFVkGm6B4nHr&amp;t=651" target="_blank" rel="noopener noreferrer">https://youtu.be/fbLgFrlTnGU?si=o0xlAFVkGm6B4nHr&amp;t=651</a></p><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>The implementation of the reverse or denoising process typically uses the <a href="/cs-notes/deep-learning/u-net">U-Net architecture</a> for image data or a <a href="/cs-notes/deep-learning/transformers/transformers-architecture">transformers</a> for non-image data.</p></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="training-objective">Training Objective<a href="#training-objective" class="hash-link" aria-label="Direct link to Training Objective" title="Direct link to Training Objective">​</a></h4><p>The forward and reverse process can be understood as process that transform data or distribution (the input image) in two different directions. The forward process involves adding noise that will make the data distribution approach Gaussian distribution. The reverse process involves transforming the data back to its original distribution. This is done by approximating the unnoised data, by doing this, we effectively generate new data points during this process.</p><p><img loading="lazy" alt="The objective of diffusion model" src="/cs-notes/assets/images/objective-57e94625624526004f3b851095df306f.png" width="283" height="291" class="img_ev3q"><br>
<!-- -->Source : <a href="https://youtu.be/fbLgFrlTnGU?si=lOk5eb9Au4EJumjW&amp;t=376" target="_blank" rel="noopener noreferrer">https://youtu.be/fbLgFrlTnGU?si=lOk5eb9Au4EJumjW&amp;t=376</a> (with modification)</p><p>The process and objective of diffusion model is similar to <a href="/cs-notes/deep-learning/variational-autoencoder"><strong>variational autoencoder (VAE)</strong></a>. In VAE, the encoder takes the input data and maps it to a lower-dimensional representation called the <strong>latent variables</strong>. This latent variables serves as a compressed representation that captures the essential information and underlying structure present in the input data. Latent variables will then be modeled in a probability distribution with some mean and variance, this is now called a <strong>latent space</strong>. The decoder sample from the latent space distribution, to generate new data samples. The objective is to approximate the true data distribution from the sampled distribution.</p><p>The similar objective can be applied to diffusion model, &quot;Given transformed data, how to untransform it?&quot;. The primary aim of a diffusion model is to enhance the inference process, particularly by focusing on the reverse process that involves computing the preceding state of the Markov chain.</p><p>The training objective can be summarized with the following formula (<!-- -->[similar to loss in VAE]<!-- -->) :</p><p><img loading="lazy" alt="Training objective formula" src="/cs-notes/assets/images/training-formula-17beda6f8e60e6a73cc78b5ecc67d695.png" width="795" height="57" class="img_ev3q"><br>
<!-- -->Source : <a href="https://youtu.be/fbLgFrlTnGU?si=D6pH6EyDnxSz0j1P&amp;t=458" target="_blank" rel="noopener noreferrer">https://youtu.be/fbLgFrlTnGU?si=D6pH6EyDnxSz0j1P&amp;t=458</a> (with modification)</p><p>The <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log p_{\theta}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> represent the likelihood of the original data, it must be greater or equal to the first term subtracted by second term. We represent the original data as a likelihood because our goal is to reconstruct it. By maximizing the likelihood, we aim to ensure that the reconstructed data closely resembles the original data.</p><p>The first term is the <strong>reconstruction term</strong>, which is the comparison of generated data and the original data. The second term is the <strong>KL divergence</strong> measures the difference of probability distribution between the target distribution (input data) and the learned distribution (generated distribuion).</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary">​</a></h4><p>In summary, diffusion model starts with the forward process where we add Gaussian distribution noise to the image. After a bunch of step, we obtained a distribution approximately close to Gaussian distribution. In the reverse process, the model aims to remove the noise by inferring the previous distribution based on the current distribution, which is where the model learns. This process is often described as sampling because it entails generating samples from an approximated distribution obtained at the current step.</p><p><img loading="lazy" alt="Diffusion process summary" src="/cs-notes/assets/images/diffusion-summary-ce581a3d73b6962da4914a730053f3b6.png" width="968" height="279" class="img_ev3q"><br>
<!-- -->Source : <a href="https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/foundation-diffusion-generative-models" target="_blank" rel="noopener noreferrer">https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/foundation-diffusion-generative-models</a></p><p>After training the model, to generate new data using a trained diffusion model, we start with a random noise sample and then perform the reverse diffusion steps.</p><p><img loading="lazy" alt="The process of generating image from a random noise, after model training" src="/cs-notes/assets/images/generation-process-268c9bb8f2a23fbea3ce8844804fbaf9.gif" width="608" height="608" class="img_ev3q"><br>
<!-- -->Source : <a href="https://tree.rocks/make-diffusion-model-from-scratch-easy-way-to-implement-quick-diffusion-model-e60d18fd0f2e" target="_blank" rel="noopener noreferrer">https://tree.rocks/make-diffusion-model-from-scratch-easy-way-to-implement-quick-diffusion-model-e60d18fd0f2e</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="denoising-diffusion-implicit-models-ddim">Denoising Diffusion Implicit Models (DDIM)<a href="#denoising-diffusion-implicit-models-ddim" class="hash-link" aria-label="Direct link to Denoising Diffusion Implicit Models (DDIM)" title="Direct link to Denoising Diffusion Implicit Models (DDIM)">​</a></h3><p>The type of denoising diffusion model previously we talked about uses the probabilistic Markov chain as the framework, which can be slow during the sampling process in the reverse diffusion (there is some technique to skip the diffusion in forward process). Furthermore, DDPM require alot of forward diffusion step, which also increase the step in reverse process.</p><p>DDIM is a non-Markovian diffusion model, it removed the use of Markov chain. In Markov chain, during the reverse process, we assume that previous step only depends on current step (e.g. <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">x_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> depends on <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">x_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> = <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>2</mn></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">q(x_2|x_1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>).</p><p><img loading="lazy" alt="DDIM without Markov chain" src="/cs-notes/assets/images/ddim-270de147713f25c4160c0427e93dbd96.png" width="924" height="161" class="img_ev3q"><br>
<!-- -->Source : <a href="https://betterprogramming.pub/diffusion-models-ddpms-ddims-and-classifier-free-guidance-e07b297b2869" target="_blank" rel="noopener noreferrer">https://betterprogramming.pub/diffusion-models-ddpms-ddims-and-classifier-free-guidance-e07b297b2869</a> (with modification)</p><p>DDIM does not rely on a strict sequential dependence on previous steps. The elimination of the Markov chain allows for the consideration of multiple states beyond just the previous one in the reverse diffusion process. This transformation turns the process into an optimization problem, as the inclusion of more states introduces complex dependencies and variations in the data.</p><p>The goal is to find an optimal <strong>latent code</strong>, latent code is nothing but a set of parameters that captures the information required to generate a denoised or clean sample from a corrupted input. We adjust the parameters by minimizing the reconstruction loss between the generated sample and the corrupted input.</p><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>DDPM can be seen as a special case of DDIM, where we only consider the previous state for current state.<br>
<!-- -->The term &quot;implicit&quot; in DDIM means that we do not explicitly try to denoise the image, instead we just find the best parameters that minimize the lost.</p></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="conditional--unconditional-generation">Conditional &amp; Unconditional Generation<a href="#conditional--unconditional-generation" class="hash-link" aria-label="Direct link to Conditional &amp; Unconditional Generation" title="Direct link to Conditional &amp; Unconditional Generation">​</a></h3><p>Diffusion model can also be integrated with a conditional prompt. The conditional prompt serves as a guide or constraint during the generation process, allowing for more controlled and targeted generation of data. The conditional prompt can be in the form of text, images, or any other type of input.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="unconditional">Unconditional<a href="#unconditional" class="hash-link" aria-label="Direct link to Unconditional" title="Direct link to Unconditional">​</a></h4><p>In the unconditional generation, we do not include any additional constraint. The model will start with a random noise sample and applies the reverse process to progressively reduce the noise and generate new data samples. The resulting data will be similar to the training data.</p><p><img loading="lazy" alt="Unconditional generation" src="/cs-notes/assets/images/unconditional-generation-58bbcbb30fb288cd4bde16f7a76b2511.png" width="628" height="335" class="img_ev3q"><br>
<!-- -->Source : <a href="https://betterprogramming.pub/beginners-guide-to-unconditional-image-generation-using-diffusers-c703e675bda8" target="_blank" rel="noopener noreferrer">https://betterprogramming.pub/beginners-guide-to-unconditional-image-generation-using-diffusers-c703e675bda8</a></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="conditional">Conditional<a href="#conditional" class="hash-link" aria-label="Direct link to Conditional" title="Direct link to Conditional">​</a></h4><p>In the conditional generation, the model is trained to learn the distribution of the training data given certain input conditions. During the generation process, the model takes the input conditions into account and adjusts the reverse process accordingly to generate samples that satisfy the given conditions.</p><p>The target data in reverse process which is <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_{\theta}(x_{0})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> becomes <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mi mathvariant="normal">∣</mi><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_{\theta}(x_{0}|y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mclose">)</span></span></span></span></span>. The reverse step also takes the additional condition. The conditional generation can actually simplify the generation process by constraining the range of possible outputs.</p><p><img loading="lazy" alt="Reverse step network" src="/cs-notes/assets/images/reverse-step-network-c1bb16987ba49e214988fa131a44268a.png" width="251" height="96" class="img_ev3q"><br>
<!-- -->Source : <a href="https://youtu.be/fbLgFrlTnGU?si=luVZ1O9GCWV8_2PH&amp;t=714" target="_blank" rel="noopener noreferrer">https://youtu.be/fbLgFrlTnGU?si=luVZ1O9GCWV8_2PH&amp;t=714</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="score-based-diffusion-model-sbdm">Score-Based Diffusion Model (SBDM)<a href="#score-based-diffusion-model-sbdm" class="hash-link" aria-label="Direct link to Score-Based Diffusion Model (SBDM)" title="Direct link to Score-Based Diffusion Model (SBDM)">​</a></h2><p><strong>Score-Based Diffusion Model (SBDM)</strong> is also known as <strong>Noise Conditional Score Network (NCSN)</strong> or <strong>Score-Matching with Langevin Dynamics (SMLD)</strong>. SBDM introduces the concept of <strong>score</strong>, mathematically, it is the gradient of the log-likelihood of the data distribution.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="score-function">Score Function<a href="#score-function" class="hash-link" aria-label="Direct link to Score Function" title="Direct link to Score Function">​</a></h3><p><img loading="lazy" alt="Score function" src="/cs-notes/assets/images/score-function-f623c8ae9a665225f7ac00e05f949146.png" width="284" height="45" class="img_ev3q"><br>
<!-- -->Source : <a href="https://youtu.be/fbLgFrlTnGU?si=lEL8BBYdL0f1cetc&amp;t=907" target="_blank" rel="noopener noreferrer">https://youtu.be/fbLgFrlTnGU?si=lEL8BBYdL0f1cetc&amp;t=907</a></p><p>A gradient with respect to some variables tells us the direction and magnitude of the steepest ascent or descent of a function at a particular point. Specifically in the context of SBDM, the gradient measures the direction in which the noise should be adjusted to move towards denoising or generating the desired output. It act as a guide for diffusion process by providing information on how to update the noise, but it does not provide a direct measure of similarity or dissimilarity.</p><p>The integration of score-based guidance with diffusion model fall between each diffusion step, score or gradient of the log-likelihood of the data distribution will be estimated with respect to the noise process. After that, the noise is updated in the direction of the estimated score.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="continuous-diffusion-model">Continuous Diffusion Model<a href="#continuous-diffusion-model" class="hash-link" aria-label="Direct link to Continuous Diffusion Model" title="Direct link to Continuous Diffusion Model">​</a></h2><p>Continuous diffusion model is a type of diffusion model that model the noise distribution of the data in a continuous manner. Instead of modeling how the noise distribution changes over discrete time step, SBDM instead treat the distribution as a continous process that evolves over a continuous time.</p><p>The diffusion process is modeled using <strong>stochastic differential equations (SDEs)</strong>. SDEs are mathematical equation that describe how a system transitions from one state to another, which is affected by deterministic and random factor.</p><p>The score-based diffusion model can be combined together with SDE-based modeling. The SDE-based deterministic and random factor terms can be incorporated into the score-based model to guide the denoising and transformation steps of the diffusion process.</p><p><img loading="lazy" alt="Continous diffusion model using SDE" src="/cs-notes/assets/images/continous-sde-3417be96666dc01780ef8757df1a7e00.png" width="639" height="276" class="img_ev3q"><br>
<!-- -->Source : <a href="https://theaisummer.com/diffusion-models/#score-based-generative-modeling-through-stochastic-differential-equations-sde" target="_blank" rel="noopener noreferrer">https://theaisummer.com/diffusion-models/#score-based-generative-modeling-through-stochastic-differential-equations-sde</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sampler">Sampler<a href="#sampler" class="hash-link" aria-label="Direct link to Sampler" title="Direct link to Sampler">​</a></h3><p>Continous diffusion model uses SDE to model the system, in order to know the system state over time and sample from it to actually generates data, the SDE needs to be solved.</p><p>While solving an SDE, we do not find the exact analytical solution, we instead approximate the solution (also called numerical integration). Some of the methods are Euler–Maruyama method, Heun&#x27;s method, and linear multistep methods, these are also called <strong>sampler</strong>.</p><p><img loading="lazy" alt="Sampler that gradually reduce noises" src="/cs-notes/assets/images/sampler-02eb7fd903abc65e796354f63ca620eb.gif" width="512" height="512" class="img_ev3q"><br>
<!-- -->Source : <a href="https://stable-diffusion-art.com/samplers/" target="_blank" rel="noopener noreferrer">https://stable-diffusion-art.com/samplers/</a> (sampler that gradually reduce the noise)</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="ancestral-sampler">Ancestral Sampler<a href="#ancestral-sampler" class="hash-link" aria-label="Direct link to Ancestral Sampler" title="Direct link to Ancestral Sampler">​</a></h4><p>There is another type of sampler called <strong>ancestral sampler</strong>, these sampler introduce randomness to the sampling process. At each sampling step, they add noise to the image to introduce variations in the generated images. This makes the image produced may not reach a consistent and reproducible state, the image can turn differently at each step.</p><table><thead><tr><th align="center">Ancestral sampler</th><th align="center">Normal sampler</th></tr></thead><tbody><tr><td align="center"><img loading="lazy" alt="Ancestral sampler" src="/cs-notes/assets/images/ancestral-sampler-7a2b65e1513263ee78614e60f7cafaf7.gif" width="512" height="512" class="img_ev3q"></td><td align="center"><img loading="lazy" alt="Normal sampler" src="/cs-notes/assets/images/normal-sampler-f39c18a18adc1d2b71984ec059492c60.gif" width="512" height="512" class="img_ev3q"></td></tr></tbody></table><p><a href="https://stable-diffusion-art.com/samplers/" target="_blank" rel="noopener noreferrer">https://stable-diffusion-art.com/samplers/</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="latent-diffusion-model-ldm">Latent Diffusion Model (LDM)<a href="#latent-diffusion-model-ldm" class="hash-link" aria-label="Direct link to Latent Diffusion Model (LDM)" title="Direct link to Latent Diffusion Model (LDM)">​</a></h2><p>In diffusion model, we are approximating what an unnoised image will look like from a random noised image. In other word, we are approximating the probability distribution of a target image from a simple base distribution. This makes diffusion model is often called a general method to model a probability distribution.</p><p>An encoder-decoder pair is a type of network that consist of two component. The encoder, serve as the one that takes input and transform it into <strong>latent variables</strong> or lower-dimensional representation of the input data. The other component, decoder, will take the output of encoder and do the reverse process. An example of encoder-decoder pair are <a href="/cs-notes/deep-learning/autoencoder">autoencoder</a> and <a href="/cs-notes/deep-learning/variational-autoencoder">Variational autoencoder (VAE)</a>, which model the latent variables in probability distribution, called <strong>latent space</strong>.</p><p><strong>Latent diffusion model</strong> or <strong>LDM</strong> is a type of diffusion model combined with an encoder-decoder pair (typically a VAE). The idea of using diffusion model with an encoder-decoder pair is because the encoder outputs a probability distribution which we can use as the input for diffusion model.</p><p>Instead of forward diffusing a raw image and then do the reverse process, LDM instead takes the input from encoder and model the probability distribution. To actually generate image, we can sample from the output of the diffusion model and use the decoder to decode it back to image. By using latent space as the input, we can reduce the dimensionality of the input for diffusion process, which will save alot of computation resources.</p><p>LDM can also be integrated with additional condition such as text, image, or any other meaningful representation. This integration can leverage technique like <a href="/cs-notes/deep-learning/transformers/transformers-architecture">cross-attention</a>, which is also used in the transformers architecture. First, the encoder encodes the data into the latent space, followed by the forward diffusion process. The conditional input, typically the encoded representation of the original input, is concatenated together with the output of forward diffusion process. Subsequently, the cross-attention mechanism is incorporated to guide the reverse diffusion or denoising process. Once the reverse diffusion is completed, we can sample the output and fed it into the decoder.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="stable-diffusion">Stable Diffusion<a href="#stable-diffusion" class="hash-link" aria-label="Direct link to Stable Diffusion" title="Direct link to Stable Diffusion">​</a></h3><p><strong>Stable Diffusion</strong> is a deep learning model based on <a href="/cs-notes/deep-learning/diffusion-model">diffusion model</a>, specifically, it&#x27;s an open source implementation of <a href="/cs-notes/deep-learning/diffusion-model#latent-diffusion-model-ldm"><strong>latent diffusion model (LDM)</strong></a>. It is typically used to generate image conditioned on text or image, inpainting, outpainting, super-resolution, and etc.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="architecture">Architecture<a href="#architecture" class="hash-link" aria-label="Direct link to Architecture" title="Direct link to Architecture">​</a></h4><p>Stable diffusion is based on Latent Diffusion Model (LDM), which uses <a href="/cs-notes/deep-learning/variational-autoencoder">variational autoencoder (VAE)</a> as both the encoder and decoder. The diffusion process takes place within the latent space generated by the encoder.</p><p><img loading="lazy" alt="LDM or stable diffusion architecture" src="/cs-notes/assets/images/stable-diffusion-architecture-d8858e95aa717952f8c7e30455abe2df.png" width="725" height="357" class="img_ev3q"><br>
<!-- -->Source : <a href="https://towardsdatascience.com/stable-diffusion-best-open-source-version-of-dall-e-2-ebcdf1cb64bc" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/stable-diffusion-best-open-source-version-of-dall-e-2-ebcdf1cb64bc</a> (stable diffusion or LDM architecture)</p><ol><li><p><strong>Image Encoding</strong> : An image is provided as the input, which is passed through the encoder to transform it into a representation in the latent space. Transforming it into latent space allows for smaller dimension which can significantly reduce the computational resources.</p></li><li><p><strong>Diffusion Process</strong> : Diffusion process which consist of forward diffusion that adds noise gradually and the reverse diffusion process which removes noise gradually. The reverse diffusion process is implemented using a neural network, specifically a denoising <a href="/cs-notes/deep-learning/u-net">U-Net</a>.</p><p>The U-Net predicts what is the denoised image in the previous time step, given a noised image in the current time step. However, rather than directly predicting the denoised image, the U-Net predicts the noise present in the input image. Subsequently, this predicted noise is subtracted from the noisy input image to obtain the actual denoised image.</p><p>To ensure a gradual reduction of noise, the predicted noise is multiplied by a fraction (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">E</mi></mrow><annotation encoding="application/x-tex">\mathcal{E}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.08944em">E</span></span></span></span></span>) before subtracting it from the input image. This gradual process enhances stability and reliability while also accommodating the injection of conditional information, such as a text prompt.</p><p><img loading="lazy" alt="Forward diffusion and reverse diffusion using U-Net" src="/cs-notes/assets/images/diffusion-process-29754ac9266f3203420340ab9762dfc9.png" width="1264" height="412" class="img_ev3q"><br>
<!-- -->Source : <a href="https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166" target="_blank" rel="noopener noreferrer">https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166</a>, <a href="https://youtu.be/J87hffSMB60?si=_iCGv-rnh_sXt_dL&amp;t=352" target="_blank" rel="noopener noreferrer">https://youtu.be/J87hffSMB60?si=_iCGv-rnh_sXt_dL&amp;t=352</a> (with modification)</p></li><li><p><strong>Conditioning</strong> : The model also takes a conditional input such as text, it will be encoded or converted into lower-dimensional representation, this can be done using <a href="/cs-notes/deep-learning/transformers/transformers-architecture#encoder">transformers encoders</a> or <a href="#contrastive-language-image-pre-training-clip">CLIP text encoder</a>.</p><p>The integration with conditional input is incorporated in two ways :</p><ul><li>The encoded conditioning will be concatenated with the output of forward diffusion which is used for the input of reverse diffusion process.</li><li>We will also utilize the cross-attention mechanism during the reverse diffusion process in U-Net. Inside the attention mechanism, the encoded conditioning act as the query vector.</li></ul><p><img loading="lazy" alt="Conditioning process in the stable diffusion architecture" src="/cs-notes/assets/images/conditioning-d69618aaa8f5b75166cfbae6fc31ae58.png" width="583" height="285" class="img_ev3q"><br>
<!-- -->Source : <a href="https://towardsdatascience.com/stable-diffusion-best-open-source-version-of-dall-e-2-ebcdf1cb64bc" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/stable-diffusion-best-open-source-version-of-dall-e-2-ebcdf1cb64bc</a> (with modification)</p></li><li><p><strong>Output &amp; Decoder Generation</strong> : Once the reverse process is completed and a refined latent space representation is obtained, it is fed into a decoder that performs an upscaling operation on the image. However, the resulting upscaled image may not have the highest resolution since the input is typically a low-resolution image to reduce computational costs. To address this, another diffusion model specialized for super-resolution tasks can be used.</p><p><img loading="lazy" alt="The output of stable diffusion with additional super-resolution" src="/cs-notes/assets/images/output-c6ffe3f4277bc5f7893ca2ec2de8deea.png" width="627" height="283" class="img_ev3q"><br>
<!-- -->Source : <a href="https://youtu.be/J87hffSMB60?si=Y01BDAdH9hNa-dqQ&amp;t=486" target="_blank" rel="noopener noreferrer">https://youtu.be/J87hffSMB60?si=Y01BDAdH9hNa-dqQ&amp;t=486</a></p></li></ol><p>After completing the training process, in the actual image generation, we have the option to generate new images by inputting only text, or we can choose to input both text and an image simultaneously, which will modify the input image based on the text we provide.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contrastive-language-image-pre-training-clip">Contrastive Language-Image Pre-Training (CLIP)<a href="#contrastive-language-image-pre-training-clip" class="hash-link" aria-label="Direct link to Contrastive Language-Image Pre-Training (CLIP)" title="Direct link to Contrastive Language-Image Pre-Training (CLIP)">​</a></h2><p>CLIP is a model that combines vision and language understanding, it is a model that learns the similarity between image and text. CLIP takes an input image with its corresponding text description, they will be encoded. In other words, the higher-dimensional data will be converted into a lower-dimensional representation. The place where all the encoded input is combined is called the <strong>embedding space</strong>.</p><p>During training, CLIP will learn how to map each image and text into the shared embedding space. The objective is to group the pairs of encoded representation (image and text) together, while pushing the dissimilar pairs apart.</p><p>The loss function in CLIP consists of two main components: the <strong>image-text similarity loss</strong> and the <strong>contrastive loss</strong>. Both of the loss is calculated in the embedding space, the similarity metrics such as cosine similarity can be used. Similarity loss is maximized to encourage the maximum similarity score between correct pair of image and text. On the other hand, the contrastive loss is minimized to encourage a minimum similarity score between mismatched pairs of image and text.</p><p><img loading="lazy" alt="Joint embedding space" src="/cs-notes/assets/images/joint-embedding-space-3be368c34ec2bd51145600e5b2bf1b9b.png" width="531" height="265" class="img_ev3q"><br>
<!-- -->Source : <a href="https://blog.dataiku.com/leveraging-joint-text-image-models-to-search-and-classify-images" target="_blank" rel="noopener noreferrer">https://blog.dataiku.com/leveraging-joint-text-image-models-to-search-and-classify-images</a></p><p>The encoding process, involves the use of image and text encoder. We can choose variety of model for text and image encoding, for example, we can use <a href="/cs-notes/deep-learning/transformers/transformers-architecture#encoder">transformers encoder</a> for text encoding and <a href="/cs-notes/deep-learning/cnn">CNN</a> (without its classifier) for image encoding.</p><p><img loading="lazy" alt="CLIP" src="/cs-notes/assets/images/clip-3b88f43c06e497a38332e6d74c2131c9.png" width="1090" height="394" class="img_ev3q"><br>
<!-- -->Source : <a href="https://towardsdatascience.com/simple-implementation-of-openai-clip-model-a-tutorial-ace6ff01d9f2?gi=de7f9822c57a" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/simple-implementation-of-openai-clip-model-a-tutorial-ace6ff01d9f2?gi=de7f9822c57a</a></p><p>After training, CLIP can be used for variety of tasks including image classification, image retrieval, text-to-image generation.</p><p>For example in an image classification tasks, an image is fed into the encoder, the encoder encodes the image and generates the encoded representation. In the embedding space, the model will find which label is the most similar with the encoded representation of the image. The highest similarity is considered the predicted class label for the image.</p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>CLIP objective is used in text-to-image models model like <a href="https://en.wikipedia.org/wiki/DALL-E" target="_blank" rel="noopener noreferrer">DALL-E</a>.</p></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/glennhenry/cs-notes/tree/main/docs/deep-learning/15-diffusion-model/diffusion-model.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2023-10-21T10:14:15.000Z">Oct 21, 2023</time></b> by <b>glennhenry</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/cs-notes/deep-learning/transformers/vision-transformers"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Vision Transformers</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/cs-notes/deep-learning/reinforcement-learning/reinforcement-learning-fundamental"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Reinforcement Learning Fundamental</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#denoising-diffusion-model" class="table-of-contents__link toc-highlight">Denoising Diffusion Model</a><ul><li><a href="#denoising-diffusion-probabilistic-model-ddpm" class="table-of-contents__link toc-highlight">Denoising Diffusion Probabilistic Model (DDPM)</a><ul><li><a href="#forward-process" class="table-of-contents__link toc-highlight">Forward Process</a></li><li><a href="#reverse-process" class="table-of-contents__link toc-highlight">Reverse Process</a></li><li><a href="#training-objective" class="table-of-contents__link toc-highlight">Training Objective</a></li><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li></ul></li><li><a href="#denoising-diffusion-implicit-models-ddim" class="table-of-contents__link toc-highlight">Denoising Diffusion Implicit Models (DDIM)</a></li><li><a href="#conditional--unconditional-generation" class="table-of-contents__link toc-highlight">Conditional &amp; Unconditional Generation</a><ul><li><a href="#unconditional" class="table-of-contents__link toc-highlight">Unconditional</a></li><li><a href="#conditional" class="table-of-contents__link toc-highlight">Conditional</a></li></ul></li></ul></li><li><a href="#score-based-diffusion-model-sbdm" class="table-of-contents__link toc-highlight">Score-Based Diffusion Model (SBDM)</a><ul><li><a href="#score-function" class="table-of-contents__link toc-highlight">Score Function</a></li></ul></li><li><a href="#continuous-diffusion-model" class="table-of-contents__link toc-highlight">Continuous Diffusion Model</a><ul><li><a href="#sampler" class="table-of-contents__link toc-highlight">Sampler</a><ul><li><a href="#ancestral-sampler" class="table-of-contents__link toc-highlight">Ancestral Sampler</a></li></ul></li></ul></li><li><a href="#latent-diffusion-model-ldm" class="table-of-contents__link toc-highlight">Latent Diffusion Model (LDM)</a><ul><li><a href="#stable-diffusion" class="table-of-contents__link toc-highlight">Stable Diffusion</a><ul><li><a href="#architecture" class="table-of-contents__link toc-highlight">Architecture</a></li></ul></li></ul></li><li><a href="#contrastive-language-image-pre-training-clip" class="table-of-contents__link toc-highlight">Contrastive Language-Image Pre-Training (CLIP)</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/cs-notes/assets/js/runtime~main.30bb93ae.js"></script>
<script src="/cs-notes/assets/js/main.73f11487.js"></script>
</body>
</html>