<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-deep-learning/deep-learning-foundation/deep-learning-foundation" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.1">
<title data-rh="true">Deep Learning Foundation | CS Notes</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://glennhenry.github.io/cs-notes/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://glennhenry.github.io/cs-notes/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://glennhenry.github.io/cs-notes/deep-learning/deep-learning-foundation"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Deep Learning Foundation | CS Notes"><meta data-rh="true" name="description" content="Deep Learning Foundation"><meta data-rh="true" property="og:description" content="Deep Learning Foundation"><link data-rh="true" rel="icon" href="/cs-notes/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://glennhenry.github.io/cs-notes/deep-learning/deep-learning-foundation"><link data-rh="true" rel="alternate" href="https://glennhenry.github.io/cs-notes/deep-learning/deep-learning-foundation" hreflang="en"><link data-rh="true" rel="alternate" href="https://glennhenry.github.io/cs-notes/deep-learning/deep-learning-foundation" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/cs-notes/assets/css/styles.319c3a75.css">
<script src="/cs-notes/assets/js/runtime~main.641d9014.js" defer="defer"></script>
<script src="/cs-notes/assets/js/main.149c9038.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ navbar--dark"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/cs-notes/"><div class="navbar__logo"><img src="/cs-notes/img/logo.svg" alt="Docusaurus Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/cs-notes/img/logo.svg" alt="Docusaurus Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Notes</b></a><a href="https://github.com/glennhenry/cs-notes" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Made with Docusaurus<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a class="navbar__item navbar__link" href="/cs-notes/index">Index</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS darkNavbarColorModeToggle_X3D1" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/cs-notes/"><img src="/cs-notes/img/logo.svg" alt="Docusaurus Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/cs-notes/img/logo.svg" alt="Docusaurus Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"><b>Notes</b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/">Intro</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/index">Index</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/digital-signal-processing">Digital Signal Processing</a><button aria-label="Expand sidebar category &#x27;Digital Signal Processing&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/computer-and-programming-fundamentals">Computer &amp; Programming Fundamentals</a><button aria-label="Expand sidebar category &#x27;Computer &amp; Programming Fundamentals&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/digital-media-processing">Digital Media Processing</a><button aria-label="Expand sidebar category &#x27;Digital Media Processing&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/computer-networking">Computer Networking</a><button aria-label="Expand sidebar category &#x27;Computer Networking&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/data-structures-and-algorithms">Data Structures &amp; Algorithms</a><button aria-label="Expand sidebar category &#x27;Data Structures &amp; Algorithms&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/computer-organization-and-architecture">Computer Organization &amp; Architecture</a><button aria-label="Expand sidebar category &#x27;Computer Organization &amp; Architecture&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/operating-system">Operating System</a><button aria-label="Expand sidebar category &#x27;Operating System&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/theory-of-computation-and-automata">Theory of Computation &amp; Automata</a><button aria-label="Expand sidebar category &#x27;Theory of Computation &amp; Automata&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/compilers">Compilers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/programming-language-theory">Programming Language Theory</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/database-system">Database System</a><button aria-label="Expand sidebar category &#x27;Database System&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/computer-graphics">Computer Graphics</a><button aria-label="Expand sidebar category &#x27;Computer Graphics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/frontend-web-development">Frontend Web Development</a><button aria-label="Expand sidebar category &#x27;Frontend Web Development&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/backend-development">Backend Development</a><button aria-label="Expand sidebar category &#x27;Backend Development&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/computer-security">Computer Security</a><button aria-label="Expand sidebar category &#x27;Computer Security&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/machine-learning">Machine Learning</a><button aria-label="Expand sidebar category &#x27;Machine Learning&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/cs-notes/deep-learning">Deep Learning</a><button aria-label="Collapse sidebar category &#x27;Deep Learning&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/cs-notes/deep-learning/deep-learning-foundation">Deep Learning Foundation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/deep-learning-tasks">Deep Learning Tasks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/neural-network">Neural Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/cnn">CNN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/resnet">ResNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/u-net">U-Net</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/siamese-network">Siamese Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/rnn">RNN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/lstm">LSTM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/gru">GRU</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/autoencoder">Autoencoder</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/variational-autoencoder">Variational Autoencoder</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/gan">GAN</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/cs-notes/deep-learning/transformers/attention-mechanism">Transformers</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/diffusion-model">Diffusion Model</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/cs-notes/deep-learning/reinforcement-learning/reinforcement-learning-fundamental">Reinforcement Learning</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/software-engineering">Software Engineering</a><button aria-label="Expand sidebar category &#x27;Software Engineering&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/cloud-computing-and-distributed-systems">Cloud Computing &amp; Distributed Systems</a><button aria-label="Expand sidebar category &#x27;Cloud Computing &amp; Distributed Systems&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/cs-notes/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/cs-notes/deep-learning"><span itemprop="name">Deep Learning</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Deep Learning Foundation</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Deep Learning Foundation</h1></header><p><strong>Main Source :</strong></p>
<ul>
<li><strong><a href="https://youtu.be/rBKWVHhFqGU?si=K8imDow_FIeEfvXq" target="_blank" rel="noopener noreferrer">Mengenal Cross Entropy Loss - Anak AI</a></strong></li>
<li><strong><a href="https://youtu.be/hBBOjCiFcuo?si=DZdZmAXYcw_M49zC" target="_blank" rel="noopener noreferrer">Deep learning lesson 3 - fastai</a></strong></li>
<li><strong><a href="https://youtu.be/UmathvAKj80?si=_OhMXYlZYrCc0xIp" target="_blank" rel="noopener noreferrer">The Unreasonable Effectiveness of Stochastic Gradient Descent (in 3 minutes) - Visually Explained</a></strong></li>
<li><strong><a href="https://youtu.be/VyWAvY2CF9c?si=254TjhySXqspex_B" target="_blank" rel="noopener noreferrer">Deep Learning Crash Course for Beginners - freeCodeCamp</a></strong></li>
<li><strong><a href="https://youtu.be/p4ZZq0736Po?si=aTGIzD1mBcOsmiYs" target="_blank" rel="noopener noreferrer">Deep learning lesson 7 - fastai</a></strong></li>
<li><strong><a href="https://youtu.be/V_xro1bcAuA?si=sErbB-7m2MBagldq" target="_blank" rel="noopener noreferrer">PyTorch for Deep Learning &amp; Machine Learning – Full Course - freeCodeCamp</a></strong></li>
<li><strong><a href="https://developers.google.com/machine-learning/data-prep/transform/normalization" target="_blank" rel="noopener noreferrer">Normalization - Google Machine Learning Course</a></strong></li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="loss-function">Loss Function<a href="#loss-function" class="hash-link" aria-label="Direct link to Loss Function" title="Direct link to Loss Function">​</a></h3>
<p><strong>Loss function</strong> is a function that measure how well does a machine learning model performs. Loss function is typically calculated from mathematical function that takes actual or true output and the predicted output from the model. For example, a simple loss function in machine learning is the Mean Squared Error (MSE) function, commonly used in <a href="/cs-notes/machine-learning/linear-regression">linear regression</a>.</p>
<p><img decoding="async" loading="lazy" alt="MSE formula" src="data:image/png;base64,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" width="300" height="168" class="img_ev3q"><br>
<!-- -->Source : <a href="https://suboptimal.wiki/explanation/mse/" target="_blank" rel="noopener noreferrer">https://suboptimal.wiki/explanation/mse/</a></p>
<p>The model predict what may be the y value for specific x value. It sums all the error or the difference between actual and predicted value and then square it and get the average. In the case of MSE, the larger means the worse performance and the lower means the better.</p>
<p>The point of machine learning is we keep measuring the performance of our model and adjust our model to make it performs better. The less result we get from the loss function (or more depending on the loss function itself) reflect of how our model performs. We need to optimize the loss function, there are many way to optimize it, such as the <a href="/cs-notes/machine-learning/linear-regression#gradient-descent">gradient descent algorithm</a></p>
<p><img decoding="async" loading="lazy" alt="Machine learning flow" src="/cs-notes/assets/images/machine-learning-flow-cbcfb640e9a093a5bcb9a85644ddf507.png" width="653" height="232" class="img_ev3q"></p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="entropy">Entropy<a href="#entropy" class="hash-link" aria-label="Direct link to Entropy" title="Direct link to Entropy">​</a></h3>
<p>In machine learning, entropy is a measure of uncertainty or randomness in a set of data. It is often used as a criterion to quantify the impurity or disorder within a group of samples.</p>
<p>The formula for entropy is (base of the log can vary) :</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>=</mo><mo>−</mo><mi mathvariant="normal">Σ</mi><mo stretchy="false">(</mo><msub><mi>p</mi><mi>i</mi></msub><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E = - \Sigma (p_{i} \log_{2} (p_{i}))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">−</span><span class="mord">Σ</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span></p>
<p>Entropy is calculated from a set of data or event, each of it has a probability of occuring which is the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> or the probability of event <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span>.</p>
<p>A higher entropy means the data is uncertain, it&#x27;s the opposite when entropy is low.</p>
<p>For example, an equally likely probabilities of coin flip has high entropy. If probabilities of getting head and tail is same, it&#x27;s hard to predict what happen next. If the head has an odds of 0.9 and the tail has an odds of 0.1, then entropy will be lower.</p>
<p>Entropy can be thought as calculating the disorder of probability distribution of the event. Probability distribution describes the probability of all different outcomes in an event.</p>
<p><img decoding="async" loading="lazy" alt="Entropy distribution of an event" src="/cs-notes/assets/images/entropy-distribution-2fcefccfa560f47344acd94d43a805e6.png" width="220" height="229" class="img_ev3q"><br>
<!-- -->Source : <a href="https://twitter.com/page_eco/status/1631267143890407426" target="_blank" rel="noopener noreferrer">https://twitter.com/page_eco/status/1631267143890407426</a></p>
<h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="cross-entropy">Cross Entropy<a href="#cross-entropy" class="hash-link" aria-label="Direct link to Cross Entropy" title="Direct link to Cross Entropy">​</a></h4>
<p>Cross entropy has a similar concept with entropy, the difference is cross entropy calculate the disorder of 2 probability distribution of an event instead.</p>
<p>The formula for cross entropy is :</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo separator="true">,</mo><mi>Q</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mi mathvariant="normal">Σ</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(P, Q) = - \Sigma (P(x) \log (Q(x)))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">Q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">−</span><span class="mord">Σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)))</span></span></span></span></p>
<p>Machine learning is typically used for prediction, the prediction output can be probabilities. Cross entropy is used in the context of machine learning, the probability distribution included is the actual probability <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> and the predicted probabilities by the machine learning model <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span>.</p>
<h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="cross-entropy-loss">Cross Entropy Loss<a href="#cross-entropy-loss" class="hash-link" aria-label="Direct link to Cross Entropy Loss" title="Direct link to Cross Entropy Loss">​</a></h4>
<p>Loss function in machine learning, measure how well a model performs in a training. Knowing how well it performs make us able to train the model to improve it.</p>
<p>The cross entropy function explained before can be used to calculate a loss function, typically for classification tasks that outputs probabilities. It still use the same formula, however, the notation for actual probability is typically denoted as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span> and the prediction is denoted as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span></span></span></span>.</p>
<h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="binary-cross-entropy-loss">Binary Cross Entropy Loss<a href="#binary-cross-entropy-loss" class="hash-link" aria-label="Direct link to Binary Cross Entropy Loss" title="Direct link to Binary Cross Entropy Loss">​</a></h4>
<p>Binary cross entropy loss is another form of cross entropy loss which is used for binary classifcation, or a classification that only has 2 output. The formula is below :</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>y</mi><mo separator="true">,</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mo stretchy="false">(</mo><mi>y</mi><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(y, \hat{y}) = - (y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">−</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="mclose">))</span></span></span></span></p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="stochastic-gradient-descent">Stochastic Gradient Descent<a href="#stochastic-gradient-descent" class="hash-link" aria-label="Direct link to Stochastic Gradient Descent" title="Direct link to Stochastic Gradient Descent">​</a></h3>
<p>In traditional gradient descent, model&#x27;s parameter (e.g. the slope and y-intercept in linear regression) are updated every iteration, this can be slow for large datasets. Traditional gradient descent &quot;walks&quot; slowly, it may reach a bad local minima or even stuck at saddle point.</p>
<p><strong>Stochastic Gradient Descent (SGD)</strong> is a variant of <a href="/cs-notes/machine-learning/linear-regression#gradient-descent">gradient descent</a> which is suited for larger datasets. The idea of SGD is, instead of considering all dataset to calculate the gradient and update the parameters, SGD randomly selects a single data point (or a small batch of data points) at each iteration and calculate that particular gradient and use it to update the model&#x27;s parameters.</p>
<p>By not considering all the data, SGD may not be stable. However, with the faster computation, we can update more and eventually catch up with traditional gradient descent.</p>
<p><img decoding="async" loading="lazy" alt="SGD comparison with traditional gradient descent" src="/cs-notes/assets/images/stochastic-gradient-descent-b4f4e05e666a7216ca4825529e469657.png" width="735" height="532" class="img_ev3q"><br>
<!-- -->Source : <a href="https://youtu.be/UmathvAKj80?si=jHExCTVk7diEA6_6&amp;t=92" target="_blank" rel="noopener noreferrer">https://youtu.be/UmathvAKj80?si=jHExCTVk7diEA6_6&amp;t=92</a></p>
<p>With randomness, SGD able to escape saddle point or bad local minima, this is because it allows the algorithm to explore different directions and not get stuck in a single negative curvature.</p>
<p><img decoding="async" loading="lazy" alt="Escaping saddle point" src="/cs-notes/assets/images/escape-saddle-point-23557dade512500474a272a32f2a0abf.png" width="554" height="353" class="img_ev3q"><br>
<!-- -->Source : <a href="https://youtu.be/UmathvAKj80?si=Tpo1K5_hXo94UGJh&amp;t=107" target="_blank" rel="noopener noreferrer">https://youtu.be/UmathvAKj80?si=Tpo1K5_hXo94UGJh&amp;t=107</a></p>
<h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="adam">Adam<a href="#adam" class="hash-link" aria-label="Direct link to Adam" title="Direct link to Adam">​</a></h4>
<p>Adaptive Moment Estimation (Adam) is an upgrade to SGD. In high-level, Adam is able to adapt the model learning to different case we are facing. Adam have several parameters, these parameters will be adjusted based on past gradients.</p>
<p>Adam uses several adaptation technique including :</p>
<ul>
<li><strong>Adaptive Learning Rate</strong> : SGD has a fixed learning rate which is set before model training begins. Adam is able to adjust the learning rate for each parameter based on their past gradients.</li>
<li><strong>Momentum</strong> : Momentum is a configuration that helps us goes into minimum region like saddle point in the loss function faster by making the model adjust the parameter in the successful direction.</li>
<li><strong>Bias Correction</strong> : During the early training, we typically set the parameters of our model (weight and bias) to some specific value. This may make our model biased toward that specific value, in simple term, Adam adjust our estimation based on the number of iterations or epochs.</li>
</ul>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="activation-function">Activation Function<a href="#activation-function" class="hash-link" aria-label="Direct link to Activation Function" title="Direct link to Activation Function">​</a></h3>
<p>While predicting in machine learning, we often find that the relationship between dependent and independent variable is non-linear. Sometimes, it can&#x27;t be easily approximated using simple line like we did in linear regression. Remember that the goal is to predict next data by fitting a line, so we must construct a function that fits the data. The more complex relationship the variables have, the more complex the function would be.</p>
<p>We can construct any complex function that captures non-linear relationship by summing up several function or lines. However, we may not achieve a desired function just by summing up all the line.</p>
<p>This is where an <strong>activation function</strong> comes, an activation function is used to filter or &quot;decide&quot; which line should we sum, this way we control the sum and make any function we want. This is when we use activation function to introduce non-linearity and constructing complex function.</p>
<p>There are many activation function, for example <strong>Rectified Linear Unit (ReLU)</strong> is a simple linear activation function that defined as : <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>ReLU(x)</mtext><mo>=</mo><mtext>max(0, x)</mtext></mrow><annotation encoding="application/x-tex">\text{ReLU(x)} = \text{max(0, x)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">ReLU(x)</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">max(0, x)</span></span></span></span></span>, in other word, it filters any negative value.</p>
<p>In image below, we summed 2 relu function, it now looks like an elbow like graph which obviously can&#x27;t be constructed by linear function. However, if we sum all the linear function and only takes what we need, we may be able to construct it.</p>
<p>The relu line is defined with the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">mx + b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span>, we can sum as many relu as we want, and in a more complex problem, the equation may also involve more variable.</p>
<p><img decoding="async" loading="lazy" alt="2 relu summed" src="/cs-notes/assets/images/relu-sum-8fadd676df42a53fbcc46803278b3295.png" width="619" height="354" class="img_ev3q"><br>
<!-- -->Source : <a href="https://youtu.be/hBBOjCiFcuo?si=B8PDlJRj4QTa99k-&amp;t=2721" target="_blank" rel="noopener noreferrer">https://youtu.be/hBBOjCiFcuo?si=B8PDlJRj4QTa99k-&amp;t=2721</a></p>
<p><img decoding="async" loading="lazy" alt="Activation function example" src="/cs-notes/assets/images/activation-function-259005443ab63a55a88e82aafe85c67d.png" width="558" height="282" class="img_ev3q"><br>
<!-- -->Source : <a href="https://medium.com/@shrutijadon/survey-on-activation-functions-for-deep-learning-9689331ba092" target="_blank" rel="noopener noreferrer">https://medium.com/@shrutijadon/survey-on-activation-functions-for-deep-learning-9689331ba092</a></p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="other-terminology">Other Terminology<a href="#other-terminology" class="hash-link" aria-label="Direct link to Other Terminology" title="Direct link to Other Terminology">​</a></h3>
<ul>
<li>
<p><strong>Hyperparameter</strong> : Settings or configuration choices that are set before the learning process begins. They are not learned from the data but are determined by the practitioner or researcher. For example, learning rate is a hyperparameter that controls rate at which a model adjusts its parameters during training (e.g. used in gradient descent). After we trained the model and evaluate it, we may want to adjust the hyperparameter to explore if there is a more optimal result, this is called <strong>hyperparameter tuning</strong>.</p>
</li>
<li>
<p><strong>Epoch</strong> : Sometimes, training data is used more than one times while training model. The number of how many iteration pass through the entire dataset is called epoch. For example, when we say 5 epoch, it means that we are training the model with the same dataset for 5 times. Epoch is considered as hyperparameter as it is set before learning process.</p>
</li>
<li>
<p><strong>Batch</strong> : Batch is the number of samples or data points that is processed together in a single learning process. If we have 1000 data and we used batch of 32, it means during each iteration, we will process 32 at each time.</p>
</li>
<li>
<p><strong>Iteration</strong> : Iteration is the number of time we want to repeat each batch. It is different with epoch, epoch is the iteration of whole dataset, while this is just an iteration of a batch.</p>
</li>
<li>
<p><strong>Metric</strong> : Metric is a measure of the quality of model&#x27;s predictions. Metric is different with loss function, loss function is used to train while metric is used to evaluate the prediction. By evaluating the models, we can compare with different model or configuration. For example, a simple metric is accuracy, it is defined as the number of correct predictions divided by the total number of predictions.</p>
</li>
<li>
<p><strong>Data Split</strong> : Data is typically split into 3 :</p>
<ul>
<li><strong>Training Data</strong> : Training data is the part of data used to train the machine learning model, the model will learn from this data by adjusting its parameters (weight and bias) based on the actual data.</li>
<li><strong>Test Data</strong> : Test data is the data used to evaluate performance of the model after the training is done.</li>
<li><strong>Validation Data</strong> : Validation data is a subset of training data used to validate or assess model&#x27;s performance, this way we can adjusts the hyperparamater if required.</li>
</ul>
</li>
<li>
<p><strong>Fine Tuning</strong> : Fine tuning is the process of training a pre-trained model, typically with a smaller dataset or new task. The purpose of fine-tuning is to leverage the knowledge and learned representations from the pre-trained model and adapt it to the new task or dataset.</p>
</li>
<li>
<p><strong>Overfitting</strong> : Overfitting, also known as high variance, is a situation where a model learns too much from the training data, making it becomes too specialized and performs poorly on new, unseen data. This is the same when you practice alot on a math problem without studying the concept and only memorizing it, turns out the actual exam asked different questions. For a machine learning model, the concept is the pattern of the data, it is important to capture the broader patterns rather than a specific example.</p>
</li>
<li>
<p><strong>Underfitting</strong> : Underfitting, also known as high bias, is where the model is too simple or didn&#x27;t capture the underlying patterns in the data. It fails to learn the relevant relationships or make a very general assumptions for the training data.</p>
<p><img decoding="async" loading="lazy" alt="Overfit and underfit" src="/cs-notes/assets/images/underfit-overfit-6abff391da2e545f14d7fe27f635b139.png" width="826" height="287" class="img_ev3q"><br>
<!-- -->Source : <a href="https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76" target="_blank" rel="noopener noreferrer">https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76</a></p>
</li>
<li>
<p><strong>Optimizer</strong> : An optimizer is an algorithm or method used to optimize the loss function. Example of optimizer are <a href="/cs-notes/machine-learning/linear-regression#gradient-descent">gradient descent</a>, <a href="/cs-notes/deep-learning/deep-learning-foundation#stochastic-gradient-descent">stochastic gradient descent</a>, <a href="/cs-notes/deep-learning/deep-learning-foundation#adam">Adam</a>, and etc.</p>
</li>
<li>
<p><strong>Gradient Accumulation</strong> : While training model using optimizer like gradient descent, model&#x27;s parameters are updated after computing gradient for each batch. Gradient accumulation is a technique that defer parameters update, the gradient for each batch is accumulated and updated once with the accumulated gradient. Using this technique can helps reduce memory as it reduce the training process.</p>
</li>
<li>
<p><strong>Regularization</strong> : In simple term, regularization is a technique to make model prediction simpler (prevent overfitting) by forcing some feature coefficients to be 0. In other word, we exclude some variable contribution to our prediction. The features we are excluding are the one that has low influence toward the overall prediction. This way we can focus on the more important and influential features.</p>
</li>
<li>
<p><strong>Data Augmentation</strong> : Data augmentation is the process of increasing the diversity and variability of the training data, to make our model more generalized to unseen data. This technique is typically used when our data is limited or in an image classification task. For example, while training a dog or cat classifier, with the same image data, we can do some transformation such as rotating the image, skew it, flip, scale, or adjust its color in order to create more variety of data to reduce overfitting.</p>
</li>
<li>
<p><strong>Multi-target Model</strong> : A multi-target model is a model trained to predict multiple target variables simultaneously. For example, a multi-target model would also predict the number of bedrooms a house has rather than just predicting the house price. The training process will minimize overall loss of each target variable. The loss function itself is calculated individually and will be combined. The metric (e.g. error) for evaluating the prediction is also measured individually.</p>
</li>
<li>
<p><strong>Transfer Learning</strong> : A technique which a pre-trained model, initially trained on a specific task, is tweaked on a related task. Instead of training a model from scratch, an existing model trained on a similar task is tweaked to suit it to the new task. For example, if there is a model for classifying cars, we can suit it to classify trucks.</p>
</li>
<li>
<p><strong>Early Stopping</strong> : A technique to prevent overfitting by stopping the training of a model in an optimal time. One way to determine the optimal time to stop, is to plot the model&#x27;s loss curve and find out when the loss is slowly decreasing.</p>
<p><img decoding="async" loading="lazy" alt="Early stopping graph" src="/cs-notes/assets/images/early-stopping-ab7227c08b32c386b8ab2a23a22cd44f.png" width="423" height="250" class="img_ev3q"><br>
<!-- -->Source : <a href="https://miro.medium.com/v2/resize:fit:708/1*LSjaVNMa-ku85I35of-mAw.png" target="_blank" rel="noopener noreferrer">https://miro.medium.com/v2/resize:fit:708/1*LSjaVNMa-ku85I35of-mAw.png</a></p>
</li>
<li>
<p><strong>Learning Rate Decay</strong> : Learning rate is a hyperparameter that is set before the training of a model. In some scenario, the model successfully learn and reach the minima of a loss function, this mean it has reached an optimal loss. However, if we set a large learning rate, the model will still try to optimize the loss function, making the optimization become unstable. <strong>Learning rate decay</strong>, also called <strong>learning rate scheduling</strong>, helps us reduce the learning rate over time during the optimization process. The idea is to start with a high learning rate for rapid progress in the early stages of training. Then, as the we get closer to the optimal loss, the learning rate is decreased to reach more precise convergence.</p>
</li>
<li>
<p><strong>State-of-the-Art (SOTA)</strong> : A model is called SOTA if that particular model or technique achieve the highest known performance or accuracy in a specific benchmark or task. It represents the current best-known solution or method for a particular problem.</p>
</li>
<li>
<p><strong>Cutting-Edge</strong> : A cutting-edge model means a model or technique that represents the latest advancements in a field. These model are characterized by most recent innovation, methodologies, or architectural designs.</p>
</li>
<li>
<p><strong>Normalization</strong> : Normalization is the process of transforming data into a common scale without distorting its shape, meaning the relative relationships and patterns within the data are preserved even after the normalization process. The purpose of normalization is to generalize the data to prevent a feature with large number dominate the learning process. This will be useful for case when absolute values of the features are not as important as their relative positions within the range.</p>
<p>There are several technique to normalize data, an example is the min-max technique that scale all the data value in a specific range (e.g. 0 to 1). It uses the minimum and maximum value of the data as the baseline of the scaled value, the formula is <code>scaled_value = (value - min_value) / (max_value - min_value)</code>.</p>
<p><img decoding="async" loading="lazy" alt="Normalization" src="/cs-notes/assets/images/normalization-2f4a7422d31ff9d6dd0f381a70e57fad.png" width="660" height="231" class="img_ev3q"><br>
<!-- -->Source : <a href="https://zaffnet.github.io/batch-normalization" target="_blank" rel="noopener noreferrer">https://zaffnet.github.io/batch-normalization</a></p>
</li>
<li>
<p><strong>Batch Normalization</strong> : Batch normalization is the process of normalizing data in a batch, they are done by adding extra layer that does normalization process in the networks.</p>
</li>
<li>
<p><strong>Standarization</strong> : If normalization scale the data to a specific range, standarization, also known as z-score normalization, transforms the data to have a mean of 0 and standard deviation of 1. By centering the data around the mean and scales it based on the standard deviation, standardization make the data less affected by outliers while also preserving the shape of distribution.</p>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/glennhenry/cs-notes/tree/main/docs/deep-learning/01-deep-learning-foundation/deep-learning-foundation.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-03-17T10:48:01.000Z">Mar 17, 2024</time></b> by <b>glennhenry</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/cs-notes/deep-learning"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Deep Learning</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/cs-notes/deep-learning/deep-learning-tasks"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Deep Learning Tasks</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#loss-function" class="table-of-contents__link toc-highlight">Loss Function</a></li><li><a href="#entropy" class="table-of-contents__link toc-highlight">Entropy</a><ul><li><a href="#cross-entropy" class="table-of-contents__link toc-highlight">Cross Entropy</a></li><li><a href="#cross-entropy-loss" class="table-of-contents__link toc-highlight">Cross Entropy Loss</a></li><li><a href="#binary-cross-entropy-loss" class="table-of-contents__link toc-highlight">Binary Cross Entropy Loss</a></li></ul></li><li><a href="#stochastic-gradient-descent" class="table-of-contents__link toc-highlight">Stochastic Gradient Descent</a><ul><li><a href="#adam" class="table-of-contents__link toc-highlight">Adam</a></li></ul></li><li><a href="#activation-function" class="table-of-contents__link toc-highlight">Activation Function</a></li><li><a href="#other-terminology" class="table-of-contents__link toc-highlight">Other Terminology</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024, myself. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>