<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-deep-learning/transformers/transformers-architecture/transformers-architecture" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">Transformers Architecture | CS Notes</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://glennhenry.github.io/cs-notes/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://glennhenry.github.io/cs-notes/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://glennhenry.github.io/cs-notes/deep-learning/transformers/transformers-architecture"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Transformers Architecture | CS Notes"><meta data-rh="true" name="description" content="Transformers Architecture"><meta data-rh="true" property="og:description" content="Transformers Architecture"><link data-rh="true" rel="icon" href="/cs-notes/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://glennhenry.github.io/cs-notes/deep-learning/transformers/transformers-architecture"><link data-rh="true" rel="alternate" href="https://glennhenry.github.io/cs-notes/deep-learning/transformers/transformers-architecture" hreflang="en"><link data-rh="true" rel="alternate" href="https://glennhenry.github.io/cs-notes/deep-learning/transformers/transformers-architecture" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/cs-notes/assets/css/styles.56c058b7.css">
<link rel="preload" href="/cs-notes/assets/js/runtime~main.e1185037.js" as="script">
<link rel="preload" href="/cs-notes/assets/js/main.9d1d09fc.js" as="script">
</head>
<body class="navigation-with-keyboard" data-theme="light">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ navbar--dark"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/cs-notes/"><div class="navbar__logo"><img src="/cs-notes/img/logo.svg" alt="Docusaurus Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/cs-notes/img/logo.svg" alt="Docusaurus Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Notes</b></a><a href="https://github.com/glennhenry/cs-notes" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Docusaurus<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS darkNavbarColorModeToggle_X3D1" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="dsla-search-wrapper"><div class="dsla-search-field" data-tags="default,docs-default-current"></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/cs-notes/"><img src="/cs-notes/img/logo.svg" alt="Docusaurus Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/cs-notes/img/logo.svg" alt="Docusaurus Logo" class="themedImage_ToTc themedImage--dark_i4oU"><b>Notes</b></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/">Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/digital-signal-processing">Digital Signal Processing</a><button aria-label="Toggle the collapsible sidebar category &#x27;Digital Signal Processing&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/computer-and-programming-fundamentals">Computer &amp; Programming Fundamentals</a><button aria-label="Toggle the collapsible sidebar category &#x27;Computer &amp; Programming Fundamentals&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/digital-media-processing">Digital Media Processing</a><button aria-label="Toggle the collapsible sidebar category &#x27;Digital Media Processing&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/computer-networking">Computer Networking</a><button aria-label="Toggle the collapsible sidebar category &#x27;Computer Networking&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/data-structures-and-algorithms">Data Structures &amp; Algorithms</a><button aria-label="Toggle the collapsible sidebar category &#x27;Data Structures &amp; Algorithms&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/computer-organization-and-architecture">Computer Organization &amp; Architecture</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/operating-system">Operating System</a><button aria-label="Toggle the collapsible sidebar category &#x27;Operating System&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/theory-of-computation-and-automata">Theory of Computation &amp; Automata</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/compilers">Compilers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/programming-language-theory">Programming Language Theory</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/system-programming">System Programming</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cs-notes/database-system">Database System</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/computer-graphics">Computer Graphics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Computer Graphics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/frontend-web-development">Frontend Web Development</a><button aria-label="Toggle the collapsible sidebar category &#x27;Frontend Web Development&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/backend-development">Backend Development</a><button aria-label="Toggle the collapsible sidebar category &#x27;Backend Development&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/computer-security">Computer Security</a><button aria-label="Toggle the collapsible sidebar category &#x27;Computer Security&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/machine-learning">Machine Learning</a><button aria-label="Toggle the collapsible sidebar category &#x27;Machine Learning&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/cs-notes/deep-learning">Deep Learning</a><button aria-label="Toggle the collapsible sidebar category &#x27;Deep Learning&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/deep-learning-foundation">Deep Learning Foundation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/deep-learning-tasks">Deep Learning Tasks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/neural-network">Neural Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/cnn">CNN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/resnet">ResNet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/u-net">U-Net</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/siamese-network">Siamese Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/rnn">RNN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/lstm">LSTM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/gru">GRU</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/autoencoder">Autoencoder</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/variational-autoencoder">Variational Autoencoder</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/gan">GAN</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/cs-notes/deep-learning/transformers/attention-mechanism">Transformers</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/transformers/attention-mechanism">Attention Mechanism</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/cs-notes/deep-learning/transformers/transformers-architecture">Transformers Architecture</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/transformers/transformers-audio">Transformers Audio</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/transformers/bert">BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/transformers/gpt">GPT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/transformers/bart">BART</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/transformers/vision-transformers">Vision Transformers</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cs-notes/deep-learning/diffusion-model">Diffusion Model</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/cs-notes/deep-learning/reinforcement-learning/reinforcement-learning-fundamental">Reinforcement Learning</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/software-engineering">Software Engineering</a><button aria-label="Toggle the collapsible sidebar category &#x27;Software Engineering&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/cloud-computing">Cloud Computing</a><button aria-label="Toggle the collapsible sidebar category &#x27;Cloud Computing&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/cs-notes/extras">Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/cs-notes/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/cs-notes/deep-learning"><span itemprop="name">Deep Learning</span></a><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Transformers</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Transformers Architecture</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Transformers Architecture</h1></header><p><strong>Main Source :</strong></p><ul><li><strong><a href="https://youtu.be/z1xs9jdZnuY?si=_rWDHRCle8k-x8SG" target="_blank" rel="noopener noreferrer">Transformer (Attention is all you need) - Minsuk Heo 허민석</a></strong></li><li><strong><a href="https://youtu.be/4Bdc55j80l8?si=hHjopC6GvZl-mZTv" target="_blank" rel="noopener noreferrer">Illustrated Guide to Transformers Neural Network: A step by step explanation - The A.I. Hacker - Michael Phi</a></strong></li><li><strong><a href="https://huggingface.co/learn/nlp-course/chapter1/1" target="_blank" rel="noopener noreferrer">Hugging Face NLP course part 1</a></strong></li></ul><p><strong>Transformers</strong> is a type of deep learning architecture that specifically uses the <a href="/cs-notes/deep-learning/transformers/attention-mechanism">attention mechanism</a> as the key component. While <a href="/cs-notes/deep-learning/transformers/attention-mechanism#rnn-with-attention">RNN with attention</a> also use the attention mechanism, transformers is a standalone architecture that doesn&#x27;t need traditional sequential model like <a href="/cs-notes/deep-learning/rnn">RNN</a>.</p><p>RNN processes information in sequence, each step has to wait for the previous step to complete, this will makes computation slow. The removal of RNN will indeed allow us to get better performance. The motivation behind transformers is to address the performance issue we are facing in RNN. The question is, how can we remove RNN while preserving the attention mechanism?</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="transformers-architecture">Transformers Architecture<a href="#transformers-architecture" class="hash-link" aria-label="Direct link to Transformers Architecture" title="Direct link to Transformers Architecture">​</a></h2><p><img loading="lazy" alt="The main architecture of transformers" src="/cs-notes/assets/images/transformers-architecture-979b9f67f275bed7004cc50535d25569.png" width="413" height="594" class="img_ev3q"><br>
<!-- -->Source : <a href="https://machinelearningmastery.com/the-transformer-model/" target="_blank" rel="noopener noreferrer">https://machinelearningmastery.com/the-transformer-model/</a> (with modification)</p><p>Transformers follow the encoder and decoder architecture, meaning one component should capture and summarize information from input and another is the one that produces output.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="encoder">Encoder<a href="#encoder" class="hash-link" aria-label="Direct link to Encoder" title="Direct link to Encoder">​</a></h3><p>The encoder is responsible for taking and processing the input sequence, it consist of multiple layer that works together. Here is the walkthrough of input processing in encoder :</p><ol><li><p><strong>Input Embedding</strong> : The input sequence is first transformed into a dense vector (often called as <strong>token</strong>) that of course contain numbers. The input embedding is a learnable process, meaning it can be adjusted during the backpropagation process.</p></li><li><p><strong>Positional Encoding</strong> : Since transformers doesn&#x27;t include RNN that process input sequentially, <strong>positional encoding</strong> is a technique to capture the relative position of tokens within a sequence.</p><p>Positional encoding is done by adding a sinusoidal function (sin and cos) of different frequency to the token vector. Basically the sinusoidal function will model the position information of each element in the sequence using different variation of frequency. There are few reason why sinusoidal function is used, one of the important is the <strong>periodic</strong> nature offers an unlimited encoding regardless of our input length. Other reason are its smooth transition and non-linear function.</p><p><img loading="lazy" alt="Positional encoding" src="/cs-notes/assets/images/positional-encoding-45f699c2543499baa834ada806243631.png" width="589" height="357" class="img_ev3q"><br>
<!-- -->Source : <a href="https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model" target="_blank" rel="noopener noreferrer">https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model</a></p></li></ol><p>The first and second is the input pre-processing, the third and so on is the actual processing inside an encoder layer.</p><ol start="3"><li><p><strong>Self-Attention</strong> : Transformers uses a mechanism called <strong>self-attention</strong>. The standard attention mechanism produces a context vector (vector that contains important information about the sequence we are processing) by considering the input sequence and the current output we are generating. In other word, it needs two sequence which is the input and current output to generate another output.</p><p>On the other hand, self-attention only need a single sequence. Self-attention weigh the relevance or importance of each element by comparing it with other element in the sequence itself. This will make different element have different relevance in the sequence.</p><p>What makes it superior than the standard attention mechanism, it offers us <strong>parallelization</strong>. Remember that in <a href="/cs-notes/deep-learning/transformers/attention-mechanism#attention">RNN with attention</a> on each output step, the attention vector is different, they are computed based on current output step. Self-attention that uses single sequence allows us to compute attention scores for all elements simultaneously, leading to better performance.</p><ul><li><strong>Self-attention process</strong> :<br>The self-attention is implemented by matrix multiplication. The token embedding are fed into the first layer in encoder. The self-attention mechanism is applied, it is done by calculating three types of vector called <strong>query, key, and value vectors</strong>. The tokens embedding are combined in a matrix, it will be multiplied by three types of matrices that correspond to each vector mentioned. The matrix are <strong>Wq, Wk, and Wv</strong>, they are basically weights in form of matrix, which mean they are learnable.</li></ul><p>These three vector carries the information of token, they will be used to calculate an attention weight, which is the value of importance of a token in sequence.</p><ul><li>The query vector represent specific position of token in the input sequence that we want to compute the attention weights for.</li><li>Key vector represent the other token in the sequence that is being compared to the query vector. As explained in self-attention mechanism, we will compare all the element with each other to consider which one is more important than other.</li><li>Value vector contain the actual information or the features of each token in the sequence.</li></ul><p>Visualization of the relation between each word or token in self-attention.<br>
<img loading="lazy" alt="Self-attention visualization 2" src="/cs-notes/assets/images/self-attention-visualization-11a9fbd1c07e6fa7f140ec5827291ab7.png" width="933" height="497" class="img_ev3q"><br>
<!-- -->Source : <a href="https://www.researchgate.net/figure/A-visualization-of-a-learned-self-attention-head-on-a-sentence-The-visualization-shows_fig5_346522738" target="_blank" rel="noopener noreferrer">https://www.researchgate.net/figure/A-visualization-of-a-learned-self-attention-head-on-a-sentence-The-visualization-shows_fig5_346522738</a>, <a href="https://babich.biz/transformer-architecture/" target="_blank" rel="noopener noreferrer">https://babich.biz/transformer-architecture/</a></p><p>The query and key vector (transposed) will be multiplied together, producing something called <strong>attention scores</strong>, it can be interpreted as the similarity between two token. The higher the result is the stronger the relevance. They will be divided by the square root of the dimensionality of the key vectors, to prevent large number. <a href="/cs-notes/deep-learning/neural-network#softmax-activation-function">Softmax activation function</a> will be applied to the previous result, resulting normalized value (they sum up to 1).</p><p>The normalized value will be multiplied with the value vector. The result of it is what we call <strong>attention weights</strong>. The normalized value which represent the similarity of information is multiplied by the actual information of the token in input sequence. This mean we are assigning the similarity of information to each token.</p><p>The output (called <strong>attention layer output</strong>) will be the sum of all attention weights and this will be done for all element in sequence. The properties of encoder that consider each token in sequence including the previous and subsequent token to capture the information that lies on the input is called <strong>bi-directional</strong>. In other word, it can pay attention to every token in the sequence.</p><p><img loading="lazy" alt="Self-attention matrix multiplication" src="/cs-notes/assets/images/self-attention-a556567684fdea41975866e764f76c8e.png" width="907" height="241" class="img_ev3q"><br>
<!-- -->Source : <a href="https://theaisummer.com/transformer/" target="_blank" rel="noopener noreferrer">https://theaisummer.com/transformer/</a></p><p><img loading="lazy" alt="Another visualization of self-attention" src="/cs-notes/assets/images/self-attention-2-f32b43e6f7b09d16f9cac0f2e824bd1b.png" width="759" height="293" class="img_ev3q"><br>
<!-- -->Source : <a href="https://youtu.be/z1xs9jdZnuY?si=czJyixA7IV3DxG7k&amp;t=475" target="_blank" rel="noopener noreferrer">https://youtu.be/z1xs9jdZnuY?si=czJyixA7IV3DxG7k&amp;t=475</a></p></li><li><p><strong>Multi-Head Attention</strong> : Multi-head attention is an extension of the self-attention mechanism that improves its efficiency through parallel computation.</p><p>The set of query, key, and value vector we have obtained is grouped in something called <strong>attention head</strong>. They first goes into a linear layer to be projected into different vector spaces, basically projecting them mean we are looking through these vector from different perspective. This will allows the model to capture different representation of the input.</p><p>So, to calculate attention weights in parallel, we will compute all the attention head simultaneously. The same calculation that includes multiplication between query and key vector, softmax normalization, and multiplication with the value vector, is also performed.</p><p>By calculating them simultaneously, the model can have longer dependencies when processing the input sequence. The model attends to all other tokens and can access different parts of the sequence during the matrix multiplication process.</p><p>Each result of attention head will be concatenated together and will be passed into a linear layer again, producing the final output of multi-head attention layer.</p><p><img loading="lazy" alt="Multi-head attention" src="/cs-notes/assets/images/multi-head-attention-3944081d5838af9adaaecb5c7beb0abb.png" width="351" height="452" class="img_ev3q"><br>
<!-- -->Source : <a href="https://paperswithcode.com/method/multi-head-attention" target="_blank" rel="noopener noreferrer">https://paperswithcode.com/method/multi-head-attention</a></p></li><li><p><strong>Residual Connection and Layer Normalization (Add &amp; Norm)</strong> : We did alot of calculation, during the backpropagation process, we may lose some information including the positional encoding we did in the earlier step. Transformers uses the <a href="/cs-notes/deep-learning/resnet#residual-connection">residual connection concept</a> to help prevent the vanishing gradient issue. This is implemented by adding the input that bypass the attention layer with the same input that goes to the attention layer. This layer also include a normalization process to normalize the output of attention layer, to prevent large number and stabilize the training process.</p></li><li><p><strong>Feed-Forward Networks (FFN)</strong> : The attention output is passed through a feed-forward network within the encoder layer. The FFN that includes activation function like ReLU introduces non-linearity. The output of the FFN is then passed through another residual connection and layer normalization. Similar to residual connection in multi-head attention layer, we will add the input that bypass the feed-forward network with the one that goes through it.</p><p><img loading="lazy" alt="Residual connection" src="/cs-notes/assets/images/add-norm-a83cbb62aef4b9965812c9379658d813.png" width="645" height="307" class="img_ev3q"><br>
<!-- -->Source : <a href="https://deepgram.com/learn/visualizing-and-explaining-transformer-models-from-the-ground-up" target="_blank" rel="noopener noreferrer">https://deepgram.com/learn/visualizing-and-explaining-transformer-models-from-the-ground-up</a></p></li></ol><p>The step from 3 to 6 represent the process of a single encoder layer. In conclusion, a single layer of encoder processes and transform the input sequence to capture relevant information in the sequence. The output of encoder is a key and value pair that represent the information about the input sequence.</p><p>Transformers architecture may includes multiple encoder layer, they have identical architecture but they don&#x27;t share weights.</p><p><img loading="lazy" alt="The encoder layer" src="/cs-notes/assets/images/encoder-layer-6a205a39c14cf4505b729a703469f5f0.png" width="214" height="393" class="img_ev3q"><br>
<!-- -->Source : <a href="https://machinelearningmastery.com/the-transformer-model/" target="_blank" rel="noopener noreferrer">https://machinelearningmastery.com/the-transformer-model/</a> (with modification)</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="decoder">Decoder<a href="#decoder" class="hash-link" aria-label="Direct link to Decoder" title="Direct link to Decoder">​</a></h3><p>The decoder is responsible for generating the output sequence, it consist of similar layer with encoder like multi-head attention, add &amp; norm, and feed-forward network. The decoder takes input from encoder layer (the relevant information) and from the previous output as well.</p><p>The decoder first process the previous output and then it will be combined with the output from encoder.</p><ol><li><p><strong>Output Embedding</strong> : The previously generated output embedded or will be turned into a vector representation (token), similar to input embedding in encoder.</p></li><li><p><strong>Positional Encoding</strong> : The similar encoding process to capture the relative position of each token in the sequence.</p></li><li><p><strong>Masked Multi-Head Attention</strong> : In the normal multi-head attention, we calculated the attention weights using matrix multiplication which include multiplying every element with each other. Multiplying with every element includes accessing its query, key, vector, meaning we have information about them. However, we don&#x27;t want this to happen in decoder, we don&#x27;t want to generate output with the information from future. This is implemented by changing some of the value in the matrix during the matrix multiplication to a very large negative number. The properties of decoder that only access previously generated token is called <strong>uni-directional</strong>.</p><p><img loading="lazy" alt="Masked multi-head attention" src="/cs-notes/assets/images/masked-multi-head-attention-46ddaaea0aa6d6fd268a3e2ba570cc84.png" width="784" height="242" class="img_ev3q"><br>
<!-- -->Source : <a href="https://tigris-data-science.tistory.com/entry/%EC%B0%A8%EA%B7%BC%EC%B0%A8%EA%B7%BC-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-Transformer4-Masked-Multi-Head-Attention%EA%B3%BC-Decoder" target="_blank" rel="noopener noreferrer">https://tigris-data-science.tistory.com/entry/%EC%B0%A8%EA%B7%BC%EC%B0%A8%EA%B7%BC-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-Transformer4-Masked-Multi-Head-Attention%EA%B3%BC-Decoder</a></p></li><li><p><strong>Add &amp; Norm</strong> : It then goes to the add &amp; norm layer again which consist of residual connection and normalization layer.</p></li><li><p><strong>Multi-head Attention</strong> : Also known as <strong>encoder-decoder attention</strong> or <strong>cross-attention</strong>, in this step, the multi-head attention will be done again. Multi-head attention will need query, key, and value vector, the input for them will be the combination of the output from encoder and previous result from decoder. Encoder provides the key and value pair, the decoder&#x27;s previous output is transformed into a query. So basically, the encoder provides the contextual information from key and value pair and its combined with the query from decoder that represent what we need to generate the output now. It then goes to add &amp; norm layer again.</p></li><li><p><strong>Feed-Forward Network + Add &amp; Norm</strong> : The next component in the decoder layer is the feed-forward network. Same as the encoder, it consist of fully connected layer with non-linear activation function. Next, it will be normalized again in the add &amp; norm layer.</p></li><li><p><strong>Output</strong> : Finally, the output from previous layer will go into a linear layer, followed with softmax activation function to produces the probability of each token. The model select the token with highest probability and use it as the input for next decoder step. Decoder is considered as  <strong>auto-regressive</strong> or <strong>causal-attention</strong>, meaning it output sequence one step at a time, in an iterative and sequential manner.</p><p><img loading="lazy" alt="Transformers output" src="/cs-notes/assets/images/transformers-output-eda0dc134579527e86a626998bcc2abd.png" width="479" height="291" class="img_ev3q"><br>
<!-- -->Source : <a href="https://www.linkedin.com/pulse/intro-transformer-architecture-jithin-s-l" target="_blank" rel="noopener noreferrer">https://www.linkedin.com/pulse/intro-transformer-architecture-jithin-s-l</a></p></li></ol><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="learning-process">Learning Process<a href="#learning-process" class="hash-link" aria-label="Direct link to Learning Process" title="Direct link to Learning Process">​</a></h3><p>After output is generated, the prediction or whatever the output is will be compared with the actual label of the input. For example :</p><ul><li>In machine translation, the labels would be the target sequences or translations corresponding to the source sequences.</li><li>In text classification, the labels represent different categories of the input text.</li></ul><p>After loss is calculated, the similar learning process will be done, including the backpropagation process through all the layer of transformers model from the decoder output until the encoder input.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="extras">Extras<a href="#extras" class="hash-link" aria-label="Direct link to Extras" title="Direct link to Extras">​</a></h4><p>Transformers consists of encoder and decoder, however, they are not necesarry used together. In tasks like text classification or sentiment analysis, where understanding of the input is the primary objective, the encoder captures contextual information about the data, the information can be fed into classifier directly. A decoder-only model is designed for generating output. It is applicable in tasks such as dialogue generation, where it generates the subsequent word based on previously generated words.</p><p>They are often used together for sequence-to-sequence tasks like text translation that require understanding of input data and the output generation.</p><p>Transformers model is considered as semi-supervised learning. The semi-supervised learning involve techniques like pre-training and fine tuning. The pre-training technique mean the model is trained on unlabeled data. During the pre-training process, the model learns the general language representation and capture how each word relate with each other. It will then be fine tuned, a smaller labeled dataset will be fed to the model to adapt it on specific tasks or specific topic (the method is also called <strong>transfer learning</strong>).</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/glennhenry/cs-notes/tree/main/docs/deep-learning/14-transformers/02-transformers-architecture/transformers-architecture.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2023-10-13T10:50:28.000Z">Oct 13, 2023</time></b> by <b>glennhenry</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/cs-notes/deep-learning/transformers/attention-mechanism"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Attention Mechanism</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/cs-notes/deep-learning/transformers/transformers-audio"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Transformers Audio</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#transformers-architecture" class="table-of-contents__link toc-highlight">Transformers Architecture</a><ul><li><a href="#encoder" class="table-of-contents__link toc-highlight">Encoder</a></li><li><a href="#decoder" class="table-of-contents__link toc-highlight">Decoder</a></li><li><a href="#learning-process" class="table-of-contents__link toc-highlight">Learning Process</a><ul><li><a href="#extras" class="table-of-contents__link toc-highlight">Extras</a></li></ul></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/cs-notes/assets/js/runtime~main.e1185037.js"></script>
<script src="/cs-notes/assets/js/main.9d1d09fc.js"></script>
</body>
</html>