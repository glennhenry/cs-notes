{"searchDocs":[{"title":"Authentication","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/authentication","content":"","keywords":"","version":"Next"},{"title":"Authentication Factor​","type":1,"pageTitle":"Authentication","url":"/cs-notes/backend-development/authentication#authentication-factor","content":" The charateristics of someone's identity can be categorized into 5, these are called authentication factor :  Knowledge Factors : Knowledge factors rely on something the user knows, such as a password, passphrase, PIN, or answers to security questions. Possession Factors : Involve something the user possesses, typically physical object or devices such as smart cards or mobile devices. Inherence Factors : Also known as biometric factors, these are unique physical or behavioral characteristics of the user. Biometric authentication factors include fingerprint scans, iris or retinal scans, voice recognition, facial recognition, or even typing patterns. Location Factors : Location factors takes account the geographic location or network information associated with the user. For example, a system may authenticate a user based on their IP address, GPS location, or proximity to a specific Wi-Fi network. Behavior Factors : Behavior factors focus on the user's patterns and habits such as user's typing speed, mouse movements, navigation patterns, or even the timing and frequency of their interactions.  ","version":"Next","tagName":"h3"},{"title":"Authentication Factor Number​","type":1,"pageTitle":"Authentication","url":"/cs-notes/backend-development/authentication#authentication-factor-number","content":" Authentication may also include more than just one evidence from the user, there can be more than one factor of consideration, this is called authentication factor number.  Single-Factor Authentication (SFA) : This involves just a single authentication factor, typically a password or PIN. Two-Factor Authentication (2FA) : 2FA requires the user to provide two different authentication factors to verify their identity. For example, the first factor is password and the system may generate some number which is sent to user's mobile device as the second factor. Multi-Factor Authentication (MFA) : MFA is the use of two or more authentication factors. It can include a combination of all authentication factor including the 5 listed above (knowledge, possession, inherence, location, behavior).  ","version":"Next","tagName":"h3"},{"title":"General Authentication Process​","type":1,"pageTitle":"Authentication","url":"/cs-notes/backend-development/authentication#general-authentication-process","content":" User Request / Login : The authentication process begins when a user sends a request to access a protected resource or initiate a login operation, the user will also enter their credentials. User Identification : The backend system receives the user request and verify it against the stored credentials. Based on the verification, the system decide whether to grant access to the user or not. Additional Factors (optional) : Depending on the auth factor number used, the system may need additional credentials. Session Establishment : Upon successful authentication, the system establishes a session for the authenticated user. A session is a period of time the user has been actively authenticated in a system, typically the user will receive their own session identifier, which will be used for subsequent access and is limited in time (expirable). Authorization &amp; Access Grant : The system decide whether the user has appropriate permission to do certain actions, if the user is allowed, the system will grant access to the user's requested resource or operation. The user can proceed to interact with the system or perform the desired actions. Session Termination : After the user is done with their activity, the session can be terminated, whether explicitly through a logout operation, or automatically by the system after a period of inactivity or upon reaching the session expiration time.   Source : https://swoopnow.com/user-authentication/  tip See authentication technique for implementation. ","version":"Next","tagName":"h3"},{"title":"Backend Development","type":0,"sectionRef":"#","url":"/cs-notes/backend-development","content":"","keywords":"","version":"Next"},{"title":"All pages​","type":1,"pageTitle":"Backend Development","url":"/cs-notes/backend-development#all-pages","content":" Web ServerAPIs APIs &amp; Server LogicREST APISOAPGraphQLRPCWebhookWebSocket AuthenticationAuthentication TechniqueAuthorizationSearch EngineMessage BrokerArchitecture MonolithicMicroserviceSOAServerless ContainerizationDocker &amp; KubernetesBackend &amp; Server SecurityCachingBackend Optimization ","version":"Next","tagName":"h3"},{"title":"APIs & Server Logic","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/apis-server-logic","content":"","keywords":"","version":"Next"},{"title":"What is API​","type":1,"pageTitle":"APIs & Server Logic","url":"/cs-notes/backend-development/apis-server-logic#what-is-api","content":" Application Programming Interface (API) is a contract between two component in a system, the contract has rules that define how these two component can interact with each other. We can think API as the bridge that connects between software component.  While making a program, we often interact with other software component (function, class, library). The purpose of API is to add a layer of abstraction and hide all the implementation details, making the client (the programmer) that interact with it only access the necessary functionality. API is something a programmer really use most of the time, calling function, classes, or modules is considered as calling an API.  APIs Types​  Some types of APIs are :  Library API : Library is a collection of pre-written ready-to-use code, this includes function, classes, methods, constants, and other code components. Libraries are used to make programmers develop their application without needing to build everything from scratch. An example of library include graphics API like OpenGL that lets you render 2D or 3D computer graphics. OS API : Operating system APIs is considered as low-level API, this includes accessing necessary function or classes that interact directly with OS component such as file system, memory management, task manager, process management, and etc. Example of OS level API can be a class that access internal storage in Android OS. Web API : Web API is an API accessed over the internet using protocols like HTTP. Web APIs can be used if the code we are interacting to is stored on the internet. Common Web APIs include Google Maps API that let you know about geolocation information, Spotify API that allows developers to search for songs, albums, and artists, retrieve user playlists.  Here is an example of API in Kotlin :  logOutput(&quot;Calling an API&quot;)   Let's say a programmer wanted to log some string, he expects the log to be stored in some list and the log should be printed to the console with current time. The programmer expect everything to be handled by the logOutput just by passing string in the parameter, because an API should hide implementation details.  import java.time.LocalTime val logs = mutableListOf&lt;String&gt;() fun logOutput(out: String) { val currentTime = LocalTime.now() logs.add(out) println(&quot;Log : $out - at $currentTime&quot;) }   Code above shows actual implementation of the logOutput function. Inside logOutput, it does all the work including saving the log and printing it out on the console. Even the logOutput function still uses an API, it gets current time through some java API which is the java.time.LocalTime (and that API calls again another system-level API that access the default time).  API is considered as a contract because the caller and the API needs to agree what they must provide and what they will return. In this scenario, the caller must provide a string as the log and the API must print it out on console for the caller.  ","version":"Next","tagName":"h3"},{"title":"Routing & Endpoint​","type":1,"pageTitle":"APIs & Server Logic","url":"/cs-notes/backend-development/apis-server-logic#routing--endpoint","content":" When the client sends a request to the server, the request must be handled with the appropriate actions. Routing is the mechanism of handling specific request specified by request's URL and HTTP method to the appropriate codes that should handle them.  The request's URL can vary depending on the request, a URL must contain some component including scheme, domain name, and etc (find more about URL here). A web URL typically have a starting point or the root of all URL, it represents the primary address that clients use to communicate with the server. The primary address is called base URL.  When making request to the base URL, the server will typically respond by returning the default page of the website. If we want to do certain operations or access specific resources, we need to add additional path, this is called endpoint. Endpoint is typically concatenated to the base URL and is separated by /.  For example, the https://jsonplaceholder.typicode.com is an online web API we can use. The https://jsonplaceholder.typicode.com/ is the base URL or the root of all page the site has, accessing the base URL directly will give you the HTML file which is then rendered by the browser to display the website's content.  The website provides some endpoint including /posts, /comments, /albums, and some other. Accessing the base URL concatenated with the /posts endpoint https://jsonplaceholder.typicode.com/posts will give us a JSON file containing some user posts.  Overall, routing and endpoint determines how the server should respond to different requests by mapping them to specific endpoints or routes. The behavior of accessing specific endpoints will depend on the web API.   Source : https://apipheny.io/api-endpoint/  ","version":"Next","tagName":"h3"},{"title":"Middleware​","type":1,"pageTitle":"APIs & Server Logic","url":"/cs-notes/backend-development/apis-server-logic#middleware","content":" In backend, Middleware is a software that bridges between application and server. It is an intermediary that takes incoming requests from application and pass it into server. Middleware is not restricted to a single component, multiple middleware can be connected with other middleware or with other application logic before it is connected into the server.  The purpose of middleware is to pre-process incoming request before passing it into server to make the server can focus only on the actual logic. For example, middleware in the context of backend can be used check if user has logged in or not, checking for cookies (file that stores basic user information), add additional data or configuration, and handling request error.   Source : https://medium.com/@seymarslan/what-ismiddleware-fdb2ad163388  ","version":"Next","tagName":"h3"},{"title":"API Gateway​","type":1,"pageTitle":"APIs & Server Logic","url":"/cs-notes/backend-development/apis-server-logic#api-gateway","content":" Middleware general definition is a software that bridges between one system to another, an API gateway is a type of middleware focused to bridges between the client and the backend system. It is a centralized entry point to access the server, a unified interface for APIs consumer.  An API gateway may provide some functionality including :  Routing &amp; Endpoint Management : API gateway handles the routing of requests to the appropriate backend services based on predefined rules. It manages the endpoints and exposes a unified interface for API consumers. Protocol Translation : API Gateways can translate between different protocols. For example, it can handle requests in RESTful format from clients and convert it into another format such as GraphQL, WebSockets, or another API protocol. Security &amp; Authentication : API Gateways provide security mechanisms to protect APIs, they can handle authentication and authorization. Request &amp; Response Modifier : API Gateways can modify, validate, or transform requests and responses as they pass through, this can include validating or filtering requests. Rate Limiting : API Gateways can limit the number of requests made by a client in a period of time. Caching : API Gateways may store its previous response from previous API request, this is called caching. If the further request requested the same data, the API gateway doesn't need to forward it to the server, instead it responds with the data stored in the cache. Monitoring &amp; Analytics : API Gateways may log or monitor incoming request, API usage and performance. This allows administrators to gain insights into the traffic patterns, identify potential issues, and make optimal decision for optimization problem. Load Balancing : Load balancing is the technique to distribute incoming request across multiple backend server to balance the load.   Source : https://stackoverflow.com/questions/53477140/asp-net-core-api-gateway-middleware ","version":"Next","tagName":"h3"},{"title":"Authorization","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/authorization","content":"","keywords":"","version":"Next"},{"title":"OAuth​","type":1,"pageTitle":"Authorization","url":"/cs-notes/backend-development/authorization#oauth","content":" Open Authorization (OAuth) is an open-standard authorization protocol used to allow users to grant access to their resource on their application without sharing their credentials directly to the application by involving third-party application.  When we are trying to register on some application, often times they provide a way to register using a Google or Facebook account, this is an example of OAuth. Instead of giving credentials directly to the app by registering and obtaining the permission, the application ask another trusted app whether to give permission to the user. With OAuth, the application we are accessing can access our information in the third party app used to authorize.  In other word, OAuth enables us to log in to an application using our existing accounts from other platforms (e.g. Google or Facebook) without needing to enter our credentials to the application. Under the hood, they are not sharing our credentials, instead, they uses something called access tokens. The access token can be thought as the proof of permission given by the user to access their own resource.   Source : https://thedailybeast.freshdesk.com/support/solutions/articles/43000627842-how-do-i-add-google-facebook-login-for-one-step-sign-in-  This is the process of OAuth :  User initiates the process : The user wants to access a service or application that requires authorization. They initiate the OAuth process by clicking on a &quot;Sign in with [Provider]&quot; button or a similar action. Redirect to the authorization server : The user is redirected to the authorization server (e.g., Google, Facebook) that holds their account and controls access to their resources. User authentication : The user is prompted to enter their login credentials (username and password) on the authorization server's login page to verify their identity. User authorization : After successful authentication, the authorization server presents the user with a consent screen. The screen explains what information the requesting application wants to access and asks the user to grant or deny permission. Authorization grant : If the user grants permission, the authorization server generates an authorization grant in the form of authorization code. An authorization code is a short-lived token that represents the user's consent to grant access to their protected resources. Redirect back to the client application : The authorization server redirects the user back to the client application (the application that requested access) and includes the authorization code as a parameter in the redirect URL. Token exchange : The client application takes the authorization code and sends a request to their authorization server to exchange it for an access token. Access token is the actual token used to access the user's protected resources. Access Granted : With the access token, the client application can make requests to the resource server (the server that holds the user's resources) on behalf of the user. The access token acts as proof of authorization. Token expiration &amp; refresh token : The token have a limited lifespan, the authorization server may issue a refresh token along with the access token. The refresh token can be used to obtain a new access token when the current one expires without requiring the user to go through the entire authentication process again. Source : https://youtu.be/CPbvxxslDTU?si=2EZK6waoD46nbE8U&amp;t=251 ","version":"Next","tagName":"h3"},{"title":"Authentication Technique","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/authentication-technique","content":"","keywords":"","version":"Next"},{"title":"Session-based Authentication​","type":1,"pageTitle":"Authentication Technique","url":"/cs-notes/backend-development/authentication-technique#session-based-authentication","content":" Session is a period of time the user has been actively authenticated in a system . After a user provide a valid credentials and successfully authenticated, the system will generate a unique session identifier. These are used to uniquely identify and track the user's session throughout their interaction with the system or application, this way the user doesn't need to authenticate everytime they made a request.  The session identifier (often referred as session ID) is typically also stored in user's device. They are stored in small piece of data called cookie. Cookie is a small text files sent by web server and stored in user's browser. Web browsers use cookies to store information about a user's preferences, activity, or session ID.  Session-based Authentication Process​  User Authentication : The authentication process begins when a user submits their credentials such as a username/email and password to the server. The server verifies the provided credentials against its user database or authentication system. If the credentials are valid, the user is considered authenticated. Session Establishment &amp; Generation : After successful authentication, the server creates a session for the user. The server generates a unique identifier or session ID for the session. Session ID is stored on the user's device in the form of cookie, when the user wants to make request to the system, the browser will sends this along with the request to identify the user. Session Validation : Whenever the server receives a request from the user, it validates the session ID. It checks whether the session ID is valid, associated with an authenticated user, and has not expired or been invalidated. Expiration &amp; Session Termination : Sessions often have a defined expiration time to ensure security by allowing limited access to the system. Session can also be terminated before the expiration time, for example, if the user remains inactive for a specified period (idle timeout), or manually logs out, the session is terminated. The session ID is invalidated, and subsequent requests with that session ID will not be accepted.   Source : https://sherryhsu.medium.com/session-vs-token-based-authentication-11a6c5ac45e4  Session-based authentication has some downsides :  Session Management : The server needs to store each of the user's session ID, in a large-scale application with high user traffic, this can be memory-intensive. The server also need to validate and maintain the session state which require additional processing. In a load balancing server architecture, session data also needs to be shared or replicated across servers to maintain session continuity. Security Concern : Session-based authentication can raise some security concern including cookie hijacking and cross-site request forgery which steal user's session ID to make request on behalf of the user.  ","version":"Next","tagName":"h3"},{"title":"Token-based Authentication​","type":1,"pageTitle":"Authentication Technique","url":"/cs-notes/backend-development/authentication-technique#token-based-authentication","content":" Token-based authentication is a method of authentication where a token is used to verify the identity of a user. A token is a unique and encoded string of characters that contains all the necesarry information to authenticate user.  In session-based authentication, the user stores session ID and the server also store it aswell along with the session state. Instead of that, when the user wanted to authenticate, the server send the user a token encoded in a secret key which only the server has. When the user make a request, the server will verify if it is encoded using the same secret key. This way, we can reduce the burden of the server. However, token-based authentication still have some security downsides similar to session-based authentication, because the information is still stored on the client-side, it can still be stolen by an attacker.  JWT​  In token-based authentication, JSON Web Token (JWT) is typically used as the implementation. JWT is encoded in base64 uses JSON as the format for representing the token's actual data. JWT consists of header, payload, and signature.  Header : Header specify algorithm used to sign the token and the type of the token (which is &quot;JWT&quot;).Payload : Payload contains the actual user's data such as username and email.Signature : Signature contains a string which is generated by signing the concatenated string of the header and payload with . using the algorithm specified in the header and the secret key owned by the server. While making request, the user send the token and the server will unsign the token and verify it.   Source : https://jwt.io/  Token-based Authentication Process​  User Authentication : The user provides their credentials (e.g., username and password) to an authentication endpoint or service. Authentication &amp; Token Generation : The authentication server verifies the user's credentials and if successful, the server generates a token. This token is associated with the authenticated user and contains relevant information such as user ID, roles, and permissions. Token Issuance : The server issues the token to the client in the response. The token is typically returned as part of the authentication response payload. Token Usage : The client stores the token securely and include it in subsequent requests to the server. This is done by adding the token to request header, such as the authentication bearer method. This is done by including the token in the HTTP header, for example : Authorization: Bearer abc123. Token Validation : When the server receives a request with a token, it validates the token using the secret key as well as other checks such as expiration time, token format, and any additional custom validation rules. Authorization &amp; Access Grant : Once the token is validated succesfully, the server will only allow user action they are allowed to do based on the information in the token. Token Expiration : Tokens may have an expiration time to ensure security and session management. If a token expires, the client needs to obtain a new token by repeating the authentication process.   Source : https://www.freecodecamp.org/news/how-to-sign-and-validate-json-web-tokens/  ","version":"Next","tagName":"h3"},{"title":"SSO​","type":1,"pageTitle":"Authentication Technique","url":"/cs-notes/backend-development/authentication-technique#sso","content":" Signle Sign-On (SSO) is an authentication solution to authenticate user to log in once and gain access to multiple device without needing to authenticate again. SSO is implemented by protocol, the two popular are Security Assertion Markup Language (SAML) and OpenID. The difference is SAML uses XML while OpenID uses JWT to exchange data, both provide secure connection.  In SSO, a service provider (SP) is an application that relies on identity provider (IdP), the one who provide user's identity. This is how SSO works in general using SAML :  User Accesses an Application : The user initiates the SSO process by attempting to access an application or service that supports SAML-based SSO. Application Redirects to Identity Provider (IdP): IdP is a service that authenticate user and provide identity information to other system, it is responsible for verifying user's identity.The application will redirect the user to the chosen IdP. User Authentication at IdP: The user is presented with the IdP's login page or authentication form. The user enters their credentials (e.g., username and password) to authenticate themselves to the IdP. IdP Generates SAML Assertion : Upon successful authentication, the IdP generates a SAML assertion, which contains information about the user's authentication status and attributes. The SAML assertion is digitally signed by the IdP, meaning it is encrpyted using a public/private key encryption so that only the IdP and SP itself knows the information to ensure the integrity and origin of the data. SAML Assertion Sent to Service Provider : The IdP sends the SAML assertion back to the original application or browser and sent it to the service provider. The assertion is sent to Assertion Consumer Service (ACS) URL, which is a specific endpoint provided by service provider used when the authentication is successful. SP Validates the SAML Assertion : The SP receives the SAML assertion from the IdP. The SP validates the assertion by verifying the digital signature to ensure the assertion's integrity and authenticity. User Authorized &amp; Session Established : Upon successful validation of the SAML assertion, the SP considers the user authenticated and establishes a session for the user. The session allows the user to access the application or service without providing additional credentials, basically the authentication process will be skipped. SAML Assertion Expiration : SAML assertions have an expiration time, typically set by the IdP. If the assertion expires, the SP may need to request a new assertion from the IdP to continue the SSO session. The SP may also supports single logout, which is a mechanism to log out from the SP and terminate the SSO session.   Source : https://support.google.com/a/answer/6262987?hl=id ","version":"Next","tagName":"h3"},{"title":"Backend Optimization","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/backend-optimization","content":"","keywords":"","version":"Next"},{"title":"Horizontal vs Vertical Scaling​","type":1,"pageTitle":"Backend Optimization","url":"/cs-notes/backend-development/backend-optimization#horizontal-vs-vertical-scaling","content":" Backend architecture can be scaled to handle increased traffic and load. They can be scaled in two ways :  Horizontal : Horizontal scaling involves adding more server to distribute the loadVertical : Vertical scaling involves upgrading the server hardware (e.g. CPU, RAM, storage).  ","version":"Next","tagName":"h3"},{"title":"Network Optimization​","type":1,"pageTitle":"Backend Optimization","url":"/cs-notes/backend-development/backend-optimization#network-optimization","content":" Minimize Requests : Combine multiple requests into a single request or use techniques like HTTP/2 multiplexing to reduce the overhead of establishing multiple connections. Minify responses file, compress or resizing image can reduce amount of data transferred over the network especially for large file. Content Delivery networks (CDN) : Distribute static content, such as images, CSS, and JavaScript files, to a CDN. CDN is distributed servers that are strategically placed in different locations worldwide to reach user faster. Source : https://www.cloudflare.com/learning/cdn/what-is-a-cdn/  ","version":"Next","tagName":"h3"},{"title":"Server Optimization​","type":1,"pageTitle":"Backend Optimization","url":"/cs-notes/backend-development/backend-optimization#server-optimization","content":" Server Caching : Implement caching mechanisms like Redis caching or reverse proxies to store frequently accessed data closer to the server. Load Balancing : Distribute incoming traffic across multiple servers using load balancers, this will reduce the burden of a single server. Source : https://herza.id/blog/load-balancing-pengertian-cara-kerja-jenis-metode-kelebihan-dan-kekurangan/  ","version":"Next","tagName":"h3"},{"title":"Resource Optimization​","type":1,"pageTitle":"Backend Optimization","url":"/cs-notes/backend-development/backend-optimization#resource-optimization","content":" Efficient Database Queries : Optimize database queries by ensuring proper indexing, avoiding unnecessary joins or subqueries, and optimizing data retrieval. Efficient Algorithms and Data Structures : Use algorithms and data structures that provide efficient operations for your application's use cases. For example, if you frequently search or write data, consider using hash map. Resource Pooling : Which is the technique of reusing resources by creating a shared repository of resources rather than creating a new resource for each request. This includes sharing database connections, network sockets, or other expensive resources. Source : https://www.thecrazyprogrammer.com/2022/01/resource-pooling-in-cloud-computing.html ","version":"Next","tagName":"h3"},{"title":"Backend & Server Security","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/backend-server-security","content":"Backend &amp; Server Security See Computer Security &gt; Backend &amp; Server Security","keywords":"","version":"Next"},{"title":"Docker & Kubernetes","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/docker-kubernetes","content":"Docker &amp; Kubernetes See Cloud Computing &gt; Docker &amp; Kubernetes.","keywords":"","version":"Next"},{"title":"Containerization","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/containerization","content":"Containerization See Cloud Computing &gt; Containerization.","keywords":"","version":"Next"},{"title":"GraphQL","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/graphql","content":"","keywords":"","version":"Next"},{"title":"Schema​","type":1,"pageTitle":"GraphQL","url":"/cs-notes/backend-development/graphql#schema","content":" We need to define schema, it serves as contract for the client and the server for querying and manipulating data.  Here is an example of schema in GraphQL :  type User { id: ID! name: String! email: String! posts: [Post!]! } type Post { id: ID! title: String! content: String! author: User! } type Query { user(id: ID!): User post(id: ID!): Post } type Mutation { createUser(name: String!, email: String!): User createPost(title: String!, content: String!, authorId: ID!): Post }   A data type is defined using type keyword followed with its name. In this example, a type User contains fields id, name, email, and posts, each has its own type as well. The exclamation sign indicate the field is non-null or required. The square bracket indicate a relationship between data types (e.g. the User has relation with Post).  The Query (not reserved word) is the actual query we are defining, to define a query, we include field (e.g. user and post) with argument and we specify the type we are returning. The argument can be used to filter or to query specific data.  The Mutation (also not reserved word) is the mutation we are defining. Mutation is an operation to modify data on server including create, update, or delete data. We define the createUser to take name and email.  The type name and its field has no actual meaning, they are just random word. The way of how the server decides when to query and when to mutate data is included in the request.  ","version":"Next","tagName":"h3"},{"title":"GraphQL Request​","type":1,"pageTitle":"GraphQL","url":"/cs-notes/backend-development/graphql#graphql-request","content":" GraphQL consist of the following structure :  Operation Type : GraphQL has 2 type of operation : Query : Request data from a server (read operation)Mutation : Modify data on the server such as create, update, or delete (write operation) Operation Name (optional) : Text specified to describe the action. Variables (optional) : We can include our variable and it will be defined with $ Query Fields : The main part of a GraphQL request, these fields represent the data that the client wants to retrieve from the server. The client can specify the fields it needs, including nested fields and relationships which mean the data are related. Arguments (optional) : Arguments are used to filter or provide additional information to the server to customize the response. Arguments are passed using the variable we defined before. Directive (optional) : Directives enable clients to modify the execution behavior of a GraphQL query. They provide additional instructions to the server about how to handle certain parts of the query. Directive can only modify functionality of an operation compared to argument that capable of modifying the data received.  Here is an example of a GraphQL query (different from the schema above) :  query ($userId: ID!, $withPosts: Boolean!) { user(id: $userId) { name email @include(if: $withPosts) } }   We defined the variable userId with types of ID!. The query fields is based on the server schema, we also specify argument id that uses the userId variable defined before. user field includes name and email nested on it. The email field uses directive @include which specify the response should include something, in this case it should include posts.  By making the request flexible, GraphQL help to address REST API issues, sometimes the client may receive some data they don't need in a GET request, this is called overfetch. Another the client may not receive enough data in a single GET request, therefore making him need to make another network request which can waste resource, this is called underfetch. ","version":"Next","tagName":"h3"},{"title":"Caching","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/caching","content":"","keywords":"","version":"Next"},{"title":"Caching Example​","type":1,"pageTitle":"Caching","url":"/cs-notes/backend-development/caching#caching-example","content":" An example of caching in social media application.  In social media applications, there are often feeds containing posts from users and their friends. Each post may include text, images, and the profile photo of the poster. The typical operation for retrieving post information involves the application making a request to a remote server, which then retrieves the requested data from a database.  Instead of repeating this process multiple times, we can store post information in local storage and modify the application logic to check if the data is present before requesting it from the remote server. This approach leads to faster response times, reduced bandwidth usage, and a decrease in the server workload for serving requests.  ","version":"Next","tagName":"h3"},{"title":"Type of Caching​","type":1,"pageTitle":"Caching","url":"/cs-notes/backend-development/caching#type-of-caching","content":" Client-side​  The caching technique is implemented in the client side, they are typically handled by the web browsers (in the case of web application). This includes caching static resources such as HTML pages, images, files that are loaded when the user visits a website. Web browser also include local storage API, which can be used to store frequently accessed data such as user credentials.  Another use case of client-side caching is, as explained in the social media caching example.  Server-side​  Caching technique where the server or the developer is the one who handles it, they are typically implemented on the server to store and serve frequently accessed data or resources.  CDN Caching : CDN is a geographically distributed network of servers that are strategically placed in different locations worldwide. The primary purpose of a CDN is to improve the delivery speed and performance of web content to end-users. For example, a person connecting from Asia is likely to have a better connection to a server located in Asia compared to a server located in a distant region, such as North America or Europe. We may also cache the resource in the CDN itself, which is what CDN caching is, they are considered distributed caching, which is the practice of distributing cache around multiple servers to allows efficient data access across different machines or locations. Source : https://www.wallarm.com/what/difference-between-a-cdn-and-web-accelerator Database Query : When a common type of query is executed, the result is stored in a cache. If the same query is requested again, the server can return the cached result instead of executing the query again.  ","version":"Next","tagName":"h3"},{"title":"Caching Strategy​","type":1,"pageTitle":"Caching","url":"/cs-notes/backend-development/caching#caching-strategy","content":" Cache-Aside : The application is responsible for managing the cache. When data is requested, the application first checks the cache, if the data is found, it is retrieved from the cache and returned to the requester. If the data is not in the cache (called cache miss), the application retrieves it from the data source, stores it in the cache for future use, and then returns it to the requester. Write-Through : This is a cache writing strategy where, every time data is written or updated, it is written to both cache and data source simultaneously. Write-through caching ensures data consistency but may have higher write latency due to the additional write operation to the data source. Write-Back : Write-back caching involves writing or updating data in the cache first and deferring the write to the underlying data source. This will reduce latency compared to write-through strategy, however, this approach introduce the risk of data loss. Read-Through : When data is requested and not found in the cache (a cache miss), the cache automatically retrieves the data from the underlying data source. The retrieved data is then stored in the cache and returned to the requester.   Source : https://medium.com/@mmoshikoo/cache-strategies-996e91c80303  ","version":"Next","tagName":"h3"},{"title":"Cache Invalidation​","type":1,"pageTitle":"Caching","url":"/cs-notes/backend-development/caching#cache-invalidation","content":" Cache Invalidation is the process of removing or marking data as invalid in a cache when the corresponding data in the underlying data source is updated or deleted. It ensures that the cached data remains consistent with the source of truth.  The methods are :  Explicit Invalidation : The application explicitly triggers the invalidation of specific data in the cache when changes are made to the corresponding data in the data source. This can involve calling cache-specific methods or APIs to remove or update the affected data in the cache. Time-Based Invalidation : Associates a time-to-live (TTL) value with each cached item. When the TTL expires, the cached item is considered invalid and is evicted from the cache. Event-Based Invalidation : Cache is invalidated by observing events or triggers that signify changes in the data source. These events can be database triggers or message queue notifications.  ","version":"Next","tagName":"h3"},{"title":"Cache Replacement​","type":1,"pageTitle":"Caching","url":"/cs-notes/backend-development/caching#cache-replacement","content":" Cache Replacement, also known as cache eviction, is the process of selecting which data should be evicted from a cache when the cache reaches its capacity limit and a new item needs to be stored.  Least Recently Used (LRU) : This policy assumes that recently accessed data is more likely to be accessed again in the near future. The data item that has not been accessed for the longest period is evicted. Least Frequently Used (LFU) : This policy assumes that frequently accessed items are more valuable and should be retained in the cache. The data item that has been accessed the least number of times will be evicted. First-In-First-Out (FIFO) : The data item that was inserted into the cache first is evicted when the cache is full. This follows a queue-like behavior, where the oldest data is removed. Random Replacement : This will select a random data item from the cache for eviction. It does not take into account the recency or frequency of access. Source : https://www.interviewcake.com/concept/java/lru-cache  ","version":"Next","tagName":"h3"},{"title":"Redis​","type":1,"pageTitle":"Caching","url":"/cs-notes/backend-development/caching#redis","content":" REmote DIctionary Server (Redis) is a popular use case for a remote, distributed, in-memory data structure store. It is typically used for distributed caching, particularly its hash map data structure.  The reason a hash map is suitable for caching is that, in a hash map, data is stored as key-value pairs. The keys are unique identifiers, and the corresponding values which is where we store our actual data. Given a key, the data structure will return the value. So, as long as we know the key, then an efficient average constant O(1) for read and write speed can be achieved.  More than a hash-map, Redis can be used to store other data structure, such as lists, sets, sorted sets, strings, bitmaps, etc.  One important thing about Redis is, its distributed nature. Redis supports sharding and replication, it allows developers to distribute data across multiple Redis instances deployed on multiple servers for improved scalability and fault tolerance. This can be beneficial for applications that need to handle large amounts of data or high traffic loads. In contrast, a traditional hash maps typically operate within a single process or machine.  For example, a backend application is deployed across multiple distinct servers. The application is supposed to serve request from anywhere around the world. If the application uses a traditional hash-map for storing data and serving the request, then all the instances would have inconsistent data.  This is because traditional hash maps don't have a synchronization mechanism implemented. Unless we implement a synchronization mechanism across multiple instances of the application, it simply won't work. So basically, Redis makes it easier for developers to handle synchronizing between distributed servers.   Source : https://blog.hackajob.com/how-to-implement-redis-in-go/  tip See also distributed systems. ","version":"Next","tagName":"h3"},{"title":"Microservice","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/microservice","content":"","keywords":"","version":"Next"},{"title":"Advantages & Disadvantages​","type":1,"pageTitle":"Microservice","url":"/cs-notes/backend-development/microservice#advantages--disadvantages","content":" Advantages :  Scalability : Microservices allow independent scaling of different components of an application. Scaling heavyweight service won't affect lightweight service, thus allowing for efficient resource utilization.Flexibility : Different services can be developed using different programming languages, frameworks, and databases as long as they can communicate with each other.Maintainability : Complex application can be broken down into smaller and self-contained services. Each service can be developed, deployed, and maintained independently, making it easier to add new features, fix bugs, and perform updates without impacting the entire system.Fault Isolation : If one microservice were to break, the isolation characteristics of microservices will help to prevent impacting other service.Team Development : Different teams can work on different services easily, as each service should be independent. This allows for faster development cycles and easier team coordination.  Disadvantages :  Complexity : Microservices still need to talk with each other, meaning they should be as general as possible to adapt with other service. The management of distributed systems, inter-service communication, and data consistency between services can be challenging.Operational Overhead : Each service will require separate deployment, monitoring, and infrastructure management. This can increase the complexity and cost of system operations.Network Latency &amp; Performance : Microservices rely on inter-service communication, which introduces latency compared to in-process communication in monolithic architectures.Testing &amp; Debugging : Ensuring all service to work correctly while also having external dependencies if they talk to other services can be quite challenging. Coordinating between each service's logs, events, and data across different services can be time-consuming (distributed debugging)  Overall, microservices architecture are suitable for complex application where each component need to be scaled independently, diverse development team, or frequent update for each component.   Source : https://medium.com/hengky-sanjaya-blog/monolith-vs-microservices-b3953650dfd ","version":"Next","tagName":"h3"},{"title":"Message Broker","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/message-broker","content":"","keywords":"","version":"Next"},{"title":"Message Queue​","type":1,"pageTitle":"Message Broker","url":"/cs-notes/backend-development/message-broker#message-queue","content":" Message broker may store messages received from the client in something called message queue, which is a queue data structure that is used to store messages until they can be processed by the intended service. By using queue, this ensures the messages are processed in appropriate request order.  Another benefits of message queue :  Asynchronous Communication : Message queues also enable asynchronous communication patterns, where the sender and receiver do not need to be active at the same time. They can also operate independently at their own pace. For example, messages can be held while the user or the service is down, and sent after it's up again. Fault Tolerance : Message queues can provide fault tolerance by storing messages persistently. If the service fails, the messages in the queue remain, ensuring that no data is lost. Once the failed component becomes available again, it can resume processing messages from the queue. Source : https://ademcatamak.medium.com/what-is-message-broker-4f6698c73089  ","version":"Next","tagName":"h3"},{"title":"Message Brokers Model​","type":1,"pageTitle":"Message Broker","url":"/cs-notes/backend-development/message-broker#message-brokers-model","content":" There are several pattern used to define how messages are processed in message brokers :  Publish-Subscribe (one to many) : In the publish-subscribe pattern, a message sender (publisher) sends messages to the message broker and the recipients (subscribers) express interest in receiving messages from specific criteria. They both don't know who sent and who receipt the message, all message are handled by the message broker. Point-to-Point Messaging (one to one) : In the point-to-point pattern, messages are sent from a sender to a specific recipient. The sender puts messages into a message queue, and the broker ensures that each message is delivered to exactly one recipient. Once a message is consumed by a recipient, it is removed from the queue. Request-Reply (one to one) : The request-reply pattern involves a sender (requester) sending a request message to a recipient (replier) and expecting a response in return. The message broker acts as a mediator, routing the request message to the appropriate recipient and forwarding the response back to the requester. Source : https://medium.com/tech-sauce/introduction-to-message-queuing-4a7ab8968b59  ","version":"Next","tagName":"h3"},{"title":"Messaging Protocol​","type":1,"pageTitle":"Message Broker","url":"/cs-notes/backend-development/message-broker#messaging-protocol","content":" Messaging protocol is a set of rules that define how message brokers deliver messages. It involves the structure of a message, message broker model, and the network transport layer used. Two common protocols are Advanced Message Queuing Protocol (AMQP) and Message Queue Telemetry Transport (MQTT). AMQP is designed to be flexible that allows any pattern to be implemented on top of it while MQTT only supports the publish-subscribe pattern. MQTT is more lightweight than AMQP, typically used for Internet of Things (IoT).  As explained before, the message brokers can be used as converting one protocol to another, such as converting REST API from application call to desired messaging protocol like AMQP.   Source : https://www.cloudamqp.com/blog/amqp-vs-mqtt.html  ","version":"Next","tagName":"h3"},{"title":"RabbitMQ​","type":1,"pageTitle":"Message Broker","url":"/cs-notes/backend-development/message-broker#rabbitmq","content":" RabbitMQ is an open-source message broker, it follows the similar traditional message broker system. It uses the AMQP protocol and provide features like message queuing, publish-subscribe, and request-reply patterns.  Messages from producers are received by exchanges, these will be held in queue until it will be consumed by the consumers. Exchanges and queue are connected together by a binding, it will determine how message should be delivered from exchanges to queue, which is called routing. Each message will have a routing key to determine which queue should receive the message.  There are types of exchange to determine the routing process :  Direct Exchange : Use the exact match between the routing key of a message and the routing key specified in the binding of the exchange and the queue.Topic Exchange : Routes messages to queues based on matching patterns in the routing key.Fanout Exchange : Routes messages to all queues that are bound to it.  The consumer will then receive a message from a queue.   Source : https://halovina.com/konfigurasi-rabbitmq-untuk-message-broker/  ","version":"Next","tagName":"h3"},{"title":"Apache Kafka​","type":1,"pageTitle":"Message Broker","url":"/cs-notes/backend-development/message-broker#apache-kafka","content":" Apache Kafka on the other hand, is designed to be more scalable for real-time data streaming and processing. A unit of data produced by producers consisting of key-value pair, timestamp, and additional metadata is called a record. These records are organized into topics and topics are further divided into partitions, record are distributed within the partitions. Each partition is an ordered, immutable sequence of records. These topics are handled by a broker which is responsible for receiving records from producers and serve records to consumers. A group of broker are then grouped together to form a Kafka Cluster, the cluster act as the entry point for producers and consumers to interact with the messaging system.   Source : https://www.projectpro.io/article/apache-kafka-architecture-/442 ","version":"Next","tagName":"h3"},{"title":"Monolithic","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/monolithic","content":"","keywords":"","version":"Next"},{"title":"Backend Architecture​","type":1,"pageTitle":"Monolithic","url":"/cs-notes/backend-development/monolithic#backend-architecture","content":" In software, architecture is the overall design and structure of a software system. Defining an architecture provide a principle of how one component talk to each other. This way we can achieve better organization, modularity, and reusability of our software.  In backend, architecture refers to the design and organization of the server-side components, it focuses on how different backend components interact, handle data, process requests, and provide services to the frontend or client-side of an application.  There are many backend architecture, the example are monolithic, microservices, SOA, and serverless.  Source : https://stackoverflow.com/questions/76415176/explain-microservice-architecture  ","version":"Next","tagName":"h3"},{"title":"Monolithic​","type":1,"pageTitle":"Monolithic","url":"/cs-notes/backend-development/monolithic#monolithic","content":" In monolithic architecture, the entire application is built as a single, indivisible unit. In the context of backend, the backend application handles various actions such as data access, business logic, user interface, and third-party integrations within a single runtime process.  This mean all the code and dependencies are packaged and deployed together, and any changes or updates to one part of the application require redeploying the entire monolith.  An example use case of monolithic architecture is when each component are tightly coupled to each other, in other word, one component is really dependent on the other.  For example, monolithic architecture may be used for an e-commerce platform that allows customers to browse products, add them to a shopping cart, and complete the checkout process. All the component are interconnected, using a unified codebase would simplify the development process.  ","version":"Next","tagName":"h3"},{"title":"Advantages & Disadvantages​","type":1,"pageTitle":"Monolithic","url":"/cs-notes/backend-development/monolithic#advantages--disadvantages","content":" Advantages :  Simplicity : Straightforward to develop, debug, test, deploy, and manage since all the components are bundled together within a single codebase.Ease of Development : Developers can work on the entire application without worrying about complex inter-component communication or integration.  Disadvantages :  Inflexibility : When a small changes or updates in one component often require redeploying the entire application, causing longer release cycles and limiting the ability to adopt new technologies. This will also make the application hard to test consideringReliability : A failure in one component of a monolithic application can potentially bring down the entire system, as there is no isolation between components.Technology Limitations : Monolithic architectures often require using a single technology stack or programming language throughout the application, limiting the ability to leverage the best tools and frameworks for specific components. Migrating them would mean to migrate the entire codebase.Scalability : Considering the application is tightly coupled, having more server means we need to distribute the entire monolithic app to each of the server. This will make these servers interdependent, as they will rely on shared resources and data, which in turns, making synchronization of the shared resources becomes complex.  Overall, monolithic architecture is typically suited for smaller applications with simpler requirements, limited scalability needs, and smaller development team which are familiar with the codebase.   Source : https://techaffinity.com/blog/microservices-architecture-vs-monolithic-architecture/ ","version":"Next","tagName":"h3"},{"title":"RPC","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/rpc","content":"","keywords":"","version":"Next"},{"title":"RPC​","type":1,"pageTitle":"RPC","url":"/cs-notes/backend-development/rpc#rpc","content":" Remote Procedure Call (RPC) is a communication protocol that allows a computer program to run procedule on a remote server. By procedure, it is a sequence of instruction that such as codes, it will be executed as if it were a local function call.  Function invocation can be achieved through a contract called IDL (Interface Definition Language) or a schema definition. This definition is used to specify which function or procedures that can be called remotely. The IDL is designed to be language-agnostic, this mean even if the client requested a function call on a server that uses different programming language, the server still know what to do.  RPC Process​  The RPC framework will generate language-specific code for the client and the server to use to communicate. The function or procedure used may also include argument to pass additional data.  Serialization : Serialization is the process of transforming data into something that can be transmitted over the network. The RPC framework will generate code following the IDL definition and will serialize the code into format like JSON, XML, or Protocol Buffers. The generated code is called client stub. The serialized request will be transmitted over the network using protocol like TCP or UDP and the server will receive it. Deserialization : The RPC framework in the server will deserialize the data received and transform it into the language the server use, this is called stub decoding. Execution &amp; Responses : The server will execute the function with its parameters and will generate the response. The response can be the result data or an error information including its error code and message. The response will be sent to the client following the similar step that includes serialization and deserialization as before. Source : https://www.javatpoint.com/what-is-rpc-in-operating-system  RPC provide a way for client to communicate with the server with code or function call instead of communicating by specifying specific operation like GET or POST request in REST API.  ","version":"Next","tagName":"h3"},{"title":"gRPC​","type":1,"pageTitle":"RPC","url":"/cs-notes/backend-development/rpc#grpc","content":" Google Remote Procedure Call (gRPC) is an implementation of RPC that can run in any environment. gRPC utilize HTTP/2 as the transport protocol, it gained performance advantages of HTTP/2 including multiplexing, single TCP connection, and concurrent request.  Protocol Buffers​  gRPC uses protocol buffers as the default Interface Definition Language (IDL) for data serialization and deserialization. Protocol buffers is a binary serialization format developed by Google designed to be language-agnostic which mean it can be used by any language.  The protocol buffer is what makes RPC able to bridge between the difference of the language used, it provide a contract for both client and server language.  Here is an example of protobuf message definition :  message Person { string name = 1; int32 age = 2; repeated string emails = 3; }   In the example, we are defining a Person type of message. It has 3 fields and each of it has unique field number as identifier. The repeated keyword indicate it can have multiple values.  When the client make an gRPC call, the function executed will be serialized into binary format based on the protobuf definition and will be transmitted over the network. On the receiving end, it will be deserialized with the same definition.  For example, the Person type defined above can be generated in kotlin code :  data class Person( val name: String, val age: Int, val emails: List&lt;String&gt; )  ","version":"Next","tagName":"h3"},{"title":"Search Engine","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/search-engine","content":"","keywords":"","version":"Next"},{"title":"Search Engine Techniques​","type":1,"pageTitle":"Search Engine","url":"/cs-notes/backend-development/search-engine#search-engine-techniques","content":" Search engine is a vital component in an application, it should be fast and accurate to provide a positive user experience. Below are key techniques used in search engine :  Indexing : Indexing is a fundamental technique where data is structured to allow for quick retrieval of relevant information. Indexing maps the data to specific search terms or keywords, making it easier and faster to locate. For example, if the search query starts from letter &quot;A&quot;, we can just skip anything that doesn't start with letter &quot;A&quot;. Tokenization : Tokenization is the process of breaking down a content into smaller units, known as tokens. Tokens can be words, phrases, or other meaningful units. For example, the sentence Delicious Cake Recipes can be broken down into [&quot;Delicious&quot;, &quot;Cake&quot;, &quot;Recipes&quot;]. Tokenization helps indexing process by searching the word one by one, it can also benefits the user to find the relevant information if they make mistakes or typos in their search queries. Inverted Index : An inverted index is a data structure commonly used in search engines. It maps terms or tokens to the documents or records that contain them. For example, in a blog website, we may map some words into a specific blog posts to allows user to locate the posts faster. Ranking : Ranking is the process of determining the relevance of search results based specific criteria. The factors can be popularity, recent search behavior, and user preferences can influence the ranking of search results. Filtering : Filtering is a techniques used to filter search results based on specific criteria. Filters allow users to narrow down the search results based on attributes such as category, date range, location, or other custom metadata. Caching : Caching is a technique where frequently accessed search results or frequently executed queries are stored in memory for faster retrieval.  ","version":"Next","tagName":"h3"},{"title":"Types of Search Engine​","type":1,"pageTitle":"Search Engine","url":"/cs-notes/backend-development/search-engine#types-of-search-engine","content":" Three example types of search engine are :  Web Search Engine : Web search engines, such as Google, Bing, and Yahoo, focus on indexing and searching web pages available on the internet. They use something called web crawlers, a component used to traverse, retrieve, and gather data to populate the search engine's index. Their algorithms consider various factors like relevance, popularity, and page ranking to provide search results. Full-Text Search Engine : These search engines are designed to index and search through the entire content of documents or web pages. They analyze the text and index it based on keywords, allowing users to search for specific terms or phrases within the indexed content. Database Search Engine : These search engines focus on searching within structured databases. They enable users to query and retrieve specific data from databases using SQL-like queries or other query languages.  ","version":"Next","tagName":"h3"},{"title":"Elasticsearch​","type":1,"pageTitle":"Search Engine","url":"/cs-notes/backend-development/search-engine#elasticsearch","content":" Elasticsearch is a search and analytics engine, at its core, it is a document-oriented database that stores data in JSON-like format. It allows you to index and search structured and unstructured data, making it suitable for many use cases such as full-text search, log analytics, and real-time analytics.  Elasticsearch operates in a distributed architecture. An engine of elasticsearch capable of storing data and performing search operation is considered as a node. These nodes work together to form a cluster.  A similar data is grouped together and stored in a logical container called index. It can be thought of as a separate database within Elasticsearch. An index is identified by a name, which should be unique within the Elasticsearch cluster.  In elasticsearch, every time we index data, it is divided into smaller subsets called shards. These shards are then distributed on separate node in a cluster to allows parallel search and indexing operations, making them faster and scalable.  Also, as an analytics engine, elasticsearch allows us to perform analytics and statistical analysis (using tool called logstash); and data visualization (using tool called Kibana).  Overall, the key features of elasticsearch is combining distributed computing and scalable storage with also various analytics feature. ","version":"Next","tagName":"h3"},{"title":"REST API","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/rest-api","content":"REST API Main Source : What Is A RESTful API? - AWSRESTful APIs in 100 Seconds - Fireship Representational State Transfer API (REST API) is an architectural style for designing an API. Web services that implement REST architecture are called RESTful web services. REST works by leveraging or extending HTTP concepts. The underlying architecture of REST API follows the HTTP protocol to perform specific types of operation such as accessing resource. Every resource in the server are uniquely identified by Uniform Resource Identifiers (URI), basically a unique address. An example of URI : file:///C:/Users/username/Documents/file.txt, https://www.example.com/index.html. In the case of REST API, the URI used typically uses the concept of routing and endpoint. REST includes another HTTP concepts like : HTTP methods : Such as GET, POST, DELETE, PUT followed with the endpoint (e.g. GET /books).HTTP format and syntax : Including header, request line, request body, response format, and version.HTTP Responses : Codes to indicate the result of the response, for example, 200 OK signifies a successful response, 404 Not Found denotes that the requested resource does not exist. Example of REST API GET request : GET /books HTTP/1.1 Host: api.example.com User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Accept: application/json In this example, a GET request is made to the /books endpoint of the api.example.com server. User agent contains information about the client. The client expects a response in JSON format. HTTP/1.1 200 OK Content-Type: application/json [ { &quot;id&quot;: 1, &quot;title&quot;: &quot;Book 1&quot;, &quot;author&quot;: &quot;Author 1&quot; }, { &quot;id&quot;: 2, &quot;title&quot;: &quot;Book 2&quot;, &quot;author&quot;: &quot;Author 2&quot; } ] In the response, the server returns a status line with the status code (200 OK) along with the JSON data. The response body is a JSON array that includes information about two books, each with an ID, title, and author.","keywords":"","version":"Next"},{"title":"Serverless","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/serverless","content":"","keywords":"","version":"Next"},{"title":"Advantages & Disadvantages​","type":1,"pageTitle":"Serverless","url":"/cs-notes/backend-development/serverless#advantages--disadvantages","content":" Advantages :  Reduced Operational Overhead : With serverless architectures, developers are relieved from the burden of managing servers and infrastructure. The cloud provider handles tasks such as server provisioning, scaling, and maintenance and developers can focus more on writing code.Scalability :The cloud provider able to scale the infrastructure automatically based on the incoming workload. As the number of requests or events increases, the provider provisions the necessary resources to handle the load.Cost Efficiency : Serverless architectures payment is often based on pay-per-use pricing model. You only pay for the actual execution time and resources consumed by your functions. This allows for cost optimization since you don't incur costs for idle resources.  Disadvantages :  Limited Execution Time : Serverless platforms often limits execution time on functions. For example, AWS Lambda has a default maximum execution time of 15 minutes.Vendor Lock-In : Serverless platforms relies a lot on cloud provider, we need to suit with their APIs, making it challenging to migrate to a different provider.Debugging &amp; Monitoring : Application which is distributed in functions and the lack of direct access to underlying infrastructure can make it difficult to troubleshoot issues and gather comprehensive monitoring data.Limited Control : Serverless architectures abstract away much of the infrastructure management, which means developers have limited control and customization options. Certain low-level optimizations, specialized configurations, or integration with specific libraries might not be possible in a serverless environment.  Serverless can be beneficial for inconsistent app that have little or no traffic as it uses pay-per-use pricing model. It is suitable for developers who don't want to be burdened with managing servers and infrastructure.   Source : https://pueblerino.info/serverless-functions-vs-microservices ","version":"Next","tagName":"h3"},{"title":"SOAP","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/soap","content":"SOAP Main Source : Wikipedia SOAP Simple Object Access Protocol (SOAP) is a protocol to exchange information or message using XML. It follows XML standard such as schema to define message in a structured way. Specifically, XML uses the WSDL (Web Services Description Language), a standard used for XML-based services on the web. It still uses HTTP as the transport protocol over the network. When SOAP messages are transmitted over HTTP, they are typically encapsulated within the payload of an HTTP request (POST) or response. The HTTP headers and body are used to transport the SOAP message between the client and the server. Here is an example of SOAP request encapsulated in HTTP POST method : POST /exampleWebService HTTP/1.1 Host: www.example.com Content-Type: application/soap+xml; charset=utf-8 Content-Length: [length] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;soap:Envelope xmlns:soap=&quot;http://www.w3.org/2003/05/soap-envelope/&quot;&gt; &lt;soap:Header&gt; &lt;!-- Optional header --&gt; &lt;/soap:Header&gt; &lt;soap:Body&gt; &lt;getWeather xmlns=&quot;http://example.com/weather/&quot;&gt; &lt;city&gt;New York&lt;/city&gt; &lt;/getWeather&gt; &lt;/soap:Body&gt; &lt;/soap:Envelope&gt; Request is sent to the /exampleWebService endpoint on the www.example.com server encapsulated in POST method. The content type specified is in SOAP XML format. The soap:Envelope is the root element of a SOAP message, it defines the namespace with the soap envelope URI. The header contains optional information. The body specify the actual SOAP message. The SOAP message here is using the &lt;getWeather&gt; operation with the New York city as the parameter. info SOAP is not very used in modern days, using XML may add complexity for a relatively simple use case. REST API is more used due to its better performance, XML parsing can be slower than JSON data format as JSON has a simpler structure and syntax.","keywords":"","version":"Next"},{"title":"Web Server","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/web-server","content":"Web Server Main Source : What is a web server? - MDNApache vs NGINX - IBM Technology When we attempt to access a website using a web browser, the browser initiates communication by sending a message to a server through an HTTP request. A server that is responsible for responding to client requests, specifically for delivering web pages, is known as a web server. Web server will respond to client request (e.g. web browser request) by delivering web pages resources such as HTML documents. A web server is typically hosted in dedicated computer equipped with specialized hardware to reduce downtime and system troubles. Static vs Dynamic Web Server​ After receiving client request, the web server will search for the required file and sends it back to the client. There are two types of web server, static and dynamic. A static web server will deliver static content, these are simple web pages which doesn't need more server-side processing and will be same to all client. Dynamic web server, on the other hand, will deliver dynamic content. These content can change based on user input, and the server will generate content on-the-fly. Source : https://about.gitlab.com/blog/2016/06/03/ssg-overview-gitlab-pages-part-1-dynamic-x-static/ Web Server Feature​ The previous explanation explain about a simple web server, in reality a web server is provided with much more complex feature. Some of them are : Reverse Proxy : A technique which forwards client requests to the appropriate backend servers on behalf of the clients. This is useful for load balancing and improving security by hiding the server's IP address.Other server optimization technique : Other technique such as caching, CDN, and load balancing. Most popular example of web server are Apache and NGINX. Apache follows a process-based model where each incoming request is handled by a separate process. Nginx uses an event-driven model, where a small number of processes can handle multiple connections simultaneously.","keywords":"","version":"Next"},{"title":"SOA","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/soa","content":"","keywords":"","version":"Next"},{"title":"Advantages & Disadvantages​","type":1,"pageTitle":"SOA","url":"/cs-notes/backend-development/soa#advantages--disadvantages","content":" Advantages :  Flexibility : SOA breaks down complex systems into smaller, independent services. These services can be developed, tested, deployed, and maintained separately, allowing for easier code reuse and increased flexibility.Interoperability : Services communicate with each other using standard protocols and predefined interface. It will be easy to integrate new services without disrupting the entire architecture.Scalability : SOA allows for a distribution of services across multiple servers that can be scaled independently based on the demand, without needing to improve the entire server just to scale a certain service.Reusability : SOA architecture that consists of smaller services make it possible to utilize available services combined to create new functionalities.  Disadvantages :  Complexity : Implementing SOA introduces additional complexity due to the requirement to have proper coordination and integration among services.Performance Overhead : Services that talks to each other using message routing, serialization, and network latency can introduce performance overhead.Increased Development Effort : Implementing SOA requires additional effort in designing service contracts, defining interfaces, and maintaining service repositories. It may involve more development effort compared to traditional monolithic architectures.  Overall, SOA architecture are suitable for complex application where it needs to be decomposed into smaller service to help manage complexity and promote code reuse. When performance is critical, SOA can be optimized by increasing server and applying load balancing technique.   Source : https://www.integrate.io/blog/soa-vs-microservices/ ","version":"Next","tagName":"h3"},{"title":"WebSocket","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/websocket","content":"","keywords":"","version":"Next"},{"title":"WebSocket Process​","type":1,"pageTitle":"WebSocket","url":"/cs-notes/backend-development/websocket#websocket-process","content":" Handshake : The WebSocket process begins with a handshake between the client and the server. The client sends an HTTP request to the server with WebSocket version included to initiate a communication using WebSocket. If the server supports WebSocket, it responds with an HTTP 101 status code indicating the switch to the WebSocket protocol from HTTP protocol. Data Exchange &amp; Events : After connection is established, both the client and the server can start sending data to each other in real-time. Client or server may receive events or callbacks such as connection open, message received, error, and connection close events. Connection Termination : When the client or the server decides to close the WebSocket connection, a close message is exchanged, indicating the intention to terminate the connection. Once both sides have acknowledged the close message, the connection is closed, and the WebSocket session ends.   Source : https://www.wallarm.com/what/a-simple-explanation-of-what-a-websocket-is ","version":"Next","tagName":"h3"},{"title":"Webhook","type":0,"sectionRef":"#","url":"/cs-notes/backend-development/webhook","content":"","keywords":"","version":"Next"},{"title":"Webhook​","type":1,"pageTitle":"Webhook","url":"/cs-notes/backend-development/webhook#webhook","content":" Webhook is a mechanism for two system to communicate with each other in real-time. The system will make an automated HTTP request containing notification or data to a specific URL (called Webhook URL) whenever particular event or trigger occurs. Webhooks are event-driven, it help to address the continous request in pooling that waste resource.  Webhook Process​  Registration : The system or application that wants to receive WebHooks (called receiver) exposes an endpoint or URL to accept incoming WebHook requests. This endpoint is usually provided by the receiver as part of its WebHook registration process. WebHook Configuration : The sender or the one that will trigger the WebHook needs to configure the WebHook including the URL to specify where the WebHook requests should be sent, as well as additional required by the receiver. Monitor Event : The sender system monitors for specific events or triggers that should initiate a WebHook. These events could be actions performed by users, changes in data, or any other predefined conditions. WebHook Request : When an event or trigger occurs, the sender system constructs an HTTP request with relevant data (can be in JSON format) and sends it to the WebHook URL provided during the configuration step. Receiver Processing : The receiver system receives the WebHook request at the specified URL. It parses the request and process it based on the information received. Response (optional) : After processing the WebHook request, the receiver system may send a response back to the sender system to acknowledge the receipt and processing of the WebHook. This response can be a simple HTTP status code or a more detailed response.   Source : https://www.zoho.com/blog/id/general/apa-itu-webhooks-dan-bagaimana-cara-pakainya.html ","version":"Next","tagName":"h3"},{"title":"Cloud Models","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/cloud-models","content":"","keywords":"","version":"Next"},{"title":"Cloud Services Model​","type":1,"pageTitle":"Cloud Models","url":"/cs-notes/cloud-computing-and-distributed-systems/cloud-models#cloud-services-model","content":" Cloud services model are the different types or categories of services that are offered by cloud computing providers. These models define the level of control and responsibility that the cloud provider and the customer have over various aspects of the cloud infrastructure, platforms, and applications.  Common Models​  The standard and commonly used model of cloud service is classified into three :  Infrastructure as a Service (IaaS) : In the IaaS model, the cloud provider offers virtualized computing resources such as virtual machines, storage, and networking infrastructure. Customers have control over the operating systems, applications, and data running on the infrastructure. They can manage and configure the virtual machines and storage according to their needs while the cloud provider is responsible for maintaining, as well as providing the abstraction for the users to interact with the underlying hardware and infrastructure. Example of IaaS: Amazon Web Services (AWS), Elastic Compute Cloud (EC2), Microsoft Azure, and Google Compute Engine (GCE). Platform as a Service (PaaS) : PaaS provides a higher level of abstraction compared to IaaS. In this model, the cloud provider offers a platform that includes the underlying infrastructure as well as development tools, runtime environments, and middleware. Customers can focus on building and deploying applications without worrying about managing the underlying infrastructure. The cloud provider takes care of the hardware, operating systems, and runtime environments, while the customer is responsible for developing and deploying their applications. Example of PaaS: Google App Engine and Heroku. Software as a Service (SaaS) : SaaS is the highest level of abstraction in the cloud services model. In this model, the cloud provider offers fully managed applications that are accessed over the internet. Customers can use the software applications without having to manage the underlying infrastructure, platform, or software stack. Example of SaaS: Gmail, Google Docs, and Zoom. Source : https://en.wikipedia.org/wiki/Cloud_computing#/media/File:Cloud_computing_service_models_(1).png  More Models​  Function as a Service (FaaS) : FaaS, also known as serverless computing, is a model where developers can execute code in the cloud without having to manage the underlying infrastructure. The cloud provider takes care of the infrastructure provisioning and scaling, and developers only focus on writing and deploying functions or small pieces of code that are triggered by specific events or requests. Example of FaaS: Amazon's AWS Lambda, Google Cloud Functions, and Microsoft Azure Functions.Container as a Service (CaaS) : CaaS is a cloud service model that provides a platform for managing and orchestrating containerized applications. CaaS platforms provide runtime environments, handle containerization, deployment, orchestration, and scaling tasks. Example of CaaS: Kubernetes and Amazon Elastic Container Service (ECS).Backend as a Service (BaaS) : BaaS, also known as Mobile Backend as a Service (MBaaS), is a cloud service model that provides a backend infrastructure and services specifically tailored for mobile and web application development. BaaS platforms offer pre-built features and functionalities, such as user authentication, data storage, push notifications, and social media integration. Example of BaaS: Firebase, AWS Amplify, and Backendless.Disaster Recovery as a Service (DRaaS) : DRaaS is a cloud service model that provides backup, replication, and recovery capabilities for applications and data in the event of a disaster or system failure. Example of DRaaS: AWS Backup and Microsoft Azure Site Recovery.  ","version":"Next","tagName":"h3"},{"title":"Cloud Deployment Model​","type":1,"pageTitle":"Cloud Models","url":"/cs-notes/cloud-computing-and-distributed-systems/cloud-models#cloud-deployment-model","content":" Cloud deployment model refer to the ways in which how cloud computing resources and services are made available to users. Some common models are :  Private : Private cloud deployment involves the use of dedicated cloud infrastructure and services exclusively for a single organization. The infrastructure can be located on-premises within the organization's data center or hosted by a third-party provider. Private clouds offer more control, security, and customization options compared to public clouds, but they require significant upfront investment and ongoing maintenance. Public : Cloud resources and services are provided by a third-party cloud service provider and made available to the public over the internet. Public clouds allow for ease-of-use and cost-effectiveness option but may have limitations in terms of security, as the infrastructure and services are shared among multiple customers. Hybrid : Hybrid cloud combines the use of both public and private clouds. In a hybrid cloud, certain workloads and data are hosted in the private cloud, while others are deployed in the public cloud. Hybrid clouds may be used by organizations to achieve ease-of-use and cost-efficiency while maintaining control over sensitive data on their private cloud. Community : Community cloud deployment shares cloud services among multiple organizations or entities with similar interests or requirements. The infrastructure, resources, and services are provisioned and maintained for the exclusive use of a specific community of users, such as government agencies, educational institutions, healthcare organizations, or industry consortiums. Multicloud : Involve the use of multiple cloud vendors, which can reduce the risk of a single point of failure. When one provider experience failure, workloads can be shifted to another provider. Another benefit is to reduce dependency of cloud provider. When using specific cloud services, they might provide technologies, APIs, or services that we must follow. Switching to another provider becomes challenging and costly in this scenario. Source : https://www.interhyve.com/what-are-the-cloud-computing-deployment-models/ ","version":"Next","tagName":"h3"},{"title":"Cloud Computing & Distributed Systems","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems","content":"","keywords":"","version":"Next"},{"title":"All pages​","type":1,"pageTitle":"Cloud Computing & Distributed Systems","url":"/cs-notes/cloud-computing-and-distributed-systems#all-pages","content":" FundamentalsDistributed Systems Distributed Systems ModelDistributed Systems CommunicationDistributed File SystemDistributed Database Architecture &amp; Model Client-ServerEvent-DrivenMaster-SlavePeer-to-PeerMicroserviceMapReduceLambda Cloud-Native Technologies VirtualizationContainerizationDocker &amp; Kubernetes Cloud ModelsCloud DatabaseCloud SecurityCloud Services ","version":"Next","tagName":"h3"},{"title":"Client-Server","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/client-server","content":"","keywords":"","version":"Next"},{"title":"Communication​","type":1,"pageTitle":"Client-Server","url":"/cs-notes/cloud-computing-and-distributed-systems/client-server#communication","content":" The interaction between client and server is done in a specific protocol. They are specific rules and format that controls how the two should exchange data. Typically, we are only concerned with the application-level protocol.  The examples are :  HTTP/HTTPS : HTTP is a protocol used for communication between web browsers (clients) and web servers. HTTPS is the secure version of HTTP that adds encryption and authentication using SSL/TLS protocols.FTP : FTP is a protocol used for transferring files between a client and a server. The protocol will specify commands, which are used for file operations such as uploading, downloading, renaming, and deleting files on a remote server.SMTP : SMTP is a protocol for sending email messages between clients and servers. It provides a set of commands and responses for the transfer of email across networks.  For example, the HTTP protocol specify that the client (web browser) must send a request consisting :  Request Line : Specify the HTTP method, which specifies our desired action of requesting, target URL or resource, and HTTP version.Headers : Provide additional information about the request, such as content type, authentication credentials, or cookies.Body (optional) : If we are sending data to the server (e.g., submitting a form), they are sent along with the request.  The server that responses to the request must also follow the format rules :  Status Line : Includes the HTTP version, a three-digit status code indicating the result of the request (e.g., 200 for OK, 404 for Not Found), and a brief status message.Headers : Additional information about the response, such as content type (specify what type of data is returned), caching directives, or authentication challenges.Body (optional) : Contains the requested resource or additional data sent as part of the response.  Here are a simple HTTP request and its response :  GET /example-page HTTP/1.1 Host: www.example.com User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.1234.56 Safari/537.36 Accept: text/html,application/xhtml+xml   This request specifies that we are requesting the www.example.com webpage. The User-Agent is a header that provides information about the client application making the request (in this case, a web browser).  HTTP/1.1 200 OK Date: Fri, 19 Feb 2024 12:00:00 GMT Server: Apache/2.4.29 (Ubuntu) Content-Type: text/html; charset=utf-8 Content-Length: 1234 &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Example Page&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Welcome to the Example Page&lt;/h1&gt; &lt;p&gt;This is a sample page for demonstration purposes.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;   The response is an HTML page, which the browser will render and show it to us. ","version":"Next","tagName":"h3"},{"title":"Cloud Database","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/cloud-database","content":"","keywords":"","version":"Next"},{"title":"Deployment Models​","type":1,"pageTitle":"Cloud Database","url":"/cs-notes/cloud-computing-and-distributed-systems/cloud-database#deployment-models","content":" The two methods to run database on the cloud :  Cloud provider could provide cloud database by providing a service. The service that provide database as a service is called Database-as-a-service (DBaaS), and is a type of SaaS.Run database by using virtual machine image. The idea is, a pre-configured template that contains an operating system and any required software, including the database system is packaged into format called image. The format will be uploaded to the cloud service provider, where they will execute the image on a virtual machine.  ","version":"Next","tagName":"h3"},{"title":"Usage​","type":1,"pageTitle":"Cloud Database","url":"/cs-notes/cloud-computing-and-distributed-systems/cloud-database#usage","content":" Upon deploying the database, user can now configure the service based on their needs. This involves selecting the infrastructure, as well as the location, selecting the desired database type or storage capacity, and selecting the number of database instances.  The database type correspond to how the data are stored within the cloud database. Typically, the data are classified into two model, SQL database and NoSQL database.  To actually use the database, we will use APIs provided by the cloud providers to access it. The APIs vary between provider, and it may use protocols like SQL for relational databases or RESTful APIs for NoSQL databases.  Example​  For example, below is how we can connect our application to a MongoDB database in Kotlin.  // Source : https://www.mongodb.com/docs/drivers/kotlin-sync/ import com.mongodb.ConnectionString import com.mongodb.kotlin.client.sync.MongoClient import com.mongodb.kotlin.client.sync.MongoDatabase import com.mongodb.MongoClientSettings // Replace the placeholder with your Atlas connection string val uri = &quot;&lt;connection string&gt;&quot; val settings = MongoClientSettings.builder() .applyConnectionString(ConnectionString(uri)) .retryWrites(true) .build() // Create a new client and connect to the server val mongoClient = MongoClient.create(settings) val database = mongoClient.getDatabase(&quot;test&quot;)   And then for the actual query, we can use code like this :  val collection = database.getCollection(&quot;myCollection&quot;) val query = Document(&quot;name&quot;, &quot;John Doe&quot;) val result = collection.find(query) for (document in result) { println(document) }   Getting the collection using getCollection method passing in the collection name. A collection is similar to table, but it is not called so because MongoDB is a document database. In this case, we are trying to find specific document, that is someone with field name equal to &quot;John Doe&quot;. The find method takes a query object (in this case, a Document object) that specifies the criteria for the search. It returns a cursor, which we can iterate over to access the retrieved documents.  Furthermore, some cloud provider provide a way to inspect and manage databases hosted on their platforms. We can see what collections and documents are available, monitor traffic, setting up access for security purposes, and adding or deleting a document directly from there is also possible.   ","version":"Next","tagName":"h3"},{"title":"Cloud Security","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/cloud-security","content":"","keywords":"","version":"Next"},{"title":"Security Concerns​","type":1,"pageTitle":"Cloud Security","url":"/cs-notes/cloud-computing-and-distributed-systems/cloud-security#security-concerns","content":" Overall, the security risks in cloud computing can be categorized based on whether they affect the cloud service provider or the customer.  Some security concerns from the provider :  Infrastructure Vulnerabilities : Cloud service providers are responsible for maintaining the underlying infrastructure, including servers, networks, and storage systems. Vulnerabilities in the provider's infrastructure can expose customer data to risks such as unauthorized access, data breaches, or service disruptions. They could implement access control to prevent DDoS, implement network firewall to filter traffic, or using a VPN to establish secure and encrypted connections between customer environments and the cloud infrastructure.Insider Threats : Employees or insiders within the cloud service provider may have unauthorized access to customer data. Insider threats can include malicious activities, data theft, or unintentional actions that lead to data exposure.Redundancy : If a significant data is stored in the cloud, it is a significant concern for both the cloud provider and the customers. Cloud providers could replicate user's data by having multiple copies of customer data stored in different locations. This redundancy helps protect against data loss due to data leakage, data loss, or service disruptions due to attacks.  Some security concerns from the customer :  Securing Application : Customer are responsible for securing their applications deployed in the cloud. This includes implementing secure coding practices to prevent attacks such as SQL injection, cross-site scripting (XSS), etc.Data Encryption : Customer data should be encrypted both in transit and at rest.Authentication : Implement a strong authentication system, such as enforcing strong passwords, adding a multi-factor authentication, and using role-based access control (RBAC), in which users are given specific role that makes them eligible to access certain resources.  Cloud computing use virtualization technology heavily. It provides a good isolation even when multiple applications are being run on single host. This isolation helps prevent interference between applications, thereby reducing the potential for the malicious activities of one app to affect the system, which could, in turn, impact others.  Therefore, it's important for the technology to be implemented correctly. The one that manages virtual machines, hypervisor, must be regularly updated to protect against vulnerabilities and potential attacks.  tip Cloud security is just a subdomain of a larger umbrella, see also computer security. ","version":"Next","tagName":"h3"},{"title":"Distributed Database","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/distributed-database","content":"","keywords":"","version":"Next"},{"title":"Database Sharding​","type":1,"pageTitle":"Distributed Database","url":"/cs-notes/cloud-computing-and-distributed-systems/distributed-database#database-sharding","content":" Database sharding is a technique used to horizontally partition a database into multiple smaller and independent pieces called shards. Horizontal partitioning involves splitting a large dataset with a high number of rows into smaller shards, each consisting of a subset of the rows. These shards can be hosted on separate servers or nodes in a distributed system.  info In contrast, a vertical partition split the dataset by columns, and what normalization also does similarly.  To actually know which node stores a particular data, we can use consistent hashing technique.  Benefits of sharding :  Performance : Less number of rows means the reduction in index size, which potentially improves search performance.Scalability : We can scale the system horizontally by adding more nodes and shards.Availability &amp; Fault Tolerance : If one shard becomes unavailable or experiences a failure, the remaining shards continue to function, minimizing the impact on the overall system.   Source : https://www.digitalocean.com/community/tutorials/understanding-database-sharding  ","version":"Next","tagName":"h3"},{"title":"Distributed Query​","type":1,"pageTitle":"Distributed Database","url":"/cs-notes/cloud-computing-and-distributed-systems/distributed-database#distributed-query","content":" The distribution of database across multiple nodes enables query to be parallelized. The process of query in distributed system :  Query Decomposition : Complex query is broke down into subqueries that can be executed in parallel across multiple nodes. Each subquery operates on a subset of the data distributed across the nodes.Query Optimization : Query optimization aims to find the most efficient execution plan for a query. In a distributed setting, query optimization considers factors such as data distribution, network latency, and the cost of data transfer between nodes to determine the optimal plan. Distributed query optimizers often use techniques like cost-based optimization and statistical information about the distributed data.Data Localization : Get the data that needs to be accessed to execute a query and identifying the nodes where that data resides. Data localization will reduce latency by minimizing unnecessary data transfer across the network.Parallel Execution : Each node processes its portion of the query and returns the intermediate results to be combined at a later stage.Data Exchange &amp; Communication : During the execution of a distributed query, nodes need to exchange intermediate results and coordinate their actions.Result Aggregation : Once the subqueries are executed in parallel, the intermediate results need to be combined or aggregated to produce the final result of the query. Aggregation can involve merging or joining intermediate results, sorting, grouping, or applying other operations to obtain the desired output.   Source : https://www.javatpoint.com/query-processing-in-dbms  ","version":"Next","tagName":"h3"},{"title":"Concurrency Mechanism​","type":1,"pageTitle":"Distributed Database","url":"/cs-notes/cloud-computing-and-distributed-systems/distributed-database#concurrency-mechanism","content":" With a bunch of nodes communicating and potentially sharing data, concurrency will always be a concern. Mechanism used to manage and coordinate concurrent access to shared resources in a distributed system are required.  A data operation can be classified as read or write. A read retrieves the value of a data item from a storage location or a shared resource, without modifying. A write operation modifies the value of a data item, updating or overwriting the existing value. A concurrency problem typically occurs when there are two nodes accessing the same resource, and one of them are writing operation.  Consider a scenario of node A and node B, where node A writes and node B reads, both accessing the resource C. Assume that node A and B execute their operation at the same time. Even if their operation is started at the same time, the end result depend on the actual execution. If the initial value of C is 100, B is supposed to read 100, but if node A executed first, then it will read whatever node A was writing. This scenario is often called race condition, and may lead to unexpected results.  Locking​  Access of resource by nodes are exclusive, meaning a resource can only be accessed by a single node. A lock can be acquired by a node, meaning that particular node gain exclusive access to the resource, or released, the node makes it available for other node to acquire.  The component that holds the resource should implement the locking mechanism, keeping track of which node is currently acquiring the lock. When a node wants to access a resource, it will need to request the lock to the lock tracker. If there's another node acquiring the lock, then the access request will simply be delayed. Otherwise, that particular node will now acquire the lock and gain access to the resource.  tip See also mutual exclusion and two-phase locking.  ","version":"Next","tagName":"h3"},{"title":"Distributed Transactions​","type":1,"pageTitle":"Distributed Database","url":"/cs-notes/cloud-computing-and-distributed-systems/distributed-database#distributed-transactions","content":" Transactions done in traditional database system can also be applied to distributed database. A transaction is a logical unit of work that consists of multiple operations that need to be executed atomically, or as an indivisible unit.  Atomic Commit​  Atomic commit is the protocol or mechanism that ensures a set of operations or transactions are either all committed or all aborted, maintaining consistency across multiple nodes or databases.  If any part of the transaction fails, all changes made by the transaction are rolled back, leaving the system in its original state. On the other hand, if all parts of the transaction succeed, all changes are committed, and the system reflects the updated state.  Two-Phase Commit​  One implementation of atomic commit is the two-phase commit (2PC) protocol. This protocol consist of two phases, prepare and commit phase.  A coordinator node responsible for managing the transaction communicates with all participating nodes and asks them to prepare for the commit. Each participating node performs its portion of the transaction and prepares to commit. If any node encounters an issue during this phase or cannot prepare for the commit, it indicates a failure.Based on the responses received during the prepare phase, the coordinator makes a final decision about whether to commit or abort the transaction. If all nodes are prepared to commit, the coordinator sends a commit message to all participating nodes, instructing them to permanently apply the changes. If any node is unable to commit or if there is a failure detected, the coordinator sends an abort message, instructing all nodes to roll back the changes.   Source : https://medium.com/geekculture/distributed-transactions-two-phase-commit-c82752d69324  tip See also similar topic about concurrency control in database system. ","version":"Next","tagName":"h3"},{"title":"Distributed File System","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/distributed-file-system","content":"","keywords":"","version":"Next"},{"title":"HDFS​","type":1,"pageTitle":"Distributed File System","url":"/cs-notes/cloud-computing-and-distributed-systems/distributed-file-system#hdfs","content":" HDFS (Hadoop Distributed File System) is an example of distributed file system used in Hadoop, which is a framework for distributed storage and processing of large datasets.  tip See also MapReduce, the programming model for Hadoop that is used for data processing.  HDFS is built with master-slave architecture. Hadoop provides 5 services :  Name Node : Hadoop keeps track a master node, which acts as the central repository for the file system. It stores the file system namespace, including file and directory metadata such as file names, permissions, and block locations.Data Node : The data node is also known as the slave node, it stores the actual data as blocks. They are responsible for storing and serving data, as well as report their status to the master node.Secondary Name Node : A helper component for Name Node, provide check pointing functionality in case of Name Node fails. It's worth noting that it doesn't act as backup.Job Tracker : It is the master node responsible for managing the execution of MapReduce jobs submitted by clients, including asking the Name Node which node has the particular data.Task Tracker : The slave node for MapReduce, responsible for actually executing the tasks.  tip In contrast to distributed/clustered file system that typically uses master-slave architecture, a file system that uses client-server architecture typically called network file system (NFS). NFS provide a remote file access mechanism where clients mount remote file systems to local environment provided by the server. ","version":"Next","tagName":"h3"},{"title":"Cloud Services","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/cloud-services","content":"","keywords":"","version":"Next"},{"title":"Terminology & Metrics​","type":1,"pageTitle":"Cloud Services","url":"/cs-notes/cloud-computing-and-distributed-systems/cloud-services#terminology--metrics","content":" The quality of cloud computing services are measured by various metrics. Some common metrics are :  Availability : Refer to the ability of a system to remain operational and accessible to users. Uptime Percentage : The percentage of time that the cloud service is available without interruptions or downtime within a specified timeframe (e.g., 99.9% uptime means the system is up 99.9% of the time).Mean Time Between Failures (MTBF) : The average time between system failures.Mean Time to Repair (MTTR) : The average time it takes to repair the system.Mean Time to Recover (MTTR) : The average time it takes to restore service after a failure or outage.Mean Time to Resolve (MTTR) : The average time to fully resolve a failure. Source : https://www.atlassian.com/incident-management/kpis/common-metrics Performance : The speed, responsiveness, and efficiency of the system. Response Time : The time it takes for the system to respond to a request or action.Throughput : The rate at which the system can process or transfer data.Latency : The time delay between a request and the corresponding response.Bandwidth : The amount of data transferred to and from your cloud server over a given period.Transactions Per Second (TPS) : The number of transactions or operations the system can handle in a second. Source : https://medium.com/@sandeep15mca/latency-bandwidth-throughput-and-response-time-0ee4d9028277 Scalability : The ability of a system to handle increasing workloads and growing user demands. Vertical Scalability : The ability to increase or decrease the resources (such as CPU, memory, storage) of a single instance or virtual machine.Horizontal Scalability : The ability to add or remove instances or virtual machines to accommodate increased or decreased workload demands.Auto-Scaling Efficiency : The effectiveness and efficiency of the auto-scaling mechanisms in responding to workload changes. Utilization : The extent to which computing resources are used. CPU Utilization : Measured as the percentage of time the CPU is actively executing instructions.Memory Utilization : Can be measured by the percentage of available physical or virtual memory that is actively used by running processes or applications.Disk Utilization : Measured by monitoring the I/O operations performed on the disk, such as the rate or the number of read or write requests. Compliance : The degree of adherence of cloud providers. Regulatory Compliance : The cloud provider's adherence to industry-specific regulations and standards, such as HIPAA, GDPR, or PCI DSS.Certifications : The attainment of certifications such as ISO 27001 (information security management), SOC 2 (security and privacy controls), or FedRAMP (for government agencies) to demonstrate compliance with specific security and privacy frameworks.  ","version":"Next","tagName":"h3"},{"title":"Pricing Model​","type":1,"pageTitle":"Cloud Services","url":"/cs-notes/cloud-computing-and-distributed-systems/cloud-services#pricing-model","content":" Cloud providers offer some pricing model :  Pay-as-you-go (On-Demand) : Charges customer based on usage, typically on an hourly or per-minute basis. This model offers flexibility and cost-effectiveness as customers only pay for what they use.Reserved Instances : Customers can commit to a specific usage level for a contracted period (usually one to three years) and receive discounted pricing compared to pay-as-you-go rates. This model is suitable for predictable or steady workloads.Function Pricing : For FaaS services, the pricing can be based on the number of function invocations and the execution duration.  Configuration​  Customers can configure various settings to match their needs and expectations, and these configurations will impact the price.  Type of Services : Customer choose what services they need, such as compute, storage, networking, databases, machine learning, or serverless functions.Location : The location of where the services will be hosted, typically based on region.Tenancy : Decide how cloud services are deployed. Can be shared instances in which customer shares the same physical hardware, dedicated instances, customers are provided with their own isolated virtual machines, or dedicated hosts, customers are provided an entire physical server or host dedicated exclusively to their use.Compute Resources : Decide the resources, including virtual machine instance type, the number of instances, operating system, how many vCPU (virtual CPU), amount of memory, cache size, and storage capacity.Networking : Estimate the number of request and responses per period of time, amount of bandwidth, network performance, and scaling options for traffic spikes.  ","version":"Next","tagName":"h3"},{"title":"Examples of Cloud Services​","type":1,"pageTitle":"Cloud Services","url":"/cs-notes/cloud-computing-and-distributed-systems/cloud-services#examples-of-cloud-services","content":" The top three cloud services are Amazon Web Services (AWS), Google Cloud Platform (GCP), and Microsoft Azure. Below is a list of commonly used services on each of them.  Amazon Web Services​  Amazon EC2 : Virtual servers in the cloudAmazon S3 : Object storage serviceAmazon RDS : Managed relational database serviceAmazon VPC : Virtual private cloud for networkingAmazon SNS : Simple Notification Service for messaging and notificationsAmazon SQS : Simple Queue Service for message queuingAmazon DynamoDB : Fully managed NoSQL databaseAmazon CloudFront : Content delivery network (CDN)Amazon Route 53 : Scalable domain name system (DNS) web serviceAmazon Lambda : Serverless compute serviceAmazon Redshift : Fully managed data warehouseAmazon Elastic Beanstalk : Platform as a Service (PaaS) for deploying and managing applicationsAmazon CloudWatch : Monitoring and observability serviceAmazon Glacier : Low-cost archival storage service  Google Cloud​  Google Compute Engine : Virtual machines in the cloudGoogle App Engine : Platform as a Service (PaaS) for building and deploying applicationsGoogle Kubernetes Engine : Managed Kubernetes service for container orchestrationGoogle Cloud Storage : Object storage serviceGoogle Cloud SQL : Fully managed relational database serviceGoogle Cloud Firestore : Flexible, scalable NoSQL document databaseGoogle Cloud Functions : Serverless compute platform for event-driven applicationsGoogle Cloud DNS : Scalable domain name system (DNS) serviceGoogle Cloud CDN : Content delivery network for low-latency and high-throughput content deliveryGoogle Cloud AutoML : Automated machine learning service for building custom ML modelsGoogle Cloud Vision : Image recognition and analysis service  Microsoft Azure​  Azure Virtual Machines : Virtual machines for Windows and LinuxAzure App Service : Platform as a Service (PaaS) for building and deploying web and mobile applicationsAzure Kubernetes Service (AKS) : Managed Kubernetes service for container orchestrationAzure Storage : Scalable and secure object, file, and block storageAzure SQL Database : Fully managed relational database serviceAzure Service Bus : Cloud messaging service for connecting distributed systemsAzure Functions : Serverless compute service for event-driven applicationsAzure DNS : Scalable domain name system (DNS) serviceAzure CDN : Content delivery network for fast and secure content deliveryAzure Active Directory : Identity and access management serviceAzure Monitor : Monitoring and observability serviceAzure Log Analytics : Centralized logging and analytics for collecting and analyzing dataAzure Machine Learning : Cloud-based machine learning service for building and deploying modelsAzure Databricks : Unified analytics platform for big data and machine learning ","version":"Next","tagName":"h3"},{"title":"Containerization","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/containerization","content":"","keywords":"","version":"Next"},{"title":"Container Engine​","type":1,"pageTitle":"Containerization","url":"/cs-notes/cloud-computing-and-distributed-systems/containerization#container-engine","content":" Containers are already fully-functional and can be executed directly in isolated instances using a container engine (e.g., Docker). The container engine is responsible for managing and running containers on a system. It achieves this by leveraging virtualization techniques, where the container engine sits on top of the operating system (OS) and provides OS-level virtualization. The container engine will then interact with the OS to execute the libraries and application in the container.   Source : https://bito.ai/blog/containerization-a-beginners-guide-to-its-impact-on-software-development/  Because containerization is OS-level virtualization, it can run directly on the operating system without the need for additional virtualization. Although virtualization is commonly associated with containerization, as containers are frequently used in hosting services such as cloud services.  ","version":"Next","tagName":"h3"},{"title":"Benefits​","type":1,"pageTitle":"Containerization","url":"/cs-notes/cloud-computing-and-distributed-systems/containerization#benefits","content":" Containerization offers more advantages than dependency management.  Isolation​  Multiple containers can be run on a single computer. Containers have their own isolated runtime environment, including their own file system, process space, and network interfaces. It can be thought that we tricked each container to think they own a whole computer, whereas the actual physical resources, such as CPU, memory, storage, and network bandwidth, are shared among containers running on the same host.  Isolation also improves degree of security by making boundaries between applications. Malicious app can no longer impact other containers or the underlying host system.  Efficiency​  The cloud provider owns many physical computers (potentially a powerful ones), each running multiple virtual machines (VMs) with different configuration. Within each VM, multiple containers or actual applications can be hosted. So, the usage of host operating system's resources is maximized.  Portability​  As said before, containers are fully-functional and can be deployed anywhere. Containers encapsulate an application, its dependencies, and runtime environment into a single package. This package is highly portable and can be run consistently across different environments, such as different systems or platforms. ","version":"Next","tagName":"h3"},{"title":"Distributed Systems Communication","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/distributed-systems-communication","content":"","keywords":"","version":"Next"},{"title":"Coordination​","type":1,"pageTitle":"Distributed Systems Communication","url":"/cs-notes/cloud-computing-and-distributed-systems/distributed-systems-communication#coordination","content":" There are numerous coordination concerns in a distributed system. These include the synchronization and handling of concurrent operations, scheduling tasks as well as assigning them to nodes, and the ordering of message to ensure they are delivered in the correct order.  Synchronization &amp; Concurrency Control​  This includes :  To avoid concurrency issues, such as race condition, we can utilize locking mechanism to treat resource access as exclusive. With exclusive access, we restrict a resource to be accessed by only a single node.When a node generates a value, maybe from executing business logic or processing data, all other nodes need to be aware of it. In other words, all nodes should maintain a synchronized view of the shared data to achieve consistency. Such techniques can be employed with techniques like consensus, with one implementation being the Paxos protocol.  Task Assignment​  To make sure that nodes have a balanced workload or meet some performance constraints, we require a component capable of assigning and scheduling tasks.  Based on &quot;who&quot; will assign the task :  Centralized : A central controller or scheduler is responsible for receiving task requests and making assignment and scheduling decisions. The central controller maintains information about the state and availability of nodes and assigns tasks based on predefined policies or algorithms. It is possible to optimize task assignment because the centralized system have a system-wide knowledge, but it can also introduce a single point of failure and scalability challenges. The controller can be chosen with leader election algorithm, with one example being the Bully algorithm. Decentralized : In decentralized approach, each node makes its own decisions regarding task assignment and scheduling. Nodes can communicate with each other to exchange information about their capabilities, workload, and availability. This approach provides more fault tolerance and scalability but may require communication overhead within nodes. One algorithm for decentralized approach is the distributed hash table. Load Balancer : We can own a dedicated component that handles task distribution among nodes. Load balancing algorithm consider factors like the current workload, processing capabilities, and network conditions to make task assignment decisions. Task assignment strategies includes round-robin, which assign task in circular manner, or least-loaded, where task is assigned to node with the least workload. Additionally, to make sure message or request received and processed in the correct order, we can employ a message queue, which can included within the load balancer. The message queue holds the submitted tasks in a queue, ensuring that they are stored in the order of arrival.   Source : Colored server icon  ","version":"Next","tagName":"h3"},{"title":"Distributed Hash Tables​","type":1,"pageTitle":"Distributed Systems Communication","url":"/cs-notes/cloud-computing-and-distributed-systems/distributed-systems-communication#distributed-hash-tables","content":" A distributed hash table (DHT) is a hash table that is decentralized across a network of nodes. Just like a traditional hash table, a key, which is the unique identifier is associated with a value, the actual thing that we store.  There are two reasons why traditional hash table can't be used in distributed systems :  Duplication : If we were to copy all the content of hash table from one node to others, making the nodes maintain the same hash table, this will be a significant duplication and require additional overhead for synchronization. In a scenario where node joins or leaves the network (which is a common), this will require us to redistribute the entire data set whenever a node joins or leaves. Load Balancing &amp; Fault Tolerant : The idea of DHT is, rather than having a centralized system keeping track all the data, we will instead spread the data across nodes. When there is a data retrieval, we will redirect the request to node whom own the data. This will obviously balance the workload of nodes, and will also be beneficial in case of node failure, as we can still redirect the request without needing to duplicate the data (back to first reason). Source : https://upload.wikimedia.org/wikipedia/commons/9/98/DHT_en.svg  The general DHT process :  Node Identification : Each node in the DHT network is assigned a unique identifier, which is used to determine the node's position in the DHT's addressing space.Key Space Partitioning : The key space, which represents all possible keys in the DHT, is divided among the participating nodes. This partitioning or mapping between keys and nodes is based on algorithm like consistent hashing or rendezvous hashing.Data Storage and Lookup : When an application wants to store or retrieve a key-value pair in the DHT, it performs a lookup operation to determine the node responsible for storing that key. The lookup algorithm will depend on the key space partitioning algorithm.Routing Table : Additionally, each node maintains a routing table that helps facilitate key lookup and routing. The routing table contains information about neighboring nodes and their positions in the DHT's addressing space.Node Joins and Leaves : Whenever a node joins, leaves, or fails, reinitialization and redistribution will be performed.  Consistent Hashing​  Consistent hashing uses a ring or circle to represent the key space. Nodes are mapped to positions on the ring using a hash function applied to their identifiers. Keys are also hashed and assigned to the closest node encountered on the ring.  For retrieval, the key will be hashed, and an algorithm to find the closest node on the ring to the hash value will be used. When a node joins or leaves the system, only a fraction of the keys needs to be remapped. The initialization that occurs when a node joins is determining its position on the ring. Whenever a node leaves, the other node closest to the position will take responsibility for its associated key.   Source : https://www.toptal.com/big-data/consistent-hashing  Rendezvous Hashing​  Rendezvous hashing involves calculating a hash value for each key-node pair using a hashing function. Given a node identifier and a key, we will apply hash function, and we call the result as hash weight. Whatever nodes with the highest resulting hash weight will be assigned with the key.  For retrieval with rendezvous hashing, we would keep track all the node identifier, so we can produce all the hash weight for each node and find the node associated with the key. When a node joins, a recalculation of hash weight by each server may be done. When a node leaves, the associated keys with the particular node will be rehashed and redistributed to other nodes.  ","version":"Next","tagName":"h3"},{"title":"Message Passing​","type":1,"pageTitle":"Distributed Systems Communication","url":"/cs-notes/cloud-computing-and-distributed-systems/distributed-systems-communication#message-passing","content":" There are several mechanisms to exchange message between nodes :  Remote Procedure Calls (RPC) : Involve making a request for other node to execute particular procedure or method as if it were a local procedure call. The caller sends a message containing the procedure name and arguments to the remote process, which executes the procedure and returns the result.Message Broker : Message broker with a message queue can also be used to store and deliver messages.Publish-Subscribe (Pub/Sub) : Publish-subscribe is actually a model of message broker, in which there exists publishers and subscribers. Subscribers express interest in specific types of messages by subscribing to the corresponding topics. Publishers send messages to the topics, and the pub/sub system delivers the messages to the interested subscribers. ","version":"Next","tagName":"h3"},{"title":"Distributed Systems Model","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/distributed-systems-model","content":"","keywords":"","version":"Next"},{"title":"Terminology​","type":1,"pageTitle":"Distributed Systems Model","url":"/cs-notes/cloud-computing-and-distributed-systems/distributed-systems-model#terminology","content":" Node : Node is the individual computer or device that make up the distributed system. Each node has its own processing capabilities and can communicate with other nodes in the distributed system. Link : A link represent a communication channel or connection between nodes in a distributed system. It can be a physical link, which uses physical medium, such as Ethernet cables, fiber optic cables, or wireless communication. Or a logical link, which is an abstraction or virtual representation of a communication channel. Types of links : Point-to-Point Links : Direct communication channel between two nodes.Broadcast Links : A channel for simultaneously sending a message to all other nodes connected to the link in the system.Multicast Links : A channel for sending a message to a specific group of nodes. Multicast links operate concurrently, thus requiring a specific collision protocol, unlike broadcast links, which facilitate one-to-many communication. Resource : Resource is anything that is managed or utilized by nodes. It can be computational resources (e.g., processing units), network resources (e.g., routers, switches), or data resources, which are the data accessed and manipulated by nodes. Topology : The structure of computers connected together forms a network topology. The network topology defines the physical or logical arrangement of the nodes and the communication links between them, which can have an impact on factors such as performance, fault tolerance, scalability, and communication patterns within the distributed system. For example, a bus topology, in which nodes are connected into a common communication channel can be beneficial for data sharing system. Each node can share data in the common bus. On the other hand, a ring topology, in which nodes are arranged in a ring-like structure may be inefficient, as the data must pass through all the intermediate nodes, causing delays and potential bottlenecks. Network Partition : Network partition is a phenomenon in which nodes are disconnected, making the overall network divided into multiple subnetworks due to a loss of connectivity. In fact, this is a common issues even in modern systems with many factors causing it, such as hardware failures, network congestion, or any other configuration errors. Network partition can impact the running application, such as reducing availability, as explained in CAP theorem. Types of Failures : Apart from the general hardware, software, and network failure, other types of failures are follows. Crash Failures : A component or process in a system abruptly stops or becomes unresponsive.Omission Failures : A component fails to perform a required action or respond to a request, resulting a loss of information.Timing Failures : Situations where components or processes in a system do not meet the expected timing requirements, leading to delays and other synchronization issues.Byzantine Failures : Situations where components behave in unpredictable ways, such as providing incorrect information, giving conflicting responses, or intentionally attempting to disrupt the system's operation.  ","version":"Next","tagName":"h3"},{"title":"Theoretical Model​","type":1,"pageTitle":"Distributed Systems Model","url":"/cs-notes/cloud-computing-and-distributed-systems/distributed-systems-model#theoretical-model","content":" We call each computer or device in the distributed systems as a node, in fact we can model a distributed systems as a graph.  By modeling the distributed system as a graph, we can utilize graph theory as the foundational model for studying distributed systems. Graph-based models allow us to abstract complex systems and provide various properties and algorithms associated with graph theory.  For example, the network partition explained before can be thought as a union find data structure, also known as disjoint-set data structure. We can consider each node as an element of set, and the partitions as disjoint sets.  Consider there are three nodes in the network, 111, 222, and 333. Initially they are connected to each other, meaning they are in a common set A={1,2,3}A = \\{1, 2, 3\\}A={1,2,3}. When a partition occurs, a node will lose connection with the others, efficiently splitting the set into its own separate set. When the 333 is partitioned, the set becomes A={1,2}A = \\{1, 2\\}A={1,2} and B={3}B = \\{3\\}B={3}.  Let's say we wanted node 333 to communicate with 111. We can determine whether they can communicate using the find operation of union find on both nodes to find their respective root representatives.  Distributed Algorithms​  There are more algorithms related to distributed systems that uses theoretical model under the hood (not limited to graph theory).  Concurrency Mechanism​  See Concurrency Mechanism.  Consensus​  Consensus is a process of reaching agreement or a shared decision on a single value among a group of participants in a distributed system. A consensus algorithm must be fault-tolerant and capable of reaching a final decision even in the case of node failures.  Properties of consensus algorithms that must be satisfied :  Agreement : All nodes in the system eventually agree on the same value or decision.Validity : The agreed-upon value or decision must be a valid option within the defined constraints.Integrity : No incorrect value should be agreed upon, even in the presence of malicious nodes.Termination : The consensus algorithm eventually reaches a final decision or value.  Paxos​  One example of family of protocols for solving consensus is Paxos. The one we are discussing is the basic variant of Paxos.  Paxos achieves consistency across multiple nodes by informing them of a value and asking their agreement. The value is only accepted if a sufficient number of nodes agree on it. It can be thought as candidate proposing and voters that vote the proposal.  It consists of the two phases, divided into another two subphases :  Phase 1 - Prepare (1a), Promise (1b) : (1a): In this phase, a proposer (node) initiates the consensus process by sending a message of prepare to a majority of acceptors (nodes). The prepare request includes a proposal number, which uniquely identify the initial message and must be greater than any number used in any of the previous prepare message. (1b): Upon receiving the prepare request, each acceptor can make a promise. A promise can be made if the received proposal number is larger than the previously requested proposal. Upon promising, it will ignore subsequent proposal with lower number than the currently promised request. After that, the acceptor will return the promise message to the proposer. If an acceptor has accepted a proposal in the past, it must include the previous proposal number and the corresponding accepted value in its response to the proposer. Phase 2 - Accept (2a), Accepted (2b) : (2a): If the proposer receives promises from a majority of acceptors, it can proceed to the accept phase. The proposer sends an accept request to the same set of acceptors, including the proposal number and the value it wants to propose. If an acceptor receives an accept request and has not promised to ignore requests with lower proposal numbers (a node should promise in phase 1b), it accepts the proposal and broadcasts its acceptance to all nodes. (2b): Once the proposer receives acceptances from a majority of acceptors, the consensus is achieved, and the value is considered agreed upon.  Paxos can be designed synchronously with fixed voting time or asynchronously. Both approach tolerate potential node failures or message delays.  Leader Election​  Leader election is type of algorithm used to select a leader or coordinator among a group of nodes (also called processes) in a distributed system.  A leader election algorithm must satisfy following conditions :  Agreement : All processes know who the leader is.Uniqueness : Only one process can be elected, the rest should be non-elected.Termination : The algorithm should finish within a finite time once the leader is selected.  Depending on the network topology, size, communication mechanism, the algorithm may vary.  Bully Algorithm​  One example of leader election algorithm is the bully algorithm. The idea is, a leader is chosen initially. The leader can stay as a leader as long as it does not fail. When the leader fail, a new leader will be chosen dynamically.  The algorithm assumes that :  The system is synchronous that consist of processes that knows each other ID and address.A failure detector which detects failed process.Process may fail at any time, including during execution of algorithm.When a process fails, it should stop and returns from failing by restarting.Message delivery is reliable.  The algorithm :  Each process is assigned a unique identifier or process ID. The process with the highest ID is considered the highest-ranked process and assumes the role of the leader initially.When a process detects that the leader is unresponsive or fails, it initiates an election by sending an election message to all processes with higher IDs.Upon receiving an election message, a process with a higher ID responds with an Alive message to acknowledge the election.If a process does not receive any response after sending the election message, it assumes that it has the highest ID among the active processes and declares itself as the new leader. Otherwise, it sends no further message and wait for the next steps.The newly elected leader should broadcast a coordinator (victory) message to inform all other processes of its leadership status.If a process receives a coordinator message, it recognizes the new leader and updates its internal state accordingly.  ","version":"Next","tagName":"h3"},{"title":"Physical Model​","type":1,"pageTitle":"Distributed Systems Model","url":"/cs-notes/cloud-computing-and-distributed-systems/distributed-systems-model#physical-model","content":" Physical model is the hardware and infrastructure components that make up the distributed system. It can be physical machines, server, network equipments, storage devices, or power and cooling systems.  Based on the organization, there are two paradigms of distributed computing :  Cluster computing : Cluster computing consist of interconnected computers that work together as a single system called cluster. The nodes in a cluster are typically located in proximity to one another and are connected through a high-speed local area network (LAN). Source : Book page 28 Grid computing : Grid computing, on the other hand, focuses on the coordinated use of geographically dispersed and heterogeneous resources to solve complex problems or perform large-scale computations. It creates a virtual organization or virtual resource pool by connecting distributed resources over a network, often the internet. Source : https://www.spiceworks.com/tech/cloud/articles/what-is-grid-computing/  ","version":"Next","tagName":"h3"},{"title":"Architectural Model​","type":1,"pageTitle":"Distributed Systems Model","url":"/cs-notes/cloud-computing-and-distributed-systems/distributed-systems-model#architectural-model","content":" Architectural model describe the high-level design and structure of the system, including the arrangement and interaction of its components. Some of architectural model (see other notes) :  Client-ServerEvent-DrivenMaster-SlavePeer-to-Peer ","version":"Next","tagName":"h3"},{"title":"Docker & Kubernetes","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/docker-and-kubernetes","content":"","keywords":"","version":"Next"},{"title":"Docker​","type":1,"pageTitle":"Docker & Kubernetes","url":"/cs-notes/cloud-computing-and-distributed-systems/docker-and-kubernetes#docker","content":" Docker is a platform that provides OS-level virtualization or containerization, allowing developers to package their applications in a format that can be easily distributed and run on any system that supports Docker.  Docker uses the idea of containers, these are isolated environment of our application. A container is fully-fledged format that contains all the necessary components to run an application, including the application's code, dependencies, libraries, and runtime environment.  Docker includes a container engine, which is the software that makes possible for our application hosted on it, to interact with the underlying system that the container is run on.  tip Find more about containerization and virtualization.  How containers are made​  A Docker container is made using a Docker images, the &quot;images&quot; doesn't refer to images we see every day. An image is a set of instruction to build a container, images can be thought as a template of making container.  Dockerfile : A Dockerfile is a text file containing the dependencies and configurations required by the application. Docker containers are designed to be lightweight, containing only the necessary dependencies. More general dependencies, such as the requirement of the system being a Linux kernel, may be included in a base image, which is specified in the Dockerfile. A base image is a pre-built image that serves as the starting point for building a Docker container. It may contain a minimal operating system and other basic components needed to run an application. Build Command : Docker provide a CLI to perform various operations, such as building Docker images, running Docker containers, and managing Docker networks and volumes. Once the Dockerfile is created, we can use the Docker CLI to build the image. Image Registry : Once the image is built, it can be stored in a registry—a repository for storing and distributing Docker images. Docker Hub is an example of a registry, it as a public repository for distributing many pre-built Docker images. For instance, there may be Docker images created by other developers that can run an application with specific dependencies. Container Creation &amp; Management : A Docker container is created from a Docker image using the run command. The run command creates a container based on the image, and it will start the container in a Docker environment. Once the container is running, we can use the Docker CLI to manage the container, such as starting, stopping, and deleting it.   Source : https://itnext.io/getting-started-with-docker-facts-you-should-know-d000e5815598  ","version":"Next","tagName":"h3"},{"title":"Kubernetes​","type":1,"pageTitle":"Docker & Kubernetes","url":"/cs-notes/cloud-computing-and-distributed-systems/docker-and-kubernetes#kubernetes","content":" While Docker is used to build containerized applications, Kubernetes, on the other hand, is a container orchestration platform—or simply, a platform that helps manage and automate the deployment, scaling, and management of containerized applications.  Kubernetes becomes useful when application is containerized at scale, across multiple nodes or machines. This includes task scheduling of containers, ease of adding or removing containers, a way to monitor the health of containers, as well as restarting or replacing them. Additionally, Kubernetes provides a load balancing service, capable of distributing incoming traffic across the available replicas of the service.  Concepts​  Kubernetes organizes containers into something called pods, they are the smallest deployable unit in Kubernetes and can contain one or more containers. Containers within a pod share the same network namespace and can communicate with each other.  One or more pods grouped together and run on a machine is called a node. Node is responsible for running Pods of containers and provide resources like CPU and memory for the Pods running on it.  When there is a set of nodes that run containerized applications, these are called Kubernetes cluster. A cluster can be managed from a control plane, which is the component that is used to manage the state of the cluster and provide the API interface for managing and deploying containerized applications.  Component of Control Plane​  API Server : The API server is the front-end for the Kubernetes control plane and provides a REST API for managing and deploying containerized applications. It processes REST API requests, validates them, and updates the etcd datastore with the desired state of the cluster. etcd Datastore : The etcd datastore is a distributed key-value store that stores the desired state of the Kubernetes cluster. It serves as the single source of truth for the cluster's configuration, state, and metadata. This will help application deployed across multiple physical server to be able to synchronize their data. Controller Manager : The controller manager is responsible for managing the state of the cluster by running a set of controllers that watch the etcd datastore for changes and take actions to ensure that the actual state of the cluster matches the desired state. For example, the replication controller ensures that the desired number of replicas of a Pod are running, and the node controller monitors the health of Nodes and takes action if a Node becomes unhealthy. Scheduler : The scheduler is responsible for scheduling Pods to run on Nodes in the cluster based on factors like resource availability, node affinity, and anti-affinity. The scheduler selects the most suitable Node for each Pod and assigns it to that Node.   Source : https://www.nginx.com/resources/glossary/kubernetes/ ","version":"Next","tagName":"h3"},{"title":"Event-Driven","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/event-driven","content":"","keywords":"","version":"Next"},{"title":"Event Handling​","type":1,"pageTitle":"Event-Driven","url":"/cs-notes/cloud-computing-and-distributed-systems/event-driven#event-handling","content":" The idea of event-driven architecture is, an entity in the system produces an event, and another entity handles it if necessary.  The entity responsible for generating an event is referred to as a producer (or publisher, emitter). The event is then broadcasted to the system. The entity interested in the event is known as a consumer and should subscribe to the specific producer to receive notifications whenever an event occurs. Once the event is received, the consumer will then consume it.  Broadcast of events are sent to channel, which is a medium for publishing and subscribing to events. It serves as a central hub where events are published by event producers and then delivered to the interested event consumers. Channel can be implemented in many ways, such as event bus or message broker.   Source : https://medium.com/elixirlabs/event-bus-implementation-s-d2854a9fafd5   Source : https://www.vmware.com/topics/glossary/content/message-brokers.html  The characteristics of event-driven architecture that decouple producer and consumer make it scalable. Event producers generate events without needing to know if there is someone interested in them. Consumers do not need to periodically check for the occurrence of events; instead, they can be notified about events they are interested in simply by subscribing to the producer.  tip See also reactive programming.  ","version":"Next","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"Event-Driven","url":"/cs-notes/cloud-computing-and-distributed-systems/event-driven#example","content":" Event-driven architecture is typically used in GUI systems. A user interface has buttons which you can click to trigger some event. A theoretical GUI system that follows event-driven architecture would look like :  class ClickDetector { val subscribers = mutableListOf&lt;Button&gt;() fun addSubscriber(btn: Button) { subscribers.add(btn) } fun removeSubscriber(btn: Button) { subscribers.remove(btn) } fun didUserClick(): Boolean { return true } fun notifySubscribers() { for (subs in subscribers) { subs.click() } } }   ClickDetector class is responsible for detecting click and notifying all the subscribers. The didUserClick is a simplified implementation of click detection.  class Button { private var listener: (() -&gt; Unit)? = null fun setOnClickListener(listener: () -&gt; Unit) { this.listener = listener } fun click() { listener?.invoke() } }   The button class consist of listener, which is a lambda expression or function that is initially null. This mean initially button won't do anything if clicked. To actually set an action to do when a button is clicked, we provide the lambda and set it from the setOnClickListener method. The click method invoke the listener that we have set, which will be called from the ClickDetector.  Sample usage :  fun main() { val clickDetector = ClickDetector() val button = Button() button.setOnClickListener { println(&quot;Button clicked!&quot;) } clickDetector.addSubscriber(button) while (true) { if (clickDetector.didUserClick()) { clickDetector.notifySubscribers() } } }   The action we set whenever a button is clicked is to print &quot;Button clicked!&quot;. Again, this is a simplified implementation, the actual logic of didUserClick may involve asking mouse click from OS.  tip See also observer design pattern. ","version":"Next","tagName":"h3"},{"title":"Fundamentals","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/fundamentals","content":"","keywords":"","version":"Next"},{"title":"Distributed System​","type":1,"pageTitle":"Fundamentals","url":"/cs-notes/cloud-computing-and-distributed-systems/fundamentals#distributed-system","content":" Distributed System is a system that consist of a collection of interconnected computers (often called nodes) that coordinate and communicate to achieve a common goal. In a traditional centralized system, a single computer handles all tasks. On the other hand, distributed systems distribute the workload across multiple computers.   Source : https://aosabook.org/en/v2/distsys.html  tip When discussing servers, distributed systems, or cloud computing, we will refer to the entity making a request as a client, and the one capable of processing it is called a server.  Benefits​  Benefits of distributed systems :  Performance : A task or problem can be divided into many subtasks, which is assigned to each node in the distributed system. This is useful when the problem is large and can be decomposed into smaller parts. This approach is called task partitioning or task parallelism, in which one task is distributed to multiple nodes, making the overall execution faster. Scalability : Another way of distributing load is assigning one task only to one node, this is called task distribution. There are various ways to assign task to each node, but the point is to distribute the workload evenly across the nodes. It's useful when dealing with large number of tasks, and each task do not require coordination or communication with other tasks (e.g., a web server serving webpages). This allows the system to be scaled horizontally, meaning we can increase the number of nodes in the system to scale the overall performance. Fault Tolerance : In a centralized system, a typical scenario involves a client sending a request to the server, and the server responds to it. However, if the system experiences a failure, it won't be able to respond to clients anymore. All clients relying on that centralized server would be affected. Distributed system has the ability to continue functioning properly even in the presence of faults, failures, or disruptions. If one node experiences failure, the system can transfer the load to another node. Furthermore, we can also replicate data across nodes so that if data is lost, we have a backup on other nodes.  Distributed vs Parallel System​  A distributed system differs with parallel system. A parallel system computes a task by dividing it into smaller subtasks that can be executed simultaneously on multiple processing units (in essence, a node is processing unit). These processing units typically share memory and communicate with each other through shared memory.  On the other hand, a distributed system operates independently (i.e., they have separate memory), and communicate through dedicated message passing mechanism.   (a) and (b) are distributed system, while (c) is a parallel system Source : https://en.wikipedia.org/wiki/Distributed_computing#/media/File:Distributed-parallel.svg  ","version":"Next","tagName":"h3"},{"title":"Cloud Computing​","type":1,"pageTitle":"Fundamentals","url":"/cs-notes/cloud-computing-and-distributed-systems/fundamentals#cloud-computing","content":" The widespread adoption of the internet and the increasing workload on servers created challenges for businesses in terms of managing and scaling their computing infrastructure.  Cloud Computing can be thought as a service, in which businesses can rent and utilize computing resources, such as virtual machines, storage, and networking, from dedicated providers known as cloud service providers. These providers specialize in managing and maintaining the underlying infrastructure, including hardware, networking, and data centers.  Their computers are dedicated to handle large workload, high computational tasks, and store huge amount of data. These computers are stored in a place called data centers and distributed across multiple locations. Cloud computing often relies on distributed system principle, it can be considered as an application of distributed system.  Components of cloud computing :  Infrastructure : Infrastructure refers to the underlying hardware and software components. It includes computing powers, physical servers, storage devices, networking equipment, etc. Platform : Platform is the set of tools, services, and frameworks that developers use to build, deploy, and manage applications. It can be an operating system, development tools, runtime environments, and libraries. Different infrastructure or platform means different way of interacting with the system, the cloud platform will abstract away this issue and provides developers with a ready-to-use environment. Application : Application represents the actual software applications or services that run on the cloud infrastructure and platform layers. These applications can be custom-developed by developers that buy the service, or provided as pre-built software packages or services by the vendors. Source : https://en.wikipedia.org/wiki/Cloud_computing#/media/File:Cloud_computing.svg  Benefits​  Benefits of cloud computing :  Cost Efficiency : Some cloud services offer a payment method based on pay-as-you-go basis. This mean that users only pay for the resources they actually use, rather than making upfront investments in hardware and infrastructure.Flexibility &amp; Agility : Developers can deploy their application on a server easily with cloud services. They don't need to set up or configure physical servers, as these are the concern of the cloud providers. Furthermore, developers can dynamically change and customize their infrastructure to suit the application needs.Scalability : Developers can easily scale up or down the resources allocated to their applications based on demand. For example, in weekend, the number of user visiting your website increase. Then, we can add more storage capacity or increase network bandwidth. Cloud providers also offer auto-scaling features that automatically adjust resources based on predefined rules or metrics.Security : Developers and users don't need to invest or be concerned in security. Cloud providers often have dedicated security teams and compliance certifications, ensuring that applications hosted in the cloud adhere to industry security standards. ","version":"Next","tagName":"h3"},{"title":"Lambda","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/lambda","content":"","keywords":"","version":"Next"},{"title":"Batch Processing​","type":1,"pageTitle":"Lambda","url":"/cs-notes/cloud-computing-and-distributed-systems/lambda#batch-processing","content":" Batch processing is a method to run software program, capable of running tasks that we call jobs in batches automatically by scheduling them at certain times. The method is typically used to process large amounts of data in a single operation. By processing, this may involve backup, filtering, sorting, or aggregating.  For example, a retail company wants to analyze its sales data to gain insights and make data-driven business decisions. The company can schedule batch processing which may involve collecting sales data, calculating total sales, identifying popular products, and generating sales reports. This process can be scheduled on daily basis, specifically at night, when the system experiences lower usage or during off-peak hours, to minimize user impact.  A batch processing job is defined by specific language, framework, or tools. The details to be specified :  Data sources, location of data, operations to be performed, and other transformations.Dependencies or the order in which these steps need to be executed.Size of batch, how many data or work units that needs to be processed or done in one batch.Time and schedule of which the job executes.We can monitor the progress and performance of job, and set error or retry mechanism in case the execution fails.   Source : https://estuary.dev/batch-data-processing/  ","version":"Next","tagName":"h3"},{"title":"Stream Processing​","type":1,"pageTitle":"Lambda","url":"/cs-notes/cloud-computing-and-distributed-systems/lambda#stream-processing","content":" Stream processing (or real-time processing) is a method of data processing that involves continuously processing and analyzing streams, which is data or events produced or generated in real-time.  A typical process of stream processing :  Data Ingestion : Data is produced from various data sources, such as sensors, log files, message queues, or external systems. Data is imagined as water that flows through a system continuous and potentially infinite. They are treated as a continuous stream of events or records. Windowing : Data can be processed in window, a way to group similar or related data within specific time intervals or fixed-size data subsets. In time-based windowing, data may be divided into fixed-duration windows, such as 5-minute, 1-hour, or daily windows. On the other hand, fixed-size window may contain fixed number of events or data points, such as every 100 records or every 1,000 records. Data Processing : The data is applied to series of operations or functions. Operations are typically pipelined or broke down into smaller, interconnected stages or steps. Stateful Processing : Some stream processing may be stateful, meaning the data are related to each other, therefore we require a component that keeps track state or context of the processing. This can involve maintaining counters, aggregating statistics, or correlating events based on common attributes (e.g., detecting if specific sensor events is a valid gesture). Output : After processing, stream processing systems generate outputs based on the defined operations and computations. This can include generating alerts, triggering actions, storing results in databases, sending data to external systems, or visualizing real-time analytics.   Source : https://www.ververica.com/what-is-stream-processing (cropped and upscaled)  ","version":"Next","tagName":"h3"},{"title":"Lambda Architecture​","type":1,"pageTitle":"Lambda","url":"/cs-notes/cloud-computing-and-distributed-systems/lambda#lambda-architecture","content":" By combining batch and stream processing, lambda architecture enables the processing of all the available data within specific period, as well as the real-time produced data in one go.  Lambda works by dividing the batch and stream processing stage into three layers :  Batch Layer : Handles the batch data that contains large volumes of historical data. Batch layer involve the use of distributed processing system capable of handling large quantities of data. It performs batch processing on the entire dataset, generating comprehensive and accurate views or summaries of the data. The technology used for this may be Apache Hadoop.Speed Layer : Speed layer deals with real-time data processing. It ingests and processes the data streams in real-time, allowing for immediate analysis and low latency to incoming data. The speed layer frameworks or technologies includes Apache Kafka or Apache Spark.Serving Layer : Serving layer provides a unified view of the processed data from both the batch layer and the speed layer. The serving layer may use technologies like Apache HBase or Apache Cassandra.  The combination of batch processing which is slower due to the need to wait for data accumulation, but provides accurate data insights. Complements stream processing, which is faster as it allows for real-time data processing and insights, although it may not be accurate.   Source : https://medium.com/@bryzgaloff/how-to-implement-lambda-architecture-using-clickhouse-9109e78c718b ","version":"Next","tagName":"h3"},{"title":"Microservice","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/microservice","content":"Microservice See Backend Development &gt; Microservice","keywords":"","version":"Next"},{"title":"Master-Slave","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/master-slave","content":"","keywords":"","version":"Next"},{"title":"Example​","type":1,"pageTitle":"Master-Slave","url":"/cs-notes/cloud-computing-and-distributed-systems/master-slave#example","content":" One common example of the master-slave architecture is database replication. Database stores a bunch of data, having a copy of them would minimize data loss. The concept of database replication is storing all or some subset of data in different device.  Any database operation that involves writing new data will only be applied to a single master database. One or more slave databases contains the replicate of the data. They are only responsible for handling read operation. The slave database will synchronize their data with the master over time.  Not only increasing fault tolerance, this also increase read performance, as we have multiple device that is capable of serving read operations.   ","version":"Next","tagName":"h3"},{"title":"Peer-to-Peer","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/peer-to-peer","content":"","keywords":"","version":"Next"},{"title":"MapReduce","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/mapreduce","content":"","keywords":"","version":"Next"},{"title":"Map & Reduce​","type":1,"pageTitle":"MapReduce","url":"/cs-notes/cloud-computing-and-distributed-systems/mapreduce#map--reduce","content":" MapReduce is inspired from map and reduce from functional programming  Map : A map operation applies a given function to each element of a collection and returns a new collection containing the transformed elements. A simple usage of map in Kotlin : fun main() { val numbers = listOf(1, 2, 3, 4, 5) val squaredNumbers = numbers.map { num -&gt; num * num } println(squaredNumbers.joinToString()) // Output: 1, 4, 9, 16, 25 } The given function is a lambda, which is defined inside the braces. It takes an input that we call num, and it will square that number to produce a result. This is done for each element in the collection. Reduce or Fold : A reduce combines the elements of a collection into a single value by repeatedly applying a binary function to pairs of elements. As a result, it reduces the collection to a single value. Another usage in Kotlin : fun main() { val numbers = listOf(1, 2, 3, 4, 5) val accumulation = numbers.reduce { accumulatedSoFar, nextNum -&gt; accumulatedSoFar + nextNum } println(accumulation) // Output: 15 } The reduce function takes two parameters, the first being value accumulated so far (or initially), and the second being the value we are currently processing. At first, accumulatedSoFar will be 0 and nextNum is 1, next accumulatedSoFar will be 1 (because 0 + 1 = 1), and nextNum is 2. This is done until we reach the last element.  ","version":"Next","tagName":"h3"},{"title":"MapReduce Operations​","type":1,"pageTitle":"MapReduce","url":"/cs-notes/cloud-computing-and-distributed-systems/mapreduce#mapreduce-operations","content":" MapReduce system typically consist of three operations or steps :  Map : Each node in the distributed system owns a subset of data locally, which are possibly messy. Each data will be associated with a key-value pair, with the key being the unique identifier of the data, and the value is the actual data. A map function is applied to the data independently based on the keys and in parallel, producing an intermediate key-value pairs as output. Shuffle : The intermediate key-value pairs are partitioned based on their keys and distributed across the cluster. This will make the data with a particular key all contained within the same node. Reduce : The reduce function is applied to each unique key and its associated set of intermediate values. The reduce function aggregates, combines, or analyzes the intermediate values for each key and produces final output key-value pairs. The final output key-value pairs are collected and stored in the desired output location (or combined). Source : https://datascientest.com/en/mapreduce-how-to-use-it-for-big-data  ","version":"Next","tagName":"h3"},{"title":"Benefits​","type":1,"pageTitle":"MapReduce","url":"/cs-notes/cloud-computing-and-distributed-systems/mapreduce#benefits","content":" Performance : As emphasized before, MapReduce operation is done in parallel, dedicated to handle large data datasets and complex computations.Scalability : Allows for horizontal scale, increasing more machines to enhance the parallel processing.Fault Tolerance : Classic benefits of distributed system, when a node fails during processing, we can redistribute the work to other available nodes, ensuring fault tolerance and continuous execution. ","version":"Next","tagName":"h3"},{"title":"Compilers","type":0,"sectionRef":"#","url":"/cs-notes/compilers","content":"","keywords":"","version":"Next"},{"title":"Temporary​","type":1,"pageTitle":"Compilers","url":"/cs-notes/compilers#temporary","content":" Source code typically need an entry point, which acts as the starting point of execution for the program. These entry points are typically called main function.  Software is made using human-readable programming languages. The source code needs to be converted into machine-code that can be executed by computers, this converting process is called compilation.  A compiled source code can either be an executable program that can be executed directly, or an object file. Relocations  ","version":"Next","tagName":"h3"},{"title":"All pages​","type":1,"pageTitle":"Compilers","url":"/cs-notes/compilers#all-pages","content":" Compiliation Process preprocessing,lexical analysis,parsing,semantic analysis (syntax-directed translation),conversion of input programs to an intermediate representationcode optimizationmachine specific code generation. Compiler OptimizationDecompilationBuilding Process ","version":"Next","tagName":"h3"},{"title":"Virtualization","type":0,"sectionRef":"#","url":"/cs-notes/cloud-computing-and-distributed-systems/virtualization","content":"","keywords":"","version":"Next"},{"title":"Virtualization vs Emulation​","type":1,"pageTitle":"Virtualization","url":"/cs-notes/cloud-computing-and-distributed-systems/virtualization#virtualization-vs-emulation","content":" Emulation is the technique that involve creating a software-based replica of a hardware platform or system. By replica, it is the complete imitation of the hardware architecture, including its CPU, memory, etc. On the other hand, virtualization is just creating an environment that makes it possible for guest platform to operate.  Emulation is used when the guest platform cannot run or is incompatible with the machine hardware. This incompatibility can arise from differences in CPU architecture. For example, a typical desktop with x86 architecture will not be able to run an application designed for ARM architecture, commonly found in mobile platforms or embedded devices. Conversely, an operating system like Linux, with its distribution, can run on x86 architecture. Therefore, a Windows desktop can virtualize Linux without the need for emulation.  It is said to be a virtualization if at least one functionality of the guest can be run natively on the host machine. By running natively, this mean the guest can leverages the underlying hardware capabilities of the host machine. This is not possible in emulation, because the architecture itself is different, so it wouldn't be able to rely on the host. This is one of the reason that makes virtualization faster than an emulation.  ","version":"Next","tagName":"h3"},{"title":"Hypervisor​","type":1,"pageTitle":"Virtualization","url":"/cs-notes/cloud-computing-and-distributed-systems/virtualization#hypervisor","content":" Hypervisor, also known as virtual machine manager (VMM), is the software that made virtualization possible that creates and manages virtual machine. In virtualization, it is possible for a single machine to run three different OSes, and each of these OSes can run three different applications that cannot be executed on the other OS except for themselves.  The primary use of hypervisor is to abstract and virtualize the physical hardware, presenting it to the virtual machines as if they were running on dedicated hardware. It provides a layer of software that sits between the physical hardware and the virtual machines, allowing the virtual machines to operate independently and securely.  There are two types of hypervisor :  Type 1 Hypervisor (Bare-Metal Hypervisor) : In this type, the hypervisor runs directly on the host computer's hardware, without the need for an underlying operating system. Examples include VMware ESXi, Microsoft Hyper-V, and Citrix XenServer.Type 2 Hypervisor (Hosted Hypervisor) : This hypervisor runs on top of an existing operating system. Examples include VMware Workstation, Oracle VirtualBox, and Microsoft Virtual PC.   Source : https://superuser.com/questions/1553794/are-hardware-drivers-needed-to-be-installed-on-the-management-os-of-a-type-1-hyp  ","version":"Next","tagName":"h3"},{"title":"Benefits of Virtualization​","type":1,"pageTitle":"Virtualization","url":"/cs-notes/cloud-computing-and-distributed-systems/virtualization#benefits-of-virtualization","content":" Virtualization has several benefits for individuals or data centers.  Improved hardware utilization : Rather than having 3 machines to run 3 distinct operating system, virtualization allows us to utilize a single machine to run it all. This benefit make it possible to scale up the architecture. This mean we can boost the hardware capabilities of a single computer rather than having multiple computer with weaker hardware.Testing in Development &amp; Isolation : Virtualization creates isolated test environments, allowing developers to test applications and configurations on different system without affecting the production environment. This can be useful for data center, where the computer is used to run multiple program.Templating : Related to development, templating is the process of creating a reusable template or image of a virtual machine, which is a file or collection of files that contains a complete representation of a virtual machine's operating system, software, configurations, and data. Having a reusable copy of a virtual machine allows us to quickly deploy multiple instances with the same configuration and software setup, which can be useful for computers in data center.Snapshot : While running a virtual machine, it is possible to capture the current state of a virtual machine (VM) at a specific point in time. By state, this includes the VM's disk and memory state. Taking a snapshot effectively allowing us to revert to that snapshot later if needed.  ","version":"Next","tagName":"h3"},{"title":"Virtualization Level​","type":1,"pageTitle":"Virtualization","url":"/cs-notes/cloud-computing-and-distributed-systems/virtualization#virtualization-level","content":" Virtualization level is the level of how depth is virtualization implemented in an environment.  Instruction Set Architecture Level (ISA) : This is the lowest level implementation that emulates the low-level operations that a processor can perform, such as arithmetic, logic, and memory operations. Hardware Abstraction Level (HAL) : This level implement the abstraction of hardware components to higher-level interface for software. It implements the device drivers and APIs that allow software to interact with hardware. Operating System Level : This level implement the operating system (OS). The OS manages system resources, provides services to applications, and facilitates communication between software and hardware. It includes components such as the kernel, file system, memory management, process management, and device drivers. Library or API Level : This level implements software libraries and frameworks that provide pre-built functions and modules to simplify application development. Application Level : Also known as process-level virtualization, this level focuses on virtualizing specific applications or software frameworks. It allows applications to run in isolated environments, separate from the underlying operating system and other applications. Source : https://www.brainkart.com/article/Levels-of-Virtualization-Implementation_11329/  Level Implementation​  Virtualization can be implemented in four level :  Full Virtualization : In full virtualization, the virtualization layer (hypervisor) provides complete hardware abstraction, it can run multiple virtual machines to run simultaneously on a single physical host. Each virtual machine operates as if it has its own dedicated hardware resources, including CPU, memory, storage, and network interfaces. Examples of full virtualization hypervisors are VMware ESXi and Microsoft Hyper-V. Para-virtualization : In full virtualization, the OS is not aware that it is running in a virtualized environment, para-virtualization instead modifies the guest operating system to be aware of the virtualization layer. The operating system and the hypervisor (or the virtualization layer) can communicate directly to perform tasks such as memory management and I/O operations. Hardware-assisted Virtualization : Hardware-assisted virtualization enables full virtualization with the help of hardware capabilities such as Intel VT-x or AMD-V. For example, the hardware may support for nested page table (NPT) memory management technique. The NPT is a technique that translates guest's virtual address to host physical address. When a virtual machine makes a memory access, it first consults the guest page tables to translate the virtual address to a guest physical address. Then, it looks up the guest physical address in the host page tables to obtain the corresponding host physical address. info More about memory management and virtual memory. Operating System-level Virtualization/Containerization : Operating system-level virtualization, also known as containerization, is a lightweight form of virtualization where the virtualization layer runs on a single host operating system. Instead of virtualizing an operating system, it creates multiple virtual machines (VMs) isolated in the OS. Each of the component is called container, and they share the host operating system's kernel, libraries, and other resources.  ","version":"Next","tagName":"h3"},{"title":"Virtualization Implementation​","type":1,"pageTitle":"Virtualization","url":"/cs-notes/cloud-computing-and-distributed-systems/virtualization#virtualization-implementation","content":" Virtualization can be implemented using three technique explained below. Most of the technique uses the concept of virtual CPU (vCPU).  vCPU is a virtualized representation of a physical CPU within a virtual machine. A physical CPU is supposed to execute instruction from program. When a virtual machine is assigned CPU resources, it doesn't mean it is given a portion of CPU, like a core.  Instead, the virtualization software manages the distribution of CPU resources among the VMs running on the host machine by creating a virtual CPU. The virtual CPU will be executed by the actual CPU, it will use scheduling algorithms to allocate CPU time to each VM.  Trap-and-Emulate​  Computer system run in two modes, user and kernel mode. The user mode is run by user, it has restricted access to the system. The kernel mode is run by system, in which it has full control over system resources and can perform privileged operations.  Implementing virtualization will involve implementing the user and kernel mode for the guest machine. The trap-and-emulate technique is used in virtualization to handle privileged instructions executed by a guest operating system.  When a guest operating system attempts to execute a privileged instruction, it triggers a trap. The trap transfers control to the hypervisor, which emulates the behavior of the privileged instruction, performs the necessary operations, and then returns control to the guest operating system. This allows the guest operating system to run unmodified while still providing isolation and control to the hypervisor.   Source : https://slideplayer.com/slide/15680322/ (cropped)  Binary Translation​  Binary translation is a technique to execute code compiled for one architecture on a different architecture. It is typically used in cases where virtualization is not feasible due to architectural differences. It is when the kernel mode operation on the guest machine is not possible to be executed on the hypervisor.  This technique involves dynamically translating the machine code of the guest operating system or application from the source architecture to the target architecture at runtime. The translated code is then executed on the target architecture, which can be directly within the host machine.   Source : https://www.oreilly.com/library/view/enterprise-cloud-security/9781788299558/e301e0ae-2518-4b8f-82c3-b073b4ee8732.xhtml  info An example of a system that uses binary translation is QEMU, which is an emulator that emulates a computer's processor through dynamic binary translation. ","version":"Next","tagName":"h3"},{"title":"Architecture​","type":1,"pageTitle":"Peer-to-Peer","url":"/cs-notes/cloud-computing-and-distributed-systems/peer-to-peer#architecture","content":" The specific architecture of P2P define how peers are connected to each other. The network topology can be structured, unstructured, or hybrid.  Structured : Peers are organized in a specific structured topology, using a distributed hash table (DHT). Unique identifier is associated to each peer and resources or data onto the identifier space. So, when a peer are looking for particular resource, they will connect to specific peer that stores it. Source : https://en.wikipedia.org/wiki/Peer-to-peer#/media/File:Structured_(DHT)_peer-to-peer_network_diagram.png Unstructured : Peers connect to each other in a random or ad-hoc manner, without any specific organization or topology. Peers typically maintain a list of other peers they are aware of, forming a network of connections. This is suitable in the case of all peer own the same resources. If not, the search for resources spread through the network can be expensive, as it requires to find the resource in neighboring peers or by using random walks. Source : https://en.wikipedia.org/wiki/Peer-to-peer#/media/File:Unstructured_peer-to-peer_network_diagram.png Hybrid : Hybrid P2P networks combine elements of both structured and unstructured topologies to leverage their respective advantages.  ","version":"Next","tagName":"h3"},{"title":"Usage​","type":1,"pageTitle":"Peer-to-Peer","url":"/cs-notes/cloud-computing-and-distributed-systems/peer-to-peer#usage","content":" P2P architectures are used in various applications :  File Sharing : Platforms such as BitTorrent, eMule, and Gnutella utilize P2P architectures to enable users to share files directly with each other, without relying on centralized servers.Content Delivery : P2P architectures are also used in content delivery networks (CDNs) to distribute and deliver content efficiently. P2P CDNs are particularly useful for distributing large media files, software updates, and streaming video content.Communication : Real-Time Communication (RTC) protocol, such as WebRTC uses P2P architecture under the hood. However, they are not a complete P2P connection, as it relies on servers during the initial connection setup phase.Blockchain : Blockchain networks, such as Bitcoin and Ethereum, are built upon P2P architectures. In these decentralized systems, peers (or nodes) maintain a distributed ledger, validate transactions, and reach consensus without relying on a central authority. ","version":"Next","tagName":"h3"},{"title":"Computer & Programming Fundamentals","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals","content":"","keywords":"","version":"Next"},{"title":"All pages​","type":1,"pageTitle":"Computer & Programming Fundamentals","url":"/cs-notes/computer-and-programming-fundamentals#all-pages","content":" Computer Representation Number SystemBinary RepresentationBitwise OperationFloating NumberData Representation Computer Fundamentals Operating SystemMemory Programming Fundamentals Programming ConceptsData Structures &amp; AlgorithmsProgramming Paradigm Imperative Imperative &amp; Procedural ProgrammingObject-Oriented Programming Declarative Declarative &amp; Functional ProgrammingQuery Language Concurrency &amp; Parallelism Code Execution CompilationInterpreterRuntime Environment Computer &amp; Programming Terminology ","version":"Next","tagName":"h3"},{"title":"Binary Representation","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/binary-representation","content":"","keywords":"","version":"Next"},{"title":"Binary Operation​","type":1,"pageTitle":"Binary Representation","url":"/cs-notes/computer-and-programming-fundamentals/binary-representation#binary-operation","content":" Similar to decimal number system we uses in everyday life, binary numbers can also be subject to basic arithmetic operations such as addition, subtraction, multiplication, and division. The overall idea is the same as decimal number operation, however, it differs in the representation of numbers and the rules for carrying out the operations.  Addition​  There are four possible combination of addition in binary :  0 + 0 = 00 + 1 = 11 + 0 = 11 + 1 = 0 (with a carry of 1)  Carry refers to the situation where the sum of two digit in a column exceeds the base (which is 2 in binary). When a carry occurs, it is carried over to the next column to the left. In decimal addition, when the sum of two digits in a column is greater than 9, we carry over the tens place value to the left column. (e.g. 5 + 6 = 11, we carry 10 to left column, resulting the digit of 1 in the current column).  Here is an example of binary addition :   Source : https://www.tes.com/teaching-resource/r354-ks3-computer-science-theory-sow-12775550 (edited)  Subtraction​  There are four possible combination in binary subtraction :  0 - 0 = 0: When subtracting 0 from 0, the result is 0.1 - 0 = 1: When subtracting 0 from 1, the result is 1.1 - 1 = 0: When subtracting 1 from 1, the result is 0.0 - 1 = 1 with a borrow: When subtracting 1 from 0, borrowing occurs from a higher place value. This is similar to decimal subtraction, where you borrow when subtracting a larger number from a smaller number. In binary, when subtracting 1 from 0, you borrow 1 from the next higher bit, resulting in 1 in the current place value and a borrow to the left. If the higher bit is also 0, we will keep borrowing until a nonzero bit is encountered or until there are no more higher bits (resulting in negative number in decimal).   Source : https://joeherbert.dev/revision/computing (edited)  Multiplication &amp; Division​   Source : https://medium.com/@karlrombauts/building-an-8-bit-computer-in-logisim-part-2-arithmetic-ae7861c82e79 (edited)   Source : https://medium.com/@karlrombauts/building-an-8-bit-computer-in-logisim-part-2-arithmetic-ae7861c82e79 (edited)  ","version":"Next","tagName":"h3"},{"title":"Binary Representation​","type":1,"pageTitle":"Binary Representation","url":"/cs-notes/computer-and-programming-fundamentals/binary-representation#binary-representation","content":" Binary representation have some properties and utilization to represent concept like negative numbers.  Least &amp; Most Significant Bit​  They are the terms used in binary representation to refer to the position or significance of bits within a binary number.  The least significant bit (LSB) is the rightmost bit in a binary number, and it holds the least amount of significance, which is the 202^020.  The most significant bit (MSB), on the other hand, is the leftmost bit in a binary number, and it holds the most significant value. Its value corresponds to the highest power of 2 within the number. For example, in a byte or 8-bit binary numbers, it holds the 272^727 value.   Source : https://www.researchgate.net/figure/Least-significant-bit_fig2_351128700  Signed Magnitude​  In binary numbers, we don't use negative sign to represent negative number, we instead uses another binary digit to determine whether its a positive or negative number, the extra digit is called the sign bit, and it doesn't contribute to overall value. The other binary digit that contributes to overall value is called the magnitude bits.  Signed magnitude is a way to represent negative or positive number, where the most significant bit (MSB) is used as the sign bit. The MSB of 1 represent the binary number is negative, while 0 represent positive number. In some cases, the usage of MSB as the sign bit makes us need an extra binary digit to represent a larger number.  The representation using signed magnitude is very simple, however, it introduces complexity when performing arithmetic operations, as special rules are required to handle the sign bit separately from the magnitude bits.   Source : https://www.geeksforgeeks.org/difference-between-signed-magnitude-and-2s-complement/  One Complement​  One complement is another signed number representation or the method of representing both positive and negative numbers, it uses a specific method to represent negative number.  In one complement, we still use sign bit (same as signed magnitude) to indicate the sign of the number (positive or negative), and the magnitude bits are obtained by inverting (flipping) all the bits of the positive number.  For example, the number +5 is represented as 0101. To obtain the one's complement representation of 5, we invert each bit to get 1010, which represent -5. Similarly, the number -5 in one's complement representation 1010 can be converted back to its positive form by inverting each bit, resulting in 0101.   Source : https://www.meracalculator.com/math/ones-complement.php  Two Complement​  The two complement is another signed number representation. To obtain the two complement of a binary number, the method is very similar to one complement, we first obtain the one complement (flip the bit), and then we add extra 1 to the resulting binary number.  For example, the number 5 in binary form, which is 00000101. By inverting the bits, we obtain 11111010, and when we add 1 to this result, we arrive at the two's complement representation, which is 11111011. This representation signifies the value of -5.   Source : https://www.codingninjas.com/studio/library/2-s-complement  To actually determine the decimal value of a binary number in two complement, we will need to add each positional value along with the power of base 2 and include the sign bit by negating the value of the power of base 2.   Source : https://youtu.be/sJXTo3EZoxM?si=j67uhtI6_sfZauME&amp;t=457  7 is represented as 0111 in binary, the two complement is 1001. To calculate 1001, we will do (1×201 \\times 2^01×20) + (0×210 \\times 2^10×21) + (0×220 \\times 2^20×22) + (1×−(23)1 \\times -(2^3)1×−(23)), which is equal to 1 + 0 + 0 + (-8) = -7. ","version":"Next","tagName":"h3"},{"title":"Bitwise Operation","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/bitwise-operation","content":"","keywords":"","version":"Next"},{"title":"Bit Shifting​","type":1,"pageTitle":"Bitwise Operation","url":"/cs-notes/computer-and-programming-fundamentals/bitwise-operation#bit-shifting","content":" In binary representation, each binary digits (0 or 1) represent a positional value, starting from 202^020 from the rightmost, the power will be increased by 1 as we go to the left.  Bit shifting is the process of moving a binary digit either to left (shift left) or to the right (shift right).  Shift Left​  The shift left operator (often denoted as &lt;&lt;), shift a binary digit to left. While shifting a digit to the left, we will discard the leftmost value and add a 0 to the rightmost value. If we calculate our binary representation again after shifting it to the left once, our binary representation will be multiplied by 2. This occurs because each binary digit holds a positional value, and shifting them to the left increases their power by 1.   Source : https://youtu.be/BKzB6gdRyIM?si=QxyxFt9htR3Q2OQT  Shift Right​  The shift right operator (often denoted as &gt;&gt;), shift a binary digit to the right. It is the opposite of shift left, it will discard the rightmost value and change the leftmost value to a 0. Also, instead of being multiplied by 2, it will be divided by 2, because the power of two decrease as we go to the right.   Source : https://youtu.be/BKzB6gdRyIM?si=XtRX1QwknUl8yhxM&amp;t=205  Bit shifting can be beneficial for multiplying or dividing number by 2, we can turn the decimal number into binary representation and then do the bit shifting operation. This can be more efficient especially on systems that can perform bitwise operations quickly.  ","version":"Next","tagName":"h3"},{"title":"Logical Operation​","type":1,"pageTitle":"Bitwise Operation","url":"/cs-notes/computer-and-programming-fundamentals/bitwise-operation#logical-operation","content":" Logical operations on binary digits are logical operations performed at bit level of binary values. In binary representation, a binary digit of 1 represent a &quot;true&quot; value and a binary digit of 0 represent a &quot;false&quot; value. By logical operations, it means we are evaluating the logical relationship between boolean values and produce another boolean result.  NOT​  A NOT (often denoted as ~), perform a logical negation on each bit of binary value. It flips the logical state of a binary, which itself is a boolean value, changing true to false and false to true, or 1 to 0 and 0 to 1.  ~10000010 (binary) --------------- 01111101 (binary)   OR​  An OR (often denoted as |), will take two binary values and produce another binary where each bit will be set to 1 if either or both of the corresponding bits in the operands are 1 and the result bit is set to 0 only if both corresponding bits are 0.  10101010 (binary) | 11001100 (binary) ---------------- 11101110 (binary)   XOR​  A XOR (often denoted as ^), is similar to OR, however, it is a stricter OR operation. The result of the operation between bit will be 1 if both of the bit in the operands are 1 and will produce 0 otherwise.  10101010 (binary) ^ 11001100 (binary) ---------------- 01100110 (binary)   AND​  An AND (often denoted as &amp;), takes two binary values and produce a binary digit of 1 if both of the bit are 1, otherwise produce 0.  10101010 (binary) &amp; 11001100 (binary) ---------------- 10001000 (binary)   ","version":"Next","tagName":"h3"},{"title":"Bit Masking​","type":1,"pageTitle":"Bitwise Operation","url":"/cs-notes/computer-and-programming-fundamentals/bitwise-operation#bit-masking","content":" Bit masking is the process of selectively manipulate or extract specific bits within a binary value using various bitwise operations.  Extracting specific bits : Consider a binary values, we want to extract first four digits from the right. We will then choose the appropriate binary mask, which is the binary values pattern used to manipulate the target binary. Value: 10101010 Mask: 00001111 ------------------- AND Result: 00001010 Given a binary values 10101010, to extract the first four binary digits from the right, we choose the mask 00001111 and do a bitwise AND operation. Setting specific bits : We can perform a bitwise OR operation with a mask that consist of 1 to set the binary values to 1 and use binary digit of 0 to keep other bits unchanged. Value: 10101010 Mask: 00001111 ------------------- OR Result: 10101111 Clear specific bits : By performing a bitwise AND operation between a binary value and a mask with specific bits set to 0, the result will have the corresponding bits in the value set to 0, while preserving the other bits. Value: 10101010 Mask: 11110000 ------------------- AND Result: 10100000  ","version":"Next","tagName":"h3"},{"title":"Compilation","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/compilation","content":"","keywords":"","version":"Next"},{"title":"Language Abstraction​","type":1,"pageTitle":"Compilation","url":"/cs-notes/computer-and-programming-fundamentals/compilation#language-abstraction","content":" Computer executes the lowest-level codes, which is binary instruction, this has gone through many abstractions up to high-level programming language source code. Here are some of different levels of language abstraction :  High-Level Code : High-level code refers to code written in a high-level programming language, such as Python, Java, C++, etc. These languages provide a high level of abstraction and are designed to be human-readable. High-level code is typically written using some programming constructs like variables, functions, or even some OOP concepts like object and classes. Bytecode : Bytecode is an intermediate representation of code for certain programming languages. It is a lower-level representation compared to high-level code, and is designed to be executed by a virtual machine. In the virtual machine itself, the code must undergo translation into machine code before being executed. Examples of languages that use bytecode include Java and Python. Assembly Language : Assembly language is a low-level programming language that represent machine code instructions in human-readable form. It provides a close representation to machine code and is specific to a particular hardware architecture. Machine Language &amp; Binary : Machine language is the lowest level of abstraction in programming. It can be binary code (sequences of 0s and 1s) or hexadecimal that directly represents the instructions and data that a computer's hardware can execute. Machine language instructions are specific to the hardware architecture and are directly understood and executed by the processor.  Another term that is often encountered is operation code (opcode). Opcode refers to the instruction of machine language that specifies the operation to be performed by the computer's processor. It represents the fundamental operations that the hardware can execute, such as arithmetic operations, memory access, and control flow instructions.   Source : High-level code, Bytecode, assembly and machine code  ","version":"Next","tagName":"h3"},{"title":"Compilation Process​","type":1,"pageTitle":"Compilation","url":"/cs-notes/computer-and-programming-fundamentals/compilation#compilation-process","content":" The compilation process typically involves the following stages :  Preprocessor : The preprocessor is a tool or component that processes the source code before it goes through compilation. It performs preprocessing directives tasks, which is special instructions in programming language like C and C++. The instruction starts with the &quot;#&quot; symbol, basically it provides a way to modify the source code before it is passed to the compiler. Compiler : The compiler takes the preprocessed source code as input and translates it into low-level code or an intermediate representation such as bytecode. The compiler checks the syntax and semantics of the code, processes the code, generates optimized code if applicable, and produces output. Compiler does the following process : Lexical Analysis : In this stage, the source code is broken down into a sequence of tokens. Tokens are the smallest meaningful units in the programming language, such as keywords, identifiers, operators, and literals. The lexical analyzer (lexer) scans the source code character by character and groups characters into tokens based on predefined rules and patterns. The tokens are then passed to the next stage. Syntax Analysis : The syntax analysis, also known as parsing, checks whether the sequence of tokens generated by the lexical analyzer is valid according to the grammar rules of the programming language. It builds a parse tree or an abstract syntax tree (AST) that represents the structure of the code in a tree-like structure. The parser analyzes the relationships and arrangements of the tokens according to the language's grammar rules and detects syntax errors if the code violates those rules. Semantic Analysis : Once the syntax analysis is complete and the code is determined to be syntactically correct, the compiler moves on to the semantic analysis stage. Here, the compiler checks the meaning and validity of the code in terms of its semantics. It verifies things like checking if variable has been declared before it used, checking if variables types are valid, scoping rules, function calls, and other language-specific rules. The semantic analyzer ensures that the code adheres to the language's semantics and detects potential semantic errors or compile-time error. Code Generation : After the semantic analysis, the compiler proceeds to generate target code based on the analyzed and validated input. The code generation stage involves transforming the high-level representation (such as the AST) into a lower-level form, which may be assembly language, machine code, bytecode, or an intermediate representation. The code generator translates the code into a format that can be executed by the target platform, while also applying optimizations to improve performance and efficiency. Source : Abstract syntax tree, Lexer - parser Assembler : The assembler is responsible for translating assembly language code into machine code. It takes the output of the compiler and converts it into machine code that can be directly executed by the computer's processor. The assembler replaces the instructions and symbolic representations with their corresponding binary representations (opcodes). Linker : The linker is a separate program that combines multiple object files, libraries, and other dependencies to create a complete executable program or a shared library. It resolves external references and ensures that all the necessary components are linked together correctly. The linker also handles tasks like memory address assignment, relocation, and generating the final executable file or library. Source : https://medium.com/@3681/steps-of-compilation-5c02935a3904  tip Find out more about compilers ","version":"Next","tagName":"h3"},{"title":"Computer & Programming Terminology","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/computer-and-programming-terminology","content":"","keywords":"","version":"Next"},{"title":"Programming Terms​","type":1,"pageTitle":"Computer & Programming Terminology","url":"/cs-notes/computer-and-programming-fundamentals/computer-and-programming-terminology#programming-terms","content":" Pseudocode​  Pseudocode is a representation of computer program's algorithm, which is not written in any programming language. Pseudocode uses the combination of natural language and programming constructs and common programming concepts to describe the steps or operations performed by a program.  The purpose of pseudocode is to provide a clear and understandable outline of the program's logic without getting into the specifics of the programming language syntax. It serves as a bridge between the problem-solving phase and the actual implementation in a programming language.   Source : https://www.computerscience.gcse.guru/theory/pseudocode  Data Structures​  Hash map vs Hash set vs Dictionary​  Hash map : Hash table or Hash map is a data structure where we access element based on a unique identifier called key. An element is a set of key-value pair, the key is the identifier and the value is the actual value associated with it.Hash set : Set is a data structure that stores unique element. Hash set is one way to implement set data structure using hash code. It uses hash table internally, which mean it uses hashing function similar to hash map. While it uses hash table, it is not a key-value pair data structure unlike hash map.Dictionary : Just another name for hash map in language like Python.  Array vs List vs ArrayList​  Array : Array is a fixed-sized data structure that stores same type of element in a contiguous block of memory.List : List is a variable-sized data structure that can store different type of element. A list can be implemented using a linked list, which allows for non-contiguous memory allocation, enabling each value to be located at an arbitrary position.ArrayList : ArrayList is an implementation of list data structure using a dynamic array. Unlike traditional array, a dynamic array can be resized.    Type System​  Typing Strength​  Typing strength refers to the level of strictness or flexibility in type checking and type enforcement within a programming language.  Strong vs Weak : In a strongly typed language, implicit type conversions or operations between incompatible types without explicit casting are not allowed. For instance, if you try to add an integer with a string, a strongly typed language will not automatically change the type of the string to a number to perform the operation. On the other hand, weakly typed languages perform automatic type checking and conversions as needed, which can potentially lead to unexpected errors.Static vs Dynamic : In a static typed language, variable types are checked at compile-time. Variables are required to have their types declared explicitly, a variable type can't be changed, and the type checking is performed before the program is executed. On the other hand, dynamic typing is the opposite, variable types are checked at runtime. Variables do not require explicit type declarations and can hold values of different types during the execution of the program.   Source : https://prepinsta.com/python/why-python-is-a-strongly-typed-language/  Type Inference​  In a static typed language, we must declare a type of a variable. Type inference is a feature in some programming language that allows the compiler or interpreter to deduce the type of a variable or expression based on its usage and context, without requiring explicit type annotations.  For instance, we can compare Java and Kotlin programming language :    We don't need to specify the type of variable and function return types if we make it inline in Kotlin.  Type Safety​  Type safety refers to the degree to which a programming language helps prevent type-related errors during the execution of a program. When a system is called type-safe, it means that it can be guaranteed that the type of the values will be compatible with the expected type.  For example, if a type-safe function is declared to return an integer, it guarantees that the function will consistently return an integer whenever it is called.  Standard Library​  Standard library is a collection of pre-written code modules or libraries that are included with a programming language. It is a programming language built-in functionality consisting a wide range of common function, data structures, and algorithms that can be used to develop program.  Abstraction​  Abstraction is a fundamental concept in computer science and programming that involves simplifying complex systems by focusing on essential features and hiding unnecessary details. Abstraction is made to allow programmers to work with higher-level concepts and operations without needing to worry about internal details.  API​  Application Programming Interface (API) is an interface for programmer to interact with different software component in a system. An API defines how different components of software systems should interact, specifying the methods, data formats, and conventions that should be used.  tip See a more detailed explanation about API  Reflection​  Reflection is a feature that enables program to examine internal detail about a previously compiled program. Examples are getting class name, interface methods, object type, and other information during runtime. It allows us to programmatically analyze and manipulate the structure and behavior of code without having prior knowledge or compile-time access to the specific elements.  Serialization​  Serialization is the process of converting an object or data structure into a format that can be easily stored, transmitted, or reconstructed later. For example, when transferring data in the form of object through the network, we will need to convert it to format that can be transmitted such as JSON file.  For example, here is a simple object in Kotlin :  data class User(val name: String, val age: Int)   We have a User object in Kotlin, if we need to send user data to a server over a network, we can convert the user object to JSON format.  // user data that wants to be transfered val user = User(name = &quot;Serial&quot;, age = 23)   // result of serialization { &quot;name&quot;: &quot;Serial&quot;, &quot;age&quot;: 23 }   In the case of HTTP request, this JSON file will be included in the payload. This will be sent over the network, the JSON data will be received on the other end by the recipient. In order to use this data in the form of Kotlin object, it will need to go through the deserialization or the reverse process of serialization that converts it back to object.  Brute Force​  Brute force is a strategy of solving a problem that involves trying out all possible solution, without any additional optimization or anything that might improve the overall outcome. Brute force is typically used in the case where the problem is very small or there are just too many solution and constraint.  Example of brute forcing is when you try to guess a 3-digit PIN code. Each of the digit can be anything from 0 to 9, and each number can repeat in other digit place. Starting from 000, you would incrementally try each possible combination, such as 001, 002, 003, and so on, until you reach 009. After that, you would move on to the next hundreds place, trying combinations like 010, 011, 012, and so forth until the last 999.  ","version":"Next","tagName":"h3"},{"title":"Other Terms​","type":1,"pageTitle":"Computer & Programming Terminology","url":"/cs-notes/computer-and-programming-fundamentals/computer-and-programming-terminology#other-terms","content":" Command Line, Command Prompt, Shell, Bash, Terminal, Console​  Command Line (CLI) : Also known as command-line interface (CLI), it refers to the interface where users can enter commands to interact with a computer system or execute programs. It is a text-based interface where commands are entered as text strings.Command Prompt : Command prompt is the specific CLI in Windows OS. There are two command prompts in Windows : Windows CMD : Windows CMD is the default CLI for Windows OSWindows PowerShell : PowerShell is a more advanced CLI, it also supports scripting with its own scripting language, which is based on the .NET Framework. Shell : Shell is the program that runs in CLI, it is the one that interprets commands.Bash : Bash (Bourne Again SHell) is a shell in Unix operating system, including Unix-like operating system like macOS and Linux (PowerShell is also a shell with more scripting capabilities).Terminal : Simply any program that uses text-based interface for input/output, it can be a CLI, but it doesn't have to be.Console : Historically, it is a terminal which is connected into physical machine (a physical terminal). Nowadays, console is commonly referred as a terminal that displays output.  Environment Variables​  Environment Variables are variables that contains value defined by user that are set outside the program, typically within the environment in which processes operate. Environment variables are commonly used in operating systems and programming frameworks to store information such as system paths, network configurations, authentication credentials, and application-specific settings.  Some common environment variable :  PATH : Specifies the directories where the operating system looks for executable files. In order to run a program, it is necessary to obtain the precise location of the executable file (e.g., find the executable file and double click it to run it). Specifying the program's location in the environment variable allows us to run the program from any directory without specifying the full path to its executable file. This is a common practice among programs that are executed within the command line.USER : Stores the username of the currently logged-in user.LANG : Determines the language and localization settings for the system.DATABASE_URL : Holds the connection details for a database server.   Source : https://docs.oracle.com/cd/E83411_01/OREAD/creating-and-modifying-environment-variables-on-windows.htm ","version":"Next","tagName":"h3"},{"title":"Data Structures & Algorithms","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/data-structures-and-algorithms","content":"","keywords":"","version":"Next"},{"title":"Algorithm​","type":1,"pageTitle":"Data Structures & Algorithms","url":"/cs-notes/computer-and-programming-fundamentals/data-structures-and-algorithms#algorithm","content":" An algorithm is a set of step-by-step instructions or rules to solve a specific problem or perform a particular task. Computer works by executing instruction, an algorithm in computer programming act as the building block of a program.  Algorithm is really just an instruction to solve a problem, it is also not specific to computer. For example, cooking a food can be formulated in set of instruction, it may include instruction starting from preparing the ingredients, combining the ingredients, heating the pan, inserting the ingredients, and so on.  Basic Type of Algorithm​  There are many types of algorithm that are frequently used in computer programming to solve a certain task. The two basic type of algorithm are sort and search, they are the most intuitive algorithm and commonly used real-life.  Sorting : Sorting algorithms are used to arrange a collection of elements in a specific order. Element can be sorted in ascending (increasing) or descending (decreasing) order. For instance, your computer may sort file on a folder by its name in ascending order. A file name contains alphabet character, the smallest alphabet character is &quot;a&quot; and the largest alphabet character is &quot;z&quot;. Sorting in ascending order involve comparing each alphabet in the file name, the file name with smaller alphabet should be placed first before the larger alphabet. There are many kinds of sorting algorithm, each with its own characteristics and efficiency. Example of sorting algorithm includes bubble sort, insertion sort, and selection sort. Source : https://emre.me/algorithms/sorting-algorithms/ Searching : Searching algorithms are used to find the presence or location of a specific element within a collection of elements. For example, in real-life, we may try to find a document within a filling cabinet. We will take the document and check if it's the one we are looking for, if yes, we will take it and if not, we will put it back and take another document. The common searching algorithm are the linear search and the binary search, they are used to find the location of specific element in some collection. The linear search is a very intuitive algorithm that sequentially checks each element in a collection until a match is found or the end of the collection is reached. The binary search instead is a more efficient algorithm, however, it is only appliable for sorted collection. Below is the comparison for linear and binary search. Source : https://tenor.com/view/binary-search-sequence-search-gif-20595028 In the GIF above, we are looking to find the location of specific element, which is 37, in a group of numbers. The steps indicate the number of time we access or check a number. The linear or sequential search begins from the left and sequentially check if it's the element we are looking for. The search will continue until we find the element. As seen in the GIF, in this case, the linear search takes more step than the binary search. The binary search algorithm assume that the collection of numbers is sorted, whether in ascending or descending order, but from now on, we will assume the collection is sorted in ascending order. In a sorted collection, the elements on the left is always smaller than the elements on the right, and vice versa. The idea is, if we managed to find a smaller number than the number we are looking for, because the numbers are sorted, then there is no way that the element would be in the left of the smaller number. Therefore, we don't need to search the portion of the collection preceding the smaller number, instead we can just eliminate that range and focuses to search on the other range. As seen in the GIF, we started the search in the middle, while it is possible to start at the 3/4 or 1/4 of the elements, there is no assurance that doing so would result in a more significant range reduction.  Algorithm Complexity​  We can decide which algorithm is better than which by comparing the efficiency. The efficiency of an algorithm is typically measured by time and space complexity. Time and space complexity refers to the amount of time and amount of memory or storage required by the algorithm to run.  Big-O Notation​  The time and space complexity is typically measured by the dimension of the input, they are denoted by the Big-O notation. The big-O notation describe the efficiency of algorithms in terms of their growth rates as the input size increases.  Some example of big-O notation :  O(1) - Constant Complexity : A constant time complexity indicates that the algorithm will always run in the same time regardless of how many inputs are given. Similarly, a constant space complexity indicates the algorithm will always use the same amount of memory for any input. O(log n) - Logarithmic Complexity : A logarithmic complexity indicates the time or space complexity grows logarithmically with the input size. O(n) - Linear Time Complexity : A logarithmic complexity indicates the time or space complexity grows linearly with the input size, there is a proportional relationship between the algorithm's resource usage and the input size.   Source : https://www.freecodecamp.org/news/all-you-need-to-know-about-big-o-notation-to-crack-your-next-coding-interview-9d575e7eec4/  Example​  For example, in the linear search, when we start searching from the left, there is a chance that the element we are looking for is at the rightmost, therefore we have to check each element in the collection. This is the worst case scenario of the linear search, the time complexity is O(n) or it will be proportional to the size of the input which is the number of elements in the collection being searched.  On the other hand, the linear search algorithm require no extra memory, making the space complexity is O(1).  ","version":"Next","tagName":"h3"},{"title":"Data Structures​","type":1,"pageTitle":"Data Structures & Algorithms","url":"/cs-notes/computer-and-programming-fundamentals/data-structures-and-algorithms#data-structures","content":" Data Structures are specialized formats of storage used to store data efficiently. Each data structure has different characteristics that are suited for different kinds of tasks, and these characteristics correspond to the algorithm's time and space complexity.  Here are commonly used data structures :  Array : Array is a data structure where a collection of elements is grouped together within a contiguous block of memory, where each element is next to each other. Each element in the array has its own position, called index and it starts from 0. To access specific element in the array, we need to know its index. The time complexity to access a single element will be in O(1) time, this is because no matter the size of the array size is, we will always access it the same way. Stack : Stack is a data structure where the elements are stacked on top of each other. It follows the Last-In-First-Out (LIFO) principle, where element can be added or removed only from the top of the stack. The operation of adding element is often called &quot;push&quot; and the operation of removing element is called &quot;pop&quot;. The time complexity to add or remove element is also O(1), no matter the size, the operation will always be the same. Hash Table : Hash table is a data structure where we access element based on a unique identifier called key. An element is a set of key-value pair, the key is the identifier and the value is the actual value associated with it. Under the hood, hash table uses an array to store the element. An array has a constant time complexity to access an element, however, in some cases the index of the element we are looking for is not known. Sometimes, the index of the element also changes overtime when we remove an element from the array. The idea of hash table is, we do not directly use index to access an element, we instead use a key as a unique identifier that will never change and always known by the programmer. To determine in which position of the array would the element be stored, hash table takes the key and input it in function called hash function. The hash function is a mathematical function that takes input and output a hash code, which is an integer value that can be used as the index of the element in the array. The array used in hash table is typically fixed in size, the hash function will be configured such that it will always return a value in the range of the array size. So, to add an element to a hash table, we will need a key and the value which is the element we are going to store. Hash table does the same way to access the element, given a key, transform it into hash code from the hash function, then use the result to access the element from the array. The hash function typically has a constant time complexity, making it a very efficient data structure to quickly access element. However, in some cases, the hash function may return the same hash code for different key, this is called collision and it's a common problem in hash table. Technique such as chaining is employed to mitigate this problem. Source : array, stack, hash table  tip Refer to data structures and algorithm for more detailed explanation ","version":"Next","tagName":"h3"},{"title":"Concurrency & Parallelism","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/concurrency-and-parallelism","content":"","keywords":"","version":"Next"},{"title":"Concurrency​","type":1,"pageTitle":"Concurrency & Parallelism","url":"/cs-notes/computer-and-programming-fundamentals/concurrency-and-parallelism#concurrency","content":" Computer executes tasks one by one sequentially, often times some tasks require a lot of time to finish. For example, in a web application like e-commerce, our browser will need to load some data from the server to display it to us. If the server takes a long time to process the request or the network connection is slow, it can result in a noticeable delay before the data is available to the browser. Imagine if there are more tasks than just requesting data to server, such as processing and transforming the data. If computer can only execute one task at a time, then it may take a long time.  Many techniques can be employed to mitigate this issue. Concurrency is the ability of a system to handle multiple tasks or processes simultaneously. Instead of waiting all the data to be received and then process it one by one, we can download the data and process it simultaneously.  Another example is a mobile app that loads data from the phone's storage. The app is supposed to display loading animation in the screen, without the ability of processing multiple task simultaneously, the screen may freeze because the system is focusing on retrieving data from the storage and can't update the animation.    ","version":"Next","tagName":"h3"},{"title":"Process & Thread​","type":1,"pageTitle":"Concurrency & Parallelism","url":"/cs-notes/computer-and-programming-fundamentals/concurrency-and-parallelism#process--thread","content":" Process​  A process is an instance of a running program. A process encapsulates the running program's code, data, and resources required to execute a program. Each process has its own memory space and system resources. Process is managed by the operating system, and is independent with each other process, they are executed in its own isolated memory space.  Process has its own memory space and system resource, they are not directly accessible or affected by another process. In order to communicate with other processes, a mechanism called Inter-Process Communication (IPC) is employed.  Operating system will keep track the state of each process, a process will run through several states :  Created : When a process is created or initialized, the process is being set up by the operating system, allocating resources such as memory and initializing data structures.Waiting : A process in the waiting state is prepared but is waiting to be executed by the operating system scheduler.Running : When a process is selected by the scheduler and is executing on the CPU, it is in the running state. In this state, the process is actively using the CPU to execute its instructions. Only one process can be in the running state at a given time on a single CPU core.Blocked : If a process is unable to proceed further and must wait for an event or resource, it enters the blocked state. For example, if a process is waiting for user input such as selecting file, it will be blocked and temporarily suspended until the required event occurs.Terminated : When a process finishes its execution or is explicitly terminated by the operating system or a user, it enters the terminated state. In this state, the process is no longer active, and its resources are released by the operating system.   Source : https://en.wikipedia.org/wiki/Process_%28computing%29#/media/File:Concepts-_Program_vs._Process_vs._Thread.jpg  Thread​  Thread, on the other hand, is a basic unit of execution within a process. A thread is a single sequence of instruction that can be executed by the CPU. Thread exists inside a process, they share the same memory space and resources of the process that created them.  A single process can have multiple thread to execute multiple task, the technique is called multithreading. In a process where multiple thread exist, they can work together to complete a task as they share the same memory space.  Process vs Thread​  The obvious difference between them is that a process is the running program, while a thread is the one that executes tasks within that program. The process owns memory and system resource, while thread do not have their own and access the one owned by the process. Each process has at least one thread, called the main thread, which starts executing the program from its entry point.   Source myself, inspired by : https://www.java67.com/2012/12/what-is-difference-between-thread-vs-process-java.html  ","version":"Next","tagName":"h3"},{"title":"Concurrent & Asynchronous Programming​","type":1,"pageTitle":"Concurrency & Parallelism","url":"/cs-notes/computer-and-programming-fundamentals/concurrency-and-parallelism#concurrent--asynchronous-programming","content":" Concurrent Programming is a programming paradigm that involves executing multiple tasks or processes concurrently. It aims to make efficient use of system resources by allowing tasks to overlap in execution.  In traditional programming, a task is executed sequentially and the program waits for each task to finish before moving on to the next one, this is called synchronous programming. On the other hand, asynchronous programming is a programming paradigm where the program continue doing other task instead of just waiting for a specific task to complete.  There are many techniques to achieve concurrency and asynchronous.  Multithreading, Multiprocessing, Multitasking​  Multithreading​  A process can have multiple threads, multithreading refers to the ability of a program or process to execute multiple threads concurrently within a single process. It allows for the concurrent execution of multiple tasks within the same program, to improve performance and resource utilization.  Threads within the same process share the same memory space. This means they can access and modify shared data and resources, enabling communication and coordination between threads. This make it possible for thread to substitute for each other and continue each other task.  Threads are used in the case where multiple tasks need to be performed, such as the mobile application scenario earlier. One thread may request data to a server, another thread processes the data on the fly, and another thread updates the screen to display animation.   Source : https://www3.ntu.edu.sg/home/ehchua/programming/java/j5e_multithreading.html  Multiprocessing​  Multiprocessing is a technique that takes advantages of computer with multiple CPU or processor to execute tasks simultaneously. Unlike multithreading, which involves multiple threads within a single process, multiprocessing utilizes multiple processes that can run independently and concurrently.  In multiprocessing, each process will have different memory and system resource. They are also not able to access each other resource directly, to be able to execute a single task with multiple process, they will need a proper coordination through the IPC mechanism.  Multiprocessing can be beneficial for computer that has multiple cores, it can significantly improve performance. However, it can be heavier and tend to consume more power. On the other hand, creating and managing threads in multithreading is generally faster and more lightweight compared to creating separate processes. Thread can also coordinate and communicate with each other easily because they share the same memory.   Source : https://www.shiksha.com/online-courses/articles/difference-between-multiprocessing-and-multiprogramming/  Multitasking​  Multitasking refers to the ability of an operating system to execute multiple processes concurrently. The operating system gives each process CPU processing time. The CPU switches rapidly between tasks, each task will have small-time execution. This switching occurs so quickly that it creates the perception of simultaneous execution.  For example, in our desktop, we can run multiple application at the same time. This is actually an illusion, the operating system rapidly switches between these processes, giving each one a small portion of CPU time to make it appear simultaneously.   Source : https://en.wikipedia.org/wiki/Computer_multitasking#/media/File:Desktop-Linux-Mint.png  Summary​  Multithreading : The use of multiple thread within a single process.Multiprocessing : The use of multiple CPU cores to create multiple process.Multitasking : The concurrent execution of multiple process.  All of them are technique to concurrently execute task.   Source : Single vs Multi-threaded, Multiprocessing vs Multithreading, Multiprocessing vs Multitasking vs Multithreading  Coroutine​  In a long operation such as waiting for I/O event or doing computation intensive task, the specific thread which executes the task will be blocked. When a thread is blocked, it means the thread is unable to proceed with its execution and is waiting for a particular operation to complete. When a thread is blocked by an I/O operation, it does not consume CPU resources because it is not actively executing instructions. In contrast, when a thread is doing a long computation, it utilizes CPU resources as it performs calculations and executes instructions.  Thread shouldn't be blocked, one reason is because they are expensive, they require memory to store its own stack space to store execution-related information and the operating system needs to manage them, which result in additional computation. Another reason is in a scenario where application is included with UI (user interface). UI should be responsible to react at user interaction (e.g., updating screen when user click on something). However, because the thread is currently blocked, it can't respond to UI event which will result in a poor user experience.  One way to mitigate this thread blocking is coroutine. Coroutine is a unit of execution that can be cooperatively scheduled, meaning it can be paused and resumed at specific points in its execution. We call a piece of code that can be paused and resumed later on suspendable.  When a piece of code can be suspended, it means that it can yield control to other tasks without blocking the thread it is running on. Instead of the thread waiting for specific task to complete, coroutine allows it to perform another task without waiting. When the task is complete, the thread will continue its previous task.   Source : https://blog.adacore.com/coroutines-in-ada-a-clean-but-heavy-implementation  Coroutine differs with multithreading, multiple thread can work together in parallel, they can execute code simultaneously. Having multiple thread can be resource intensive (e.g., require memory and the switches between them can be intensive). On the other hand, coroutine doesn't use a lot of thread, they take turns executing their code. It is a concurrent construct rather than a parallelism construct, which allows for efficient multitasking within a smaller number of threads. In summary, coroutine can be thought as the lightweight version of thread.  tip Thread is considered as preemptive multitasking, which mean it is the responsibility of the operating system to manage the execution between them. On the other hand, coroutine is considered as cooperative multitasking, where each coroutine is responsible for yielding control to others.  Callbacks​  A callback is a function passed as argument to another function. The purpose of passing function is to allow the receiving function to invoke the provided function at a specific point in its execution or in response to a certain event or condition.  For example, consider a scenario where we need to load data from server and then process it after. We have function A that loads data from a server and function B that processes the data. Function A will complete its execution when all the data is loaded, however, we don't know when it will complete. We can pass function B to function A, in the body of function A, we can include a code that invoke function B. The point where function B is invoked is when function A finishes. By using callbacks, we can ensure that the operations execute in the correct order and at the appropriate time.  Here is an example in the Python programming language.  def load_data(on_data_loaded): data = load_data_from_server() # Simulate long computation to load data on_data_loaded(data) # Call the provided function and pass the loaded data def process_data(data): # Process the loaded data... # When data is loaded, we want to process the data load_data(on_data_loaded = process_data)   Future, Promise, Async Await​  Future and Promise are both abstractions that represent the eventual result of an asynchronous operation. An operation regardless of the amount of time it takes to complete and whether they are success or not, they must eventually have a result. A future or promise is an object that represent the result.  Future or promise typically have some set of methods to handle the result, it may include method that indicates when the operation is finished, a method that is responsible to handle success operation, where we can use the result for our use case, or method to handle error operation.  Here is an example in Kotlin programming language (not a real code) :  // load data must complete at one point, represent by Future object fun loadData(): Future&lt;String&gt; { // load data... } fun main() { val futureString = loadData() // When loadData is completed, we will pass callback that will be executed // in the event of a successful data load or an error. futureString.onDone( onSuccess = { data -&gt; // Handle success scenario }, onError = { error -&gt; // Handle error scenario } ) }   ","version":"Next","tagName":"h3"},{"title":"Reactive Programming​","type":1,"pageTitle":"Concurrency & Parallelism","url":"/cs-notes/computer-and-programming-fundamentals/concurrency-and-parallelism#reactive-programming","content":" Reactive Programming is a programming paradigm that focuses on asynchronous data streams and the propagation of changes. Reactive programming is used in the scenario where data or events occurs over time and we need to react over it.  There is another term called event-driven programming, which is a programming paradigm that focuses on designing software systems around the concept of events such as user actions and sensor inputs.  Stream, Channel​  Stream and channel are commonly encountered concepts in the reactive programming. A stream refers to asynchronous sequences of data items or events. On the other hand, channel is communication mechanism which data can flow between different components of a system.  For example, in a mobile application with sensor, the sensor is considered as a data source that capable of outputting a sequence of sensor information. We can represent it as a stream and process the data to use it in the app. In the application itself, we can use channel as a way to communicate between component in the application. When the UI detect user's interaction, send the interaction information to channel and then another component may check the channel periodically for event.    Observable, Consumer, Subscribe​  These three concepts are related to each other. Something is called observable if it is capable of being a source of events or data (e.g., a data stream). The component that have an intention to receive data emitted by that stream is called the observer, also known as consumer. Subscribe is the action of establishing a connection between an observable and a consumer. When a component &quot;subscribe&quot; to an observable, it will be notified whenever an event occurs.    ","version":"Next","tagName":"h3"},{"title":"Terminology​","type":1,"pageTitle":"Concurrency & Parallelism","url":"/cs-notes/computer-and-programming-fundamentals/concurrency-and-parallelism#terminology","content":" Race Condition​  A race condition happens when multiple threads try to use the same data at the same time, and the final outcome of the program depends on the order in which the threads are executed. For example, imagine two threads, thread 1 and thread 2, they will access some number and increment it. If thread 1 starts first and thread 2 starts very quickly afterward, there is a chance that thread 2 might access the number before thread 1 has finished modifying it. This can lead to unexpected and incorrect results because thread 2 should ideally wait for thread 1 to finish modifying the number before accessing it.    Thread Lock​  Thread lock, also known as mutual exclusion (mutex), is a synchronization mechanism used prevent race condition. It simply ensures that only one thread can access the resource at a time.    Thread Safe​  Thread safe refers to the property of a program or data structure that can be safely accessed and manipulated by multiple threads concurrently, without causing race conditions or other synchronization issues.  Deadlock​  Deadlock is a situation where two or more threads are unable to proceed because each is waiting for the other to take a specific action. Deadlock can occur when attempting to prevent race conditions through synchronization mechanisms, but those mechanisms are implemented incorrectly.   Source : https://en.wikipedia.org/wiki/Deadlock  Thread Pool​  Thread pool is a collection of pre-initialized threads. Instead of creating a new thread for each task, a thread pool maintains a pool of reusable threads, reducing the overhead of creating and destroying threads.  tip In relation with multitasking and concurrency, see also multithreading. ","version":"Next","tagName":"h3"},{"title":"Data Representation","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/data-representation","content":"","keywords":"","version":"Next"},{"title":"Units of Data​","type":1,"pageTitle":"Data Representation","url":"/cs-notes/computer-and-programming-fundamentals/data-representation#units-of-data","content":" These are standardized measures to count the amount of information or data stored in computer :  Bit (b) : The smallest information is a single binary digit (0 or 1).Byte (B) : A byte is a group of 8 bits. It is the most common unit used for representing characters and data in computer systems.Kilobyte (KB) : 1 kilobyte is equal to 1,000 bytes (10310^3103 bytes). It is often used to describe small amounts of data, such as text documents or small images.Megabyte (MB) : 1 megabyte is equal to 1,000 kilobytes (10610^6106 bytes). It is commonly used to measure the size of files, larger documents, images, or short audio recordings.Gigabyte (GB) : 1 gigabyte is equal to 1,000 megabytes (10910^9109 bytes). It is used to describe larger files, such as high-resolution images, longer audio recordings, or small videos.Terabyte (TB) : 1 terabyte is equal to 1,000 gigabytes (101210^{12}1012 bytes). It is used for large-scale data storage, such as hard drives, servers, or high-definition video recordings.Petabyte (PB) : 1 petabyte is equal to 1,000 terabytes (101510^{15}1015 bytes). It is used to measure large amounts of data, such as data centers or big data analytics.  And many more measurements that follows the same pattern.  There are also measurements such as KiB or MiB. The difference between KB and KiB or MB and MiB differs in their base. KB uses base 10 (decimal) while KiB uses base 2 (binary).  In summary:  1 KB = 10310^3103 bytes (decimal)1 KiB = 2102^{10}210 bytes (binary)  ","version":"Next","tagName":"h3"},{"title":"File Format​","type":1,"pageTitle":"Data Representation","url":"/cs-notes/computer-and-programming-fundamentals/data-representation#file-format","content":" In computer, a file is a collection of data or information stored on a storage device such as hard drives. When a file is created, modified, or saved, it is typically represented as a sequence of binary data, consisting of 0s and 1s. The file's contents, along with its metadata (such as file name, size, creation date, and permissions), are stored on the storage device.  A file format defines how a file is structured and organized. A file format describes how data is stored, encoded, and interpreted in a computer file. For example, a document file may include what font used in the document so that the computer that reads it know what to display.  Various amount of file format can be found in digital media processing, digital media formats section.  ","version":"Next","tagName":"h3"},{"title":"Data Representation​","type":1,"pageTitle":"Data Representation","url":"/cs-notes/computer-and-programming-fundamentals/data-representation#data-representation","content":" Color​  In computer, color is represented as number in binary format. Each combination of binary format represent a different color.  RGB (Red, Green, Blue) : RGB is the most widely used color model in computer graphics and digital displays. It represents colors by specifying the intensities of red, green, and blue primary colors. By combining different intensities of these three primary colors, a wide range of colors can be produced. RGB contains 3 different color components (also called color channel), where each component is typically represented as 8-bit value ranging from 0 to 255. For example : Red : RGB(255, 0, 0) / RGB(11111111, 00000000, 00000000) in binary.Green : RGB(0, 255, 0) / RGB(00000000, 11111111, 00000000).Blue : RGB(0, 0, 255) / RGB(00000000, 00000000, 11111111).Purple : RGB(128, 0, 128) / RGB(10000000, 00000000, 10000000). Source : https://www.programiz.com/blog/working-of-binary-numbers-in-computers/ CMYK (Cyan, Magenta, Yellow, Key/Black) : CMYK is primarily used in printing and represents colors in terms of the amounts of cyan, magenta, yellow, and black inks required to reproduce a specific color. It uses subtractive color mixing, where the more ink is added, the darker the color becomes. Similar to RGB, CMYK is typically represented as a set of 8-bit. Cyan : CMYK(100, 0, 0, 0) / CMYK(11111111, 00000000, 00000000, 00000000)Magenta : CMYK(0, 100, 0, 0) / CMYK(00000000, 11111111, 00000000, 00000000)Yellow : CMYK(0, 0, 100, 0) / CMYK(00000000, 00000000, 11111111, 00000000)Black : CMYK(0, 0, 0, 100) / CMYK(00000000, 00000000, 00000000, 11111111)Orange : CMYK(0, 50, 100, 0) / CMYK(00000000, 01111111, 11111111, 00000000) HSL/HSV (Hue, Saturation, Lightness/Value) : HSL and HSV are alternative color models that represent colors based on their perceived attributes. Hue represents the dominant wavelength of the color, saturation represents the intensity or purity of the color, and lightness or value represents the brightness. HSL and HSV values are usually represented as angles for hue (ranging from 0 to 360 degrees) and percentages or decimal values for saturation and lightness. Red : HSL(0, 100%, 50%) / HSL(00000000, 10000000, 01100100)Lime Green : HSL(120, 100%, 50%) / HSL(01111000, 10000000, 01100100)Blue : HSL(240, 100%, 50%) / HSL(11110000, 10000000, 01100100)Light Yellow : HSL(60, 100%, 75%) / HSL(00111100, 10000000, 11001000)Magenta : HSL(300, 100%, 50%) / HSL(10010110, 10000000, 01100100) Hexadecimal Color : Hexadecimal color is another commonly used representation for colors in computer systems. It uses the hexadecimal numbering system to represent colors, where each color component is represented by a two-digit hexadecimal value ranging from 00 to FF. Red : #FF0000Green : #00FF00Blue : #0000FFYellow : #FFFF00Purple : #800080  Sound​  Sound is a continuous wave in its analog form, in computer, they are represented discretely. To transform continuous wave to discrete data, it will go through a process called sampling. Sampling involves measuring the amplitude of the sound wave at specific points in time. The rate at which these measurements are taken is known as the sampling rate. For example, when we say a sound is sampled at 44.1 kHz, it means we are sampling the sound wave at 44,100 times per second.  Each sample represents the amplitude of the sound wave at a particular moment. To convert this analog amplitude into a digital representation, the sample will be quantized. Quantization involves assigning a numerical value to the amplitude of each sample. Basically, we will assign binary digits for each different amplitude. However, with the many combinations of amplitude, sometimes they are rounded to the closest interval to reduce complexity of the data, sacrificing the sound accuracy. The number of numerical value we will have is calculated by 2bit depth2^{\\text{bit depth}}2bit depth, the bit depth determines the resolution or precision of the quantized representation.  After converting it to binary, we can then store it on a file. Storing and accessing the file will involve coding process which includes encoding and decoding. Simply, they are the process of representing a signal or data in a specific format or code that can be processed, transmitted, stored, or interpreted by digital systems.  As explained before, the stored file will be in a specific file format. In the case of sound or audio file, we can store it in MP3 format. By using the MP3 format, audio files can be efficiently stored, transferred, and played back on various digital devices.   Source : https://www.teachwithict.com/binary-representation-of-sound.html  tip Know more about wave in computer in digital signal processing, especially the signal transmission part. More about digital media processing.  Database​  A database is a collection of structured data. A common approach to store database is to organize data into tables consisting of rows and columns. Each row represents a record or entity, and each column represents a specific attribute or field of that record.  A database consists of multiple rows and columns, the structure and organization of the tables, data types used, etc.  A database file is typically divided into fixed-size chunk that contains a specific number of records or a portion of the database file. The database will be stored using a specific file format that defines how the database file is structured. It may consist of header containing important information about the file, the metadata, and the actual database.   Source : https://www.javatpoint.com/what-is-rdbms  tip See also database system.  ","version":"Next","tagName":"h3"},{"title":"Character Encoding​","type":1,"pageTitle":"Data Representation","url":"/cs-notes/computer-and-programming-fundamentals/data-representation#character-encoding","content":" Encoding refers to the process of converting information from one representation or format to another. It involves converting data into a specific format that can be processed, transmitted, stored, or interpreted by digital systems.  Character encoding is specific encoding used to represent characters, symbols, and textual data in computer.  ASCII​  ASCII (American Standard Code for Information Interchange) is one of the simplest character encoding, widely used in the old days of computing. ASCII represent character using a combination of binary digits. A character is represented by 7-bit code, counting all the binary digits' combination, we can represent 128 different character.  ASCII provides a standardized mapping between these characters and their corresponding numerical codes. For example, the uppercase letter &quot;A&quot; is represented by the code 65, the lowercase letter &quot;a&quot; is represented by 97, and the digit &quot;0&quot; is represented by 48.   Source : https://learnlearn.uk/binary/text-representation-binary/  The image above did a good job explaining ASCII. For example, letter &quot;g&quot; is defined as 103 in decimal or 01100111 in binary.  Unicode​  While ASCII provides a simple way to represent character, it has very limited character set, and it focuses on the English language.  Unicode is a widely used universal character encoding standard for text in all writing systems and languages worldwide. It can even represent various kind of emojis. The Unicode version 15.1, which was released in September 2023 is able to produce 149813 different character.  Unicode assigns a unique numerical value, called a code point, to each character in its repertoire. The code points are represented using hexadecimal notation, such as U+0041 for the uppercase letter &quot;A&quot; and U+4E2D for the Chinese character &quot;中&quot;.  UTF​  UTF (Unicode Transformation Format), such as UTF-8, UTF-16, and UTF-32, is the character encoding schemes used to represent Unicode characters in binary form.  UTF-8 : UTF-8 is a variable-length encoding scheme that represents Unicode characters using 8-bit units, which can be one to four bytes long. In UTF-8, characters from the ASCII character set (U+0000 to U+007F) are represented using a single byte, making it backward compatible with ASCII. Characters outside the ASCII range are represented using multiple bytes. UTF-8 uses a specific bit pattern to indicate the start of a multibyte sequence. A single-byte UTF-8 character (ASCII) starts with a '0' bit, followed by the 7-bit ASCII representation.A two-byte UTF-8 character starts with '110', followed by the remaining 11 bits of the character's code point.A three-byte UTF-8 character starts with '1110', followed by the remaining 16 bits of the character's code point.A four-byte UTF-8 character starts with '11110', followed by the remaining 21 bits of the character's code point. UTF-16 : UTF-16 is a variable-length encoding scheme that represents Unicode characters using 16-bit units, which can be one or two 16-bit code units (also known as surrogates). Characters from the ASCII character set are represented using a single 16-bit unit, while characters outside the ASCII range are represented using one or two 16-bit units. UTF-16 can handle the entire Unicode character set, including characters outside the Basic Multilingual Plane (BMP), or the most commonly used characters across various writing systems. UTF-32 : UTF-32 is a fixed-length encoding scheme that represents all Unicode characters using 32-bit units. Each character is encoded using a single 32-bit unit, regardless of its Unicode code point value. UTF-32 provides a straightforward and uniform representation for all characters, but it requires more storage space compared to UTF-8 and UTF-16.  UTF-8 Example​  ASCII Character &quot;A&quot; : The ASCII character &quot;A&quot; has a Unicode code point of U+0041. In UTF-8, since the code point for &quot;A&quot; falls within the ASCII range (U+0000 to U+007F), it can be represented using a single byte. The UTF-8 binary representation of &quot;A&quot; is: 01000001. Non-ASCII Character &quot;中&quot; : The non-ASCII character &quot;中&quot; has a Unicode code point of U+4E2D. In UTF-8, since the code point for &quot;中&quot; is outside the ASCII range, it requires multiple bytes for representation. The UTF-8 binary representation of &quot;中&quot; is: 11100100 10111000 10101101. Here, the first byte starts with three leading '1' bits followed by a '0' bit (indicating a multibyte sequence), while the subsequent bytes start with '10' bits.   Source : https://support.absorblms.com/hc/en-us/articles/18014014342291-Outlook-Encoding-for-Special-Characters  ","version":"Next","tagName":"h3"},{"title":"Base Encoding​","type":1,"pageTitle":"Data Representation","url":"/cs-notes/computer-and-programming-fundamentals/data-representation#base-encoding","content":" Base encoding is the process of representing data or information in a specific numerical base. The most common encoding in computing is the base-2 encoding, where we represent data using only two symbols: 0 and 1.  Base64​  Base64 is an encoding scheme that represents binary data in an ASCII string format. It uses a set of 64 characters from the alphabet (both lowercase and uppercase), numbers, the &quot;+&quot; symbol, and the &quot;/&quot;. It also use the &quot;=&quot; symbol as padding, to ensure that the length of the resulting encoded string is a multiple of 4 characters.  Here's how the conversion from binary data to Base64 works (Base64 encoding) :  Input Binary Data : The binary data will be divided into groups of 3 bytes.Split Into 6-bit Chunks : Each byte that contains 8-bit each will be combined producing 24-bit binary value. It will then be split onto four 6-bit chunks.Map to Base64 : Each 6-bit chunk is mapped to a corresponding character from the Base64 character set.Padding : If the input data is not evenly divisible by 3 (i.e., the last group has less than 3 bytes), padding is added to ensure that the length of the encoded string is a multiple of 4 characters.  The process to get binary data back from a string encoded in Base64 will be the reverse process of this, and it's called Base64 decoding.   Source : https://en.wikipedia.org/wiki/Base64  For example, consider ASCII characters: &quot;Man&quot;, which has 8-bit binary values of 01001101, 01100001, and 01101110, respectively. Each byte will be joined together resulting in 010011010110000101101110. We will then split it into 6-bit chunk 010011 010110 000101 101110. Each 6-bit chunk maps to T, W, F, u, respectively. Thus, &quot;Man&quot; in ASCII is equivalent to &quot;TWFu&quot; in Base64 encoded. ","version":"Next","tagName":"h3"},{"title":"Declarative & Functional Programming","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/declarative-functional-programming","content":"","keywords":"","version":"Next"},{"title":"Declarative Programming​","type":1,"pageTitle":"Declarative & Functional Programming","url":"/cs-notes/computer-and-programming-fundamentals/declarative-functional-programming#declarative-programming","content":" Declarative Programming is a programming paradigm that focuses on describing what the program should accomplish, rather than specifying how to achieve it. It is the opposite paradigm with the imperative programming.  The main characteristic of declarative programming is that it emphasizes the &quot;what&quot; rather than the &quot;how&quot; of a computation. This means that instead of writing step-by-step instructions for the computer to follow, we specify the desired outcome or the relationship between inputs and outputs.  Declarative programming works at higher level, it hides the implementation detail, making the code more readable and concise. On the other hand, imperative programming require explicit control flow. The system will decide how to execute the code based on the declaration, it doesn't have to be executed sequentially.  The abstraction of declarative programming makes it seem like magic, this can make it harder to predict how changes in the code will affect performance or behavior, especially if we are not familiar how the system works.   Source : https://steemit.com/programming/@nv-vn/getting-started-with-functional-programming  ","version":"Next","tagName":"h3"},{"title":"Functional Programming​","type":1,"pageTitle":"Declarative & Functional Programming","url":"/cs-notes/computer-and-programming-fundamentals/declarative-functional-programming#functional-programming","content":" Functional Programming (FP) is a programming paradigm (type of declarative programming) which treats computation as the evaluation of mathematical functions. Mathematical function is the framework of FP. In mathematics, a function is defined as a relation between a set of inputs (called arguments) and a set of possible outputs (called results), where each input is related to exactly one output.   Source : https://web.cecs.pdx.edu/~antoy/Courses/CS250/slides/2b/Functions_2.html, https://danielpecos.com/2014/06/24/function-composition/ (function relation in math)  Some characteristics of mathematical functions :  Deterministic Mapping : For a given input, a mathematical function must produce a unique and deterministic output (same input should produce same output). The output is solely determined by the input. No Side Effects : A mathematical function should be pure, meaning it shouldn't have side effects, which mean it doesn't modify external state. For example, producing an output from an input shouldn't change the function output for other input. Referential Transparency : Referential transparency means that a function's output should be replaceable by its equivalent output. Here's an example of referential transparency in Python programming language : # define function that takes two number and return the sum def add(x, y): return x + y result1 = add(3, 4) result2 = 3 + 4 # Because the 'add' function is referentially transparent, # 'result1' and 'result2' will always be equal.   In order to adhere with the principles of mathematical functions, functional programming has some characteristics :  Immutability : Immutability is a concept where once a variable (or data structure) is assigned a value, that value cannot be changed. In mathematics, a function maps inputs to outputs, the mapping should be deterministic. Immutability provides consistency and reduce side effects to align with the purity of mathematical function.Minimizing Global State : Mathematical functions are independent of external factors, their behavior is determined solely by their inputs. Functional programming minimize global state to make functions more self-contained and modular.  First-Class Citizen​  In programming, an entity is called as a first-class citizen if it can be treated like other basic data types or values in the programming language. In functional programming, a function is treated as first-class citizen, it has several key characteristics :  Assigned to Variables : You can assign a function to a variable, just like you would with a primitive data type or any other value.Stored in Data Structures : Functions can be stored in data structures such as arrays.Passed as an Argument : A function can be passed as an argument to another function, in other word, a function can accept other functions as parameters.Returned from a Function : Functions can produce other functions as output.  The concept of treating a function as an argument or as an output from another function is also known as a higher-order function.  Recursion​  Because functional languages depend on function, iterating is based on recursion. Quick concept, recursion is where a function call itself until some condition is achieved. For example, we could make an illusion of loop in a function like :  def loop_for(n): if (n == 0): print(&quot;loop end&quot;) print(&quot;still looping...&quot;) loop_for(n - 1) loop_for(3)   When we call the function with n = 3, the function will check if the input has reached zero or not. If not, it will print the &quot;still looping...&quot; and will call itself with its own input decremented by 1. This will be done until the n reached zero and the &quot;loop end&quot; will be printed.  So, even though there's no explicit loop, recursion provides a way to achieve iteration-like behavior in functional programming languages.  Lambda Calculus​  Lambda Calculus is a concept of expressing computation in mathematical logic and computer science that represents computation based on the concept of anonymous functions (functions without names).  A function doesn't have name, it is denoted using the lambda symbol : λ\\lambdaλ. A function can take a parameter, it is placed in front of the lambda symbol. A function with parameter xxx is denoted as λx\\lambda xλx.  After constructing a function, the next step is to apply an expression to the parameter, an expression or the function body is denoted as MMM. Putting all together, a function that takes a parameter xxx and apply expression MMM is denoted as λx.M\\lambda x.Mλx.M, where ... is just a symbol to separate the parameter from the body of the function.  Example​  A function that takes some variable and return the variable incremented by 1, the lambda calculus notation for this is λx.x+1\\lambda x.x + 1λx.x+1. When we apply this function with the argument of 333, it would be written as (λx.x+1) 3(\\lambda x.x + 1) \\space 3(λx.x+1) 3, which is equal to 444. The variable xxx is bound to the argument 333 during the function application.  That was just the basic concept of lambda calculus, there are many more concepts.  note Overall, lambda calculus serves as the theoretical foundation for many concepts in functional programming languages. It provides a formal and mathematical framework for understanding functions, function application, and the manipulation of functions.  Functional Programming Application​  In the JavaScript programming language, a lambda expression (or arrow function) can be written as const addOne = (x) =&gt; x + 1. For example, calling addOne(5) will return 6.  As we can see, the expression (x) =&gt; x + 1 looks similar to the lambda calculus notation. It takes an argument x and returns the result of x + 1. The function is also assigned to the addOne variable (function as first-class citizen).  Another example of functional programming is the map function.  const numbers = [1, 2, 3, 4, 5]; numbers.map((x) =&gt; x * 2);   The map function is a commonly used function in functional programming that is used to transform each element in a collection with the provided function. Calling numbers.map() means we are going to transform each element in the numbers with the function given to the map function.  The function we provided to the map function is defined as a lambda expression, which takes x as parameter and return the x * 2. The result of the map function applied to the numbers array will be [2, 4, 6, 8, 10]  Here is another example from Wikipedia that compares imperative and functional programming approach to perform some operation to an array of numbers.   Source : https://en.wikipedia.org/wiki/Functional_programming#Imperative_vs._functional_programming ","version":"Next","tagName":"h3"},{"title":"Imperative & Procedural Programming","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/imperative-procedural-programming","content":"","keywords":"","version":"Next"},{"title":"Imperative Programming​","type":1,"pageTitle":"Imperative & Procedural Programming","url":"/cs-notes/computer-and-programming-fundamentals/imperative-procedural-programming#imperative-programming","content":" Imperative Programming is a programming paradigm revolving around providing a sequence of statements or commands that change the program's state. It focuses on how a program should accomplish a task by specifying explicit instructions for the computer to follow.  Statements are executed in the order they appear, and each statement modifies the state of the program or performs an action. The program's execution follows a step-by-step approach, where each statement is executed sequentially.  Some characteristics of imperative programming :  Key characteristics of imperative programming include:  Assignment &amp; Variables : Making variables by assigning data to it involve storing data in the memory. It acts as the state of the program, the variable can be modified during the program execution, which will change the program's behavior. Control flow : Control flow such as loops and conditional statements which controls the flow of execution based on conditions. These structures determine which statements are executed and in what order. Expressions and operators : Expressions perform computations or combine values, operators, such as arithmetic, logical, and comparison operators, are used to manipulate and evaluate expressions. Modularity &amp; Functions : Imperative programming breaks down its code into reusable functions that encapsulate specific tasks or computations. These functions can be called from various parts of the program.  ","version":"Next","tagName":"h3"},{"title":"Procedural Programming​","type":1,"pageTitle":"Imperative & Procedural Programming","url":"/cs-notes/computer-and-programming-fundamentals/imperative-procedural-programming#procedural-programming","content":" Procedural Programming is a type of imperative programming, which emphasizes the use practice of organizing a program into procedures or functions, both share many similarities.  Procedural programming has more dedicated feature compared to imperative programming. In imperative programming which is used for lower-level language, some control flow are implemented using the goto statement, which transfers the program execution to a labeled statement elsewhere in the code. The use of goto statement may cause the program to become hard to maintain.  In contrast, procedural programming which is used in higher-level language, control flow has dedicated block scopes for control flow (e.g., while loop, if-else statement). A scope refers to the visibility and accessibility of variables within different parts of a program, it determines where a variable can be accessed and used within the program. Having dedicated scope allows the program to have more predictable and understandable control flow. ","version":"Next","tagName":"h3"},{"title":"Interpreter","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/interpreter","content":"","keywords":"","version":"Next"},{"title":"Compiler vs Interpreter​","type":1,"pageTitle":"Interpreter","url":"/cs-notes/computer-and-programming-fundamentals/interpreter#compiler-vs-interpreter","content":" A compiler takes high-level source code and translate it to lower-level language, without running it. On the other hand, an interpreter takes high-level source code, which doesn't need to be machine language and directly runs it.  An interpreter executes each statement in the source code line by line, without the need for prior translation into machine code like a compiler. The speed of execution in an interpreter can vary depending on the context. When frequently running a specific piece of code, an interpreter may execute it faster since it avoids the overhead of entire source code compilation. In contrast, a compiler initially takes time to translate the entire source code, but the resulting compiled code can execute more quickly afterwards.  Compiler solely translates code, in some case it will need an interpreter. For instance, in the case of the Java programming language, the source code is compiled into bytecode, which is an intermediate representation. This bytecode is then interpreted by the Java Virtual Machine (JVM).  ","version":"Next","tagName":"h3"},{"title":"Interpreter Process​","type":1,"pageTitle":"Interpreter","url":"/cs-notes/computer-and-programming-fundamentals/interpreter#interpreter-process","content":" Some steps are quite similar to compilation process.  Lexical Analysis : The interpreter starts by performing lexical analysis or the tokenization process. It breaks down the source code into a sequence of tokens, such as keywords, identifiers, operators, and literals. It removes whitespace, comments, and other non-essential characters. Parsing : The interpreter proceeds to the parsing phase. It analyzes the sequence of tokens according to the syntax rules of the programming language. This step involves building a representation of the code's structure, such as an abstract syntax tree (AST) (code representation in tree-like structure) or another suitable data structure. The parser ensures that the code is grammatically correct and adheres to the language's syntax rules. Semantic Analysis : Once the code has been parsed, the interpreter performs semantic analysis, which involves checking the code for semantic errors. The interpreter verifies aspects such as variable declarations, type compatibility, scoping rules, and other language-specific rules. It helps catch errors that may not be identified during parsing. Execution : After the code has been analyzed and deemed semantically correct, the interpreter proceeds to execute it. It traverses through the generated AST or data structure, interpreting each statement and performing the associated actions or operations.  ","version":"Next","tagName":"h3"},{"title":"JIT Compilation​","type":1,"pageTitle":"Interpreter","url":"/cs-notes/computer-and-programming-fundamentals/interpreter#jit-compilation","content":" Just-In-Time (JIT) compilation is a technique that compiles code during execution rather than before execution. In traditional compilation, the entire source code is compiled ahead of time (AOT) into machine code, which is then executed directly by the hardware. However, in JIT compilation, the compilation process is deferred until the code is about to be executed.  JIT is typically used for language that uses intermediate representation (e.g., Java bytecode) and usually included in an interpreter. The high-level source code is compiled into bytecode. However, rather than being directly executed by the interpreter, the code is further compiled into machine code, which can be executed directly by the underlying hardware.  The advantage of JIT compilation is that it can apply some runtime optimization, because it can access some of runtime information, such as the actual values of variables, the execution context.  An example of runtime optimization is the function inlining. Inlining involves replacing a function call with the actual code of the called function. This eliminates the overhead of the function call itself, improving performance by reducing the stack frame (data structure used by a program's to manage function or method calls) setup and teardown.  JIT compilation does have a downside related to increased startup time. This is due to the requirement of compiling the code at runtime before it can be executed, which can result in longer startup times compared to programs that are compiled ahead of time (AOT).   Source : https://aboullaite.me/understanding-jit-compiler-just-in-time-compiler/ ","version":"Next","tagName":"h3"},{"title":"Floating Number","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/floating-number","content":"","keywords":"","version":"Next"},{"title":"Whole Number Representation​","type":1,"pageTitle":"Floating Number","url":"/cs-notes/computer-and-programming-fundamentals/floating-number#whole-number-representation","content":" Computer can easily represent decimal or whole number using a binary representation consisting of specific number of binary digits, typically in a multiple of 8.  For example, we can use 32 binary digits (bits), which is able to make a 4 billion possible combination. We are able to represent decimal number ranging from 0 to 4.294.967.295. The calculation, which is explained in the binary number system, involve summing each binary digits (0 or 1) multiplied by each power of base two based on its positional value in the binary representation.  However, that is the case if we don't consider a negative number. To consider negative number, we can use the most significant bit as the sign bit, making us able to represent decimal value from -2.147.483.648 to 2.147.483.647.   Source : https://medium.com/@luischaparroc/integer-numbers-storage-in-computer-memory-47af4b59009 (cropped)  ","version":"Next","tagName":"h3"},{"title":"Real Number Representation​","type":1,"pageTitle":"Floating Number","url":"/cs-notes/computer-and-programming-fundamentals/floating-number#real-number-representation","content":" Fixed-Point Representation​  One way to represent fractional or number with decimal point is through the fixed-point representation. In fixed-point representation, we allocate some binary digits to store the integer part of the decimal value and the leftover is used to store the fractional value.  Like usual, each position represent a power of two where it increases by 1 as we go to the left. However, the part where we separate the integer and fractional value (also called binary point or radix point), is where the power of two starts to increase or decrease up to negative power of two, which correspond to fractional value (properties of exponents, e.g. 2−2=1222^{-2} = \\dfrac{1}{2^2}2−2=221​).   Source : https://www.geeksforgeeks.org/fixed-point-representation/  By using this representation, we can represent a fractional value. For example, given a binary values of 00010.110 in a fixed-point representation, we can calculate it as usual :  (0×240 × 2^40×24) + (0×230 × 2^30×23) + (0×220 × 2^20×22) + (1×211 × 2^11×21) + (0×200 × 2^00×20) + (1×2−11 × 2^{-1}1×2−1) + (1×2−21 × 2^{-2}1×2−2)  0+0+0+2+0+0.5+0.25=2.750 + 0 + 0 + 2 + 0 + 0.5 + 0.25 = 2.750+0+0+2+0+0.5+0.25=2.75  Where 2−1=121=0.52^{-1} = \\dfrac{1}{2^1} = 0.52−1=211​=0.5 and 2−2=122=0.252^{-2} = \\dfrac{1}{2^2} = 0.252−2=221​=0.25  Limitation​  In fixed-point representation, the value we can achieve for all fractional bits is fixed by the multiply negative power of two. To be able to represent value with a more complex fractional accurately, we will need more fractional bits, which will also increase the memory requirements and computational complexity. This method is also not suitable to represent irrational numbers, because by definition it cannot be expressed as a ratio of two integers, also they have an infinite number of non-repeating decimal places. We can still approximate the irrational number, but it can be hard to find the combination of binary values that will sum close to them.  Even sometimes we can't exactly represent a fractional value. This is because fixed-point representation uses binary (base-2) representation, and some decimal fractions don't have an exact binary representation. But, we can at least round or approximate those fractions.  The precision of the fractional value is determined by the number of fractional bits allocated, by having more fractional bits, we can represent smaller and smaller fractions, giving us a higher level of precision. Each additional bit allows us to represent a smaller increment or division of the value.  For instance, to represent 1100\\dfrac{1}{100}1001​ or 0.010.010.01, we can choose 8 fractional bits. We can represent it in: 0.000000110.000000110.00000011 = 0+0+0+0+0+0+127+1280 + 0 + 0 + 0 + 0 + 0 + \\dfrac{1}{2^7} + \\dfrac{1}{2^8}0+0+0+0+0+0+271​+281​ = 0.011718750.011718750.01171875, which differs by 0.001718750.001718750.00171875 from 0.010.010.01.  note This was calculated by hand, there is probably better way to approximate 0.010.010.01.  Floating Number Representation​  The floating number representation, also known as the IEEE 754 standard, is the widely used standard to represent fractional value more accurately. It also provides a wider range of representable numbers compared to fixed-point representation such as more accuracy to represent rational and irrational number and able to represent special values such as positive and negative infinity.  Scientific Notation​  Scientific notation is a way to express numbers that are very large or very small in a concise and standardized format. It is commonly used in science, engineering, and mathematics to represent numbers with many digits.  In scientific notation, a number is expressed as the product of a coefficient and a power of 10. The coefficient is a decimal number greater than or equal to 1 and less than 10, and the power of 10 indicates the scale or magnitude of the number.   Source : https://byjus.com/maths/scientific-notation/  Part of Floating Number &amp; Precision​  Scientific notation provide a framework for floating number representation. In the floating-point standard, we are going to represent decimal numbers using scientific notation. This involves three components: the coefficient (often called as mantissa or significand), the base (typically 2 for binary representation), and the exponent. We will also have single bit that stores the sign bit (indicates positive or negative number).  Still, some numbers cannot be exactly represented due to the limitations of the finite number of bits available. The floating-point standard specifies that we can choose two kinds of precision :  Single Precision : Uses 32-bit to represent a single number, this format allocates 1 bit for the sign, 8 bits for the exponent, and 23 bits for the mantissa, providing 7 decimal digits of precision.Double Precision : Uses 64-bit to represent a single number, this format allocates 1 bit for the sign, 11 bits for the exponent, and 52 bits for the mantissa, providing 16 decimal digits of precision.   Source : https://www.geeksforgeeks.org/ieee-standard-754-floating-point-numbers/  Special Cases​  The floating point standard have some special cases to handle special number :  Zero : All bits in the mantissa are set to zero, and the exponent is also set to zero. It represents a value of exactly zero.Infinity : All the exponent consists of all 1s, and the mantissa is all zeros. It represents an overflow or a result that is too large to be represented within the range of the floating-point format. Positive and negative infinity are used to represent values that are larger or smaller than the maximum or minimum representable values, respectively.Not-a-Number (NaN) : NaN is a special case that represents an undefined or nonsensical result. It is typically used to indicate operations that are mathematically undefined, such as dividing zero by zero or taking the square root of a negative number. NaN values have a specific bit pattern in the exponent and significand that distinguishes them from other floating-point numbers.  Normalization​  Normalization is the process that provides a standardized way to represent floating-point numbers, which helps to avoid ambiguity and ensures consistent interpretation of the numerical value.  Normalizing number ensure that only one non-zero digit exist to the left of the binary point. For example in decimal number, the number 53.75×10053.75 \\times 10^{0}53.75×100 can be normalized to 5.375×1015.375 \\times 10^{1}5.375×101 or even 0.5375×1020.5375 \\times 10^{2}0.5375×102.  There are two types of normalization :  Explicit​  In explicit normalization, we represent the float number in scientific notation by moving the decimal point to the right or left until there is only one non-zero digit to the left of the decimal point.  For 0.003450.003450.00345, to normalize it explicitly, we can move the decimal point three places to the right, resulting in the number 3.45×10(−3)3.45 \\times 10^{(-3)}3.45×10(−3).  Implicit​  In implicit normalization, the float number is represented in a normalized form without explicitly indicating the position of the decimal point. Instead, the decimal point is assumed to always be immediately to the right of the leftmost non-zero digit.  For 0.003450.003450.00345, in implicit normalization, we directly represent it as 345×10(−5)345 \\times 10^{(-5)}345×10(−5), assuming the decimal point is right after the leftmost non-zero digit.  Example​  Representing the number 3.143.143.14 in floating point standard :  Determine the sign bit : The number 3.143.143.14 is positive, so the sign bit would be 000. Convert 3.143.143.14 to binary : It becomes approximately 11.0010001111010111000010111.0010001111010111000010111.00100011110101110000101. Normalize the binary representation : We can use explicit normalization, we will move binary point to the left until there is only one non-zero digit to the left of the binary point. This results in 1.100100011110101110000101×211.100100011110101110000101 \\times 2^11.100100011110101110000101×21. Determine the exponent : We get that the exponent is 111. In general, the exponent can be negative, but we do not allocate a sign bit for the exponent. If we use 8 bits to represent the exponent, we can represent 256 different exponents ranging from 1 to 254 (0 and 255 have special meanings). To represent negative exponents, we need to adjust the range of the exponent. The standard specifies that we subtract a bias value (often called the bias exponent) from the true exponent. For single precision, the bias value is 127, which makes the exponent range from -126 to +127. To convert the exponent to binary, we need to do the opposite. Since the true exponent is 111, the biased exponent that will be used is 1+127=1281 + 127 = 1281+127=128. The number 128128128 in decimal is equal to 100000001000000010000000 in binary. To interpret this, we subtract 127127127 (the bias) from the exponent in binary representation: 128−127=1128 - 127 = 1128−127=1, which is the true exponent. Determine the mantissa : The mantissa is just the right-hand side of the normalized binary representation, which is the 100100011110101110000101100100011110101110000101100100011110101110000101. Putting it all together : Combining the sign bit, exponent, and mantissa, we can represent 3.143.143.14 in binary using floating point standard with : 000 100000001000000010000000 100100011110101110000101100100011110101110000101100100011110101110000101. ","version":"Next","tagName":"h3"},{"title":"Number System","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/number-system","content":"","keywords":"","version":"Next"},{"title":"Decimal​","type":1,"pageTitle":"Number System","url":"/cs-notes/computer-and-programming-fundamentals/number-system#decimal","content":" The decimal number system is the most widely used number system in everyday life. It is a base-10 system, meaning it uses 10 different symbol to write a number, which are 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9.  From the positional number system explanation, we can conclude that each digit represent the product of its value and the base raised to the power of its position.   Source : https://www.unm.edu/~tbeach/terms/binary.html  The digit 8 is in the rightmost position, representing the ones place. Its value is 888, which is equal to 8×1008 × 10^08×100.The digit 7 is in the tens place. Its value is 707070, which is equal to 7×1017 × 10^17×101.The digit 5 is in the hundreds place. Its value is 500500500, which is equal to 5×1025 × 10^25×102.The digit 3 is in the thousands place. Its value is 300030003000, which is equal to 3×1033 × 10^33×103.  The power of the base (or base-10 here), will increases by 1 as we move from right to left, and decreases by 1 as you move from left to right.  ","version":"Next","tagName":"h3"},{"title":"Binary​","type":1,"pageTitle":"Number System","url":"/cs-notes/computer-and-programming-fundamentals/number-system#binary","content":" The binary number system is a number system widely used in computer systems and digital electronics. It is a base-2 system, where numbers are represented using only two digits: 0 and 1 (each digit is often called bit). This system is favored due to its simplicity and its alignment with the characteristics of electronic circuits. Electronic circuits possess two distinct states, namely on and off, or high and low voltage. We can represent the binary digit of 1 as a high voltage level, while 0 is associated with a low voltage level.  Similar to decimal, it is a positional number system, where each binary digit represent the power of its base which is the power of two. It starts from 202^020 from the rightmost digit and the power increase by 1 as we go to the left.  For example :  The binary number 1010 represents (1×231 × 2^31×23) + (0×220 × 2^20×22) + (1×211 × 2^11×21) + (0×200 × 2^00×20), which simplifies to 8 + 0 + 2 + 0, resulting in the decimal value 10. The binary number 11100 represents (1×241 × 2^41×24) + (1×231 × 2^31×23) + (1×221 × 2^21×22) + (0×210 × 2^10×21) + (0×200 × 2^00×20), which simplifies to 16 + 8 + 4 + 0 + 0, resulting in the decimal value 28.   Source : https://www.unm.edu/~tbeach/terms/binary.html  ","version":"Next","tagName":"h3"},{"title":"Hexadecimal​","type":1,"pageTitle":"Number System","url":"/cs-notes/computer-and-programming-fundamentals/number-system#hexadecimal","content":" Hexadecimal is a number system in base-16, it is also commonly used number system in computer science and digital electronics. It uses a total of 16 digits to represent numbers: 0-9 and A-F, where A represents the decimal value 10, B represents 11, and so on up to F representing 15.  Similar to the decimal and binary systems. The positions are counted from right to left, starting with the ones place, then moving to the 16s place, the 256s place, and so on. Each position represents a power of 16.  For example, the hexadecimal number 2F8 can be expanded as follows :  The rightmost digit, 8, is in the ones or 16016^0160 place and its value is 8.The middle digit, F, is in the sixteens or 16116^1161 place. In decimal, F represents the value 15, so its value in the sixteens place is 15 multiplied by 16116^1161, which is 240.The leftmost digit, 2, is in the 256s or 16216^2162 place. Its value is 2 multiplied by 16216^2162, which is 512.   Source : https://drstienecker.com/tech-332/1-numbering-systems-and-conversions/   Source : https://kenanhancer.com/2020/08/25/binary-hexadecimal-and-decimal-number-systems/ ","version":"Next","tagName":"h3"},{"title":"Operating System","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/operating-system","content":"","keywords":"","version":"Next"},{"title":"Primary Function​","type":1,"pageTitle":"Operating System","url":"/cs-notes/computer-and-programming-fundamentals/operating-system#primary-function","content":" In theory, it is possible for a computer to function without an operating system (OS) when it's used by a user to run an application. However, it would be highly complex, time-consuming, and impractical for general-purpose computing. Operating systems provide a crucial layer of abstraction and functionality that simplifies the development and usage of computer systems.  The primary function of OS includes :  Process Management : The operating system manages the execution of software programs, known as processes. The OS will schedule when should a program be executed, considering factors like priority, fairness, and resource availability. Resource Management : An OS allocates system resources, such as CPU time and memory, to different processes, setting up data structures, and initializing the process's execution context. File System Management : Provide an abstraction layer to handle file in computer including organizing, storing, and retrieving data on storage devices, such as hard drives, solid-state drives. Device Management : OS provides device driver, which is a software that act as an intermediaries between the computer's hardware devices and the higher-level software or applications running on the system. User Interface : Provides a user interface through which users can interact with the computer system. This can be a simple command-line interface (CLI) or a graphical user interface (GUI). Security and Protection : The OS implements security and protection to protect the system and user data from unauthorized access. It provides user authentication and isolation between processes to maintain system integrity. Networking : Provide network protocols, drivers, and network stack implementations for data transmission and network services.   Source : https://ipwithease.com/what-is-operating-system-and-its-functions/  ","version":"Next","tagName":"h3"},{"title":"Kernel​","type":1,"pageTitle":"Operating System","url":"/cs-notes/computer-and-programming-fundamentals/operating-system#kernel","content":" The core component of an OS is the kernel. A kernel is lower-level component in OS that directly interacts with the computer's hardware, managing system resources and providing an interface for software applications to access those resources. It handles low-level tasks such as process and thread management, memory management, device drivers, interrupt handling, and security mechanisms.  While an OS is a broader concept that encompasses the entire software package that manages and controls a computer system.  ","version":"Next","tagName":"h3"},{"title":"Program Execution​","type":1,"pageTitle":"Operating System","url":"/cs-notes/computer-and-programming-fundamentals/operating-system#program-execution","content":" A program can be run by computer is called an executable (EXE file on Windows OS). An executable file contains machine code, which is a set of instructions that can be directly executed by the computer's CPU.  The OS will allocates memory and initializes system resources for the program that wants to be executed. The program will have some entry point, indicating the initial state of the program (e.g., the main function in C/C++ programs).  The OS creates a process to represent the executing program, the process will have important information, such as its memory allocation, execution state, open files, and other relevant details. The OS then executes the program's instructions sequentially, following the logic and flow defined by the programmer.  When the program involves system calls such as writing data to a file, the OS will handle the requested operations on behalf of the program, and returns the results back to the program. When an error occurs in the program, the OS play a role in handling and managing the error. The OS will have some default exception handler to handle the errors, and it may generate error messages for the users to provide information about the error.  ","version":"Next","tagName":"h3"},{"title":"UNIX​","type":1,"pageTitle":"Operating System","url":"/cs-notes/computer-and-programming-fundamentals/operating-system#unix","content":" UNIX is a portable, multitasking, and multi-user operating system.  Some characteristics of Unix :  Multitasking &amp; Multi-user : Unix is able to run multiple processes concurrently, it also provides a multi-user environment, multiple users can log in and use the system simultaneously, with each user having their own account and privileges.Portability : Unix separates the kernel or the core of the operating system from the hardware, making it easier to adapt Unix to different computer architectures.Hierarchical File System : Unix organizes files and directories in a tree-like structure. At the top of the hierarchy is the root directory (&quot;/&quot;), which serves as the starting point for the entire file system. All other directories and files are located within the root directory or its subdirectories. For example, &quot;/home/user/Documents&quot; represents the path to the &quot;Documents&quot; directory located within the &quot;user&quot; directory, which is in turn located within the &quot;home&quot; directory.User Interface : Unix systems provides a command-line interface, an interface for users to interact with the operating system by typing commands.  Popular operating system such as Linux and macOS are considered as Unix-like, meaning they have similarities with the original Unix operating system.  tip Refer to operating system for more detailed explanation ","version":"Next","tagName":"h3"},{"title":"Query Language","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/query-language","content":"Query Language Main Source : SQL - Wikipedia Query Language is a computer language used to communicate with and retrieve information from databases. A database is a collection of data, which can be categorized as either relational, where data is organized into tables with rows and columns, or non-relational, where data do not use fixed tabular structure and is instead unstructured, often used to store rapidly changing data. Query languages are considered as declarative programming, they focus on specifying the desired outcome or result rather than the detailed steps or procedures to achieve that outcome. Query language uses statements to command the database system to retrieve specific outcomes or perform certain operations. A query language can have keyword that indicate the type of operation to be performed, provide additional information to the operation, expression that define calculation or transformation that will be applied to data, and some conditional logic to adjust the operation based on specific condition. Last but not least, query language may have their own syntax rules and conventions that define how the keywords, expressions, or other operators should be structured. For example, consider a scenario where we need to retrieve product data from a shop database. We can use GET keyword to retrieve data, we can also provide additional information to select product by the provided name. Here is an example of the statement : GET Product name = &quot;Soap&quot; (just a fictional language). The keyword GET specify the specific type of operation, the Product denotes that we are retrieving product-related data, the expression name = &quot;Soap&quot; that uses an = symbol specify the condition for retrieving data with the name of &quot;Soap&quot;. That was just a simple example of a query language, the most popular query language especially for relational database is the Structured Query Language (SQL). Here is an example of a SQL statement : SELECT * FROM Customers WHERE Country = 'USA'. The SQL statement is using the SELECT keyword to retrieve data from a table named &quot;Customers&quot;, the asterisk * represents all columns in the table. The WHERE clause is used to specify a condition, in this case, to retrieve only the rows where the &quot;Country&quot; column value is equal to 'USA'. In summary, this statement will return all customer from the Customers table where their country is 'USA'. Source : https://www.w3resource.com/sql/sql-syntax.php Another example of SQL statement which is selecting only the &quot;country_id&quot; and &quot;country_name&quot; from &quot;countries&quot; table, where their &quot;region_id&quot; is 1 and the column should be ordered by &quot;country_name&quot;. tip Find out more about database system.","keywords":"","version":"Next"},{"title":"Memory","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/memory","content":"","keywords":"","version":"Next"},{"title":"Terminology​","type":1,"pageTitle":"Memory","url":"/cs-notes/computer-and-programming-fundamentals/memory#terminology","content":" Stack​  The stack is a region of memory used for the management of function calls and local variables. It operates on the principle of stack data structure. Each time a function is called, a new stack frame is created and pushed onto the stack. The stack frame contains information such as function parameters, return address, and local variables.  As functions complete their execution, their stack frames are popped off the stack, allowing the program to return to the calling function. This mechanism enables the program to manage the flow of execution and maintain the context of function calls.  The stack size is typically fixed and limited, determined by the compiler or the operating system. However, the stack space is generally small and can be quickly exhausted if the program uses deep recursion or has large local variables.   Source : https://stackoverflow.com/questions/32418750/stack-and-heap-locations-in-ram  Heap​  A heap (not heap data structure) is a free store or a pool of memory used to store resources like data structures, objects, and variables that have a dynamic lifetime. By dynamic, it means those resources are stored in memory based on specific needs and conditions within the program. Those resources may grow larger or shrink depending on the program, therefore they require a flexible memory allocation.  Unlike the stack, where the memory is limited and cannot be dynamically expanded at runtime, the fixed size make it not suitable to store data whose size is not known in advance or can vary significantly such us user's input.  The stack is faster than heap for some reasons :  Stack is a LIFO structure, most recently allocated memory is at the top of the stack and can be accessed quickly.The fixed size of stack makes it easy to allocate memory, we can easily move up or down the size by a fixed amount.  Heap requires additional bookkeeping, or it requires maintaining additional information to keep track of allocated and deallocated memory blocks. When a memory block is allocated, the heap manager needs to find a suitable free block of the requested size and update the bookkeeping information accordingly.   Source : https://www.javatpoint.com/stack-vs-heap-java  Buffer​  A buffer is a temporary storage area used to hold data while it is being transferred between different devices or processes. It acts as an intermediate storage space, allowing for smoother and more efficient data transfer.  For example, in video playback, we often hear the term &quot;buffering&quot; when there are delays or interruptions during the playback. The buffering process involves temporarily storing video's frame in the buffer. Playing each individual frame immediately as it arrives will make the playback looks rough, instead the video player stores the frames in a buffer until a certain amount of playback time is accumulated.  Another usage for buffer is to improve retrieval efficiency in I/O (input-output) operation. Accessing data stored in the storage (e.g., hard drive) can be slower than accessing to computer's main memory (e.g., RAM). When reading a file, the computer can load a chunk of data from hard drive into the buffer, then the application will load the data from the buffer rather than directly accessing the storage device.   Source : http://www.planetoftunes.com/computer/caching-and-streaming.php  Mutability​  Mutability refers to whether an object, data structure, or a variable can be modified after it is created. An object that can be modified after a value is assigned is called mutable, while the opposite is called immutable.  In memory, when a mutable object is created, a block if memory is allocated to store its contents. When we change the value of the object, we are modifying it in place, and the memory address remains the same.  On the other hand, when someone tries to change an immutable object, the particular address that holds the value won't be changed, instead, a new object with the modified values is created, and a new memory location is allocated to store the new object.  Here is an example in code :  a = 5 a = 3   When we assign a = 5, a memory object representing the value 5 is created and stored. The variable a is then associated with this object. Later, when we assign a = 3, assuming numbers are immutable, a new memory object representing the value 3 is created. The variable a is updated to reference this new object. The original object representing 5 remains in memory until it is cleaned up by the garbage collector or manually released in lower-level programming languages.  Pointer &amp; Reference​  A pointer is a variable that stores the memory address of another variable. It &quot;points&quot; to the location in memory where the actual data is stored.  A reference is a variable that &quot;refer&quot; to other variable. Reference provides an alternative name or alias for an existing object.   Source : https://youtu.be/sxHng1iufQE?si=qfog89ZUE5UIXQsB&amp;t=463 (cropped and edited)  Example​  Here's an example taken from a YouTube video :  In the line 1, we are declaring a variable named ptr which is a pointer (marked by * symbol), that pointer will &quot;point&quot; to a memory address of an int. Currently, it doesn't point to anything yet.In the line 2 and 3, we are declaring two variable that holds an int, named var and foo, respectively. Memory will be allocated to store these values, and both variables will hold their respective values.We are assigning an address to ptr variable, the value will be &amp;var. The symbol &amp; is the &quot;address-of&quot; operator, when we say &amp;var, it means we are taking the address of var variable. Based on the image, the ptr will now store the address of var variable, which is 0xA.We are changing the address of ptr variable to &amp;foo. Now, ptr will hold the address of foo variable, which is 0xB.In the line 5, we are creating a variable that named ref that holds reference to an int variable, which is var. The ref variable will have the same exact address as var.  Purpose of Pointer​  Consider the following code :  int x = 5; int* ptr = &amp;x; *ptr = 10;   x is an int variable with a value of 5, while ptr is an int pointer that stores the memory address of x. When we say *ptr = 10, we are directly accessing the memory address held by the ptr variable, which is the address of x, and updating the value at that address to 10. As a result, any variable associated with that address, such as x, will also be modified. Using pointer, we can effectively modify other variables indirectly through their shared memory address.  Pointer which stores memory address allows us to have direct memory access. This is useful for implementing data structures like tree or linked list, where each node need to connection with other node. We can allow connection between node by having a pointer that points to other node's address.  Purpose of Reference​  Consider the following code :  int a = 5; int b = a;   In this code, we are declaring an int variable named a with the value of 5. In the next line, we are declaring another int variable named b with the value of variable a. When we assign a variable with the value of another variable, we are essentially taking the copy of that variable and save it in the new variable. Because it's just a copy, modifying b will not affect the value of a, and vice versa.  int a = 5; int&amp; b = a;   However, in this code we added &amp; symbol, which mean we are creating a reference instead of just copying the value. When we make change, either to variable a or b, both variable will be changed. This is because a reference refer to the same memory location.  Some purpose of using reference :  We can create another name for a variable to make code more readable and expressive.In situations where multiple instances of a value are required, there is no need to create separate copies explicitly. Instead, we can utilize a single value by creating references, each instance will point to the same value stored in a single location. This approach simplifies the process of synchronizing all instances since modifying the referenced value will automatically update all the instances, reducing the processing resources required.  Type of References​  The two important type of reference :  Strong Reference : A strong reference is the default type of reference in many programming languages. It keeps an object in memory as long as there is at least one strong reference pointing to it. As long as there are active strong references, the object will not be garbage-collected. Weak Reference : A weak reference is a type of reference that does not prevent the object from being garbage-collected. In other word, when a variable have weak reference to an object, we can't guarantee that the variable will always contain that object, as the object may be garbage-collected or cleaned from the memory. If the object is garbage-collected and we are accessing the variable, we may get null value.  There are also soft, phantom, and unreachable reference.  Object​  Object is a fundamental concept in OOP, which mean a particular instance of a data structure or a variable that has its own state (data).  Null​  Null, also known as nil, is a special value that represents the absence of a value. It is often used to indicate that a variable or pointer does not currently point to any valid data.  ","version":"Next","tagName":"h3"},{"title":"Memory Management​","type":1,"pageTitle":"Memory","url":"/cs-notes/computer-and-programming-fundamentals/memory#memory-management","content":" Memory Allocation​  Memory allocation refers to the process of assigning and reserving a portion of the computer's memory for the storage and management of data during program execution. It involves determining the size and location of memory blocks that will be used to store variables, objects, data structures, and other program components.  For example, memory allocation on the heap is typically done explicitly by the program. When we store some data on the memory, we need to specify the size of memory needed, and the system finds a suitable block of memory to accommodate that size. When some data is no longer needed, the program needs to release the memory to avoid memory leaks, which a scenario when a memory is allocated but not released, leading to inefficient memory usage.  Garbage Collection​  Garbage collection is an automatic memory management technique to automatically reclaim memory that is no longer needed by the program. Its primary purpose is to free up memory occupied by objects that are no longer reachable or referenced by the program.  The garbage collector knows a data or an object is no longer needed by the program using a technique called reference counting. It is a technique where each object stores a count and keeps track of the number of references or pointers pointing to that particular object. When the reference count of an object reaches zero, it means that no references exist to that object, indicating that it is no longer reachable and can be safely deallocated.   Source : https://book.huihoo.com/data-structures-and-algorithms-with-object-oriented-design-patterns-in-java/html/page421.html (with modification)  Virtual Memory​  Virtual memory is a memory management technique used by operating systems to provide an illusion of having more memory than is physically available in a computer system.  In a computer system, the physical primary memory (RAM) is finite, and programs require memory to store their instructions and data during execution. Virtual memory allows the operating system to allocate memory to programs in smaller, fixed-size units called pages, the process is called paging. These pages can be stored in both physical memory and secondary storage, such as a hard disk.  Virtual memory logically combines primary and secondary memory to provide large contiguous memory space for programs to access. The OS component is responsible for mapping the virtual address space of a program to the physical memory or disk space, the mapping is stored in a data structure called page table.  When a program accesses a memory location using a virtual address and that particular page is not currently present in physical memory (RAM), a page fault occurs. It could be possible that the required page is stored on secondary memory. To resolve this, the operating system needs to retrieve the required page and load it into physical memory to allows the program to access it.   Source : https://iboysoft.com/wiki/virtual-memory.html  Cache​  Cache is a small, high-speed memory that is located closer to the CPU (central processing unit) than the main memory (RAM). The purpose of cache is to speed up retrieval for frequently accessed resource.  When the CPU needs to read data from or write data to the main memory, it first checks the cache to see if the required data is already present. If the data is found in the cache (called cache hit), it can be quickly accessed by the CPU without the need to access the slower main memory.   Source : https://witscad.com/course/computer-architecture/chapter/cache-memory ","version":"Next","tagName":"h3"},{"title":"Programming Concepts","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/programming-concepts","content":"","keywords":"","version":"Next"},{"title":"Variables & Data Types​","type":1,"pageTitle":"Programming Concepts","url":"/cs-notes/computer-and-programming-fundamentals/programming-concepts#variables--data-types","content":" Variable is something that can that hold values and are identified by a name. They act as storage units within a program, allowing values to be stored and manipulated.  Depending on the programming languages, we can't always manipulate or change the value of a variable. Variables can be mutable or immutable. Mutable variables can have their value changed after assignment, while immutable variables cannot be modified once assigned.  Variable allow programmers to store user input, keep track of program state, and pass values between different parts of the program, etc.   Source : https://dev.to/hermitex/what-is-a-variable-in-programming-5990  A variable holds data, the type of data held can vary. Data types determine which kind of operation can be performed on them. For example, we can perform arithmetic on number but not on characters.  When we declare a variable, memory is allocated to store its value. The size of the allocated memory depends on the variable's data type. When we read or modify a variable, the program will access the corresponding memory address for the variable and retrieve or modify the stored value.  Some common data types are :  Integer (int) : An integer variable is used to store whole numbers without decimal points. A value of integer is determined by its binary representation (e.g., when we store number 5, the binary representation is 00000101). The number of binary digits determines the unique values we can store or the range of number we can represent. In the case of 00000101, which is a 8-bit binary, it can store value ranging from 0 to 255 (28−12^8 - 128−1). Depending on the programming languages, the number of bits used to store an integer can vary. Typically, an integer has fixed size, such as 32-bit (unsigned integer ranges from 0 to approximately 4 billion) or 64-bit (unsigned integer ranges from 0 to approximately 18.4 quintillion). Example of an integer variable : int x = 5, a variable named x has an integer value of 5. Float : A float variable is used to store floating-point numbers, which are numbers with a decimal point. The number of bits represents the precision of the floating representation. In most programming languages, float is typically 32-bits or in floating-point representation, it is a single precision. Example of a float variable : float x = 5.4, a variable named x has a float value of 5.4. Double : A double variable is similar to a float but has a higher precision, it uses 64-bits, and it achieves double precision Example of a double variable : double pi = 3.141592653589793238462, a variable named pi has a double value of 3.141592653589793238462 (double is able to store more decimal point than float). Character : A character variable is used to store a single character. Characters are usually represented using the ASCII or Unicode encoding schemes. The size of a character variable depends on the character encoding used, for ASCII, it can be represented in 8-bits. Example of a character variable : char alphabet = 'a', a variable named alphabet has a character value of a. A character is typically marked using quotation mark, for a single character, we wrap the character in single quote '' Array : An array is a data structure used to store a fixed-size sequence of elements of the same type. When we declare an array, we must specify what data types it's going to store and the size or the number of element we can store inside it. In memory, array is stored in a contiguous block of memory, they are accessed using an index, which represents their position within the array. An index starts from 0 and will increment by 1 for each element in the array. The contiguous and indexing characteristics of an array make it easier to access related elements and allows for fast access when the position of the element is known. Example of declaring an array : int numbers[] = {25, 50, 75, 100}, we are creating array of integers that consist of 25, 50, 75, and 100. To access or modify an element, we need to know the index or position of the element. For example, to access element at index 0 we can use : numbers[0], which gives us 25. To modify element, we can do it similar as assigning a variable, we will access by index and assign it like : numbers[0] = 33, which will modify the element at index 0 to 33. Some programming languages provide array that can be resized, also known as dynamic arrays. When the array needs to be resized, the computer finds a new empty space in the memory, copy existing element in the array to the new space, and then free up old space used by the array. String : A string variable is used to store a sequence of characters (a word). Strings are typically represented as an array of characters. They are used to store text or combinations of characters. A size of a string depends on the number of characters it contains and the character encoding used. If we assume each character require 8-bit and we have 10 character, then the string would require 80 bits or 10 bytes of memory. Example of a string variable : str name = &quot;Poe&quot; (uses double quotation instead of single quote), a variable named name has a string value of Poe. Source : https://www.geeksforgeeks.org/what-is-array/ Boolean : A boolean variable is used to represent logical values, it only has two possible value which are true and false. Boolean are used to control flow of a program based on condition, we can also apply bitwise logical operation such as OR, AND, XOR, etc. While boolean can be represented as a single bit such as 0 (false) or 1 (true), because the CPU can't read a single bit, a boolean is stored as a byte. Example of a boolean variable : bool correct = true, a variable named correct has a boolean value of true.  Some of these data types such as Integer, Float, Double, Character, Boolean are called primitive data types, which means they are basic built-in types provided by the programming language and are not composed of other types. Primitive types are used to represent fundamental data values and are usually the simplest and most efficient way to store and manipulate data.  ","version":"Next","tagName":"h3"},{"title":"Operators​","type":1,"pageTitle":"Programming Concepts","url":"/cs-notes/computer-and-programming-fundamentals/programming-concepts#operators","content":" Operators are symbols or keywords that are used to perform operations on data. When an operator is applied to two elements, it performs a specific operation and returns a resulting value. This value can then be stored in a variable or used in further computations. Some common type of operators :  Arithmetic Operators : Arithmetic operators perform mathematical calculations on numerical data type such as integer. Examples include addition (+), subtraction (-), multiplication (*), division (/), and modulus (%). For example, we have two variables : x = 3 and y = 2. When we use the addition operator by int z = x + y, this mean we are doing the mathematical addition operation on x and y, the result of the operation will be stored in new int variable called z, which should be 5. Relational Operators : Relational operators compare the values of two data and return a boolean result (true or false) based on the comparison. Examples include checking if two elements are equal or not (==), not equal to (!=), greater than (&gt;), less than (&lt;), greater than or equal to (&gt;=), and less than or equal to (&lt;=). For example, we have two variables : x = 3 and y = 2. When we compare two values with operator such as == by x == y, this mean we are checking if x and y are equal. Because both are not equal, the operator should give us false. Whereas if we use the &gt; greater than operator and compare with x &gt; y, this should give us true because the variable x which contain 3 should be greater than value of variable y, which is 2.  ","version":"Next","tagName":"h3"},{"title":"Control Flow​","type":1,"pageTitle":"Programming Concepts","url":"/cs-notes/computer-and-programming-fundamentals/programming-concepts#control-flow","content":" Control flow refers to the order in which statements and instructions are executed in a program. It determines the flow of execution within a program, deciding which statements should be executed next based on certain conditions or criteria.  A program's code typically runs sequentially from top to bottom, following the order in which the statements and instructions are written. By using control flow, we can control how they will be executed, this will allow us to repeat sections of code, or jump to different parts of the program based on specific conditions.  Conditional Statement​  A conditional statement, is a type of control flow that allows a program to make decisions based on the evaluation of a condition.  The most common type of conditional statement used is the if-else statement, which control the program to execute a specific instruction if a condition is met, and if the condition is not met, it executes an alternative instruction. An if statement takes a boolean value and controls the program based on it.  Here is an example of if-else statement :  if (condition) { // Code to be executed if the condition is true // do something here... } else { // Code to be executed if the condition is false // do other thing here... }   In this example, the condition refers to a boolean expression used as the condition.  In real-life scenario, conditional statements are commonly used. For instance, consider a game that determines whether a user can play based on their age. If the user's age is above the minimum requirement, the game will let them play. Otherwise, a notice will be displayed indicating that they are not eligible to play.  Loop Statement​  Loop is a control flow statement used to repeat a block of code. It allows the program to execute a set of instructions multiple times, either for a fixed number of iterations or until a certain condition is met.  For Loop​  A for loop is used when the number of iterations is known beforehand. Here is the syntax of a for loop :  for (initialization; condition; update) { // Code to be executed in each iteration }   Example of a for loop  for (iteration = 0; iteration &lt; 10; iteration = iteration + 1) { // Code to be executed in each iteration }   note iteration is not a keyword required to start a for loop, we can use any name of variable we want.  A for loop will stop until certain condition is fulfilled, however, the condition is typically known beforehand. In the example, the condition is the iteration &lt; 10, this mean we will keep executing the code inside the for loop while iteration variable is below 10. The iteration variable is set in the initialization before the condition, we set it as 0. A single loop is executed when all the instruction inside the for loop block is executed. After a single loop is done, a for loop will have an update step, which is the thing we are doing in order to fulfill the condition. In the example, after every loop, we will increase the iteration variable by 1.  So, putting all together, the iteration variable act as a counter of the number of times we have executed the loop. The number of time we executed the loop is based on the initialization, condition, and update. In the example, we set the counter to 0, every time a loop is finished we will increment it by 1, and this will be executed for 10 times based on the iteration &lt; 10 condition.  While Loop​  A while loop on the other hand, executes block of code inside the loop &quot;while&quot; a condition is met. Here is an example of a while loop :  while (condition == true) { // code to be executed when condition is true }   The code inside will keep being executed while the condition is equal to true. We can also make the loop to be executed while the condition is false.  While loop executes given block of code while the condition is fulfilled, but what if the condition is never fulfilled? This is called infinite while loop, it is a loop that continues to execute indefinitely, without a condition that can terminate it naturally.  So, inside the while loop, at some point we will need to make the condition no longer fulfill the loop condition. A while loop can be beneficial when performing tasks without knowing when they will end.  There is also a keyword called break that will exit a loop even when the condition hasn't been met.  note In most programming languages, we don't need to explicitly say == true, putting the condition inside the bracket like while (condition) is enough. To make the while loop execute while the condition is false, we can negate the boolean of condition. In some programming language, the ! is used to negate a boolean, so by using while (!condition), it is the same thing as while (condition == false).   Source : https://www.guru99.com/c-loop-statement.html  ","version":"Next","tagName":"h3"},{"title":"Functions​","type":1,"pageTitle":"Programming Concepts","url":"/cs-notes/computer-and-programming-fundamentals/programming-concepts#functions","content":" A function is a block of code that performs a specific task or set of tasks. It is a &quot;mini-program&quot; or reusable unit of code that can be called or invoked from different parts of a program. The part of function that contains the set of instructions or statements to complete a task is called the body function.  A function has unique name that identify them and will be used to call within the program. A function is typically called by calling its name with a parenthesis: function().  For example, when we have a task to add two number together, instead of writing the same code, we can make a function that will add two number together, and we can call it to do it for us.  The purpose of function is different with loop, a loop repeats a block of code multiple times. On the other hand, a function encapsulate a task, it decomposes a task into smaller, manageable pieces, each performing a specific task.  A function can also take input, an input provides necessary information for the function to perform its task. A function input is called parameter and the input we are actually passing is called argument.  In the case of adding two number, the function will take input or the parameters of the function are the two number. If we call the function and give it input of 2 and 5, then 2 and 5 are the arguments.  For example, here's how you would define a function in Kotlin programming language :  fun add(num1: Int, num2: Int): Int { return num1 + num2 }   The fun keyword is used when you are defining a function, the add is the name of the function. The function will take 2 parameters, which are num1 and num2. The num1 and num2 are just placeholder name that we will use inside the function, these can be thought as the variables given to us.  In Kotlin, we need to specify the data type of parameters of the function and also what data type will the function returns. To specify, we put : and its type such as Int in front of the parameter name and we need to separate each parameter with a comma. At the end, we also specify the return type similar to the parameter, in the case of adding number, the result will be another number, therefore we will specify : Int.  To define the function body, we use curly brackets {}. Inside the brackets, we will define the function body which contains the instruction of our function. In this case, the instruction is just adding the two parameters using + operator.  The return keyword is the keyword we use to indicate the value that the function will return. The data type of the return value must correspond to the data type we specified in the function. When a return is called, the function will end, and the programmer who call it will receive the result.   Source : https://www.scaler.com/topics/c/user-defined-functions-in-c/  ","version":"Next","tagName":"h3"},{"title":"Debugging & Error Handling​","type":1,"pageTitle":"Programming Concepts","url":"/cs-notes/computer-and-programming-fundamentals/programming-concepts#debugging--error-handling","content":" A bug in software is an error or unexpected behavior in software. Debugging is the process of identifying and resolving which part of the program's code causes the error. Error handling, is the process of handling or anticipating error that may occur during the execution of program. Error handling helps us to recover the program from error and preventing it to terminate abruptly.  Type of Error​  Syntax Error : Syntax errors occur when the code violates the rules of the programming language's syntax. A syntax error typically occurs before the program execution, such as during the process of writing the code. Common examples of syntax errors include misspelled keywords, missing or mismatched parentheses or brackets, incorrect variable names, or improper use of operators.Runtime Error : Runtime errors are the error that occurs during the execution of program. These errors may arise due to unexpected conditions, such as invalid user input (e.g., entering number when the application ask you to enter a character).Logic Error : On the other hand, logic error doesn't involve the error of execution. They occur when the code executes correctly but does not produce the expected or desired result. Logical errors are typically caused by mistakes in the program's logic. For example, in a calculator application, the programmer mistakenly implemented a button intended for adding two numbers to instead perform subtraction between the numbers.  Exception​  An exception is an event or condition that occurs during the execution of a program and disrupts the normal flow of instructions (considered as runtime error). It represents an unexpected situation in program which require to be handled.  When an error occurs, the program raises or throws the exception. When we throw an exception, it typically involves creating an exception object that contains information about the exception, such as the type of exception and additional details.  Try-Catch​  The try-catch is a control flow mechanism that allows us to execute some code and handle the exception if an error occurs. It consists of a try block, where the regular program flow is contained, a catch block to handle exceptions, and an optional finally block for code that should execute regardless of whether an error occurs or not.  Here is an example of try-catch block in the Kotlin programming language :  try { // user should input number } catch (e: InvalidInput) { // user inputted character, send alert message to them } finally { // send a &quot;message received&quot; message to user }   In the catch block, we will need to specify what kind of error is expected to happen. For example, the type of error is the InvalidInput, which tells us that user has invalid input. We can also anticipate multiple kind of error by specifying more catch block. If the error we get is different to what we specified, then the catch block may be ignored, unless the type of error is a more general type or if it includes a broader exception type.  The finally block will send a &quot;message received&quot; message to the user, to indicate that their input has been acknowledged.  When an error is thrown and is not handled, the program executor will try to find which part of the program is supposed to handle the error. The program will keep finding the error handler up to the part of code that responsible for calling or invoking the function that makes the error happens, this is called error propagation or call stack unwinding. If we reach the highest-level point and error handler is still not found, the program may crash abruptly.   Source : https://www.wikitechy.com/tutorials/java/java-try-catch-finally-blocks ","version":"Next","tagName":"h3"},{"title":"Object-Oriented Programming","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/object-oriented-programming","content":"","keywords":"","version":"Next"},{"title":"Class & Object​","type":1,"pageTitle":"Object-Oriented Programming","url":"/cs-notes/computer-and-programming-fundamentals/object-oriented-programming#class--object","content":" Object​  Objects are the building block of OOP, object is a representation of an entity which has its own characteristics and behavior. The characteristics of an object is called attributes or properties and the behavior or the action it can perform is called methods.  In OOP, we model an object based on real-world entities, consider making a Bird object.  A bird can have attributes :  Species : The species attribute represents the specific type of the bird, such as &quot;sparrow,&quot; &quot;eagle,&quot; or &quot;parrot.&quot;Color : The color attribute represents the color of the bird, such as &quot;red,&quot; &quot;blue,&quot; or &quot;green.&quot;Weight : The weight attribute represents the mass of the bird. It provides information about the bird's size and physical condition.  Attributes provide a way to store data and represent the state of an object.  A bird can have methods :  Fly : A bird can fly, a fly method simulates the bird's ability to fly.Sing : The sing method represents the bird's ability to produce sounds or songs.Eat : The eat method simulates the bird's feeding behavior.  Methods are the procedures or functions associated with an object. They define the behavior or actions that an object can perform.   Source : https://blog.glugmvit.com/oops/ (cropped)  Class​  In OOP, a class defines a set of attributes and methods an object can have. A class can be thought as the blueprint or template to make an object. An object needs to created from a class, the using a process called instantiation. When an object is instantiated from a class, it is referred to as an instance or an object of that class. Class provide a generalized description of what objects of that class should look like.  Class vs Object​  Consider a class Car, it is a blueprint to create a car object. Car class might have attributes such as model and year, which represent the specific model and manufacturing year of a car. It could also have methods like start_engine(), accelerate(), and brake() to represent the actions a car can perform.  Whereas, an object is the actual content of a class, we should define the actual data and behavior it has. For example, we can make object called MyCar, which is the instance of Car class. It is created based on the Car class blueprint. For instance, the MyCar object could have attribute values like model=&quot;Camry&quot;, and year=2022, indicating that it is a Toyota Camry manufactured in 2022. The object will have method defined in the Car class, such as start_engine() to start the car's engine, accelerate() to increase its speed, or brake() to apply the brakes.   Source : https://techvidvan.com/tutorials/java-class/ (with modification)  ","version":"Next","tagName":"h3"},{"title":"Concepts​","type":1,"pageTitle":"Object-Oriented Programming","url":"/cs-notes/computer-and-programming-fundamentals/object-oriented-programming#concepts","content":" The principles and concepts of OOP are designed to mimic real-world objects and their interactions. For example, in a banking system, you can have classes like &quot;Customer,&quot; &quot;Account,&quot; and &quot;Transaction.&quot;  In customer class we might define attributes and methods such as :  Attributes : name, address, contact information, unique customer IDBehaviors : getCustomerID(), getName(), getAddress(), getContactInfo().  By organizing code using OOP, we can achieve a natural and intuitive way to structure and manage software systems.  OOP doesn't only revolve around classes and objects, there are more concepts in OOP.  Abstraction​  Abstraction is the process of hiding internal details of a system and instead presenting a simplified and high-level view. Abstraction is typically achieved through :  Interface​  Interface is a set of contract that defines a set of attributes and methods that a class must implement. It defines what are the attributes and methods a class should have. An interface doesn't provide concrete example of how a class should look like, it simply provides rules that a class should follow.  For example, an interface Vehicle is a set of contracts that defines the attributes and methods that a class considered as a vehicle should implement. Class such as Car or Bike is considered as a vehicle. A Vehicle interface may have a accelerate() method, which is used to increase the speed of the vehicle. However, each vehicle may have different behavior on how to accelerate, for instance, car accelerates by adjusting engine power, while a bike accelerate by increasing the pedaling force.  An interface, which doesn't provide any concrete example of a class can't be instantiated directly, an interface must be implemented by a class.  In abstraction, an interface provide a way to focus on the characteristics of a class. We don't care how a car or a bike differs in terms of accelerating, what we care is they must be able to accelerate.  Abstract Classes​  Abstract Classes is similar to interface with some distinctions and additional features compared to interfaces. An interface, can't have any default implementation at all, it can only declare what a class should look like. On the other hand, an abstract class can have default or partial implementation of a class.  Abstract class can't be instantiated directly and is meant to serve as a base for concrete classes. The ability to declare default implementation makes abstract classes have several advantages over an interface. An abstract class can define common behavior and characteristics that are shared across its subclasses.   Source : https://www.scaler.com/topics/abstraction-in-oop/  Encapsulation​  Encapsulation is the process of encapsulating attributes or methods of an object. It promotes the concept of visibility modifier. The main idea of encapsulation is to hide an object's state or behavior from external code.  Visibility Modifier​  Visibility modifier is a keyword that determines the accessibility and visibility of class members. A class member refers to the components of a class, including its attributes and methods.  There are typically two visibility modifiers used in OOP :  Public : A public keyword is used to make class members accessible from anywhere. External code can interact and modify the detail of an object.Private : A private keyword is used to make class members accessible only within the class in which it is declared. It cannot be accessed or modified from outside the class.  By using private modifier, we can prevent external code from making unauthorized changes or accessing sensitive information. For example, in the image below, we make an object attributes to be private and instead define methods to make them accessible to external code. Inside the method, we can define how should the external code interact with it.   Source : https://www.crio.do/blog/encapsulation-in-java/  Inheritance​  Inheritance is a concept in OOP that allows a class to inherit the properties and behaviors of another class. The inherited class is called the superclass (also called a base or parent class) and the class that inherits is called a subclass (also called a derived class or child class).  The difference between interface and inheritance lies in their respective functionalities. Inheritance enables the acquisition or &quot;copying&quot; of attributes and methods from a class, while interfaces serve as contracts for classes and are particularly suited for scenarios where classes have distinct characteristics. In situations where a class shares the exact implementation of another class but with additional features, inheritance promotes code reuse and the organization of related classes into a hierarchical structure.   Source : https://towardsdatascience.com/how-to-code-inheritance-in-java-beginners-tutorial-in-oop-d0fc0a71be98  For example, in the image below, a Dog class may have similar method with the general Animal class. The Dog class can inherit the Animal class, with another method addition, which is the bark() method.   Source : https://www.programiz.com/cpp-programming/inheritance  With the addition of inheritance and interface concept, we can introduce another visibility modifier :  Protected : A protected keyword is used to make class members accessible within the class where it is declared, as well as in its subclasses (derived classes). In other word, it is a public modifier which is exclusive to a class hierarchy.  Polymorphism​  Polymorphism is a concept that allows us to treat a single object in different ways. It allows us to have interface with many implementations. Polymorphism is related to the interface and inheritance concept.  We can define a method that will be implemented by its subclasses. For example, a base class called Shape and two derived classes called Circle and Rectangle. Each derived class has its own implementation of a method called calculateArea(). Polymorphism concepts enable us to write a single piece of code that can invoke a function responsible for calculating the area of a shape, regardless of whether the shape is a circle or a rectangle. The specific implementation of calculateArea() that will be used for our calculation will be determined dynamically at runtime based on the type of the object being used.  There are two types of polymorphism :  Compile-time Polymorphism (Static Polymorphism) : This polymorphism is resolved in compile-time, one of the example is method overloading. It is a phenomenon that occurs when multiple methods with the same name, but different parameters are defined in a class. The appropriate method to be executed is determined at the compile-time based on the number, type, and order of the arguments passed. For example, a Calculator class that has method to add number, there may be two separate method, where each method differ in how many parameters or how many numbers it can take. Here is an example of code in Kotlin programming language : class Calculator { fun add(num1: Int, num2: Int): Int { return num1 + num2 } fun add(num1: Int, num2: Int, num3: Int): Int { return num1 + num2 + num3 } } In the Calculator class, we have two method with the same exact name, when we call the add() function, depending on the argument we passed in, the appropriate add() function which will be called will be determined at compile-time.  note Compile-time is period in which code is being translated into machine language that computer can understand.  Runtime Polymorphism (Dynamic Polymorphism) : This polymorphism is resolved in rutime, one of the example is method overriding, which occurs when a subclass provides a specific implementation of a method that is already defined in its superclass. The decision of which method implementation will be executed determined at runtime (at the program execution) based on the actual type of the object being referenced. // open indicates a class, attributes, or methods is inheritable open class Shape { open fun draw() { println(&quot;Drawing a shape.&quot;) } } // The &quot;: Shape()&quot; indicates it is inheriting the Shape class class Circle : Shape() { // &quot;override&quot; indicates it is replacing the superclass or the Shape class methods override fun draw() { println(&quot;Drawing a circle.&quot;) } } class Rectangle : Shape() { override fun draw() { println(&quot;Drawing a rectangle.&quot;) } } fun main() { // Instantiating a class is similar to calling a function val shape1: Shape = Circle() val shape2: Shape = Rectangle() shape1.draw() // Calls the draw() method of the Circle class shape2.draw() // Calls the draw() method of the Rectangle class } In this example, we have a Shape superclass with Circle and Rectangle subclasses. Both classes inherit to Shape and implement their own method using the override keyword. The main() function is the entry point of a Kotlin program, it is where the program starts being executed. In the main function, we created two variables which has the type of Shape with their own respective implementation, which is the Circle and Rectangle class for shape1 and shape2, respectively. The draw() method has three different implementation, the first in the original Shape class, the second in the Circle, and the third in the Rectangle class. The specific implementation which will be used is determined at runtime based on the actual type of the object. For example, shape1 has the type of Shape, but it actually has the value of the Circle class. When shape1.draw() is called, it will invoke the draw() method of the Circle class rather than the one in the Shape or Rectangle class. Under the hood, the program stores information about the actual type of an object at runtime. This information is used to determine the appropriate method to call. ","version":"Next","tagName":"h3"},{"title":"Runtime Environment","type":0,"sectionRef":"#","url":"/cs-notes/computer-and-programming-fundamentals/runtime-environment","content":"","keywords":"","version":"Next"},{"title":"Java Runtime Environment (JRE)​","type":1,"pageTitle":"Runtime Environment","url":"/cs-notes/computer-and-programming-fundamentals/runtime-environment#java-runtime-environment-jre","content":" Java Runtime Environment (JRE) is the runtime environment for executing Java programs. It consists of several components :  note A Java program in source code (with format of .java), needs to be compiled into bytecode (intermediate representation with format of .class).  Java Class Library : The Java class library is a set of pre-compiled Java's APIs that provide a wide range of functionality for Java applications. It includes standard classes for tasks such as I/O operations, networking, database access, GUI development, concurrency, and more.Class Loader : The class loader is a part of JRE used to dynamically loads Java classes into memory as they are referenced during program execution. It is responsible for locating the compiled bytecode for a class and creating the corresponding Class objects.Bytecode Verification : Bytecode verification is a process that verifies the integrity and security of Java bytecode before it is executed. The process examines the bytecode to ensure it adheres to the rules and constraints specified by the Java virtual machine specification to prevent certain runtime errors and security vulnerabilities.Java Virtual Machine (JVM) : The JVM is the execution engine responsible for executing Java bytecode on different hardware and operating systems. The JVM provides an execution environment that abstracts the underlying system details and provides several key functionalities, including memory management, garbage collection, thread management, exception handling, and runtime support.   Source : https://www.geeksforgeeks.org/jre-full-form/  ","version":"Next","tagName":"h3"},{"title":"Node JS​","type":1,"pageTitle":"Runtime Environment","url":"/cs-notes/computer-and-programming-fundamentals/runtime-environment#node-js","content":" JavaScript is a programming language mainly used for client-side web development. It is a programming language that runs on web browser to manipulate the webpage (HTML) and its style (CSS).  Node JS is a JavaScript programming language runtime environment that allows developers to run JavaScript on the server-side. By enabling JavaScript on the server side, JavaScript can be used to build server-side applications, handle incoming requests, process data, interact with databases, and perform various server-side operations.  tip Another note about Node JS ","version":"Next","tagName":"h3"},{"title":"Computer Graphics","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics","content":"","keywords":"","version":"Next"},{"title":"All pages​","type":1,"pageTitle":"Computer Graphics","url":"/cs-notes/computer-graphics#all-pages","content":" Computer Images (Part 1)Computer Images (Part 2)2D Transformation3D TransformationGPU PipelineLow Level GraphicsCurvesSurfacesTextures (Part 1)Textures (Part 2)Shading (Part 1)Shading (Part 2)Rendering (Part 1)Rendering (Part 2)Ray TracingShadowsReflectionsSamplingSignal ProcessingComputer DisplayComputer AnimationPhysics-Based AnimationSimulation ","version":"Next","tagName":"h3"},{"title":"2D Transformation","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/2d-transformation","content":"","keywords":"","version":"Next"},{"title":"2D Transformation​","type":1,"pageTitle":"2D Transformation","url":"/cs-notes/computer-graphics/2d-transformation#2d-transformation-1","content":" 2D objects are represented using a coordinate system that consists of two axes, usually labeled x and y. Each point in the 2D space is represented using a pair of coordinates (x, y) and indicate the position in the coordinate system.   Source : https://www.cuemath.com/geometry/transformations/   Source : https://www.cs.iusb.edu/~danav/teach/c481/c481_06_trans2d.html ","version":"Next","tagName":"h3"},{"title":"3D Transformation","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/3d-transformation","content":"","keywords":"","version":"Next"},{"title":"3D Transformation​","type":1,"pageTitle":"3D Transformation","url":"/cs-notes/computer-graphics/3d-transformation#3d-transformation-1","content":" 3D objects are represented with 3 axes instead, labeled with x,y, and z.   Source : https://commons.wikimedia.org/wiki/File:Stress_transformation_3D.svg  ","version":"Next","tagName":"h3"},{"title":"Viewing in 3D​","type":1,"pageTitle":"3D Transformation","url":"/cs-notes/computer-graphics/3d-transformation#viewing-in-3d","content":" 3D object can’t be mapped directly to the computer screen, these need to be transformed into a 2D scene, this process is called projection. Camera is used in 3D scene as the viewpoint or perspective from which the scene is rendered onto a 2D surface.View space is a coordinate system that defines the position and orientation of objects in a 3D scene relative to the camera.  There are several technique for projecting 3D scene onto computer screen, such as :  Perspective projection : It simulates the way that objects appear to the human eye in the real world, by creating the illusion of depth and distance. Transformation is applied in view space to better simulate the world.Orthographic projection : Projection technique that does not create the illusion of depth and distance, but instead creates a flat, two-dimensional image of the 3D scene. In orthographic projection, all objects are projected onto the same plane, regardless of their distance from the camera. Transformation is applied in object space and projected into 2D later.   Source : https://stackoverflow.com/questions/36573283/from-perspective-picture-to-orthographic-picture ","version":"Next","tagName":"h3"},{"title":"Computer Animation","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/computer-animation","content":"","keywords":"","version":"Next"},{"title":"Procedural Animation​","type":1,"pageTitle":"Computer Animation","url":"/cs-notes/computer-graphics/computer-animation#procedural-animation","content":" In Procedural Animation, movements and behaviors of objects or characters are often determined by algorithms or rules that are designed to simulate real-world physics or behaviors. In transformating an object, it can use some matrix transformation algorithm.  It could also generate realistic environmental effects like moving water or swaying grass, based on algorithms that simulate the behavior of fluids or plants.   Source : https://www.semanticscholar.org/paper/Procedural-Animation-of-3D-Humanoid-Characters-Bhatti-Ismaili/a00187c3333cfe73ab37b77f81724ae8a3dbe15c/figure/5  ","version":"Next","tagName":"h3"},{"title":"Keyframing​","type":1,"pageTitle":"Computer Animation","url":"/cs-notes/computer-graphics/computer-animation#keyframing","content":" Keyframing uses some specific point called keyframes that is defined as starting and ending points of an animation sequence. Keyframing also involves interpolating to creates smooth animation between the sequence. The smooth transition is often referred as &quot;tweens&quot; or &quot;tweening”.   Source : https://filmora.wondershare.co.id/video-editing-tips/what-is-keyframing.html  A keyframing animation that involves transforming one object or shape into another over time is called Morphing.   Source : https://youtu.be/lhK7ZMcW5pU?t=1971  ","version":"Next","tagName":"h3"},{"title":"Motion Capture​","type":1,"pageTitle":"Computer Animation","url":"/cs-notes/computer-graphics/computer-animation#motion-capture","content":" Motion Capture is technique used in animation and film production to generate animation by capturing the movements of real-life actors or objects and apply them to digital characters or objects. Motion Capture uses specialized cameras, sensors, or markers to track the movements of a person or object.   Source : https://www.mdpi.com/1424-8220/21/18/6115 ","version":"Next","tagName":"h3"},{"title":"Computer Images (Part 1)","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/computer-images-part-1","content":"","keywords":"","version":"Next"},{"title":"Color​","type":1,"pageTitle":"Computer Images (Part 1)","url":"/cs-notes/computer-graphics/computer-images-part-1#color","content":" Human eyes is able to receive color and the 3 primary color are Red, Green, Blue (RGB). Other color can be represented using additive color mixing from these 3 colors.  Computer represent color using binary digit (bits), the most common method is to represent it in RGB format, these color separated is called color channel. The number of bits used for each channel determines the number of possible color values that can be represented. For example, an 8-bit color channel can represent 256 different shades of that color, ranging from 0 (black) to 255 (white).  There are other color models used in computer graphics and imaging, such as CMYK (Cyan, Magenta, Yellow, Black) and HSL (Hue, Saturation, Lightness).   Source : https://expertphotography.com/photoshop-channels/  ","version":"Next","tagName":"h3"},{"title":"Computer Display​","type":1,"pageTitle":"Computer Images (Part 1)","url":"/cs-notes/computer-graphics/computer-images-part-1#computer-display","content":" Computer display produces color by emitting light through tiny pixels on the screen. Each pixel on the screen is made up of three sub-pixels, which emit red, green, and blue light respectively.  There are 2 different type of monitor based on different levels of brightness and color depth :  LDR (Low Dynamic Range), which typically have a brightness range of 100 to 300 bits and a color depth of 8 bits per channel (24-bit color).HDR (High Dynamic Range), HDR monitors typically have a brightness range of 400 to 1000 bits or higher, and a color depth of 10 bits per channel (30-bit color) or higher. ","version":"Next","tagName":"h3"},{"title":"Computer Images (Part 2)","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/computer-images-part-2","content":"","keywords":"","version":"Next"},{"title":"Raster Image​","type":1,"pageTitle":"Computer Images (Part 2)","url":"/cs-notes/computer-graphics/computer-images-part-2#raster-image","content":" An image contains content which is the pixel data, also file header which is contains metadata and information about the image file, such as its format, size, resolution, color space, and compression.  Image pixel data can be stored using 2 types of storage :   Source : https://youtu.be/zllIPDaiOyk?t=1357  Interleaved Storage : Each pixel are stored in single block of memory.Separate Channel : Each color channel is stored in a separate block of memory.  ","version":"Next","tagName":"h3"},{"title":"Image Format​","type":1,"pageTitle":"Computer Images (Part 2)","url":"/cs-notes/computer-graphics/computer-images-part-2#image-format","content":" Image format is file format for digital images. Different image format has their own advantages and disadvantages in terms of quality, file size, compatibility.  Some commonly used format are :  Raster Image Format :  PNG (Portable Network Graphics), a lossless compression also supports transparency.JPEG (Joint Photographic Experts Group), lossy compression and doesn’t supports transparency.GIF (Graphics Interchange Format), a compressed image format that supports animation.   Source : https://99designs.com/blog/tips/image-file-types/  ","version":"Next","tagName":"h3"},{"title":"Gamma​","type":1,"pageTitle":"Computer Images (Part 2)","url":"/cs-notes/computer-graphics/computer-images-part-2#gamma","content":" Human eye does not perceive brightness in a linear way. Instead, our eyes perceive brightness in a logarithmic way, meaning that small differences in brightness are more noticeable in darker parts of an image than in brighter parts.  Gamma is a measure of the contrast between the darker and lighter parts of an image. A higher gamma value means that the darker parts of the image will appear darker, while a lower gamma value means that the darker parts of the image will appear lighter.  Gamma correction is used to compensate for this non-linear perception of brightness and ensure that images are displayed correctly on a wide range of display devices. One of a common gamma correction is 2.2 and 2.4.   Source : https://doc.stride3d.net/4.0/en/manual/graphics/post-effects/color-transforms/gamma-correction.html  ","version":"Next","tagName":"h3"},{"title":"Alpha​","type":1,"pageTitle":"Computer Images (Part 2)","url":"/cs-notes/computer-graphics/computer-images-part-2#alpha","content":" Alpha refer to the transparency or opacity of an image or an individual pixel within an image. Alpha have range from 0 (fully transparent) to 255 (fully opaque).  A usual image would have RGB color channel. If an image supports transparency, there will be one more channel for alpha value. RGB becomes RGBA, alpha can also be stored in separated channel.  A transparent image is able to be combined with the background color. Combining two or more images or layers is called Alpha Blending also called Alpha Compositing.  Alpha blending works by interpolating color between both foreground and background color and alpha value.   Source : https://en.wikipedia.org/wiki/Alpha_compositing  ","version":"Next","tagName":"h3"},{"title":"Blending​","type":1,"pageTitle":"Computer Images (Part 2)","url":"/cs-notes/computer-graphics/computer-images-part-2#blending","content":" Blending is a technique used in computer graphics to combine two or more images or layers by adjusting their color values and transparency.  There are many blending technique that calculate combined image such as adding image color, subtracting, multiplying, etc.   Source : https://www.pinterest.com/pin/7740630584610634/  tip Find out more about image properties in here ","version":"Next","tagName":"h3"},{"title":"Computer Display","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/computer-display","content":"","keywords":"","version":"Next"},{"title":"Type of Computer Display​","type":1,"pageTitle":"Computer Display","url":"/cs-notes/computer-graphics/computer-display#type-of-computer-display","content":" Source : PowerCert  Cathode Ray Tube (CRT)​  CRT is the oldest monitor display, it uses 3 color electron gun (red, green, blue). The electron gun will rapidly fire to the screen creating a combination of colors. The rate of fire or the re-drawing is called refresh rate. For example a refresh rate of 60 Hz means that the display will be re-drawn 60 times per second.   Source : https://youtu.be/yxygknX1AiE?t=101  Liquid Crystal Display (LCD)​  LCD displays is a flat display that consist of a layer of liquid crystal material, in front of that there exists an RGB color filter to produce the different color we need. An LCD monitor needs a light source behind.   Source : https://youtu.be/yxygknX1AiE?t=162  Light Emitting Diode (LED)​  An LED display is a type of LCD display which use light-emitting diodes (LEDs) light as the light source.   Source : https://youtu.be/yxygknX1AiE?t=215  Flat Panel Types​  Twisted Nematic (TN) : TN panels use liquid crystals that twist in response to an applied electric current. TN has a good response time, however, TN panels have limited viewing angles, meaning that color and image quality degrade when viewed from off-center angles. In-Plane Switching (IPS) : IPS uses liquid crystals that move horizontally between two glass plates. They are arranged in a parallel orientation between the glass plates. IPS offers wider viewing angles compared to TN panels, while also ensuring consistent colors and image quality even when viewed from different angles. Vertical Alignment (VA) : VA panels use liquid crystals that align vertically between two glass plates. Liquid crystals in VA panels can tilt and align in different angles to control the passage of light.   Source : https://www.benq.com/en-us/knowledge-center/knowledge/how-to-choose-between-tn-va-and-ips-panels-for-the-games-you-play.html  ","version":"Next","tagName":"h3"},{"title":"Video Interface​","type":1,"pageTitle":"Computer Display","url":"/cs-notes/computer-graphics/computer-display#video-interface","content":" A video interfaaces is used for transferring display signals from a device (such as a CPU or GPU) to a monitor or display. After the CPU and GPU generate graphics, the binary data representing the images or video is sent from the device to the display using one of these interfaces.  VGA : VGA is an analog video display interface meaning it can only transfer analog data, it is commonly found on older computers, monitors, and projectors.  DVI : DVI is made to succeed the VGA, it has 3 types, DVI-A which only sends analog signal, DVI-D that sends digital signal and DVI-I which is able to send both analog and digital signal.  HDMI : HDMI is a popular use nowadays, it transmit high-definition audio, video, and network communication signals in a single cable.  DisplayPort : DisplayPort is primarily used for video, however it can also be used for transfering USB data and audio. DisplayPort is able to send display by chaining from monitors to monitors, it is called daisy chaining.  Thunderbolt : Thunderbolt is relatively new technology released in 2011. Thunderbolt is used in Apple technologies, it offers high bandwidth and up to 6 daisy chaining.   Source : https://www.jagadmedia.id/2020/11/perbedaan-dvi-hdmi-displayport-vga.html  ","version":"Next","tagName":"h3"},{"title":"Display Measurement​","type":1,"pageTitle":"Computer Display","url":"/cs-notes/computer-graphics/computer-display#display-measurement","content":" Both animation and video uses image and the more image used will results in smoother, the measurement of how many image is called frame rate and typically in FPS (Frames Per Second)   Source : https://www.animotica.com/blog/fps-in-video-editing/  The typical FPS for some display are :  Television : North America, NTSC (National Television Standards Committee) - 30 FPSEurope, PAL (Phase Alternate Lines) - 25 FPS Movies : The standard is 24 FPS, sometimes 48 - 120 FPSComputer Monitor : 60 FPS, high-end gaming monitors can support frame rates of 120 FPS or higher.  Refresh Rate​  Computer monitor also have refresh rate, which is the measurement of how many times per second the screen is updated with new image data. It is typically measured in Hertz (Hz), which represents the number of cycles per second.  Common refresh rates for monitors are 60 Hz, 120 Hz, and 144 Hz, although some high-end gaming monitors can support even higher refresh rates, such as 240 Hz or 360 Hz.   Source : https://www.benq.com/en-us/campaign/gaming-projector/resources/gaming-projector-high-refresh-rates.html  ","version":"Next","tagName":"h3"},{"title":"Video Resolution​","type":1,"pageTitle":"Computer Display","url":"/cs-notes/computer-graphics/computer-display#video-resolution","content":" This refers to the number of pixels in an image, typically measured in terms of width and height. It is often expressed as the total number of pixels in the image, such as 1920x1080 or 4K (3840x2160).  The resolution of a video can have a significant impact on its visual quality, with higher resolutions generally providing sharper and more detailed images. The higher resolution will also require more processing power and storage space, and may not be necessary for all applications.   Source : https://youtu.be/lhK7ZMcW5pU?t=566  Low-resolution video can look blurry or pixelated because it does not have enough pixels to represent the details of the image accurately. While high-resolution video has more pixels to represent the details of the image more accurately, which results in a sharper and more detailed image.   Source : https://commons.wikimedia.org/wiki/File:YouTube-resolution-comparison.jpg  ","version":"Next","tagName":"h3"},{"title":"Video Data​","type":1,"pageTitle":"Computer Display","url":"/cs-notes/computer-graphics/computer-display#video-data","content":" Video contains data related to the digital information that makes up a video file. This can include information about :  Resolution : The number of pixels in image, e.g. 1920x1080 (also known as &quot;1080p&quot;).Frame Rate : Number of individual frames or images that are displayed per second, a video with 60 FPS will be smoother but also stores larger data than a 24 fps video.Color Depth : Number of colors that can be represented in each pixel of the image. A video with 8-bit color depth can represent up to 256 different colors per pixel, while a video with 10-bit color depth can represent up to 1024 different colors per pixel.Compression : The process of reducing the amount of data required to store and transmit a video file. Common compression are H.264 and AVC.Actual content such as visual and audio  ","version":"Next","tagName":"h3"},{"title":"Video Compression​","type":1,"pageTitle":"Computer Display","url":"/cs-notes/computer-graphics/computer-display#video-compression","content":" Video Compression is essential for efficient storage, transmission, and playback of video content, particularly over networks with limited bandwidth or storage capacity.  Compressing a video may results in losing an information, accuracy, and detail. This is called lossy compression, which reduce significant amount of data with some negative drawbacks.  There is Lossless Compression which is a compression technique that reduces the size of a file without losing any information. This make lossless compression reduce the data lesser than the lossy one.  The big idea of how video compression works :  Spatial Compression : Removing unnecessary data which refers to pixels that do not significantly contribute to the visual quality of the image. A pixel that don’t contribute visual quality means that human eye won’t perceive the image accurately. For example, some pixels in an image may be very similar to their neighboring pixels.Temporal Compression : Removing unnecessary data from frames that do not contain significant changes from previous frames. For example, a video where someone waves their hands but the background doesn’t change at all. Using this compression we can use the previous background frame and only change at frame where the motion occurs.Run-Length Encoding (RLE) : A lossless compression which involves storing data with the value and the count instead of the full length. For example, consider data with “AAAABBBCCD”, with RLE we can reduce it to &quot;4A3B2C1D” which results in smaller length. However, it’s less effective with data that has less repetition.   Source : https://www.videoconverterfactory.com/tips/h264-to-mp4.html  tip Find out more about compression in here. ","version":"Next","tagName":"h3"},{"title":"GPU Pipeline","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/gpu-pipeline","content":"","keywords":"","version":"Next"},{"title":"GPU​","type":1,"pageTitle":"GPU Pipeline","url":"/cs-notes/computer-graphics/gpu-pipeline#gpu","content":" GPU (Graphics Processing Unit), is specialized cpu designed to do graphics calculation. GPU has many small core and are optimized for parallel processing or performing many calculations simultaneously. Parallel processing are used extensively in graphics where many things happen at once and all need to be updated. The use of specialized hardware components like GPU is called hardware accelaration.   Source : https://www.electronicshub.org/apu-vs-cpu/  ","version":"Next","tagName":"h3"},{"title":"GPU Pipeline​","type":1,"pageTitle":"GPU Pipeline","url":"/cs-notes/computer-graphics/gpu-pipeline#gpu-pipeline-1","content":" GPU Pipeline is a series of stages that a graphics processing unit (GPU) uses to render and display 3D graphics. The 3 main stages are vertex shader, rasterization, and fragment shader. In other word, they are sequence of tasks done to reproduce a virtual graphics world (in the application) into a 2D representation to the screen. Out of 3, vertex shader and fragment shader are programmable meaning they can be modified or customized based specific needs.   Source : https://youtu.be/UzlnprHSbUw?t=1704  ","version":"Next","tagName":"h3"},{"title":"Scene Data​","type":1,"pageTitle":"GPU Pipeline","url":"/cs-notes/computer-graphics/gpu-pipeline#scene-data","content":" Graphics application interact with graphics API such as OpenGL or DirectX. Graphics API produce a 3D object by modeling the object using primitives such as point, lines, curve, and triangle. Modeling is a process of representing object such as shapes within the graphics environment. Shapes are defined by something called vertices, they are connected together forming shapes. Each vertices will contain an information about their position.  After objects are modelled, they are put into the 3D world and this is called scene data which represent 3D environment of the world. Scene data contain anything that describe the graphics environment including geometry, color, materials, lighting, and other properties of the objects in the scene.   Source : https://youtu.be/UzlnprHSbUw?t=2792  ","version":"Next","tagName":"h3"},{"title":"Vertex Shader​","type":1,"pageTitle":"GPU Pipeline","url":"/cs-notes/computer-graphics/gpu-pipeline#vertex-shader","content":" Vertex shader receives 2D / 3D scene data from graphics application. In this stage transformations such as translation, rotation, and scaling are applied to vertices. In addition to transforming vertices, the vertex shader can also perform other operations, such as normalizing vertex coordinates (scaling vertex position to ensure they are in certain range) and performing lighting calculations.  After these operations, 3D objects are transformed into 2D to be displayed in computer screen   Source : https://learnopengl.com/Getting-started/Hello-Triangle  ","version":"Next","tagName":"h3"},{"title":"Rasterizer​","type":1,"pageTitle":"GPU Pipeline","url":"/cs-notes/computer-graphics/gpu-pipeline#rasterizer","content":" This stage, GPU takes the 2D object and convert them into raster images or in other words, it “pixelize” the object. Rasterizer is responsible for converting primitives into something that can be displayed on screen. For example, rasterizer will draw pixel on the triangle path which is specified by 3 vertices connected together and also clips the object if it’s outside of the screen (to make sure unnecesarry things being rendered).   Source : https://www.techspot.com/article/1888-how-to-3d-rendering-rasterization-ray-tracing/ * The red color is not the actual color shown  ","version":"Next","tagName":"h3"},{"title":"Fragment Shader​","type":1,"pageTitle":"GPU Pipeline","url":"/cs-notes/computer-graphics/gpu-pipeline#fragment-shader","content":" Fragment shader are responsible for coloring each pixel and performs operations such as lighting calculations, texture mapping, and other effects. In simple term, they decide what is the final color of each pixel after taking account all important effect.  For example in the image below, there is a triangle that is formed by 3 vertices. These vertices all have different color, color interpolation are done to determine the color for any pixel inside the triangle.   Source : https://unsoundscapes.com/slides/2017-06-08-bringing-the-fun-to-graphics-programming/ ","version":"Next","tagName":"h3"},{"title":"Curves","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/curves","content":"","keywords":"","version":"Next"},{"title":"Curve Continuity​","type":1,"pageTitle":"Curves","url":"/cs-notes/computer-graphics/curves#curve-continuity","content":" Curve Continuity refers to the smoothness of the curve and how well it connects with the adjacent segments.  C0C^0C0 continuity, also known as position continuity, means that two adjacent segments of a curve meet at their endpoints, but may not have the same tangent direction. This can result in a sharp corner or kink where the two segments meet.C1C^1C1 continuity, also known as tangent continuity, means that two adjacent segments of a curve meet at their endpoints and have the same tangent direction. This results in a smooth connection between the two segments, without any kinks or sharp corners.C2C^2C2 continuity, also known as curvature continuity, means that two adjacent segments of a curve meet at their endpoints and have the same curvature. This results in a very smooth and seamless connection between the two segments, with no noticeable change in the curvature of the curve.  There are also higher-order continuity, such as C3C^3C3, C4C^4C4, and so on with additional constraint and smoothness.   Source : https://slideplayer.com/slide/12715113/  ","version":"Next","tagName":"h3"},{"title":"Control Point Representation​","type":1,"pageTitle":"Curves","url":"/cs-notes/computer-graphics/curves#control-point-representation","content":" Some method to represent curve using control point are :  Bezier CurveSplineB-SplineCatmull-Rom SplineRational CurveNon-Uniform Rational B-Spline (NURBS)  ","version":"Next","tagName":"h3"},{"title":"Bezier Curve​","type":1,"pageTitle":"Curves","url":"/cs-notes/computer-graphics/curves#bezier-curve","content":" Bezier Curve is generated by interpolating or approximating the control points using a fixed degree polynomial function. The first and last control points determine the starting and ending positions of the curve, while the intermediate control points determine the shape and curvature of the curve.   Source : https://en.wikipedia.org/wiki/Bézier_curve  The equation for quadratic bezier curve   Source : https://www.tutorialspoint.com/computer_graphics/computer_graphics_curves.htm  ","version":"Next","tagName":"h3"},{"title":"Spline​","type":1,"pageTitle":"Curves","url":"/cs-notes/computer-graphics/curves#spline","content":" Unlike Bezier curves, which use a fixed degree polynomial function to generate the curve, splines typically use a piecewise polynomial function. This mean quadratic function are joined together over different segments of the function's domain. Meaning that polynomial function may not be continous.  In spline, moving a single point would move the entire curve, this is because polynomial segments are connected end-to-end.   Source : https://en.wikipedia.org/wiki/Spline_(mathematics)  ","version":"Next","tagName":"h3"},{"title":"B-Spline​","type":1,"pageTitle":"Curves","url":"/cs-notes/computer-graphics/curves#b-spline","content":" B-Spline is a spline that have local control point. It uses a set of basis functions to generate the polynomial segments. Basis function is a function defined with combination of simpler function. This mean each point and curve in b-spline are defined locally. So changing a point doesn’t change the entire curve.   Source : https://www.researchgate.net/figure/A-quadratic-p-2-B-spline-curve-with-a-uniform-open-knot-vector-X-0-0-0-1-2_fig1_277405448  ","version":"Next","tagName":"h3"},{"title":"Catmull-Rom Spline​","type":1,"pageTitle":"Curves","url":"/cs-notes/computer-graphics/curves#catmull-rom-spline","content":" It’s a type of spline that pass through all of their control points, which means that the path of the spline is connected to all of the points. The uniqueness of Catmull-Rom Spline is used to control the exact path that an object or camera follows.   Source : https://www.researchgate.net/figure/An-example-of-a-Catmull-Rom-spline_fig1_50838845  ","version":"Next","tagName":"h3"},{"title":"Rational Curve​","type":1,"pageTitle":"Curves","url":"/cs-notes/computer-graphics/curves#rational-curve","content":" A curve that is defined by the ratio of two polynomial functions. Rational curve uses the nature of rational function to represent more complex shapes and function that has asymptote, such as conic sections like ellipses and hyperbolas.   Source : https://pages.mtu.edu/~shene/COURSES/cs3621/NOTES/spline/NURBS/RB.html  ","version":"Next","tagName":"h3"},{"title":"Non-Uniform Rational B-Spline (NURBS)​","type":1,"pageTitle":"Curves","url":"/cs-notes/computer-graphics/curves#non-uniform-rational-b-spline-nurbs","content":" This is a type of B-Spline which is rational curve and also non-uniform. Non-uniform means that unlike a uniform B-spline, where the knots are evenly spaced, in a non-uniform the knots can be placed at arbitrary positions along the curve or surface.  The rational which involves the use of weights in the function make curve smoother and more continuous curves and surfaces, compared to non-rational B-splines. It also has the ability to represent conic section shapes.   Source : https://en.wikipedia.org/wiki/Non-uniform_rational_B-spline ","version":"Next","tagName":"h3"},{"title":"Low Level Graphics","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/low-level-graphics","content":"","keywords":"","version":"Next"},{"title":"Drawing Primitives​","type":1,"pageTitle":"Low Level Graphics","url":"/cs-notes/computer-graphics/low-level-graphics#drawing-primitives","content":" This is the process of rendering basic geometric shapes or elements in computer graphics. These operation are fundamental building blocks that can be combined to create more complex images.  Point : A point is the simplest thing to draw, the only thing we do is changing a pixel color at some coordinates. Source : https://www.javatpoint.com/computer-graphics-point-clipping Line : A line need a start and end coordinates, we also need to determine the slope in case if it's not a straight line. To get the slope we can use the standard rise over run formula. After that, we will iterate over the pixel starting from the start point to the end point. The iteration is based on the slope and we will color the pixel on iteration. We can also set the thickness of a line, we only need to multiply the slope by the thickness factor. Source : https://www.programc.in/computer-graphics/points-and-lines-in-graphics.html Shape Square : A square can be made from connecting 4 line. Source : http://what-when-how.com/introduction-to-computer-graphics-using-java-2d-and-3d/basic-principles-of-two-dimensional-graphics-introduction-to-computer-graphics-using-java-2d-and-3d-part-3/ Triangle : A triangle can also be made from connecting 3 line. Source : https://cglearn.codelight.eu/pub/computer-graphics/geometry-and-transformations-i Circle : Commonly used algorithm to draw a circle is the Midpoint Circle Algorithm. A circle need a center point and radius. We will start drawing the bound outside the circle. The circle is divided by 8 and we will iterate over a small change in x and y coordinate, this can be thought as making a stair. Source : https://www.geeksforgeeks.org/draw-circle-c-graphics/ Filled Shape : A filled shape has a color inside it. To do this, we can draw the shape first, after that we will iterate over while also changing the pixel color in the region inside the shape. A simple algorithm to fill a shape is the flood fill. It begins changing pixel color at a point. The next thing is to check the neighboring pixels (up, down, left, and right). If the neighboring pixel has the same original color, move to that pixel and repeat steps 2 to 4 recursively. This process spreads the fill color throughout the connected region. Source : https://en.wikipedia.org/wiki/Flood_fill  ","version":"Next","tagName":"h3"},{"title":"Buffer​","type":1,"pageTitle":"Low Level Graphics","url":"/cs-notes/computer-graphics/low-level-graphics#buffer","content":" framebuffer, graphic malloc, back buffer  ","version":"Next","tagName":"h3"},{"title":"Framebuffer Manipulation​","type":1,"pageTitle":"Low Level Graphics","url":"/cs-notes/computer-graphics/low-level-graphics#framebuffer-manipulation","content":" A buffer is a block of memory thats temporilaly holds data. In graphics, buffer is used to store graphics information such as coordinates and pixel color. Changing a graphics involve modifying the buffer, for example changing a pixe color involves calculating the memory addresses corresponding to the desired pixel positions and writing the appropriate color values to those addresses.   Source : https://ecomputernotes.com/computer-graphics/basic-of-computer-graphics/what-is-frame-buffer ","version":"Next","tagName":"h3"},{"title":"Physics-Based Animation","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/physics-based-animation","content":"","keywords":"","version":"Next"},{"title":"Euler Integration​","type":1,"pageTitle":"Physics-Based Animation","url":"/cs-notes/computer-graphics/physics-based-animation#euler-integration","content":" Euler Integration is a numerical method used in physics animation to approximate the motion of objects over time. Euler Integration assumes that the acceleration of the object remains constant over the time step, and does not take into account any changes in acceleration that may occur. While this sound leading to inaccuracy, it is actually accurate as long as the time step is very small.   Source : https://pythonnumericalmethods.berkeley.edu/notebooks/chapter22.03-The-Euler-Method.html  Euler integration can be divided into three types :  Explicit : It calculates the new position and velocity of an object at each time step based on the object's current position, velocity, and acceleration.Implicit : This is more advanced method that updates the new position and velocity of an object at the end of a time step. To calculate the acceleration, we need to know the force acting on the object. However, to calculate the force, we need to know the velocity or position of the object. This lead to a recursive problem, it will be difficult to calculate, but it will also lead to more accurate results.Semi-Implicit : In this type, accelaration is calculated explicitly which means it uses the current force and it is available at the current time. By knowing the accelaration, we can calculate the implicit velocity and use it to update the object position.    ","version":"Next","tagName":"h3"},{"title":"Gravity Force​","type":1,"pageTitle":"Physics-Based Animation","url":"/cs-notes/computer-graphics/physics-based-animation#gravity-force","content":" Gravity is also an important force to create a more realistic animation. Gravity force is based on the object affected masses and also the gravitational accelaration. An object affected by gravity force will accelerate towards the gravitational field.   Source : https://youtu.be/F9TP48yXs3s?t=1244  ","version":"Next","tagName":"h3"},{"title":"Spring Force​","type":1,"pageTitle":"Physics-Based Animation","url":"/cs-notes/computer-graphics/physics-based-animation#spring-force","content":" Spring Force model the behavior of springs for object, it is typically used for bouncing balls, swinging pendulums, and dancing characters.  The spring force is a restoring force, which means that it always acts in the opposite direction of the displacement of the object. This means that if an object is compressed, the spring force will act to expand it, and if an object is stretched, the spring force will act to compress it.  This image below shows the equation for linear spring forces, it’s based on some spring properties.   Source : https://youtu.be/F9TP48yXs3s?t=1340  There is another type of spring force which is Spring Damping Force. This force is used to slow down or stop a motion such as a stopping car. It’s based on the object speed in changing length, meaning a faster object will be affected by more force.  Another type is mass-spring system, which is a spring force based on mass of object.   Source : https://youtu.be/F9TP48yXs3s?t=1833  ","version":"Next","tagName":"h3"},{"title":"Collision​","type":1,"pageTitle":"Physics-Based Animation","url":"/cs-notes/computer-graphics/physics-based-animation#collision","content":" Collision is any event in which two or more bodies exert forces on each other in a relatively short time. To know if two object collide with each other, we need to detect it and it’s usually called collision detection. Two object is collided if they intersect or overlap.  One of the algorithm to detect is Bounding Volume Hierarchies (BVH), This involves making a box around the object and we will know if object collided by checking if the box intersect with each other.   Source : https://developer.mozilla.org/en-US/docs/Games/Techniques/3D_collision_detection  After object is detected to collide with other object, we can know handle this. For example in a video game, a collision of a falling ball to the ground will bounce it in the opposite direction. In this case, object can have restitution coefficient which measure the bounciness.   Source : https://youtu.be/F9TP48yXs3s?t=2958 ","version":"Next","tagName":"h3"},{"title":"Ray Tracing","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/ray-tracing","content":"","keywords":"","version":"Next"},{"title":"Ray​","type":1,"pageTitle":"Ray Tracing","url":"/cs-notes/computer-graphics/ray-tracing#ray","content":" Rays are typically represented as a line segment or a vector that originates from a point on the camera or light source and travels through a pixel in the image plane and into the scene.   Source : https://youtu.be/gGKup9tUSrU?t=304  ","version":"Next","tagName":"h3"},{"title":"Ray-Sphere Intersection​","type":1,"pageTitle":"Ray Tracing","url":"/cs-notes/computer-graphics/ray-tracing#ray-sphere-intersection","content":" This technique is used to model the behavior of light as it interacts with spherical objects in a scene. The sphere formula is used in this model, to solve for the equation we can use quadratic formula.  The possible outcomes of the quadratic roots are :  If the discriminant is negative, there are no real solutions for t, and the ray does not intersect the sphere.If the discriminant is zero, there is one real solution for t, and the ray intersects the sphere at a single point.If the discriminant is positive, there are two real solutions for t, and the ray intersects the sphere at two points. In this case, the intersection point closest to the ray origin is typically chosen as the point of intersection.   Source : https://youtu.be/gGKup9tUSrU?t=969  tip In summary, ray tracing process involves iterating every ray, the ray is tracked until it finds an object, if we found an object, we will check if it intersect and also the closest intersection from the camera. If all of that is true, we start shading the object  ","version":"Next","tagName":"h3"},{"title":"Ray Tracing Accelaration​","type":1,"pageTitle":"Ray Tracing","url":"/cs-notes/computer-graphics/ray-tracing#ray-tracing-accelaration","content":" Ray Tracing Accelaration is a techniques that is used to optimize the ray tracing process and reduce the computational cost of rendering complex scenes with many objects.  One of the common technique is Axis-aligned bounding boxes (AABB), which partition the scene into a hierarchy of bounding volumes. The bounding volume hierarchy (BVH) is constructed by recursively dividing the space containing the scene's objects into two halves, with an AABB enclosing each half.  The improvement come from intersection testing, if the ray does not intersect the box, then it cannot intersect any of the objects inside it, and the ray tracing can be terminated.   Source : https://youtu.be/gGKup9tUSrU?t=2883  ","version":"Next","tagName":"h3"},{"title":"Ray Casting​","type":1,"pageTitle":"Ray Tracing","url":"/cs-notes/computer-graphics/ray-tracing#ray-casting","content":" There is also a much more lighter version of ray tracing, this is called Ray Casting, as it does not account for reflections, shadows, or other complex lighting effects. The only thing ray casting do is just tracing the light from the camera and check if it intersect with an object to determine which objects are visible. Ray casting can also create an illusion of depth or distance, for example a closer wall might be more bright than the further wall.   Source : https://id.wikipedia.org/wiki/Raycasting ","version":"Next","tagName":"h3"},{"title":"Reflections","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/reflections","content":"","keywords":"","version":"Next"},{"title":"Refractions​","type":1,"pageTitle":"Reflections","url":"/cs-notes/computer-graphics/reflections#refractions","content":" Refractions occur when light as it passes through a transparent material, such as glass or water making the light bends or changing direction. Refraction is typically created using a technique called ray tracing or path tracing.  The algorithm computes the angle of incidence of the incoming light ray, and then uses Snell's law to calculate the angle of refraction of the outgoing light ray.   Source : https://youtu.be/l_iVdRbA_4s?t=3265, https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-to-shading/reflection-refraction-fresnel.html ","version":"Next","tagName":"h3"},{"title":"Sampling","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/sampling","content":"","keywords":"","version":"Next"},{"title":"Monte Carlo Ray Tracing​","type":1,"pageTitle":"Sampling","url":"/cs-notes/computer-graphics/sampling#monte-carlo-ray-tracing","content":" In the rendering equation, we calculate the outgoing light taking account from all light source in the scene. The equation itself involves integral and can’t be solved because it’s a recursive integration.  To solve this, we can approximate the solution, one of the technique is Monte Carlo Sampling approximate a value by taking random samples. This method randomly sample the directions of incoming light and use these samples to estimate the integral in the rendering equation.  Below are rendering equation that uses Monte Carlo Sampling (Monte Carlo Ray Tracing).   Source : https://youtu.be/qgdDu-K0pZ4?t=1893  ","version":"Next","tagName":"h3"},{"title":"Path Tracing​","type":1,"pageTitle":"Sampling","url":"/cs-notes/computer-graphics/sampling#path-tracing","content":" Path Tracing adds a layer of complexity to the ray tracing algorithm, instead of tracing a single ray from the camera, path tracing simulates the path that the light takes through the scene by tracing many rays from the camera and simulating the interactions of these rays with surfaces in the scene.   Source : https://youtu.be/qgdDu-K0pZ4?t=2906  Light bounces off surfaces in many direction creating a hemisphere of light, which we can integrate to get the color for each pixel. The problem is the integral is too complex to solve analytically, so path tracing approximate the light using Monte Carlo technique which random samples the light.   Source : https://youtu.be/qgdDu-K0pZ4?t=2990  However, because we random sampled the light, this can create inaccurate representation of image. This is called noise and may look like random variations of brightness and color that aren't part of the original image.  There are some algorithm to de-noise the image, which uses machine learning techniques to analyze the noisy image and generate a smoother version of the image.   Source : https://youtu.be/qgdDu-K0pZ4?t=3112 ","version":"Next","tagName":"h3"},{"title":"Rendering (Part 1)","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/rendering-part-1","content":"","keywords":"","version":"Next"},{"title":"Rendering Equation​","type":1,"pageTitle":"Rendering (Part 1)","url":"/cs-notes/computer-graphics/rendering-part-1#rendering-equation","content":" In the step 2, there is an equation that describes how light interacts with surfaces in a 3D scene, it is commonly referred as The Rendering Equation.   Source : https://youtu.be/GOfzX7kRwys?t=154  Lo(ωo)L_o(\\omega_o)Lo​(ωo​) : Outgoing radiance at some point in direction \\omega_o, which represents the amount of light leaving the surface in that direction.∫ΩLi(ωi)\\int_{\\Omega} L_i(\\omega_i)∫Ω​Li​(ωi​) : Incoming radiance at a point on a surface from all light sources in the scene. Also weighted by the cosine of the angle between the surface normal and the incoming light direction.fr(ωi,ωo)f_r(\\omega_i, \\omega_o)fr​(ωi​,ωo​) : BRDF function which describes how light is reflected from a surface in direction \\omega_i reflected to direction \\omega_o.  ","version":"Next","tagName":"h3"},{"title":"BRDF (Bidirectional Reflectance Distribution Function)​","type":1,"pageTitle":"Rendering (Part 1)","url":"/cs-notes/computer-graphics/rendering-part-1#brdf-bidirectional-reflectance-distribution-function","content":" An object surface wouldn’t always have the perfect surface. Having a perfect surface meaning light will be reflected in a more predictable way, while a rough surface will scatter the light in many different directions.   Source : https://youtu.be/GOfzX7kRwys?t=595  BRDF describes how light is reflected from a surface in different directions. It takes two vectors as input : the incoming light direction (ωi) and the outgoing light direction (ωo). The output is a scalar value that represents the fraction of light that is reflected in the outgoing direction.  In other words, it describes how much light is reflected in a particular direction relative to the amount of light that is incident on the surface.   Source : https://youtu.be/GOfzX7kRwys?t=1587  Because of the surface structure, light will be reflected in many direction. To take account of all the light reflected, rendering equation uses integral to calculate all the light direction which forming a hemisphere on the surface. The sum of outgoing light should be less than the incoming light.   Source : https://youtu.be/GOfzX7kRwys?t=1980  The rendering equation also takes into account the contribution of all light sources in the scene and this is also calculated using integral.   Source : https://youtu.be/GOfzX7kRwys?t=2656  ","version":"Next","tagName":"h3"},{"title":"Blinn / Phong Modification​","type":1,"pageTitle":"Rendering (Part 1)","url":"/cs-notes/computer-graphics/rendering-part-1#blinn--phong-modification","content":" Rendering equation uses BRDF function, there is also the Blinn/Phong material model version for replacing the BRDF function and it uses diffuse specular component.  The Blinn/Phong material model is a simplified version of the BRDF function The that is a good approximation of the BRDF function and it is relatively easy to calculate.   Source : https://youtu.be/GOfzX7kRwys?t=2928 ","version":"Next","tagName":"h3"},{"title":"Rendering (Part 2)","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/rendering-part-2","content":"","keywords":"","version":"Next"},{"title":"Rendering Algorithm​","type":1,"pageTitle":"Rendering (Part 2)","url":"/cs-notes/computer-graphics/rendering-part-2#rendering-algorithm","content":" Rendering process which include rasterization, uses several algorithm :  Rasterization :  Scanline RenderingPainter AlgorithmZ-Buffer RasterizationA-Buffer RasterizationREYES (Renders Everything You Ever Saw)  ","version":"Next","tagName":"h3"},{"title":"Anti-Aliasing​","type":1,"pageTitle":"Rendering (Part 2)","url":"/cs-notes/computer-graphics/rendering-part-2#anti-aliasing","content":" In rasterization process, 3d object are converted into 2d image with pixel, and then will be colored. In some cases, coloring the pixel directly would make it appear jaggy, this is called Aliasing. Anti-Aliasing which is the technique to reduce aliasing in digital images, it smoothes out the edges of objects and reducing jaggedness in the image to produce a more visually pleasing result.   Source : https://youtu.be/0WrzyD8nBlk?t=334  There is also an improvement for Anti-Aliasing (AA) which is Multi Sample Anti-Aliasing (MSAA). MSAA works by sampling multiple points within each pixel, instead of just one, and using the average of the color values at those sample points to generate the final color for the pixel.   Source : https://youtu.be/0WrzyD8nBlk?t=1425  ","version":"Next","tagName":"h3"},{"title":"Scanline Rendering​","type":1,"pageTitle":"Rendering (Part 2)","url":"/cs-notes/computer-graphics/rendering-part-2#scanline-rendering","content":" Scanline Rendering is a technique used in computer graphics to generate images by processing one horizontal line of pixels, known as a scanline, at a time. It involves determining the intersection points of geometric primitives (such as triangles or polygons) with each scanline and then shading those pixels to produce the final image.   Source : https://youtu.be/TEAtmCYYKZA  ","version":"Next","tagName":"h3"},{"title":"Painter Algorithm​","type":1,"pageTitle":"Rendering (Part 2)","url":"/cs-notes/computer-graphics/rendering-part-2#painter-algorithm","content":" The painter's algorithm is a simple algorithm to render object by drawing the object furthest away to the camera to the closest.  The algorithm is based on the idea of painting each object in the scene onto an imaginary canvas from back to front, with each object partially or fully obscuring the objects behind it. This mean the object needs to be sorted.   Source : https://en.wikipedia.org/wiki/Painter's_algorithm  Painter algorithm draws object one by one from back to front, when objects overlap, the algorithm has to make assumptions about which object is in front of the other based purely on distance. This can lead to incorrect results if the objects are not sorted correctly or if there are ambiguities in the ordering.   Source : https://youtu.be/0WrzyD8nBlk?t=730  ","version":"Next","tagName":"h3"},{"title":"Z-Buffer Rasterization​","type":1,"pageTitle":"Rendering (Part 2)","url":"/cs-notes/computer-graphics/rendering-part-2#z-buffer-rasterization","content":" Z-buffer algorithm works by keeping track of the depth value of each pixel in the image plane during the rasterization process. The depth value represents the distance from the camera to the closest object at that pixel.  The depth value is used to determine which object to draw in front. This algorithm allows for efficient and accurate rendering of 3D scenes, because it allows objects to be rendered in any order and still produce correct results.   Source : https://youtu.be/0WrzyD8nBlk?t=1194  There is a problem with Z-buffer algorithm while rendering transparent objects that should blend. If the blue triangle is rendered first and then red after, it will result in the middle image. However, if we do the opposite, rendering red first and blue after, the result wouldn’t blend the color. This is because the depth value of the blue triangle is behind the red hence it’s not drew.  The solution for this is to sort the object from back to front, which require more computation.   Source : https://youtu.be/0WrzyD8nBlk?t=1585  ","version":"Next","tagName":"h3"},{"title":"A-Buffer Rasterization​","type":1,"pageTitle":"Rendering (Part 2)","url":"/cs-notes/computer-graphics/rendering-part-2#a-buffer-rasterization","content":" A-buffer rasterization is an extension of the traditional Z-buffer algorithm that uses layers to track each pixel which is stored in linked list. This means A-buffer require more memory than z-buffer and also computationally expensive because the linked lists must be traversed and sorted for each pixel.   Source : https://www.geeksforgeeks.org/a-buffer-method/  ","version":"Next","tagName":"h3"},{"title":"REYES (Renders Everything You Ever Saw)​","type":1,"pageTitle":"Rendering (Part 2)","url":"/cs-notes/computer-graphics/rendering-part-2#reyes-renders-everything-you-ever-saw","content":" REYES is a graphics rendering algorithm that was developed by Pixar in the late 1980s for use in their animated films.  The REYES algorithm works by dividing the scene into small patches, typically 16x16 pixels in size. Each patch is then subdivided into small micropolygons, which are typically 2x2 or 4x4 pixels in size. For each micropolygon, the algorithm computes the shading and other attributes of the object, such as texture mapping and lighting.   Source : https://www.semanticscholar.org/paper/The-Reyes-image-rendering-architecture-Cook-Carpenter/9477daf6e5cfc58d0daa41c893391d1eee8097e8 ","version":"Next","tagName":"h3"},{"title":"Shading (Part 1)","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/shading-part-1","content":"","keywords":"","version":"Next"},{"title":"Light​","type":1,"pageTitle":"Shading (Part 1)","url":"/cs-notes/computer-graphics/shading-part-1#light","content":" The basic principle behind light calculations is that light interacts with objects in the scene and is either absorbed, reflected, or refracted by those objects, depending on the properties of the object's surface and the characteristics of the light.  There are also some component we use to describe the appearance of objects under different lighting conditions, which is called Light Reflectance.  Three commonly used are :  Ambient, how light is scattered and reflected by surfaces in the scene such as wall.Diffuse, light scatters and reflects in all directions when it hits the surface of the material, resulting in matte appreance.Specular, how light reflects in a single direction when it hits the surface of the material. This component is responsible for the glossy highlights and reflections that are seen on many materials, such as metal or glass.   Source : https://clara.io/learn/user-guide/lighting_shading/materials/material_types/webgl_materials  Light source also have different types such as :  Directional, emits light in a specific direction.Point, emits light from a single point in all directions.Spot, emits light in a specific direction and within a cone-shaped area.Cylinder, emits light from a cylindrical shape.Disk, emits light from a disk-shaped surface.Rectangle, emits light from a rectangular-shaped surface.Photometric, light source that is based on real-world light measurements, such as the luminous intensity of a light bulb or the color temperature of a light source.Mesh, light source that is attached to a 3D object and emits light from its surface.   Source : https://youtu.be/Xg6KEmhqHCY?t=3267  Representation of the way light interacts with the surface of an object is called Material Model.  Some common material models are :  Lambertian MaterialPhong ModelBlinn-Phong Model  ","version":"Next","tagName":"h3"},{"title":"Lambertian Material​","type":1,"pageTitle":"Shading (Part 1)","url":"/cs-notes/computer-graphics/shading-part-1#lambertian-material","content":" The Lambertian model assumes that light is reflected equally in all directions from a surface, regardless of the angle of incidence or the direction of the light source.  This means that the amount of light reflected from a surface is proportional to the cosine of the angle between the surface normal and the direction of the light source.  The reflected light is scattered evenly in all directions, resulting in a diffuse, a matte appearance, with no glossy highlights or reflections.   Source : https://youtu.be/Xg6KEmhqHCY?t=1065  nnn : The normal line.ω\\omegaω : Omega represents the light direction.KdK_dKd​ : Diffuse reflectance coefficient, represents the amount of light that is reflected diffusely from a surface. A material with a high Kd value appears brighter and more reflective than a material with alow Kd value.θ\\thetaθ : The angle between light source and light reflected.III : The light source intensity.CCC : The resulting pixel color.  ","version":"Next","tagName":"h3"},{"title":"Phong Material Model​","type":1,"pageTitle":"Shading (Part 1)","url":"/cs-notes/computer-graphics/shading-part-1#phong-material-model","content":" Phong material model used both the diffuse and specular components of a material's reflectance. This results a shiny highlight that is reflected in a single direction from the surface.   Source : https://youtu.be/Xg6KEmhqHCY?t=1948  rrr : The direction light is reflected.vvv : The camera direction.ϕ\\phiϕ : Angle between the reflection vector and the direction of the viewer.KsK_sKs​ : The specular reflectance coefficient of the surface, and represents the amount of light that is reflected in a single direction from the surface.α\\alphaα : The specular exponent, which controls the size and sharpness of the specular highlights. Higher values of alpha result in smaller and sharper highlights.Kd+KsK_d + K_sKd​+Ks​ : The combined diffuse and specular components.  ","version":"Next","tagName":"h3"},{"title":"Blinn-Phong Material Model​","type":1,"pageTitle":"Shading (Part 1)","url":"/cs-notes/computer-graphics/shading-part-1#blinn-phong-material-model","content":" Modification of the Phong shading model that improves on the specular component of the model. The specular component of the Blinn-Phong model is based on the Phong reflection model that uses a halfway vector instead of the reflection vector.  The improvement comes in from halfway vector which can be calculated more efficiently than the reflection vector because it requires only a single vector addition rather than a vector reflection.   Source : https://youtu.be/Xg6KEmhqHCY?t=2949  h : Halved direction between light source (ω) and camera (v).   Source : https://en.wikipedia.org/wiki/Blinn–Phong_reflection_model  Phong model used exact reflection vector to calculate the specular reflection, which can result in sharper and more focused specular highlights that appear brighter.  Blinn-Phong may appear more blurry or softer appearance for the specular highlights.  ","version":"Next","tagName":"h3"},{"title":"Image Based Lightning​","type":1,"pageTitle":"Shading (Part 1)","url":"/cs-notes/computer-graphics/shading-part-1#image-based-lightning","content":" Image-based lighting (IBL) is a lighting technique used to simulate the lighting of a virtual scene using a high dynamic range image or a set of images.  High dynamic range image (HDRI) is used to capture the lighting information from the real world environment, such as a photograph of a physical location or a rendered image of a virtual environment.  Commonly used in applications such as video games, virtual reality, and architectural visualization   Source : https://developer.playcanvas.com/en/user-manual/graphics/physical-rendering/image-based-lighting/ ","version":"Next","tagName":"h3"},{"title":"Shading (Part 2)","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/shading-part-2","content":"","keywords":"","version":"Next"},{"title":"Smooth Shading​","type":1,"pageTitle":"Shading (Part 2)","url":"/cs-notes/computer-graphics/shading-part-2#smooth-shading","content":" There are some technique to create a smooth appearance on curved surfaces, by interpolating the surface normals across the surface of the object :  Flat ShadingGouraud ShadingPhong Shading   Source : https://opengl-notes.readthedocs.io/en/latest/topics/lighting/shading.html  Smooth Shading is important for objects with curved surfaces, such as spheres or cylinders, where the faceted appearance of flat shading can be particularly noticeable.  ","version":"Next","tagName":"h3"},{"title":"Flat Shading​","type":1,"pageTitle":"Shading (Part 2)","url":"/cs-notes/computer-graphics/shading-part-2#flat-shading","content":" Flat Shading is a simple and efficient shading technique where each polygon or triangle of the object is assigned a single color, based on the lighting and material properties of the object. After color is calculated, it applies the material properties such as diffuse and specular reflectivity.  Flat shading does not take into account the direction of the surface normals on the object, which can result in a faceted appearance on curved surfaces. This is because each polygon of the object is treated as a separate flat surface, and the shading of each polygon is not blended with the shading of adjacent polygons.   Source : https://youtu.be/Q_TYQvZS6WE?t=250  ","version":"Next","tagName":"h3"},{"title":"Gouraud Shading​","type":1,"pageTitle":"Shading (Part 2)","url":"/cs-notes/computer-graphics/shading-part-2#gouraud-shading","content":" In Gouraud Shading, the surface color of each polygon or triangle of the object is determined at its vertices, and then interpolated across the surface of the polygon using linear interpolation.  The color at each vertex is calculated based on the lighting and material properties of the object, such as the direction of the light and the surface normal, and can also take into account any textures or material maps that are applied to the surface of the object.  The interpolation of the color across the surface of the polygon creates the illusion of a smooth surface, by blending the shading of adjacent polygons together. This technique is more computationally expensive than flat shading, but produces much smoother and more natural results.   Source : https://youtu.be/Q_TYQvZS6WE?t=384  ","version":"Next","tagName":"h3"},{"title":"Phong Shading​","type":1,"pageTitle":"Shading (Part 2)","url":"/cs-notes/computer-graphics/shading-part-2#phong-shading","content":" In Phong shading, the surface color of each pixel of the object is determined using an interpolation of the vertex normals line, rather than the surface normals used in Gouraud shading.  The vertex normals are calculated by averaging the surface normals at each vertex of the polygon or triangle, and then interpolated across the surface of the polygon using linear interpolation.  Phong shading may be more computationally expensive than Gouraud shading because it calculates the surface color at each pixel of the object. This results a better lightning than Gouraud shading, especially for objects with high specular highlights or sharp edges.   Source : https://youtu.be/Q_TYQvZS6WE?t=1118  tip In summary, Flat shading assigns a single color to each polygon or triangle of the object while Gouraud calculates the surface color at each vertex of the object using interpolation, this mean Gouraud takes account neighbor polygon. On the other hand, phong shading uses vertex normals instead of surface normals in Gouraud shading.  ","version":"Next","tagName":"h3"},{"title":"Shading Transformation​","type":1,"pageTitle":"Shading (Part 2)","url":"/cs-notes/computer-graphics/shading-part-2#shading-transformation","content":" Shading Transformation is the process of transforming the surface normals of an object from its local coordinate system to a global coordinate system, in order to apply lighting and shading effects correctly.  Some of the coordinate system are :  Model Space, refers to the local coordinate system of the object being rendered. This allows the object to be easily manipulated and transformed, and enables the use of local lighting and shading effects that are based on the object's surface normals.World Space, refers to a global coordinate system that is fixed in 3D space which also include the other object in the scene. This allows global lighting and shading effects to be applied to the object, such as environmental lighting or reflections.View Space, camera space or eye space, is a coordinate system that is defined relative to the position and orientation of the camera that is viewing the scene being rendered. View space is used to perform perspective projection, which is the process of projecting a 3D object onto a 2D screen.   Source : https://youtu.be/Q_TYQvZS6WE?t=2011  Shading transformation works by applying a special matrix tranformation which is called inverse transpose matrix and is used to account for any non-uniform scaling or shearing transformations that may be present in the model-view matrix.   Source : https://youtu.be/Q_TYQvZS6WE?t=3214 ","version":"Next","tagName":"h3"},{"title":"Shadows","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/shadows","content":"","keywords":"","version":"Next"},{"title":"Ray Traced Shadow​","type":1,"pageTitle":"Shadows","url":"/cs-notes/computer-graphics/shadows#ray-traced-shadow","content":" Ray Traced Shadow is a type of shadow that is created using ray tracing techniques in computer graphics. In the case of ray traced shadows, rays of light are traced from the light source to the observer's eye or camera to the point on the object where the shadow is cast.   Source : https://youtu.be/l_iVdRbA_4s?t=393  Sometimes shadows may appear incorrect or contain artifacts in some areas of the scene. This is because there is some error in shadow calcuation.   Source : https://youtu.be/l_iVdRbA_4s?t=481  What happened is when light is traced, the intersection point may not exactly on the surface. It may go inside the surface, this is called Rounding Errors which are caused by the limitations of floating-point arithmetic used in computer calculations.   Source : https://www.pbr-book.org/3ed-2018/Shapes/Managing_Rounding_Error  One of a solution is to use bias term technique which involves adding a small offset to the shadow map depth value when determining which areas are in shadow. This offset creates a margin of error that helps to reduce the appearance of shadow artifacts caused by rounding errors.   Source : https://youtu.be/l_iVdRbA_4s?t=1041  ","version":"Next","tagName":"h3"},{"title":"Shadow Mapping​","type":1,"pageTitle":"Shadows","url":"/cs-notes/computer-graphics/shadows#shadow-mapping","content":" Shadow can be generated in rasterization process where colors are determined based on the lighting and shading information in the scene. However, it can be difficult to generate realistic shadows in real-time, particularly in dynamic scenes where the lighting and objects are constantly changing.  Shadow mapping is a technique used to handle this, the basic idea behind shadow mapping is to check if the light source can’t hit some point, which mean it is blocked by some object, then shadow will be generated.  To store the information, a depth map which is two-dimensional image that stores the distance between each pixel in the image and the nearest object in a scene. This depth map is then used to determine which areas of the scene are in shadow and which are not.   Source : https://learnopengl.com/Advanced-Lighting/Shadows/Shadow-Mapping ","version":"Next","tagName":"h3"},{"title":"Simulation","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/simulation","content":"","keywords":"","version":"Next"},{"title":"Rigid Body Simulation​","type":1,"pageTitle":"Simulation","url":"/cs-notes/computer-graphics/simulation#rigid-body-simulation","content":" Rigid Body Simulation is a type of physics simulation that model the motion and interaction of solid objects that do not deform or change shape during their motion. Examples of rigid bodies include objects such as balls, boxes, or cars.   Source : https://3dtotal.com/tutorials/t/how-to-master-rigid-body-simulation-in-blender-filippo-veniero-object-animation-rigid  The motion of a rigid body is governed by Newton's laws of motion, which relate the forces acting on an object to its acceleration. In a rigid body simulation, the forces acting on each rigid body are typically gravity, which pulls the object downwards, and any external forces such as friction or collisions with other objects.  There are also type of rigid body called Articulated Rigid Body. In an articulated rigid body simulation, each object is represented as a collection of rigid bodies connected by joints. The joints allow the rigid bodies to move and rotate relative to each other, while maintaining their rigid shape.  Articulated Rigid Body has degree of freedom which refers to the number of independent ways in which a joint or rigid body can move.   Source : https://link.springer.com/chapter/10.1007/978-1-4471-4417-5_5  ","version":"Next","tagName":"h3"},{"title":"Cloth Simulation​","type":1,"pageTitle":"Simulation","url":"/cs-notes/computer-graphics/simulation#cloth-simulation","content":" Cloth Simulation is a type of physics simulation that model the behavior of flexible materials, such as cloth, fabric, or soft bodies. Cloth simulation is a complex process which includes :  Mesh Modelling : The cloth is first modeled as a mesh of triangles. Each triangle is made up of three vertices, which are points in space.Properties Modelling : Physical properties of the cloth are then defined, such as its mass, elasticity, and friction.Calculation : The cloth is then simulated over time using a physics engine. The physics engine calculates the forces acting on each vertex of the mesh, such as gravity, wind, and friction. After that the position and velocity of each vertex of the mesh will be updated.   Source : https://min-tang.github.io/home/PCloth/  ","version":"Next","tagName":"h3"},{"title":"Particle-Based Fluid Simulation​","type":1,"pageTitle":"Simulation","url":"/cs-notes/computer-graphics/simulation#particle-based-fluid-simulation","content":" Particle Simulation involves modeling the motion and interaction of individual particles, such as dust, smoke, or sparks.  The motion of particle is governed Newton's laws of motion and the conservation of momentum. The simulation calculates the new position and velocity of each particle based on the forces acting on it, which include gravity, external forces applied by the user or other objects in the scene, and internal forces such as friction and cohesion.   Source : https://rocky.esss.co/library/presentation-investigate-a-wider-range-of-possibilities-with-particle-dynamics-simulation/  Fluid simulation, on the other hand, involves modeling the behavior of fluids such as water, air, or gases.  The motion of the fluid is governed by the Navier-Stokes equations, which describe the conservation of mass, momentum, and energy in a fluid. The simulation calculates the new velocity and pressure of the fluid at each point in space, based on the forces acting on it, which include gravity, external forces applied by the user or other objects in the scene, and internal forces such as viscosity and surface tension.   Source : https://www.semanticscholar.org/paper/Fluid-Simulation-For-Computer-Graphics%3A-A-Tutorial-Braley-Sandu/0b521fdd0b92e00b221012374cbaa70fe163feea  One of the technique used to simulate fluid is Smoothed particle hydrodynamics (SPH). This technique uses a grid of particles to represent the fluid. The particles interact with each other and with the boundaries of the fluid domain.   Source : https://en.wikipedia.org/wiki/Smoothed-particle_hydrodynamics   Source : https://www.kitware.com/sph-fluid-simulation-in-imstk/ ","version":"Next","tagName":"h3"},{"title":"Signal Processing","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/signal-processing","content":"","keywords":"","version":"Next"},{"title":"Smooth Edges​","type":1,"pageTitle":"Signal Processing","url":"/cs-notes/computer-graphics/signal-processing#smooth-edges","content":" In raster images, the colors between pixels can be thought of as a continuous signal. The color and intensity can change rapidly or has sharp transitions between the pixel often referred as high-frequency image.  To achieve smoother appearance in raster images, we can use signal processing techniques similar to those used for smoothing or filtering signals.   Source : https://youtu.be/UQl6ttthfXE?t=1405  This is also used in rendering when an object is aliased. The object is treated as a signal and is sampled using an anti-aliasing technique.   Source : https://youtu.be/UQl6ttthfXE?t=1650  ","version":"Next","tagName":"h3"},{"title":"Image Filters​","type":1,"pageTitle":"Signal Processing","url":"/cs-notes/computer-graphics/signal-processing#image-filters","content":" Image Filters are image processing techniques that are used to modify or enhance the appearance of an image. Image filters are typically applied to the pixels of an image in order to achieve a desired effect, such as smoothing, sharpening, or edge detection.  Image filter works by typically taking an nxn sized box of pixel called filter and the pixel inside will be transformed depending on the technique used. The transformation used to transform input into output is called convolution.   Source : https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html  For example, taking a 3x3 box filter of various pixel colors, and then we take the average of those colors resulting in a blurry image.   Source : https://youtu.be/UQl6ttthfXE?t=1779  There are various image filter technique such as :  Gaussian Filter, is used for blurring, smoothing, and noise reduction in an image.Sharpening Filter, enhances contrast between pixels.Median Filter, non-linear filter meaning the filter might change depending on the pixel in the image. Used for noise reduction in an image by taking median of neighboring pixels.Sobel Filter, used to detect edge.   Source :https://www.computerhope.com/jargon/p/photoshop-gaussian-blur.htmhttps://medium.datadriveninvestor.com/understanding-edge-detection-sobel-operator-2aada303b900http://community.photostockplus.com/tag/sharpen-filters/https://en.wikipedia.org/wiki/Median_filter  ","version":"Next","tagName":"h3"},{"title":"Edge Detection​","type":1,"pageTitle":"Signal Processing","url":"/cs-notes/computer-graphics/signal-processing#edge-detection","content":" Edge detection identifies the boundaries or transitions between different objects or regions in an image. The idea is edges have significant changes in intensity or color. The purpose of finding the edge is to provide important information about the structure and shape of objects in the image.  Identifying overall structure of an object is useful in many task such as robotics, video surveillance systems by detecting motion, medical imaging to detect tumor or organ segmentation.  Common methods includes :  Sobel Operator : It uses two 3x3 kernels to calculate the gradient of an image. The gradient is a measure of how quickly the brightness or color of an image changes in a particular direction. Prewitt Operator : The Prewitt operator is similar to the Sobel operator, but it uses different kernels to calculate the gradient. The Prewitt operator is sometimes preferred over the Sobel operator because it is less sensitive to noise. Canny Edge Detector : The Canny edge detector is a more sophisticated edge detection technique than the Sobel operator, and the Prewitt operator. It is able to find edges more accurately, but it is also more computationally expensive. The Canny edge detector uses a variety of steps to find edges, including : Gaussian Smoothing : The image is smoothed to remove noise.Gradient Calculation : The gradient of the image is calculated.Non-maximum Suppression : Edges that are not the strongest in their neighborhood are suppressed.Hysteresis Thresholding : Edges are classified as either strong or weak. Strong edges are kept, and weak edges are only kept if they are connected to strong edges. Source : https://www.researchgate.net/figure/Edge-detection-by-different-methods-a-Test-image-b-Cannys-method-c-Sobel-method_fig3_314204625 ","version":"Next","tagName":"h3"},{"title":"Textures (Part 2)","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/textures-part-2","content":"","keywords":"","version":"Next"},{"title":"Texture On GPU​","type":1,"pageTitle":"Textures (Part 2)","url":"/cs-notes/computer-graphics/textures-part-2#texture-on-gpu","content":" Texture fall into the fragment shader process in the gpu pipeline. In fragment shader, texture are sampled and mapped into the object pixel at particular point. This can be done using barycentric coordinate which interpolate 3 vertices to find the specific point. Fragment shader also calculate other shading information, such as lighting, material properties, and any additional effects or algorithms applied to the scene.   Source : https://youtu.be/Yjv6hc4Zqjk?t=1233  ","version":"Next","tagName":"h3"},{"title":"Texture Setup​","type":1,"pageTitle":"Textures (Part 2)","url":"/cs-notes/computer-graphics/textures-part-2#texture-setup","content":" Texture setup is the process by which a texture is loaded into memory on a graphics processing unit (GPU) and prepared for use in rendering. The texture setup process involves several steps, including loading the texture data into memory, setting up the texture parameters, and configuring the texture unit to apply the texture to 3D objects during rendering.  The component responsible for handling texture are called Texture Unit. Texture need to be set up and binded into the texture unit. Texture binding process include providing data and parameter to textures.  The data and parameter includes :  Texture sizeTexture format, such as RGB or RGBATexture filtering mode, what filtering method are usedMipmap levelTexture tiling mode, including repeat, clamp to edge, etc   Source : https://math.hws.edu/graphicsbook/c6/s4.html  ","version":"Next","tagName":"h3"},{"title":"Shader Access​","type":1,"pageTitle":"Textures (Part 2)","url":"/cs-notes/computer-graphics/textures-part-2#shader-access","content":" After binding the texture, shaders which are small programs that run on the GPU and are used to perform various tasks during rendering, such as lighting, shading, and texturing need to access the texture.  Texture mapping involves calculating the texture coordinates that are used to look up the corresponding texel in the texture map. This mean texture unit need to access vertex shader which provide information about the coordinates.   Source : https://youtu.be/WULOKMqEGA0?t=2543 ","version":"Next","tagName":"h3"},{"title":"Surfaces","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/surfaces","content":"","keywords":"","version":"Next"},{"title":"Implicit Surfaces​","type":1,"pageTitle":"Surfaces","url":"/cs-notes/computer-graphics/surfaces#implicit-surfaces","content":" Implicit surfaces, which is mathematical representation of a surface that is defined by an implicit function. Implicit surfaces can have complex and irregular shapes, and are often used to model organic or natural shapes that are difficult to represent using polygons.    Source : https://en.wikipedia.org/wiki/Implicit_surface  ","version":"Next","tagName":"h3"},{"title":"Meshes​","type":1,"pageTitle":"Surfaces","url":"/cs-notes/computer-graphics/surfaces#meshes","content":" Meshes are used to define shape, it is a collection of vertices, edges, and faces that define the shape and topology of a 3D object or surface. These vertices together are connected and forming shape, these are called mesh types, some of them are :  Polygon MeshNURBS (Non-Uniform Rational B-Splines) Mesh  ","version":"Next","tagName":"h3"},{"title":"Polygon Mesh​","type":1,"pageTitle":"Surfaces","url":"/cs-notes/computer-graphics/surfaces#polygon-mesh","content":" It is made up of a collection of polygons, typically triangles, that are connected by their edges to form a continuous surface. Those polygons are typically modeled and defined using specialized software called 3D modeling software.  Each vertices has attribute to define their position and typically specified as a set of X, Y, and Z coordinates in 3D space.   Source : https://en.wikipedia.org/wiki/Polygon_mesh  Polygonal mesh can have a smoother surface and the technique is called Subdivision. Idea behind subdivision is to take a coarse approximation of a surface, and then iteratively refine it to create a smoother and more detailed surface by subdividing the mesh into smaller sub-meshes.  One of the common used algorithm is Catmull-Clark Subdivision. It works by dividing each polygon into four smaller polygons, and then smoothing the resulting mesh to create a smoother surface.   Source : https://download.autodesk.com/us/maya/2009help/index.html?url=Subdivisionsurfaces_overview_What_are_subdivision_surfaces.htm,topicNumber=d0e248732  ","version":"Next","tagName":"h3"},{"title":"Triangular Meshes​","type":1,"pageTitle":"Surfaces","url":"/cs-notes/computer-graphics/surfaces#triangular-meshes","content":" Triangular Meshes are subtype of polygonal mesh which is a triangle. The reason why triangle is used because it is the simplest polygon shape which provides more flexibility. Triangle doesn’t have fixed angle making it easier to define shape such as a distorted shape or other complex shapes.   Source : https://www.graphics.rwth-aachen.de/publication/03149/  Triangle are defined using 3 vertices and edges, these vertices describe information such as position and color. Using triangle also provide us a way to represent a point called Barycentric Coordinates.  Barycentric Coordinates uses weighted sum of 3 points and we can represent a point by the combination of these 3 points. This way we can use interpolating technique to determine value or attribute of a point inside it.  For example, we determine the color by interpolating between these 3 vertices colors.   Source : https://youtu.be/HV2dKIQcp6k?t=1251   Source : https://youtu.be/HV2dKIQcp6k?t=1324  ","version":"Next","tagName":"h3"},{"title":"NURBS (Non-Uniform Rational B-Splines) Mesh​","type":1,"pageTitle":"Surfaces","url":"/cs-notes/computer-graphics/surfaces#nurbs-non-uniform-rational-b-splines-mesh","content":" A NURBS mesh is a mathematical representation of a surface that uses NURBS curve to represent the mesh. Because it’s defined using curve the main advantages is their ability to represent complex, smooth surfaces with a high degree of accuracy.   Source : https://holocreators.com/blog/what-is-the-difference-between-a-nurbs-model-and-a-polygon-mesh/  ","version":"Next","tagName":"h3"},{"title":"Bezier Patches​","type":1,"pageTitle":"Surfaces","url":"/cs-notes/computer-graphics/surfaces#bezier-patches","content":" Bezier Patches uses bezier curve to defined surface. The advantage is they can be evaluated efficiently using recursive subdivision. However, Bezier patches can be difficult to edit or manipulate directly, as modifying the control points can result in unexpected changes to the shape of the surface.   Source : https://en.wikipedia.org/wiki/Bézier_surface ","version":"Next","tagName":"h3"},{"title":"Textures (Part 1)","type":0,"sectionRef":"#","url":"/cs-notes/computer-graphics/textures-part-1","content":"","keywords":"","version":"Next"},{"title":"Texture Filtering​","type":1,"pageTitle":"Textures (Part 1)","url":"/cs-notes/computer-graphics/textures-part-1#texture-filtering","content":" We have a texture and we want to map it to a point, we take a texel in the texture space and map it to the object. In many cases, the texture coordinates do not align exactly with the texels in the texture map.   Source : https://youtu.be/Yjv6hc4Zqjk?t=1345  Texture Filtering is the process of interpolating between adjacent texels in a texture map to create a smooth appearance when the texture is applied to a 3D object. Texture is not directly mapped but it is just an approximation of the appearance of a surface.  There are some technique to address this issue :  Nearest Neighbor FilteringBilinear FilteringBicubic Filtering   Source : https://www.researchgate.net/figure/Interpolation-methods-Nearest-neighbour-interpolation-left-assigns-the-value-of-the_fig7_341509407  ","version":"Next","tagName":"h3"},{"title":"Nearest Neighbor Filtering​","type":1,"pageTitle":"Textures (Part 1)","url":"/cs-notes/computer-graphics/textures-part-1#nearest-neighbor-filtering","content":" With nearest neighbor filtering, the texel closest to the texture coordinate is selected and used as the texture color for the pixel. This is easiest and fastest filtering but results in a blocky or pixelated appearance, as the edges between adjacent texels are clearly visible.    ","version":"Next","tagName":"h3"},{"title":"Bilinear Filtering​","type":1,"pageTitle":"Textures (Part 1)","url":"/cs-notes/computer-graphics/textures-part-1#bilinear-filtering","content":" With bilinear filtering, the four nearest texels surrounding the texture coordinate are sampled and their colors are blended together using linear interpolation to produce the final color of the pixel. This method produces a smoother appearance than nearest neighbor filtering.   Source : https://youtu.be/Yjv6hc4Zqjk?t=1457, https://youtu.be/Yjv6hc4Zqjk?t=1741  ","version":"Next","tagName":"h3"},{"title":"Bicubic Filtering​","type":1,"pageTitle":"Textures (Part 1)","url":"/cs-notes/computer-graphics/textures-part-1#bicubic-filtering","content":" With bicubic filtering, a 4x4 matrix of the closest texels surrounding the texture coordinate is sampled, and the color of the pixel is calculated using a cubic polynomial function that takes into account the colors and positions of the surrounding texels.   Source : https://youtu.be/Yjv6hc4Zqjk?t=2135  ","version":"Next","tagName":"h3"},{"title":"Texture Flickering​","type":1,"pageTitle":"Textures (Part 1)","url":"/cs-notes/computer-graphics/textures-part-1#texture-flickering","content":" When an object is small, the texels in the texture map become larger relative to the size of the object, making individual texels more visible and causing the collide with other. In other word the texels is too big to fit in to each pixel.  In the image below, the closer texture looks okay, but further away you will see it become noised.   Source : https://youtu.be/Yjv6hc4Zqjk?t=2458Using bilinear filtering  When we move the camera the texture sampled will change, if texels is larger compared to pixel, the texels mapped can change color frequenly making it looks like flickering or shimmering.  The solution for this would to take more texel color into consideration and interpolate them.   Source : https://youtu.be/Yjv6hc4Zqjk?t=2664  ","version":"Next","tagName":"h3"},{"title":"Mipmapping​","type":1,"pageTitle":"Textures (Part 1)","url":"/cs-notes/computer-graphics/textures-part-1#mipmapping","content":" Another way to solve texture flicker issue is to use mipmapping. It’s a technique to make a lower resolution texture which are precomputed from texture maps. The texture are progressively smaller versions of the texture map and is called mipmap level which are usually halved from the previous.  The graphics hardware selects the appropriate level from the mipmap pyramid based on the distance between the object and the camera.   Source : https://youtu.be/Yjv6hc4Zqjk?t=2974   Source : https://en.wikipedia.org/wiki/Mipmap  ","version":"Next","tagName":"h3"},{"title":"Trilinear Filtering​","type":1,"pageTitle":"Textures (Part 1)","url":"/cs-notes/computer-graphics/textures-part-1#trilinear-filtering","content":" Sometimes the mipmap level can be too high or too low, so if we use bilinear filter in each texture, an another linear interpolation is done between them creating a new texture filter called Trilinear Filter.   Source : https://youtu.be/Yjv6hc4Zqjk?t=3340  ","version":"Next","tagName":"h3"},{"title":"Anisotropic Filtering​","type":1,"pageTitle":"Textures (Part 1)","url":"/cs-notes/computer-graphics/textures-part-1#anisotropic-filtering","content":" While filtering texture for surfaces that are viewed at oblique angles, such as the surface of a road or a tiled floor, we may oversampled the required texture. This makes further away textuers appear to be blurry.  Anisotropic Filtering instead try to only sample the required texture by making a more smaller texel which improve the approximation.   Source : https://youtu.be/Yjv6hc4Zqjk?t=3526   Source : https://en.wikipedia.org/wiki/Anisotropic_filtering  ","version":"Next","tagName":"h3"},{"title":"Texture Tiling​","type":1,"pageTitle":"Textures (Part 1)","url":"/cs-notes/computer-graphics/textures-part-1#texture-tiling","content":" Texture may have limited amount of texels that it doesn’t fulfill to map all object surfaces. Texture Tiling is the process of making texture sufficient for mapping to object. Some texture tiling method include :  Repeat : Texture is repeated seamlessly in both directions when the texture coordinates extend beyond the edges of the texture map. This creates a repeating pattern across the surface of the object.Mirrored Repeat : Same as repeat, but it alternates between mirroring the texture horizontally and vertically each time it repeats.Clamp to Edge : The texture coordinates are clamped to the edges of the texture map when they extend beyond the boundaries of the texture map. This creates a image color border around the edges of the texture.Clamp to Border : Same as clamp to edge but instead uses a user-defined solid border color.   Source : https://community.khronos.org/t/how-to-get-gl-clamp-to-border-effect/104085 ","version":"Next","tagName":"h3"},{"title":"Bluetooth","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/bluetooth","content":"","keywords":"","version":"Next"},{"title":"Bluetooth Packet​","type":1,"pageTitle":"Bluetooth","url":"/cs-notes/computer-networking/bluetooth#bluetooth-packet","content":" In bluetooth, data is transmitted and received in the form of packets. A packet is a unit of data that contains the information being exchanged between Bluetooth devices. Bluetooth packets consist of several components, including :  Access Code : The access code is a synchronization pattern that helps the receiving device identify the beginning of a packet. It allows devices to synchronize their data transmission. Header : The header contains control information about the packet, such as the packet type and addressing information. Payload : The payload is the actual data being transmitted. It can vary in size depending on the packet type and the purpose of the transmission. For example, payload can include an audio data. Cyclic Redundancy Check (CRC) : The CRC is an error-checking mechanism that uses mathematical calculation on the packet contents. The sender and the receiving device performs the same calculation and the result should be the same.  ","version":"Next","tagName":"h3"},{"title":"Bluetooth Channel​","type":1,"pageTitle":"Bluetooth","url":"/cs-notes/computer-networking/bluetooth#bluetooth-channel","content":" Bluetooth operates within the 2.4 GHz frequency bands. Same as Wi-Fi, within the frequency bands, the frequencies are further divided into smaller ranges called channels. These channels are where the radio waves are actually transmitted.  The Bluetooth standard divides this band into 79 individual channels, each with a bandwidth of 1 MHz. This channels are used to minimize interference from other devices operating in the same frequency range, such as Wi-Fi routers, cordless phones, and microwaves. Bluetooth devices hop between these channels at a rapid rate, typically changing channels 1600 times per second.   Source : https://youtu.be/1I1vxu5qIUM?si=43742PCya8lVvX0r&amp;t=526 ","version":"Next","tagName":"h3"},{"title":"Computer Networking","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking","content":"","keywords":"","version":"Next"},{"title":"All pages​","type":1,"pageTitle":"Computer Networking","url":"/cs-notes/computer-networking#all-pages","content":" Network Standard OSI ModelTCP/IP Model Network Fundamental Network Addressing IP AddressSubnet MaskMAC AddressRoutingGatewayPorts Network Device HubsSwitchRouterDial-up Modem Computer Connection SocketBroadbandEthernetLAN &amp; WANServer Network Topology Network Services &amp; Protocol DNSDHCPNATProxyVPNNetwork Protocol TCP ProtocolUDPFTPEmail ProtocolRTPRTCHTTP &amp; HTTPS Wireless &amp; Mobile Networking Wi-FiBluetoothCellular NetworksSim Card Network SecurityNetwork EncryptionNetworking Command ","version":"Next","tagName":"h3"},{"title":"Broadband","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/broadband","content":"","keywords":"","version":"Next"},{"title":"Broadband Technology​","type":1,"pageTitle":"Broadband","url":"/cs-notes/computer-networking/broadband#broadband-technology","content":" Broadband technologies differs in some aspects including the medium used for transmission, the data transfer rates they can achieve, and the availability of the technology. Here are some common techonology :  DSL (Digital Subscriber Line) : DSL utilizes existing telephone lines to transmit data, it uses copper wires which traditionally also used for telephone communication. DSL separates the telephone line into two frequency bands, one for voice communication and the other for data transmission. This separation enables simultaneous voice and internet access without interfering with each other. Source : https://id.aliexpress.com/item/33015194864.html?gatewayAdapt=Msite2Pc There are different variants of DSL technology, including : ADSL (Asymmetric Digital Subscriber Line) : A DSL technology that offers faster download speeds compared to upload speeds. This is useful for home user that typically download more data than they upload, such as web browsing, streaming media, and downloading files. SDSL (Symmetric Digital Subscriber Line) : Offers the same download and upload speeds, typically used in businesses. VDSL (Very High Bitrate Digital Subscriber Line) : VDSL provides faster download and upload speeds, although it is more deployed in areas with shorter distances between the user and the telephone exchange. Source : https://youtu.be/qQYiwmamq38?si=6BejcFALoPHg6Vx4&amp;t=192 Cable Internet : Cable internet is a type of broadband internet access that uses the same coaxial cable infrastructure that delivers cable television signals from (Internet Service Providers ISP) to provide high-speed internet connectivity. It offers faster data transfer rates compared to traditional dial-up and DSL connections. The cable can be attached to gateway that combines router, switch, and modem together. Cable internet connections are typically shared among multiple users within the same neighborhood or local area. This may result in slower connection speed during peak usage periods. Source : https://youtu.be/qQYiwmamq38?si=5-r9fAH45DVueiEC&amp;t=128 Fiber Optic Broadband : Also known as fiber internet or fiber optic internet, uses fiber optic cables to transmit data. It offers significantly faster speeds and higher bandwidth capacity compared to traditional copper-based broadband technologies like DSL or cable internet. Fiber optic cables are immune to electromagnetic interference, signal loss, and degradation, which can occur with copper-based cables. Fiber optic uses pulse of light to transmit data. Digital data are transformed into electrical signal and transformed again into light signal through the process called modulation. Typically uses method such as amplitude modulation (AM) where the amplitude of the light signal is varied based on the voltage levels of the electrical signal. Fiber optic broadband also supports hybrid infrastructure. We can use fiber optic to transmit data over long distance, and use copper cable to distribute the data into smaller area. Source : https://www.lightspeed.co.uk/blog/introducing-full-fibre-broadband Wireless Broadband : Wireless broadband is a type of broadband internet connection that is delivered wirelessly, without the need for physical cables or wires. Some common wireless technologies include Wi-Fi (Wireless Fidelity) and cellular networks (such as 3G, 4G, and 5G). Source : https://www.jks.co.id/wireless-boradband/ ","version":"Next","tagName":"h3"},{"title":"DHCP","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/dhcp","content":"","keywords":"","version":"Next"},{"title":"APIPA​","type":1,"pageTitle":"DHCP","url":"/cs-notes/computer-networking/dhcp#apipa","content":" Automatic Private IP Addressing (APIPA) is a feature in the Windows operating system that allows devices to automatically assign themselves a private IP address.  When a device are unable to obtain an IP address from a DHCP server for whatever reason, it can still obtain a private IP address. This mean the device can still communicate within the local network, but it can't connect to the internet.  The private IP address obtained will range from 169.254.0.1 to 169.254.255.254 with a 16-bit subnet mask.   Source : https://tipsmake.com/learn-about-apipa ","version":"Next","tagName":"h3"},{"title":"Cellular Networking","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/cellular-networking","content":"","keywords":"","version":"Next"},{"title":"Cellular Tower​","type":1,"pageTitle":"Cellular Networking","url":"/cs-notes/computer-networking/cellular-networking#cellular-tower","content":" However, there are many problem emitting the electromagnetic signal, there may be obstacle, interference, and limited range of signal transmission. To avoid this, sender's mobile device doesn't send the signal directly to the receiver device. Instead they will send it to a cellular tower.  Cellular towers are a freestanding tower designed to support wireless communication. The tower's height helps extend the range of the cellular signal and improve coverage. The tower are spread around a region called cells, the cells are hexagon shaped. The purpose of hexagon shape is to cover a region of area without overlapping with other tower.   Source : https://www.rfwireless-world.com/Tutorials/cell-phone-tower-basics-and-cell-phone-tower-types.html  Each tower are connected physically, typically using fiber optic cable. By using fiber optic cable, we can overcome the limitation of electromagnetic signal. The tower will communicate with each other to forward the data received from the sender in the form of light pulse. The tower may receive help from Mobile Switching Center (MSC). MSC will handle a request and determines the appropriate destination or which cell tower to send the data.   Source : https://www.researchgate.net/figure/Cellular-network-architecture-for-a-mobile-distributed-system_fig1_323861058  ","version":"Next","tagName":"h3"},{"title":"Cellular Networking Generation​","type":1,"pageTitle":"Cellular Networking","url":"/cs-notes/computer-networking/cellular-networking#cellular-networking-generation","content":" Cellular networking has go through different stage or generation of the technology, this includes advancement in terms of data speeds, capacity, network architecture, and capabilities.  1G (First Generation) : 1G networks were the first commercially deployed cellular networks. They were analog systems introduced in the 1980s and provided basic voice communication. It uses Frequency Division Modulation Access (FDMA), the data speed is 2.4 Kbps. 2G (Second Generation) : 2G networks is the transition from analog to digital cellular networks. They introduced digital voice communication and improved data services (such as SMS or text messaging). Notable 2G technologies include Global System for Mobile Communications (GSM) and CDMA (Code Division Multiple Access). 2G data speeds ranges from 9.6 Kbps to 384 Kbps. 3G (Third Generation) : 3G networks were a significant leap forward in terms of data transfer speeds and capabilities. They enabled higher-quality voice calls, faster data connections, and the introduction of services such as video calling, mobile internet access, and multimedia streaming. Technologies like Universal Mobile Telecommunications System (UMTS), Wideband Code Division Multiple Access (WCDMA), and CDMA2000 were used for 3G networks. On average, 3G networks offered download speeds of around 1-2 Mbps and upload speeds of up to 500 kbps. 4G (Fourth Generation) : 4G networks represented a major advancement in cellular technology. They provided significantly faster data speeds, lower latency, and improved network capacity compared to 3G. 4G networks enabled high-definition video streaming, online gaming, cloud services, and a wide range of mobile applications. The most widely used 4G technology is LTE (Long-Term Evolution). It uses the Multiple Input Multiple output (MIMO) and Orthogonal Frequency Divsion Multi Access (OFDMA). Typical 4G speeds can range from 5 Mbps to over 100 Mbps for downloads, with upload speeds ranging from 1-50 Mbps. 5G (Fifth Generation) : 5G is the latest generation of cellular networking technology. It offers even higher data transfer speeds, ultra-low latency, massive network capacity, and advanced capabilities. 5G networks are designed to support emerging technologies such as virtual reality, augmented reality, autonomous vehicles, industrial automation, and the Internet of Things (IoT). 5G speeds can range from 100 Mbps to several gigabits per second (Gbps) for downloads, with upload speeds ranging from 50 Mbps to several hundred Mbps.   Source : https://slideplayer.com/slide/16305995/ ","version":"Next","tagName":"h3"},{"title":"DNS","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/dns","content":"","keywords":"","version":"Next"},{"title":"DNS Record​","type":1,"pageTitle":"DNS","url":"/cs-notes/computer-networking/dns#dns-record","content":" DNS server has a map of different type of domain name. They are categorized based on the specific purpose. Here are some common record :  A (Address) : An A record maps a domain name to an IPv4 address. AAAA (IPv6 Address) : Similar to the A record, the AAAA record maps a domain name to an IPv6 address. CNAME (Canonical Name) : A CNAME record creates an alias for a domain. It allows multiple domain names to be directed to the same IP address. For example, we can create a CNAME record to associate www.example.com with example.com. This mean everytime user enters www.example.com, it will be redirected to example.com MX (Mail Exchanger) : An MX record specifies the mail servers responsible for receiving emails sent to a domain. TXT (Text) : A TXT record allows domain owners to add arbitrary text to their DNS records. It is commonly used for purposes such as verification, email authentication, and adding human-readable information. NS (Name Server) : NS records specify the authoritative name servers for a particular domain which is the DNS servers that are responsible for storing and providing DNS information for a domain.  ","version":"Next","tagName":"h3"},{"title":"Type of DNS Server​","type":1,"pageTitle":"DNS","url":"/cs-notes/computer-networking/dns#type-of-dns-server","content":" DNS server is distributed around the world, it also consist of hierarchy which tells the importance of the domain. Here are some types of DNS server :  Local DNS : While trying to convert a domain name into an IP address, our computer will first check if the website has ever been connected to before, check if it's exist in the local computer memory. If yes, the computer doesn't need to connect to the DNS server. Recursive Resolver : This DNS server act as the bridge between our computer to the higher hierarchy of DNS server. It will recursively ask to the higher server, also will store the responses it receive to speed up future request. Authoritative Name Server : Authoritative name servers are responsible for storing the DNS records (such as A records, CNAME records, MX records, etc.) for a specific domain. They provides record for specific domain. Top-Level Domain (TLD) Server : TLD servers are responsible for storing information about the domains within a specific top-level domain. For example, the .com TLD server stores information about all the domains ending with .com. Root Server : These are the highest-level DNS servers in the hierarchy. They store the IP addresses of the authoritative servers for top-level domains (TLDs) like .com, .net, .org, etc. There are 13 sets of root servers distributed worldwide, managed by different organizations.   Source : https://www.menandmice.com/glossary/dns-server-types ","version":"Next","tagName":"h3"},{"title":"Dial-up Modem","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/dial-up-modem","content":"","keywords":"","version":"Next"},{"title":"Modulation - Demodulation​","type":1,"pageTitle":"Dial-up Modem","url":"/cs-notes/computer-networking/dial-up-modem#modulation---demodulation","content":" The primary function of a modem is to convert the digital signals generated by a computer into analog signals that can be transmitted over a standard telephone line or cable line. This process is called modulation. On the receiving end, the modem also performs the reverse process, called demodulation, by converting the analog signals back into digital signals that can be understood by the computer or network.  Modulated analog signals are transmitted over the communication medium, such as copper cables, fiber optics, or wireless channels, depending on the type of modem. The medium carries the signals to the destination.   Source : https://techdifferences.com/difference-between-modulation-and-demodulation.html  Dial-up Connection​  The method of connecting to the internet or a computer network using a traditional telephone line is called Dial-up connection. Dial-up connection uses modem and it was a popular method of accessing the internet in the early days of home computing when broadband and high-speed internet connections were not widely available.   Source : https://hendri.staff.uns.ac.id/2009/11/koneksi-dial-up/  Modem vs Router​  A modem connects a user's network to the ISP's network, allowing access to the internet through telephone lines in modulation and demodulation process, while a router manages the traffic between different networks within a local network, directing data packets to their appropriate destinations.   Source : https://youtu.be/Mad4kQ5835Y?si=5obRuPN7CxkoxFuO&amp;t=402  ","version":"Next","tagName":"h3"},{"title":"Type of Modem​","type":1,"pageTitle":"Dial-up Modem","url":"/cs-notes/computer-networking/dial-up-modem#type-of-modem","content":" Here are some common types of modem :  Dial-Up Modem : Dial-up modems were commonly used in the past to establish internet connections over traditional telephone lines. They operated by dialing a phone number to connect to an Internet Service Provider (ISP). Dial-up modems had relatively low data transfer rates, typically ranging from 56 Kbps down to as low as 14.4 Kbps. Source : https://id.wikipedia.org/wiki/Akses_Internet_putar-nomor DSL Modem : DSL (Digital Subscriber Line) modems are used to establish high-speed internet connections over existing telephone lines. DSL technology allows simultaneous transmission of voice and data signals by utilizing different frequency bands. DSL modems can achieve significantly higher data transfer rates compared to dial-up modems, ranging from several Mbps (Megabits per second) to tens or hundreds of Mbps. Source : https://www.tp-link.com/ae/dsl-modem-router/ Wireless Modem : Wireless modems, also known as cellular modems or mobile broadband modems, use wireless communication technologies such as 3G, 4G LTE, or 5G to establish internet connections. They typically come in the form of USB dongles, portable hotspots, or modems built into devices like smartphones or tablets. Wireless modems provide internet access in areas covered by cellular networks but may have varying speeds depending on network coverage and technology. Source : https://id.aliexpress.com/item/32657887191.html ","version":"Next","tagName":"h3"},{"title":"Email Protocol","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/email-protocol","content":"","keywords":"","version":"Next"},{"title":"SMTP​","type":1,"pageTitle":"Email Protocol","url":"/cs-notes/computer-networking/email-protocol#smtp","content":" Simple Mail Transfer Protocol (SMTP) is an email protocol used for sending email messages between servers or from a mail client to a mail server. SMTP is responsible for delivering emails from the sender's side to the recipient's mail server.  SMTP operates on a client-server model, the sender of an email is called client and the recipient act as the server that receive the email. Sender and receiver can be server or an email service software such as Outlook or Gmail. The recipient server is called SMTP server, the server is responsible for handling email delivery, routing, and storing incoming emails.  When a client compose and send an email, the client establishes a connection with the SMTP server and submits the email for delivery.  SMTP Process​  SMTP is based on TCP/IP suite, it follows the TCP protocol which includes the handshakes  Connection Establishment : The SMTP client initiates a TCP/IP connection with the SMTP server. By default, SMTP uses port 25, but alternative ports like 587 for secure connections (SMTPS) are also commonly used. Greeting and Handshaking : Once the connection is established, the SMTP server sends a greeting message to the client, typically containing the server's name or identification. The client acknowledges the greeting. Sender and Recipient Information : The client sends a &quot;MAIL FROM&quot; command to specify the email sender's address. The server responds with a confirmation. Recipient Verification : The client sends a series of &quot;RCPT TO&quot; commands to specify one or more recipient addresses. The server verifies the recipients' addresses and responds with confirmation or error codes. Email Content and Transfer : The client sends the email content, including the message headers, body, and any file attachments, using the &quot;DATA&quot; command. The content is transmitted as a series of text lines. The client marks the end of the message with a special termination sequence (such as a period on a line by itself). Message Delivery : The server receives the email content and performs further processing. It may perform additional checks, such as spam filtering or antivirus scanning. If the recipient's address is valid and the message passes all checks, the server accepts the email for delivery. Response Codes : Throughout the process, the server sends response codes to the client indicating the status of each command. These codes include success (e.g., 250 OK), temporary failure (e.g., 421 Service not available), or permanent failure (e.g., 550 Mailbox not found). Connection Termination : Once the email transfer is complete, the client can issue a &quot;QUIT&quot; command to terminate the SMTP session. The server acknowledges the QUIT command, and the connection is closed.   Source : https://yespo.io/blog/how-choose-right-email-protocol  ","version":"Next","tagName":"h3"},{"title":"POP3​","type":1,"pageTitle":"Email Protocol","url":"/cs-notes/computer-networking/email-protocol#pop3","content":" Post Office Protocol version 3 (POP3) is an email retrieval protocol used by email clients to retrieve email messages from a mail server. While SMTP is used to send email, POP3 is used to retrieve email messages from a mail server. It is used for downloading email messages from the server to a local device (such as a computer or mobile device) for offline access.  POP3 Process​  Connection Establishment : POP3 also uses TCP/IP suite, the client will initiates a TCP/IP connection with the POP3 server. By default, POP3 uses port 110, but alternative ports like 995 for secure connections (POP3S) can be used. Authentication : Once the connection is established, the client sends a &quot;USER&quot; command followed by the username and a &quot;PASS&quot; command followed by the password to authenticate itself to the server. The server responds with a positive or negative acknowledgement of the authentication. Mailbox Access : After successful authentication, the client sends a series of commands to interact with the mail server and access the user's mailbox. These commands include &quot;LIST&quot; to retrieve a list of available messages, &quot;RETR&quot; followed by the message number to retrieve a specific message, &quot;DELE&quot; followed by the message number to mark a message for deletion, and &quot;QUIT&quot; to terminate the session. Message Retrieval : The client uses the &quot;RETR&quot; command to retrieve individual messages from the server. The server responds with the requested message content, including the message headers, body, and any attachments. The client buffers and stores the received messages locally. Deletion and Cleanup : If the client sends the &quot;DELE&quot; command for specific messages, the server marks those messages for deletion. The actual deletion of marked messages occurs when the client sends the &quot;QUIT&quot; command to terminate the session. Connection Termination : Once the client has finished accessing the mailbox, it sends the &quot;QUIT&quot; command to terminate the POP3 session. The server acknowledges the QUIT command, and the connection is closed. Source : https://www.javatpoint.com/pop-protocol  ","version":"Next","tagName":"h3"},{"title":"IMAP​","type":1,"pageTitle":"Email Protocol","url":"/cs-notes/computer-networking/email-protocol#imap","content":" Internet Message Access Protocol (IMAP) is an email retrieval protocol same as POP3. The difference between POP3 is IMAP provide more advanced functionallity.  POP3 is primarily designed for offline email access. When a POP3 client retrieves email from the server, it typically removes the messages from the server, storing them locally on the client device. It doesn't provide synchronization, for example, if we mark an email as read in our local computer, the server will not update it.  In contrast, IMAP allows for online access to email, clients can create, delete, and manage folders on the server. The messages remain stored on the server, and the client can interacts with the server to view and manage email as the server will synchronized across multiple devices.  Connection Establishment : The IMAP client initiates a TCP/IP connection with the IMAP server. By default, IMAP uses port 143, but alternative ports like 993 for secure connections (IMAPS) can be used. Authentication : Once the connection is established, the client sends an &quot;AUTHENTICATE&quot; or &quot;LOGIN&quot; command followed by the username and password to authenticate itself to the server. The server responds with a positive or negative acknowledgement of the authentication. Mailbox Selection : After successful authentication, the client selects a specific mailbox (folder) on the server using the &quot;SELECT&quot; command. This command allows the client to access and manage the messages within the selected mailbox. Message Synchronization : The client can request a list of available messages in the selected mailbox using the &quot;FETCH&quot; command. The server responds with information about each message, including headers, flags, and body structure. The client can choose to synchronize the message headers or retrieve the complete message content selectively. Message Manipulation : The client can perform various actions on the messages, such as marking them as read/unread, flagging them, moving them to different folders, or deleting them. These actions are typically accomplished through commands like &quot;STORE&quot; and &quot;COPY&quot; issued by the client. MIME Structure and Attachments : If a message contains attachments or multi-part content, the client can issue additional commands to retrieve specific parts or attachments of the message using the &quot;BODY&quot; and &quot;PARTIAL&quot; commands. Connection Management : The client can keep the connection open for an extended period to maintain an active session with the server. This allows for real-time updates and notifications of new messages or changes to the mailbox. When the client is finished, it can issue the &quot;LOGOUT&quot; command to terminate the IMAP session. Source : https://www.researchgate.net/figure/MAP-and-POP-protocols_fig1_329881682 ","version":"Next","tagName":"h3"},{"title":"FTP","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/ftp","content":"","keywords":"","version":"Next"},{"title":"FTP Process​","type":1,"pageTitle":"FTP","url":"/cs-notes/computer-networking/ftp#ftp-process","content":" FTP relies on TCP protocol for reliable data transfer. Because it's based on TCP, there will be TCP handshakes before connecting to the server. FTP is based on command-response communication, this means client will send command as the request to the server such as RETR to retrieve a file or STOR to store a file.  Here are the high level process of FTP :  TCP Connection : The client will initiate connection to the server that in default listens to port 21. TCP handshakes that includes sending SYN (synchronize) message and ACK (acknowledgement). Authenticate : After establishing a connection to the FTP server, you will need to authenticate, there are two types of authentication. Depending on the server, some FTP require you to register a username and password while some can connect anonymously. File Operation : Once client is authenticated, the client can perform various file operations such as uploading (putting) files to the server, downloading (getting) files from the server, renaming files, deleting files, creating directories, and navigating the directory structure, all of these are done by using FTP command. The server will respond with three-digit status code and a corresponding message indicating the success or failure of the command. Data Connection : FTP uses separate connections for conencting to the server and for transferring the data. Connecting to server that includes sending command is called control connection and the other connection for transferring data is called data connection. Data connection can be active or passive : Active Mode : The server initiates a connection to the client for data transfer. The server sends the client its IP address and a port number on which it listens for the data connection. Passive Mode : In passive mode, the client is the one who should initiates a connection to the server. The server will also provide the client its IP address and a port number to establish the data connection. Connection Close : When the FTP session is complete, the client sends the QUIT command over the control connection to terminate the session. The server acknowledges the command, and both the control and data connections are closed. Source : https://www.deskshare.com/resources/articles/ftp-how-to.aspx  ","version":"Next","tagName":"h3"},{"title":"SFTP​","type":1,"pageTitle":"FTP","url":"/cs-notes/computer-networking/ftp#sftp","content":" FTP is not really secure, it has some security issue such as :  Lack of Encryption : FTP transfers data, including usernames, passwords, and file contents, in plain text. This means that anyone with access to the network traffic can potentially intercept and view sensitive information. Authentication Vulnerabilities : FTP relies on basic username and password authentication, which can be susceptible to brute-force attacks, eavesdropping, or password sniffing.  Secure File Transfer Protocol (SFTP) is commonly used as a secure alternative to FTP (File Transfer Protocol) because it adds encryption and authentication mechanisms to protect data during transit.  SFTP uses Secure Shell (SSH) as its underlying protocol, which provides encryption and data integrity. All data, including file content, commands, and authentication credentials, are encrypted to prevent unauthorized access or eavesdropping. SFTP also uses TCP for reliable data transfer and it operates on port 22 instead.   Source : https://www.educba.com/ftp-vs-sftp/ ","version":"Next","tagName":"h3"},{"title":"Ethernet","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/ethernet","content":"","keywords":"","version":"Next"},{"title":"Process of transferring data​","type":1,"pageTitle":"Ethernet","url":"/cs-notes/computer-networking/ethernet#process-of-transferring-data","content":" While transferring data, the data is divided into small pieces called frames. Frames consist of header, payload (actual data), and trailer. The header includes source and destination MAC addresses and frame type information to check if frame is damaged or corrupted. Each device on an Ethernet network has a unique MAC address assigned to its network interface card (NIC). These address are used to uniquely identify each devices within the network. Ethernet will only forward the frames to the desired destination, this will improves network efficiency and reduces collisions. Ethernet uses a protocol called Carrier Sense Multiple Access with Collision Detection (CSMA/CD) to manage access to the network medium (the shared cable). Before transmitting data, a device checks if the network is idle to avoid collisions with other devices. If a collision occurs (multiple devices transmit data at the same time), they detect the collision and stop transmitting. Each device involved in the collision waits for a random period of time before retransmitting its data. After data is received by the target device, the device will check if the data is sent to the correct device. It will then do some error checking to determine if the frames are corrupted during transmission. The payload that contains data will be extracted and will be sent to next protocol process or application for further processing.   Source : https://yzrkiller.home.blog/2019/06/22/computer-network/ ","version":"Next","tagName":"h3"},{"title":"Hubs","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/hubs","content":"Hubs Main Source : Hub, Switch, &amp; Router Explained | PowerCert Hubs is a networking device that connects device together, typically in small network such as local area network (LAN). Hubs act as a central point that accepts data from a device and forward it to all other device connected to the hub. The devices are physically connected using cable called ethernet. Hub is a &quot;dumb&quot; device, it doesn't store or analyze packets, it simply replicates and forwards them. It does not filter or make any decisions based on the packet's source or destination addresses. As a result, all devices connected to a hub receive all the network traffic, even if the data is not intended for them. This makes hub has certain limitation which is wasting bandwidth and raise some security concerns. This is why hubs are replaced with more modern networking device such as switch and router Source : https://study-ccna.com/network-hubs-explained/ Source : https://www.fiber-optic-solutions.com/buy-ethernet-switch-hub.html","keywords":"","version":"Next"},{"title":"Gateway","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/gateway","content":"","keywords":"","version":"Next"},{"title":"Gateway Types​","type":1,"pageTitle":"Gateway","url":"/cs-notes/computer-networking/gateway#gateway-types","content":" There are several type of gateway, here are some common types :  Default Gateway : Every device connected to a local network requires a default gateway to communicate with devices on other networks, such as the internet. The default gateway is usually the IP address of router that connects the local network to the internet. Protocol Gateway : A protocol gateway connects networks that use different network protocols. It translates network protocols to enable communication between networks that would otherwise be incompatible. For example, it can be used to connect a local network using Internet Protocol version 4 (IPv4) to a network that uses Internet Protocol version 6 (IPv6). Application Gateway : An application gateway, also known as an application-level gateway or proxy server, operates at the application layer of the network stack. It acts as an intermediary for specific applications or services, providing functions such as protocol conversion, data caching, security filtering, and load balancing. Application gateways are often used for web traffic, email, or other specific applications.   Source : https://labcomsistemas.com.br/en/applications/protocol-gateway/ ","version":"Next","tagName":"h3"},{"title":"HTTP & HTTPS","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/http-https","content":"","keywords":"","version":"Next"},{"title":"HTTP​","type":1,"pageTitle":"HTTP & HTTPS","url":"/cs-notes/computer-networking/http-https#http","content":" Hypertext Transfer Protocol (HTTP) is protocol used for transmitting hypermedia documents, such as HTML files or webpages, over the internet.  HTTP operates on top of the TCP/IP protocol suite, using Transmission Control Protocol (TCP) as the underlying transport protocol. It typically uses port 80 for communication, although other ports such as 8080 are also commonly used.  ","version":"Next","tagName":"h2"},{"title":"HTTP Request & Method​","type":1,"pageTitle":"HTTP & HTTPS","url":"/cs-notes/computer-networking/http-https#http-request--method","content":" HTTP is a stateless protocol, meaning that each request from the client is independent of previous requests. To maintain state and enable more complex interactions, web applications often use cookies or session tokens to track user sessions.  The protocol follows a client-server model, where the client, typically a web browser, sends requests to the server, and the server responds with the requested data.   Source : https://www.freecodecamp.org/news/what-is-http/  HTTP requests consist of a method that indicates the type of request, a Uniform Resource Locator (URL) that specifies the resource being requested, and optional headers that provide additional information about the request.  Some of the common method are :  GET : The GET method is used to retrieve a resource from the server. It is primarily used to request data and should not have any side effects on the server. For example, retrieving a webpage or fetching data such as image or video from a server. POST : The POST method is used to submit data to the server to create a new resource. It is commonly used for submitting form data or uploading files. The data sent in the body of the request is typically processed by the server and may result in the creation of a new resource. PUT : The PUT method is used to update an existing resource on the server. It replaces the resource with the new data provided in the request body. If the resource does not exist, some servers may create it. DELETE : The DELETE method is used to remove a resource from the server. It requests the server to delete the specified resource.  For example, the image below is called an API endpoint. It is a specific URL that the client can request into and the servers should understand. An API endpoint needs a base URL, it is the starting point of a web address, typically uses the domain name itself.  Suppose we want to &quot;get&quot; all the books available in a server. The web address or the base URL is www.library.com. We would use the GET method with the /books endpoint. We will append it next to the base URL, so it will be http://www.library.com/books. We used http in the front to indicate we are going to use the HTTP protocol.   Source : https://javarevisited.blogspot.com/2016/04/what-is-purpose-of-http-request-types-in-RESTful-web-service.html  ","version":"Next","tagName":"h3"},{"title":"HTTP Response​","type":1,"pageTitle":"HTTP & HTTPS","url":"/cs-notes/computer-networking/http-https#http-response","content":" Upon requesting using an HTTP method, the server will respond a status code that indicates the outcome of the request along with the requested data and optional response headers. The status code consists of 3 digit ranging from 100 to 599 (not all number included).  Informational (1xx) : This category consists of informational response codes that indicate that the server has received the request and is processing it. These codes are used to provide preliminary information or to request the client to continue with the request. Success (2xx) : The success category includes response codes that indicate the successful processing of the request by the server. These codes indicate that the request was understood, accepted, and processed without any errors. The most common success code is 200, which signifies that the request was successful and the server is returning the requested content. Redirection (3xx) : Redirection response codes indicate that the requested resource is temporarily or permanently located at a different URL. These codes are used when a resource has been moved or when the server wants the client to access a different URL. The client may need to follow the redirection to obtain the desired resource. Client Errors (4xx) : The client error category includes response codes that indicate an error on the client's side. These codes indicate that the server was unable to process the request due to a client error, such as a malformed request, unauthorized access, or a request for a resource that does not exist. The well-known client error code is 404, which indicates that the requested resource was not found on the server. Server Errors (5xx) : Server error response codes indicate that the server encountered an error while processing the request. These codes indicate that the server was unable to fulfill the request due to an internal server error or an unexpected condition. Server error codes are returned when something goes wrong on the server side, such as a misconfiguration, a problem with the server application, or server overload.    ","version":"Next","tagName":"h3"},{"title":"HTTP Format & Syntax​","type":1,"pageTitle":"HTTP & HTTPS","url":"/cs-notes/computer-networking/http-https#http-format--syntax","content":" HTTP request and response follows a specific format and syntax for communication :  HTTP Request Format: Request Line : Specifies the HTTP method, target URL (including the path and any query parameters), and the HTTP version.Request Headers : Optional headers that provide additional information about the request, such as domain name, content type, authentication credentials, and etc.Blank Line : A blank line indicating the end of the headers section.Request Body : Optional data included with certain HTTP methods like POST or PUT. It contains the payload of the request. Example : GET /path/to/resource HTTP/1.1 Host: www.example.com User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 User-Agent provides information about the client making the request, including the browser and operating system details. HTTP Response Format: Status Line : Specifies the HTTP version, status code, and a brief status message.Response Headers : Provide additional information about the response, such as content type, server information, and etc.Blank Line : A blank line indicating the end of the headers section.Response Body : Contains the actual content of the response, such as HTML pages, JSON, or binary data. Example : HTTP/1.1 200 OK Content-Type: application/json Content-Length: 1234 { &quot;message&quot;: &quot;Hello, World!&quot;, &quot;data&quot;: { &quot;key&quot;: &quot;value&quot; } }    Source : https://www.studytonight.com/computer-networks/http-protocol  ","version":"Next","tagName":"h3"},{"title":"HTTP Version​","type":1,"pageTitle":"HTTP & HTTPS","url":"/cs-notes/computer-networking/http-https#http-version","content":" HTTP has undergone several versions over time, the oldest one is the HTTP/1.0 and the newest is the HTTP/3. Along the version, new response code, feature, performance improvement were introduced.  HTTP/1.0​  HTTP/1.0 was made in 1996, it is based on TCP and each request required a separate connection, resulting in higher overhead. HTTP/1.0 had a basic set of headers, including Content-Type, Content-Length, Host, User-Agent, and Referer. The supported methods are GET, POST, HEAD, and PUT were among the supported methods.  HTTP/1.1​  This version was made in 1997 and it was a major improvement. It allows multiple requests and responses to be sent over a single connection. Response is allowed to be sent in multiple parts or &quot;chunks&quot;. This is particularly useful for transferring large or dynamically generated content.  HTTP/1.1 supports request pipelining, where multiple requests can be sent without waiting for the corresponding responses. This allows for more efficient use of network resources and reduces latency by eliminating the need for round trips between the client and server.  It introduces the host header that specifies the domain name of the server being requested, this header allows a single server to host multiple websites.  HTTP/2​  HTTP/2 was published in 2015. It introduced multiplexing, with single TCP connection, multiple requests and responses can be sent and received concurrently. Allows a more advanced pipelining compared to HTTP/1.1, it allows multiple independent stream to be sent without order.  HTTP/2 introduces server push functionality, which allows servers to send resources to clients before they are explicitly requested. This can improve page load times by eliminating the need for subsequent round trips for additional resources.  HTTP/3​  HTTP/3, also known as HTTP over QUIC, was a draft in 2020 and was published in June 2022. It uses new protocol called Quick UDP Internet Connections (QUIC) instead of the original TCP. QUIC combines the reliability of TCP with the no handshake mechanism of UDP that provides low-latency and efficiency.  In HTTP/3, if a packet is lost or delayed in one stream, it does not affect the delivery of other streams since they are independent of each other.  QUIC introduces connection ID, it is a unique identifier of a connection. An identifier allows a faster connection establishment or migration compared to traditional protocols like TCP. An example for this would be a phone that originally uses Wi-Fi and changes to a cellular network. The server will recognize the ID and associates it with the existing connection state, allowing the connection to continue seamlessly without interruption.   Source : https://www.researchgate.net/figure/Comparison-of-HTTP-versions_fig1_312560536, https://blog.cloudflare.com/http3-the-past-present-and-future/  ","version":"Next","tagName":"h3"},{"title":"HTTPS​","type":1,"pageTitle":"HTTP & HTTPS","url":"/cs-notes/computer-networking/http-https#https","content":" While using HTTP as protocol to retrieve webpage, the information are sent in plain text. This means while information are transmitted, it can be read easily by anyone who has access to the network traffic, this is called eavesdropping. Another bad thing that could happen is someone could manipulate the sensitive information or injecting a malicious code, this is called Man-in-the-Middle Attack (MITM).  The browsing behavior and activities of users can be easily monitored and tracked by various entities, including internet service providers (ISPs), advertisers, and malicious actors. This compromises user privacy and can lead to targeted advertising, profiling, or misuse of personal information.  Hypertext Transfer Protocol Secure (HTTPS) is the secure version of HTTP. It employs encryption mechanisms to ensure secure transmission of data over a computer network. HTTPS uses an encryption protocol called Transport Secure Layer (TLS).  ","version":"Next","tagName":"h2"},{"title":"HTTPS Process​","type":1,"pageTitle":"HTTP & HTTPS","url":"/cs-notes/computer-networking/http-https#https-process","content":" TLS protocol involves a handshake process, after connection is established or the three-way TCP handshakes in HTTP is done. TLS handshakes is required to establish a secure connection, in the process there will be some information exchange including TLS protocol version, server's identity or certificate, and exchanging cryptographic keys.  Here is a high-level explanation for HTTPS process :  TCP Handshake : TCP handshakes which is the same as the beginning of HTTP process, this includes sending SYN and ACK message. Certificate Check : The client will send a &quot;Client Hello&quot; message to the server, indicating the TLS protocol versions and cryptographic algorithms it supports to encrypt the data. The server will respond with a &quot;Server Hello&quot; message, selecting the highest TLS version and cryptographic algorithms that both the client and server support. The server presents its digital certificate, which is a digital document that provide authenticity and trustworthiness of a website or server. The client verifies the certificate's authenticity and checks if it is issued by a trusted Certificate Authority (CA). After the certificate exchange, the server sends a &quot;Server Hello Done&quot; message to indicate that it has completed its part of the handshake. This message serves as a signal to the client that it can proceed with the next step. Key Exchange : In the next step, there will be an asymmetric key exchange where the client and server exchange keys. This key exchange process includes sending the necessary information required to encrypt the data. This information typically includes the key exchange algorithm, encryption algorithm, and hash function to be used for the TLS protocol. Data Transmission : After the key exchange, a session key is generated. The session key is a symmetric encryption key used to encrypt and decrypt the data. The client possesses the public key, while the server has the corresponding private key. In simple terms, the session key can only be used with the public and private keys owned by the client and server. The data is encrypted using the session key, enabling secure transmission between the client and server.  HTTPS is the combination of symmetric and asymmetric encryption algorithm. Asymmetric only happens from certificate check to the key exchange process. During data transmission, symmetric algorithm will be used.   Source : https://blog.bytebytego.com/p/how-does-https-work-episode-6  tip Find more about general encryption and its terminology including public and private key, key exchange. Also, about symmetric and asymmetric encryption. ","version":"Next","tagName":"h3"},{"title":"LAN & WAN","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/lan-wan","content":"","keywords":"","version":"Next"},{"title":"LAN​","type":1,"pageTitle":"LAN & WAN","url":"/cs-notes/computer-networking/lan-wan#lan","content":" Local Area Network (LAN) is a computer network that covers small geographic area, typically in single building or group of building that are closes. LANs are commonly used in homes, offices, schools, and other small-scale environments.  LANs connect multiple devices within the network, such as computers, printers, servers, switches, and routers. These devices are interconnected using wired technologies like Ethernet cables or wireless technologies like Wi-Fi.  LANs also supports the sharing of resources among connected devices. This includes sharing files, printers, internet access, and other network-connected devices or services.   Source : https://www.arduinoindonesia.id/2023/05/penjelasan-tentang-jaringan-lan-local-area-network.html  ","version":"Next","tagName":"h3"},{"title":"WAN​","type":1,"pageTitle":"LAN & WAN","url":"/cs-notes/computer-networking/lan-wan#wan","content":" Wide Area Network (WAN) on the other hand is a type of computer network that spans a large geographic area, connecting multiple LANs (Local Area Networks) or other networks together. These locations can include offices, campuses, data centers, or remote sites and larger area such as city and country.  WANs often rely on the services of telecommunication companies or internet service providers (ISPs) to establish the connectivity and data transmission between locations. Because of long distance, the speed may be lower than LAN, although this also depend to the cable or infrastructure used.  WANs also support a wide range of applications and services, including file sharing, email, video conferencing, remote access, cloud computing, and centralized data storage.   Source : https://www.sangfor.com/glossary/cloud-and-infrastructure/what-is-wide-area-network-wan  ","version":"Next","tagName":"h3"},{"title":"Virtual Local Area Network (VLAN)​","type":1,"pageTitle":"LAN & WAN","url":"/cs-notes/computer-networking/lan-wan#virtual-local-area-network-vlan","content":" Virtual Local Area Network (VLAN) is a technique used to create logical, isolated networks within a physical local area network (LAN). Devices within a VLAN can communicate with each other as if they were connected to the same physical network, even if they are physically located in different areas or connected to different switches.  By dividing network traffic, VLAN offers simpler network, traffic reduction, and better security. Devices in one VLAN cannot directly communicate with devices in another VLAN unless specific routing or firewall rules are in place.  Network division works by assigning VLAN ID or VLAN tag to each VLAN. These IDs are used to uniquely identify which VLAN a network packet or frame belongs to. Network devices such as switch may be configured to associate specific ports with particular VLANs. This association determines which VLAN a device belongs to. Devices connected to the same VLAN-configured ports are considered members of the same VLAN.   Source : https://bardimin.com/network/mengenal-vlan-dalam-jaringan/ ","version":"Next","tagName":"h3"},{"title":"IP Address","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/ip-address","content":"","keywords":"","version":"Next"},{"title":"Type of IP Address : IPv4 vs IPv6​","type":1,"pageTitle":"IP Address","url":"/cs-notes/computer-networking/ip-address#type-of-ip-address--ipv4-vs-ipv6","content":" There are two types of IP address, IPv4 and IPv6. The difference between them is how many possible IP address can be made.  IPv4​  In IPv4, an IP address consist of four sets of decimal number ranging from 0 to 255 separated by period. Each set of number represents 8 bits, resulting in total 32 bits for the entire address. For example, 192.168.0.1, 192.168.123.132, 102.168.212.226.  Computer will then convert the IP address to binary format that only uses 0 and 1. Find out more here to know how to convert decimal to binary or the opposite.  An IPv4 IP Address is 32 bits, this mean if we try all possible combination between each set of number, there is 2564256^42564 or approximately 4.3 billion unique combination possible.   Source : https://id.wikipedia.org/wiki/Alamat_IP_versi_4  IPv6​  Even though IPv4 has so many unique combination, in the modern day of networking, 4.3 billion is not enough to uniquely identify all device.  The difference is, IPv6 consist of eight segment of 16-bit hexadecimal value, allowing for a total of 4 hexadecimal digits per segment (1 hexadecimal digit = 4 bit), each separated by colon. For example, 2001:0db8:3a4c:0012:0000:8a2e:0370:abcd, fe80:0000:0000:0000:1234:5678:abcd:ef01, 2a02:abcd:1234:5678:9abc:def0:fedc:ba09.  The full address results in 16-bit×8=128-bit16\\text{-bit} \\times 8 = 128\\text{-bit}16-bit×8=128-bit. With 128 bit, we will be able to generate 21282^{128}2128 or approximately 340 undecillion of unique IP address.  IPv6 is designed to be compatible with existing IPv4 networks. This mean, it can still communicate with devices and networks running IPv4. The common way to do this is through the technique called dual stack. In dual stack, the device will be able to read both IPv4 and IPv6 address and choose based on the appropriate address.   Source : https://en.wikipedia.org/wiki/IPv6_address  ","version":"Next","tagName":"h3"},{"title":"Public & Private IP Address​","type":1,"pageTitle":"IP Address","url":"/cs-notes/computer-networking/ip-address#public--private-ip-address","content":" Public &amp; Private IP address are type of IP address based on where is IP address assigned.  Public IP Address​  A public IP address is assigned by an Internet Service Provider (ISP) and is globally unique. It is used to identify a device on the internet and allows it to communicate with other devices or services across the internet.  Public IP addresses are routable on the internet and can be accessed from anywhere in the world. Examples of public IP addresses for IPv4 and IPv6 respectively are 203.0.113.1 and 2001:db8🔢5678::1.  Private IP Address​  A private IP address is used within a private network such as home network. These IP addresses are not globally unique and are reserved for use within private networks only. It is assigned by the local network and you can't connect to internet with it, it will need to be converted to public IP address first.  The purpose of private IP address is to uniquely identify device while also reducing the exhaustion of IP address. So a particular device on a network may have a same IP address with other device in other network, but this is okay because all the device on that network will have a public IP address to connect to the internet or to connect to the other network.   Source : https://www.avg.com/en/signal/public-vs-private-ip-address ","version":"Next","tagName":"h3"},{"title":"NAT","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/nat","content":"NAT Main Source : NAT Explained - Network Address Translation | PowerCert Network Address Translation (NAT) is a service in a router that translates an IP address into another IP address. One of the primary purpose of NAT is to conserve the limited number of IPv4 IP address. There are approximately 4.3 billion unique IPv4 addresses. However, this is not enough to uniquely identify all devices around the world. To address this, engineers introduced two types of IP addresses: public and private. Public IP addresses are used to uniquely identify devices and are publicly registered on the internet. Any device that wants to connect to the internet needs a public address. Private IP address, on the other hand, are not registered to the internet and only used within local or private network such as homes or business. Using a combination of public and private IP addresses, the router has the public IP address while all the devices within the network use private addresses. This allows us to overcome the limitations of the limited pool of public IP addresses. NAT acts as a service that translates the public IP owned by the router into private addresses and the router will assign private address to each devices. It can also do the otherwise, if a device wants to connect to the internet, the private address will be translated into public address along with the port number and outgoing packet of data. Source : https://community.fs.com/blog/what-is-nat-and-what-are-the-benefits-of-nat-firewalls.html","keywords":"","version":"Next"},{"title":"MAC Address","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/mac-address","content":"","keywords":"","version":"Next"},{"title":"MAC Address Assignment​","type":1,"pageTitle":"MAC Address","url":"/cs-notes/computer-networking/mac-address#mac-address-assignment","content":" A MAC address is a 48-bit (6-byte) identifier, typically represented as a series of six pairs of hexadecimal digits, separated by colons or hyphens. For example, a MAC address might look like : 00:1A:2B:3C:4D:5E, 00-B0-D0-63-C2-26. With hexadecimal, it is available from 00-00-00-00-00-00 through FF-FF-FF-FF-FF-FF.  The MAC address is assigned by the manufacturer of the network interface and is usually burned into the device's hardware during production. It is divided by two parts, the first 3 segment identifies the manufacturer and the last 3 segment which is a random bytes that identifies device in a network.   Source : https://learntomato.flashrouters.com/what-is-a-mac-address-how-to-find-my-mac-address/  ","version":"Next","tagName":"h3"},{"title":"Purpose of MAC Address​","type":1,"pageTitle":"MAC Address","url":"/cs-notes/computer-networking/mac-address#purpose-of-mac-address","content":" IP address might change sometimes because of various reason such as configuration change or DHCP. MAC address act as a physical hardware identifier that never changes. As it is assigned permanently by manufacturer, no two devices should have the same MAC address.  IP addresses facilitate communication between devices across networks, while MAC addresses provide physical addressing and facilitate communication within a local network segment. Overall MAC address is used in the data link layer of OSI model while IP address operator on the network layer.   Source : https://youtu.be/TIiQiw7fpsU?t=416 ","version":"Next","tagName":"h3"},{"title":"Network Security","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/network-security","content":"Network Security See Computer Security &gt; Network Security","keywords":"","version":"Next"},{"title":"Network Encryption","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/network-encryption","content":"","keywords":"","version":"Next"},{"title":"SSL/TLS​","type":1,"pageTitle":"Network Encryption","url":"/cs-notes/computer-networking/network-encryption#ssltls","content":" Secure Sockets Layer (SSL) is one of the first cryptographic protocols to provide secure communication over a computer network. It uses public key encryption, also known as asymmetric encryption. There will be public and private key, these key are a piece of information that is used to &quot;lock&quot; or &quot;unlock&quot; data, just like a key in real life.  Handshakes (Client &amp; Server Hello) : To establish a connection, SSL protocol involves a handshaking process. Handshaking indicates that the client wants to communicate with the server, they will also exchange some required information. The client will send a Client Hello message that contains SSL version, a random number, and a list of cipher suites (encryption algorithms) that the client can use. The server will respond with a Server Hello message. This message includes the SSL version selected for the connection, another random number, and the cipher suite chosen from the client's list of supported cipher suites. Certificate Exchange : The server will send its digital certificate to the client. The certificate contains the server's public key, which is used for encryption. The client verifies the certificate to know if the server is trustworthy or not by checking its validity, the issuing authority, and the server's domain name. The certificate is necessary as Google may flag a website as not secure if they are not SSL protected and may be penalized in the search ranking. Key Generation : The server will generate a key pair consisting of a public key and a private key. The private key is kept securely by the server, while the public key is made available to anyone who wants to establish a secure connection with the server. Data Transmission : Encryption : The server sends its public key to the client. The client uses this public key to encrypt data that will be sent to the server. The encryption process ensures that only the server possessing the corresponding private key can decrypt and read the data. The data can now be transmitted securely over the network. Decryption : Upon receiving the encrypted data, the server uses its private key to decrypt the data. The private key is kept secret and known only to the server, ensuring that only the server can decrypt and access the original data.   Source : https://avinetworks.com/glossary/ssl-security/  TLS​  Transport Layer Security (TLS) is a newer cryptographic security protocol, it is the successor of SSL. Overall the process is similar, TLS supports a wider range of cryptographic algorithms and is considered more secure.  ","version":"Next","tagName":"h3"},{"title":"SSH​","type":1,"pageTitle":"Network Encryption","url":"/cs-notes/computer-networking/network-encryption#ssh","content":" Telnet​  To communicate with a remote computer such as server, we can use telnet. Telnet is client/server protocol that provides access to the remote systems on local area networks or the Internet. Telnet is a command line tool, meaning there is no graphical user interface, you can only type command using keyboard.  Telnet offers the ability to tell server to do task such as running program, delete file, transfer file, start or stop service, and etc.  Telnet is an old technology, it operates over the TCP/IP network. Telnet is not very secure, it transmits data, including usernames, passwords, and commands, in clear text, meaning the information is not encrypted. This makes it susceptible to eavesdropping and interception by malicious actors. As a result, Telnet is considered insecure for transmitting sensitive information over public networks, such as the internet.  Secure Shell (SSH) is network protocol that provides a secure and encrypted method of communication between two computers. SSH is the secure replacement over the old telnet, it is commonly used for remote login and secure file transfer over an unsecured network, such as the internet.  SSH supports password-based authentication and public key encryption, it encrypts the entire communication session, including the transmitted data, commands, and responses.  SSH Process​  Connection Initialization : The client initiates a connection request to the SSH server. The server listens on a designated port (typically port 22) for incoming SSH connections. Key Exchange : The client and server perform a key exchange process to establish a session key which is a symmetric encryption used for encrypting and decrypting of the data (typically uses the asymmetric encryption). Encryption : Once the session key is established, the client and server use symmetric encryption algorithms to encrypt the data transmitted between them. This ensures the confidentiality of the information. Additionally, SSH provides integrity checks using cryptographic hash functions to detect any tampering or modification of the data during transmission. User Authentication : The client authenticates itself to the server, by providing a username and password or by using public-key cryptography for authentication, where the client presents a public key to the server for verification. Shell or Command Execution : After successful authentication, the SSH server provides the client with a secure shell or executes a specific command requested by the client. This allows the client to interact with the remote system as if they were directly accessing it. Secure Data Transfer : SSH can also provide secure file transfer capabilities through the SFTP (SSH File Transfer Protocol) subsystem. SFTP allows users to transfer files securely between the client and server, similar to FTP but with the added security of SSH.   Source : https://course-net.com/blog/ssh-adalah/ ","version":"Next","tagName":"h3"},{"title":"Network Topology","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/network-topology","content":"","keywords":"","version":"Next"},{"title":"Wired​","type":1,"pageTitle":"Network Topology","url":"/cs-notes/computer-networking/network-topology#wired","content":" Star Topology : Device is connected directly to a central device, such as a switch or hub. All data traffic flows through the central device, which manages the communication between devices. If one device fails, it does not affect the rest of the network, but if the central device failes, it affects all devices and it's called single point of failure. Ring Topology : Devices are connected in a closed loop, where each device is connected to two neighboring devices. Data travels around the ring in one direction, and each device extracts the required data as it passes by. If one device or link fails, the entire network can be affected. Bus Topology : All devices are connected to a common backbone or a single communication line. Data is transmitted along the backbone, and each device listens to the data and picks up the relevant information. However, if the backbone fails, the entire network may become inoperable. Mesh Topology : In a mesh topology, every device is connected to every other device in the network. This provides redundant paths and enhances network reliability. Mesh topologies can be classified as full mesh, where every device has a direct connection to every other device, or partial mesh, where only some devices have multiple connections. Tree (Hierarchical) Topology : A tree topology is a combination of bus and star topologies. Devices are arranged in a hierarchical structure, where multiple star topologies are connected to a main bus backbone. This allows for scalability and easier management of large networks. Hybrid Topology : Hybrid topologies are a combination of two or more different topologies. For example, a network might combine elements of a star topology and a mesh topology to benefit from the advantages of both.   Source : https://simple.wikipedia.org/wiki/Network_topology  ","version":"Next","tagName":"h3"},{"title":"Wireless​","type":1,"pageTitle":"Network Topology","url":"/cs-notes/computer-networking/network-topology#wireless","content":" Infrastructure Topology : This topology combine wired and wireless. Switch connects to a modem and the wireless devices will connect to the to a central access point (AP) or a wireless router. The access point will then connect to the switch. Source : https://www.electroniclinic.com/network-topologies-start-ring-mesh-bus-tree-hybrid-ad-hoc-and-wireless-topology/ Ad hoc : Ad hoc is a peer-to-peer network communication, meaning the devices communicate with each other directly without the need for a central access point. Source : https://www.researchgate.net/figure/Network-topology-ad-hoc-mode_fig1_236013412 Wireless Mesh Topology : This is the wireless version of mesh topology. A device will connect to an access point, the access point will then connect to another access point until it reach the gateway. Because every devices are connected with each other, there will be many routes and the determined route should be the most efficient path. Source : https://www.intechopen.com/chapters/66938 ","version":"Next","tagName":"h3"},{"title":"Networking Command","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/networking-command","content":"","keywords":"","version":"Next"},{"title":"ipconfig​","type":1,"pageTitle":"Networking Command","url":"/cs-notes/computer-networking/networking-command#ipconfig","content":" ipconfig displays network configuration within a TCP/IP network providing information such as IP addresses, subnet masks, and default gateways. The specific command depends on the operating system. On Windows, it's ipconfig, while on Unix-like systems (Linux, macOS), it's ifconfig.  The ipconfig /all displays the full TCP/IP configuration of your network adapters. There is also a is commonly used command, ipconfig /flushdns. It is used to clear the locally stored Domain Name System (DNS) information on a device. These cache are stored to speed up future request to the same domain.   Source : https://en.wikipedia.org/wiki/Ipconfig  ","version":"Next","tagName":"h3"},{"title":"ping​","type":1,"pageTitle":"Networking Command","url":"/cs-notes/computer-networking/networking-command#ping","content":" The ping command is a commonly used network utility that sends Internet Control Message Protocol (ICMP) echo request packets to a specific IP address or domain name and measures the round-trip time (RTT) for the packets to reach their destination and return.  The primary purpose of the ping command is to check the connectivity and reachability of a network host or IP address and to measure the network latency or response time. It is often used for troubleshooting network connectivity issues and diagnosing network problems.  For example, we may send a request to a server to know how long would it take to get a response. The packet loss refers to how many data in packets are lost in delivery, the higher means the worse connectivity.   Source : https://support.n4l.co.nz/s/article/How-to-use-Ping  ","version":"Next","tagName":"h3"},{"title":"tracert​","type":1,"pageTitle":"Networking Command","url":"/cs-notes/computer-networking/networking-command#tracert","content":" tracert command is a network diagnostic tool available on Windows operating systems. It is used to trace the route that packets take from your computer to a target destination (IP address or domain name) on a network.  The tracert command works by sending a series of Internet Control Message Protocol (ICMP) Echo Request packets with incrementally increasing Time to Live (TTL) values. Each packet is sent to the target destination, and as it passes through routers along the network path, the routers decrement the TTL value. When the TTL reaches zero, the router discards the packet and sends an ICMP Time Exceeded message back to the sender. By analyzing these ICMP messages, the tracert command determines the IP addresses of the routers that the packets traverse.   Source : https://www.easeus.com/knowledge-center/tracert-traceroute-command.html  ","version":"Next","tagName":"h3"},{"title":"netstat​","type":1,"pageTitle":"Networking Command","url":"/cs-notes/computer-networking/networking-command#netstat","content":" netstat, short for network statistics is a command-line network utility available on various operating systems, including Windows, Linux, and macOS. It provides information about network connections, network interfaces, and network statistics.  Some of the information are :  Active Network Connections : netstat can show a list of currently active network connections on the system. This includes information such as the local and remote IP addresses, port numbers, connection state (e.g., established, listening, closed), and the process ID (PID) associated with each connection. Listening Ports : It can display a list of network ports on which the system is listening for incoming connections. This information can be useful to identify which services or applications are actively accepting network connections. Network Interface Statistics : netstat can provide statistics and information about the network interfaces on the system, including the number of packets sent and received, errors, collisions, and other interface-specific details. Routing Table : It can show the system's routing table, which contains information about network routes, gateways, and interface assignments. This information is crucial for determining how network traffic is routed between different networks.   Source : https://www.geeksforgeeks.org/netstat-command-linux/ ","version":"Next","tagName":"h3"},{"title":"Ports","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/ports","content":"","keywords":"","version":"Next"},{"title":"Port Forwarding​","type":1,"pageTitle":"Ports","url":"/cs-notes/computer-networking/ports#port-forwarding","content":" Port is used together with IP address to communicate. The IP address is used to know the location of the destination, while port is to specify what type of communication we want to do. For example, an HTTP request is made to http://www.google.com, the IP address of google.com is 215.114.85.17, with the port 80 it will become 215.114.85.17:80.  Port Forwarding is a technique used to redirect network traffic from one IP address and port combination to another IP address and port combination. It allows incoming connections or data packets destined for a specific port on a router or firewall to be forwarded to a different port or device on the local network.  Our router receives data from external network along with our router's IP address as the destination and also a port. Port forwarding can be thought as the router translating the IP address with the port, so it will be redirected to local device on the network.  The router doesn't know how to process the port received from external network without any port forwarding configuration.   Source : https://getquicktech.com.au/blog/benefits-of-port-forwarding/  ","version":"Next","tagName":"h3"},{"title":"Port Scanner​","type":1,"pageTitle":"Ports","url":"/cs-notes/computer-networking/ports#port-scanner","content":" A port scanner is a technique used to discover which ports are open on a specific target system or network. Port scanning is often performed by security analysts, network administrators, or attackers to gather information about network services and identify potential vulnerabilities.  Port States​  Ports can be in different states based on the response received during scanning:  Open : If a port responds to the scan, it is considered open, indicating that a service or application is actively listening on that port. Closed : If a port responds with a specific message indicating that it is closed, it means that no service or application is listening on that port. Filtered : If a port does not respond to the scan or the response is blocked by a firewall or network filtering device, it is considered filtered. The port scanner cannot determine whether the port is open or closed.  This port state is related to port forwarding. If we haven't configured the port forwarding in our router, the default state will be closed.   Source : https://codepen.io/umarcbs/pen/NWradwg  Scan Types​  Port scanners use different scan types or techniques to gather information about the target ports. Some common scan types include:  TCP Connect Scan : This scan attempts to establish a full TCP connection with the target port to determine if it is open, closed, or filtered. SYN/Stealth Scan : Also known as half-open scanning, this technique sends SYN packets to the target ports and analyzes the responses to determine the port status without completing the full connection. UDP Scan : UDP ports are connectionless, so UDP scanning involves sending UDP packets to the target ports and analyzing the responses.   Source : https://www.mdpi.com/1424-8220/20/16/4423  tip Find out about TCP and UDP ","version":"Next","tagName":"h3"},{"title":"OSI Model","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/osi-model","content":"","keywords":"","version":"Next"},{"title":"OSI Layers​","type":1,"pageTitle":"OSI Model","url":"/cs-notes/computer-networking/osi-model#osi-layers","content":" Physical : It is the stage where data is transmitted through physical medium such as coaxial cable, optic fiber, radio signal, and etc. The data is first encoded and converted into binary digits. Binary digits are then transformed into electrical signal, a simple example would to map 0s to low voltage signal and 1s to high voltage signal. This layer also involves many other signal transmission technique. Source : https://digitalmediaglobe.com/understanding-the-layers-of-the-osi-model/ Data Link : The data link layer is responsible for handling data transfer between the network. Before we establish the connection, data link layer ensure an error-free transfer to the network. Data link is divided into two sublayers : Logical Link Control (LLC) : Data to be sent over the network will be encapsulate and will be added headers and trailers, this is called frames. They can contain source and destination address, these are useful to distinguish between other data transmission. LLC layer establishes, maintains, and terminates logical connections between devices. LLC ensures that the receiving device can handle the incoming data and regulates the rate at which data is transmitted. It also detect tranmission error such as corrupted frames. Medium Access Control (MAC) : This sublayer of data link is responsible for managing the physical media data transmission in a computer network. It ensures that multiple devices sharing the same network segment take turns transmitting data to avoid collisions and ensure efficient and fair communication. Source : https://www.pearsonitcertification.com/articles/article.aspx?p=438038&amp;seqNum=4 Network : Network layer provide end-to-end packet forwarding and routing within a network. Packet is a unit of data that contains header, payload (actual data), and sometimes a trailer. While sending the packets, network layer assign a unique address as identifier to each devices on the network. Sending packets include deciding optimal path for transmission from the source to the destination. If a packets is too large or it doesn't fit within the maximum transmission unit (MTU), network layer may divide it again into small pieces called fragment and it will reassembles the fragments again at the receiving end. Source : https://abitofnoise.com/blog/networking-fundamentals-osi-model-layer-3-network-layer Transport : Transport layer ensures again the data is transmitted sent and received correctly and in the proper order by the destination device. The data, again needs to be small enough and doesn't exceed the maximum transmission unit (MTU). The transport layer will divide it into another terms called &quot;segments&quot;. Transport layer also ensures the same thing as before, it ensures data is correct and error-free, prevent overwhelming between sender and receiver, establish connection-oriented protocols, such as TCP (Transmission Control Protocol) Source : https://www.lifewire.com/layers-of-the-osi-model-illustrated-818017 Session : A data exchange at a time is called a session. Session layer function is to establish, manage, and terminate communication sessions between two network devices. When communication between devices are completed, the session layer is responsible for terminating the session. In some scenario when a session is interrupted, session layer allows to re-establish the previous session. Source : https://www.atatus.com/ask/osi-model-layers-and-its-functions Presentation : While transmitting data, we encode it and often compress it for more efficient bandwidth. Presentation layer handles data encryption, data decryption, data compression, data decompression. It ensures data will be correctly interpreted by the receiver. Source : https://realpars.com/osi/ Application : After data is received, it can be presented into user that is able to interpret data received from transmission such as web browser. This layer is responsible for providing network services directly to the end-user or application. It uses internet protocols including HTTP (Hypertext Transfer Protocol) for web browsing, SMTP (Simple Mail Transfer Protocol) for email communication, FTP (File Transfer Protocol) for file transfer, and many more. Source : https://www.corelangs.com/html/introduction/first-page.html, https://jenkov.com/tutorials/html4/the-web.html ","version":"Next","tagName":"h3"},{"title":"Proxy","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/proxy","content":"","keywords":"","version":"Next"},{"title":"Purpose of Proxy​","type":1,"pageTitle":"Proxy","url":"/cs-notes/computer-networking/proxy#purpose-of-proxy","content":" Normally when we ask a request to website or server, we will send the request directly. With proxy, it can act as a middleman. This makes proxy have several useful purpose :  Privacy : While connecting to the internet, our IP address are visible. If we use a proxy server, our address will only be visible to the proxy server itself, it will be hidden from the internet. Source : https://youtu.be/5cPIukqXe5w?si=GaAuvhmmFIJLyK-h&amp;t=83 Caching : After requesting and receiving resource such as web pages, images, or files from the internet, the proxy server can store this on their database. By storing the resource, if a clients request the same resources again, the proxy server will just retrieve them without the needs to communicate with the server again. Caching can improve performance by reducing the response time and bandwidth usage. Source : http://www.cs.cornell.edu/courses/cs519/2003sp/homework/webproxy/webproxy.html Security &amp; Filter : Proxies can act as a security barrier between clients and servers by inspecting network traffic for potential threats, such as malware, viruses, or unauthorized access attempts. This can include filtering or blocking certain types of content or websites.  ","version":"Next","tagName":"h3"},{"title":"Reverse Proxy​","type":1,"pageTitle":"Proxy","url":"/cs-notes/computer-networking/proxy#reverse-proxy","content":" Unlike a traditional forward proxy, which forwards client requests to a server, a reverse proxy receives requests from clients and forwards them to the appropriate backend servers on behalf of the clients.  The term &quot;reverse&quot; in reverse proxy refers to the direction of the proxying process. Instead of clients accessing servers directly, they interact with the reverse proxy, which then determines the appropriate backend server to handle the request.  By using reverse proxy, we can achieve :  Load Balancing : Reverse proxy server act as the interface from the client to server. It can distribute client requests across multiple backend servers to balance the load and prevent any single server from being overwhelmed. Security &amp; Privacy : It can improves security from malicious request from client and will hides server's IP address from the client, the client will only see the proxy server IP address.   Source : https://securityboulevard.com/2023/04/what-is-reverse-proxy-how-does-it-works-and-what-are-its-benefits/ ","version":"Next","tagName":"h3"},{"title":"Router","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/router","content":"","keywords":"","version":"Next"},{"title":"Hub & Switch vs Router​","type":1,"pageTitle":"Router","url":"/cs-notes/computer-networking/router#hub--switch-vs-router","content":" Hub and switch are networking device used for local or private network, it can't send data to external network. This is because hub and switch doesn't have public IP address, as you need it to communicate with external network like internet.  A network may have hub or switch to connect the local device and have a router to externally connect with other network. Data transmission from the local network won't be forwarded to external network by router. Only if the local device wants to send it outside using the destination IP address.  Hubs and switches can be thought as the device you use to make network, while routers are used to connect them.   Source : https://youtu.be/1z0ULvg_pW8?si=GkDgO98d3d0uKt43&amp;t=401 ","version":"Next","tagName":"h3"},{"title":"Routing","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/routing","content":"","keywords":"","version":"Next"},{"title":"Static & Dynamic Routing​","type":1,"pageTitle":"Routing","url":"/cs-notes/computer-networking/routing#static--dynamic-routing","content":" In packet forwarding, there are two main approach :  Static Routing : Static routing is a manual method of configuring the route of packet forwarding. In static routing, the routing paths are predetermined and do not change automatically based on network conditions. This mean if there is network failures, static routing can't adapt. Dynamic Routing : Dynamic routing protocols automate the process of deciding the route in packet forwarding. Router that forwards packet to the router will also send information about network topology, reachability, and link status. Based on this information, routers dynamically calculate the best paths for forwarding packets.  ","version":"Next","tagName":"h3"},{"title":"Routing Protocol​","type":1,"pageTitle":"Routing","url":"/cs-notes/computer-networking/routing#routing-protocol","content":" To make the decision about how to forward network traffic, routers have set of rules and algorithm called routing protocol. Routing protocol serves as :  Calculates optimal path to forward the packetIdentify and establish connections with other routers in the network to exchange information such as network addresses and network reachability.  The information related to network destinations and the paths to reach them are stored by router in routing table. Table are updated dynamically if things such as link failures, network additions, or topology modifications happens.  Routing Information Protocol (RIP)​  There are many routing protocol, one of the oldest, simplest, and relatively used for small to medium-sized network is the Routing Information Protocol (RIP).  RIP uses routing table to maintain information about the networks, these table includes how many &quot;hops&quot; or router it takes to reach them. The table is periodically shared with other neighbouring router, and each router will update their table accordingly.  RIP uses a simple metric called &quot;hop count&quot; to measure the distance to a network. Each router adds one to the hop count when it forwards a packet. So, if a packet goes through three routers to reach a network, the hop count would be three.  RIP has a limitation as it only able to hop maximum 15 times. The minimum hop count is 0, which means it is directly connected to the router. The maximum is 16 that denote it is unreachable.   Source : https://www.javatpoint.com/rip-protocol ","version":"Next","tagName":"h3"},{"title":"RTC","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/rtc","content":"","keywords":"","version":"Next"},{"title":"RTC Process​","type":1,"pageTitle":"RTC","url":"/cs-notes/computer-networking/rtc#rtc-process","content":" RTC is a peer-to-peer connection, meaning it doesn't involve server and the participants communicate directly. Server is only involved at the beginning and at the end of communication. RTC is based on session, session is a period of a particular activity. In the context of RTC, it is a period of interaction or communication between participants.  Session Initialization : The RTC protocol begins with session initialization, where participants indicate their intention to communicate. This is typically achieved through a signaling process and through a signaling server. Signaling is a process of coordinating between participants, it includes setting up and closing connection. This process is necessary to inform participants to communicate and synchronize their actions. Media Exchange : Once the session is established, the RTC protocol facilitates the exchange of real-time media streams, such as audio, video, or data. Media streams are typically packetized into smaller units and transmitted over the network. Real-Time Transport Protocol (RTP) is commonly used for this. Session Termination : When participants conclude their communication, the session will be terminated. This involves signaling the end of the session to all participants, releasing resources, and closing the communication channels. Cleanup tasks may include releasing network connections, freeing up system resources, and updating session state information.  ","version":"Next","tagName":"h3"},{"title":"WebRTC​","type":1,"pageTitle":"RTC","url":"/cs-notes/computer-networking/rtc#webrtc","content":" WebRTC is a web real-time communication technology that uses RTC protocol. It establishes the same peer-to-peer connections for voice calling, video chat, file sharing, and other forms of real-time communication.  To begin connection, the participant need to send an &quot;offer&quot; to the signaling server. It contains necessary information including protocol version, session name, session identifier, session title, email address, phone number, and other important information. The signaling server will exchange the information between each participant.  Peer-to-peer connection could bring some problems, many devices uses private IP address assigned by Network Address Translation (NAT). These private IP addresses are not directly reachable from the public internet. Another problem is some devices have a firewall as network security.  WebRTC uses standard called Interactive Connectivity Establishment (ICE). ICE helps avoiding private IP address problem by utilizing STUN (Session Traversal Utilities for NAT) servers. It enables devices to determine their public IP addresses by contacting these servers, allowing them to communicate with the outside world.  ICE also helps overcoming firewall by utilizing TURN (Traversal Using Relays around NAT) servers. TURN servers act as intermediaries, relaying data between devices when direct connections are not possible.   Source : https://www.techtarget.com/searchunifiedcommunications/definition/WebRTC-Web-Real-Time-Communications ","version":"Next","tagName":"h3"},{"title":"RTP","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/rtp","content":"","keywords":"","version":"Next"},{"title":"RTP Process​","type":1,"pageTitle":"RTP","url":"/cs-notes/computer-networking/rtp#rtp-process","content":" RTP is used for media transmission, so before the transmission happens, these media file need to be encoded first.  Payload Packetization : RTP packetizes the encoded media into smaller units called RTP packets. Each RTP packet consists of an RTP header and a payload, these packet are numbered in sequence to detect packet loss. The header contains version of RTP, payload type or the type of data being carried, timestamp to maintain synchronization, and other information. The payload is the actual media data. Transmission : RTP relies on the UDP protocol, the UDP layer delivers the packets to the appropriate receiving application based on the destination port number. Packet Reassembly : The received RTP packets are reassembled based on their sequence numbers. This ensures that the media data is reconstructed in the correct order. Playout Timing : The timestamp in each RTP packet assists in maintaining proper playout timing at the receiving end. By using the timestamp information, the receiving application or media player can synchronize the playback of the media data. Media Decoding : Once the RTP packets are reassembled, they are typically decoded using the appropriate codec. The decoding process reverses the compression applied during encoding, allowing the media data to be rendered or played out accurately. After being decoded, the media is ready to be played by the application. Source : https://www.oreilly.com/library/view/advanced-infrastructure-penetration/9781788624480/5ce761e5-1024-4556-a0b0-0864a1856de1.xhtml ","version":"Next","tagName":"h3"},{"title":"SIM Card","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/sim-card","content":"","keywords":"","version":"Next"},{"title":"How does it works​","type":1,"pageTitle":"SIM Card","url":"/cs-notes/computer-networking/sim-card#how-does-it-works","content":" SIM cards are used to connect to a cellular network, of course the card needs to be inserted first into the device.  IMSI​  As said before, SIM card has a unique identifier called IMSI. IMSI is typically 15-digit number, the first three digits represent Mobile Country Code (MCC). The next three digits represent the Mobile Network Code (MNC). The remaining digits represent the Mobile Subscription Identification Number (MSIN), which is unique to each subscriber within the network.  The process​  While connecting to a cellular network, the cell provider will generate a random number. They will transform the number by encrypting it using some secret key. The provider will send us the random number, our phone should transform it aswell and the result should be the same.  The secret key is actually the IMSI and authentication key (also stored in SIM card) owned by us. So, if we actually has a valid SIM card or are eligible to connect, than we should be able to get the same result. If it succeed, then we are allowed to connect to the network.  This authentication process is called Challenge-Response Authentication (CRA). Different cellular networking technology (such as GSM, UMTS, LTE, or 5G) may have different process and the security protocols implemented by the network operator.   Source : https://www.wharftt.com/identifying-and-fixing-a-damaged-sim-card-know-the-signs-and-solutions/ ","version":"Next","tagName":"h3"},{"title":"Subnet Mask","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/subnet-mask","content":"","keywords":"","version":"Next"},{"title":"Subnet Mask Calculation​","type":1,"pageTitle":"Subnet Mask","url":"/cs-notes/computer-networking/subnet-mask#subnet-mask-calculation","content":" To determine the network address, subnet mask and IP address needs to be converted into binary format first if they are in decimal number. Subnet mask is applied to an IP address using a bitwise AND operation, the result is the network address.  The last 1 from the binary number of subnet mask define the last part of network portion.   Source : https://youtu.be/s_Ntt6eTn94?t=319  Here is an example of how it is used :   Source : https://youtu.be/s_Ntt6eTn94?t=591 ","version":"Next","tagName":"h3"},{"title":"Socket","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/socket","content":"","keywords":"","version":"Next"},{"title":"How does it works (in high level)​","type":1,"pageTitle":"Socket","url":"/cs-notes/computer-networking/socket#how-does-it-works-in-high-level","content":" The creation of socket takes what type of IP address to use (IPv4 or IPv6) and which type of socket to use. The type of socket refers to what protocol is used to transfer data. Socket provide reliable data transfer by using protocols like Transmission Control Protocol (TCP) and User Datagram Protocol (UDP).  After the creation, socket need to be bound to a specific IP address and port number. The socket then transition to a listening state, waiting for incoming connection requests. When a client socket wants to communicate, it establish a connection to the corresponding server's IP address and port number.  Once a connection is established, both the client and server sockets can send and receive data by the appropriate protocol used. When the communication is complete or the application decides to terminate the connection, the sockets will be closed to release the network resources associated with the socket.   Source : https://www.cloudnweb.dev/2019/6/what-are-web-sockets-implementing-web-sockets-in-node-js(WebSocket that uses TCP protocol) ","version":"Next","tagName":"h3"},{"title":"Server","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/server","content":"","keywords":"","version":"Next"},{"title":"Server Function​","type":1,"pageTitle":"Server","url":"/cs-notes/computer-networking/server#server-function","content":" Server hardware are typically more powerful and have more storage capacity than client devices. It is also optimized and configured to perform a lot of tasks simultaneously. A server may also use dedicated operating system that suitable for concurrent computation.  Here are some common use case of a server :  File server : Stores and manages files that can be accessed by client devices over a network.Web server : Hosts websites and delivers web pages to client web browsers upon request.Database server : Manages databases and provides access to stored data for client applications.Mail server : Handles email communication, routing incoming and outgoing messages between clients.Application server : Runs and manages business applications and software, providing centralized processing and data management.DNS server : Resolves domain names to their corresponding IP addresses, enabling client devices to locate resources on the internet.  Overall the purpose of server is to act as a central repository for shared data or it may also be a devices runs for most of the times to provide service for client that can be accessed anytime.   Source : https://www.networkstraining.com/different-types-of-servers/  ","version":"Next","tagName":"h3"},{"title":"Server Optimization​","type":1,"pageTitle":"Server","url":"/cs-notes/computer-networking/server#server-optimization","content":" Caching : Caching is a technique used to achieve high-speed and efficient operation while accessing data. The idea is a frequently accessed data can be placed in an easier place to access. Server will first check if the requested data exists in cache, if yes, the server can quickly return the data without accessing a database or generating the data from scratch. For example, a web server may cache HTML and CSS files, or other static resources that are frequently requested by clients. Source : https://www.scylladb.com/glossary/database-caching/ Content Delivery Network (CDN) : CDN is a distributed servers strategically located in multiple data centers around the world. When a user requests content, the CDN automatically routes the request to the nearest edge server, minimizing the distance and network latency between the user and the server. For example, a company has main server located in the US. A person connecting from Asia will have a higher latency due to physical distance. The company may choose to build another server in Asia, the new server will connect to the main server. This way we let the server to experience the slower connection while allowing people from Asia can connect with a lower latency or faster connection. Source : https://www.domainesia.com/tips/apa-itu-cdn-dan-fungsi-cdn/ Load Balancing : Load balancing is a technique used to distribute incoming network traffic across multiple servers. The primary purpose of load balancing is to evenly distribute the workload among multiple servers, preventing any single server from becoming overwhelmed or underwhelmed and improving overall system efficiency. Source : https://herza.id/blog/load-balancing-pengertian-cara-kerja-jenis-metode-kelebihan-dan-kekurangan/  ","version":"Next","tagName":"h3"},{"title":"Localhost​","type":1,"pageTitle":"Server","url":"/cs-notes/computer-networking/server#localhost","content":" As said before, we can even use our own computer as server. We can use it as a public server, where people could connect to us. We can also use it for ourselves, meaning our computer will be a server and the one who connect is ourselves. It is referred as loopback network, where we send or request and receive data within the same device, without going through a network.  Localhost need an IP address as well as normal website, the IP is 127.0.0.1. We can also specify a port, for example, localhost:4000 or 127.0.0.1:3500 if using the actual IP address. These ports are used to identify specific services or applications running on your device.   ","version":"Next","tagName":"h3"},{"title":"Switch","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/switch","content":"Switch Main Source : Hub, Switch, &amp; Router Explained | PowerCertWikipedia Network switch Switch is a networking device that is very similar as hub, but it is more intelligent as it provides more advanced features such as improved performance, better bandwidth utilization, and enhanced security. Switch Table, Selective Forwarding &amp; Collision Isolation​ Switch has a memory to store MAC address of all devices connected to it. The purpose of storing device addresses is to efficiently forward packets to the correct destination. For example, computer A wants to send data to computer B. Instead of broadcasting the packet to all connected ports like a hub does, the switch selectively forwards the packet only to the port associated with the destination MAC address. Source : https://www.computernetworkingnotes.com/ccna-study-guide/how-switch-learns-the-mac-addresses-explained.html (with modification) Not only offers efficient forwarding, it also reduce collision when data is transmitted. In hub, when a device sends data to the hub, there's a chance that other device also sends data before. This makes the incoming data from hub collide with the outgoing data that destinate to hub. Security​ Switch allows administrators to control access to the network by limiting the devices that can connect to specific switch ports. Port security enables the configuration of MAC address-based restrictions, allowing only authorized devices with specific MAC addresses to communicate through the switch. This helps prevent unauthorized devices from gaining network access. Network Segmentation​ Switch also supports network segmentation through the use of Virtual Local Area Network (VLAN). VLAN is a a technology that divide a single physical switch into multiple virtual switches or networks. Each VLAN operates as a separate broadcast domain, allowing devices within the same VLAN to communicate as if they were connected to the same physical network, even if they are physically connected to different switch ports. This also reduce the risk of network attacks. Source : https://www.ciscopress.com/articles/article.asp?p=2181836&amp;seqNum=7","keywords":"","version":"Next"},{"title":"UDP","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/udp","content":"","keywords":"","version":"Next"},{"title":"UDP Process​","type":1,"pageTitle":"UDP","url":"/cs-notes/computer-networking/udp#udp-process","content":" Data Packaging : The UDP process starts by dividing data that want to be sent into smaller units called datagrams. Each datagram consist of UDP header that contains source and destination address; port number, and checksum value which is a number that is used to detect error. Checksum value from the sender datagram should be the same as the received datagram. The actual user's data in datagrams is called payload. Addressing : Sender needs to know the IP address and port number of the destination. It is available in the UDP header, it can also resolved using DNS server if needed. Datagram Transmission : The UDP datagram, including the header and payload, will be further encapsulated in Internet Protocol (IP) layer. The IP layer adds its own header containing the ource and destination IP addresses. The resulting IP packet is then sent to the network for transmission. UDP does not provide any mechanisms for reliable delivery. The network will send the IP packets, and the receiver's receives them. However, UDP does not have built-in mechanisms for error recovery, retransmission, or acknowledgment of successful delivery. If packets are lost, duplicated, or arrive out of order, it is the responsibility of the receiving application to handle these issues. Datagram Extraction : On the receiving end, incoming packets will be identified, it will examine the destination port number to determine what application should process this. The UDP layer removes the UDP header, leaving the payload (user data). The receiving application then retrieves and processes the data from the payload.   Source : https://www.spiceworks.com/tech/networking/articles/tcp-vs-udp/ ","version":"Next","tagName":"h3"},{"title":"TCP/IP Model","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/tcp-ip-model","content":"","keywords":"","version":"Next"},{"title":"TCP/IP Layers​","type":1,"pageTitle":"TCP/IP Model","url":"/cs-notes/computer-networking/tcp-ip-model#tcpip-layers","content":" Link : Link layer is the lowest layer in TCP/IP. Similar to OSI model, data to be sent is encapsulated into frames and it will be sent through physical media such as coaxial cables, fiber optic, and etc. It also involves assigning physical address to each device in the networks as a unique identifier. Internet : Sending data from source to destination is called routing. Internet layer assigns logical addresses, known as IP addresses, to devices connected to a network. IP addresses uniquely identify devices on the network and enable end-to-end communication across different networks. Data is then broken down into small pieces called packets. Delivering packets to the network may encounter some issues, internet layer is responsible for determining the best path for packet delivery. Source : https://course-net.com/blog/network-adalah/ Transport : Transport layer is responsible for the reliable and orderly delivery of data between hosts or endpoints. It ensure no data-error between transmission, data is sent and received accordingly and regularly without overwhelming the receiver or congesting the network. Transport layer uses protocol such as TCP and UDP as the set of rules or mechanism to transport the data. Source : https://www.geeksforgeeks.org/differences-between-tcp-and-udp/ Application : The application layer is the topmost layer and is responsible for providing network services directly to the user or application. The TCP/IP model doesn't consider the specific format, encryption, compression of the data received. These are considered by the defined protocol in the application layer. For example, a browser knows the HTTP protocol for hypertext content such as web pages. Example of application layer protocol are : Hypertext Transfer Protocol (HTTP)File Transfer Protocol (FTP)Simple Mail Transfer Protocol (SMTP)Dynamic Host Configuration Protocol (DHCP) Source : https://webeduclick.com/difference-between-ftp-and-smtp/ ","version":"Next","tagName":"h3"},{"title":"TCP Protocol","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/tcp-protocol","content":"","keywords":"","version":"Next"},{"title":"TCP Process​","type":1,"pageTitle":"TCP Protocol","url":"/cs-notes/computer-networking/tcp-protocol#tcp-process","content":" In TCP, data that wants to be send will be broken down into small unit of data called segments. TCP segments contains header that consists of source and destination address; ISN (Initial sequence numbers) which is a uniquely number assigned to each segment to ensure the segment are delivered in order and doesn't conflict with each other, and other header data and the actual data called payload.  Here's a simplified explanation of how TCP works :  Connection Establishment : Before data transmission can begin, TCP establishes a connection between the sender and receiver. This process is known as the &quot;three-way handshake&quot;. SYN : The sender (client) sends a message called synchronization (SYN) to the receiver (server) indicating its intention to establish a connection. SYN-ACK : If the server agrees to establish a connection, the receiver will responds back with a SYN-ACK (acknowledgment) indicating they acknowledge the request. It contains the server's own ISN and acknowledges the receipt of the client's ISN. The server also indicates its readiness to establish a connection. ACK : The client, upon receiving the SYN-ACK segment, sends back an acknowledgment. It sends a TCP segment confirming the receipt of the server's ISN. The client also acknowledges the server's readiness for the connection. At this point, the connection is established, and both the client and server can begin exchanging data. This ACK can be thought as the indicator of approvement whether data transmission or TCP connection should continues. Source : https://www.javatpoint.com/tcp Data Transfer : In TCP connection, data is not transmitted continously. After sending a segment, the sender needs to wait for an acknowledgment (ACK) or an approvement from the receiver again. This makes TCP data transfer reliable and certain, if an ACK is not received within a certain time, the sender assumes the segment was lost and retransmits it. The amount of how many segment can be sent without sending another ACK is called window size, it is also stored in the header of segment. Source : https://packetlife.net/blog/2010/jun/17/tcp-selective-acknowledgments-sack/ Flow Control : TCP employs flow control mechanisms to prevent the receiver from being overwhelmed by data. The receiver specifies a receive window indicating the amount of data it can accept. The sender adjusts its data transmission rate based on the receiver's window size, ensuring that data is sent at a pace the receiver can handle. Source : https://www.baeldung.com/cs/tcp-flow-control-vs-congestion-control Congestion Control : TCP also includes congestion control mechanisms to prevent network congestion. It monitors the network for signs of congestion, such as packet loss or increased latency. When congestion is detected, TCP reduces its sending rate to alleviate the congestion and maintain network stability. Source : https://youtu.be/kRS4J-m5n04?si=3tWpM33jMlx6schI Connection Termination : When data transmission is complete, TCP performs a connection termination process to gracefully close the connection. This involves a four-way handshake, where both the sender and receiver exchange FIN (finish) messages to indicate their intention to close the connection. Sender Sends FIN : The sender initiates the connection termination by sending a TCP segment with the FIN (Finish) flag set. This segment indicates that the sender has finished sending data and wants to close the connection. Receiver Sends ACK : Upon receiving the FIN segment, the receiver sends back an acknowledgment (ACK) segment to confirm the receipt of the FIN. The receiver acknowledges the sender's request to terminate the connection. Receiver Sends FIN : If the receiver has also finished sending data and wants to close the connection, it will send a TCP segment with the FIN flag set. Sender Sends ACK : Upon receiving the FIN segment from the receiver, the sender responds with an ACK segment to acknowledge the receipt of the receiver's FIN. This acknowledges the receiver's request to terminate the connection. Source : https://www.ionos.com/digitalguide/server/know-how/introduction-to-tcp/ ","version":"Next","tagName":"h3"},{"title":"Wi-Fi","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/wi-fi","content":"","keywords":"","version":"Next"},{"title":"Wireless Access Point (WAP)​","type":1,"pageTitle":"Wi-Fi","url":"/cs-notes/computer-networking/wi-fi#wireless-access-point-wap","content":" Wireless Access Point is a device that enables wireless devices to connect to a wired network. It acts as a bridge between the wired and wireless networks. Wireless access point is the simpler version of Wi-Fi router, the purpose is only to connect from a normal router to each device in the network wirelessly.  A Wi-Fi router is a more advanced device that combines the functions of a wireless access point, a network switch, and a network router into a single device.   Source : https://youtu.be/OxiY4yf6GGg?si=-tqndmm07MA2bh6j&amp;t=178  ","version":"Next","tagName":"h3"},{"title":"Hotspot​","type":1,"pageTitle":"Wi-Fi","url":"/cs-notes/computer-networking/wi-fi#hotspot","content":" Hotspot is a location or an area where wireless internet access is provided, typically using Wi-Fi technology.  Hotspot can be public or private, a public hotspots are Wi-Fi networks that are available for use by anyone within range. Examples include Wi-Fi networks in cafés, airports, libraries, and parks. Public hotspots are designed to be easily accessible to the public and provide internet connectivity for visitors or customers. However, using public hotspot may be dangerous as anyone can access it. A malicious actor could intercept and captures the network traffic transmitted between a user's device and the hotspot, this is called eavesdropping.  Private hotspots, also known as tethering, are Wi-Fi networks created by individuals using their smartphones, tablets, or dedicated mobile hotspot devices. These hotspots are intended for personal use and are secured with a password. Only those who know the password can connect to the network and access the internet through it. Private hotspots are commonly used when a person wants to share their data connection with other devices.   Source : https://youtu.be/ktxC3vDukbc?si=K5IbUJW8XUiRNbuB&amp;t=85 ","version":"Next","tagName":"h3"},{"title":"VPN","type":0,"sectionRef":"#","url":"/cs-notes/computer-networking/vpn","content":"","keywords":"","version":"Next"},{"title":"VPN Benefits​","type":1,"pageTitle":"VPN","url":"/cs-notes/computer-networking/vpn#vpn-benefits","content":" Privacy : VPN will secure your internet traffic and hide your IP address while connecting to a server, a VPN helps protect your privacy and makes it difficult for third parties, such as hackers or government agencies, to monitor your online activities. Security : VPNs use encryption protocols to secure your data, making it unreadable to anyone who intercepts it. In simple term, the request or data sent from server will be converted into a format that is not readable by human and it can only be transformed back by the intended recipient. This is especially important when connecting to public Wi-Fi networks, which are often unsecured and vulnerable to attacks. Remote Connection : We can use VPN as a private network. All the devices will be equiped by VPN services. Even if the devices are in remote places, we can connect it to a central location as if it was a local &quot;private&quot; network. Along with security benefits, we can be sure for transferring sensitive data. Bypass Restrictions : VPNs can help bypass censorship and geolocation restrictions by allowing you to connect to servers located in different regions. This can be useful for accessing blocked websites or services in your country. This works because we are not actually connecting to the actual server, instead the VPN server is the one who connects. We can choose to connect from a VPN server located in a country where the website is not blocked from there.  tip Refers to computer security cryptography section to know more about security and encryption. ","version":"Next","tagName":"h3"},{"title":"Computer Organization & Architecture","type":0,"sectionRef":"#","url":"/cs-notes/computer-organization-and-architecture","content":"","keywords":"","version":"Next"},{"title":"All pages​","type":1,"pageTitle":"Computer Organization & Architecture","url":"/cs-notes/computer-organization-and-architecture#all-pages","content":" COA FundamentalsBoolean LogicCPU ALURegisters &amp; RAMControl UnitCPU Design Input/OutputAssembly LanguageComputer Architecture Von NeumannHarvardISA GPUVector Processors &amp; TPU ","version":"Next","tagName":"h3"},{"title":"Boolean Logic","type":0,"sectionRef":"#","url":"/cs-notes/computer-organization-and-architecture/boolean-logic","content":"","keywords":"","version":"Next"},{"title":"Transistor​","type":1,"pageTitle":"Boolean Logic","url":"/cs-notes/computer-organization-and-architecture/boolean-logic#transistor","content":" Computer can only understand binary 1s and 0s. In electronic systems, such as computers, under the hood, anything is represented and processed using electrical voltages. These voltages can be either high (binary digit 1) or low (binary digit 0).  From high-level language, translated to machine language, then it will be interpreted by each computer hardware, which built in electronic circuits and will be able to generate electrical signals.  Transistor is the building blocks of modern electronic circuits. It is simply a switch that will let current flow from the source (collector) to the target (emitter). When the transistor is turned on, significant current will flow through it. When it is turned off, no significant current will flow through it. Transistor will be turned on or off based on the input voltage or current applied from the source (base).   Source : https://builtin.com/hardware/transistor  The electric signal will be interpreted as 0, when the transistor is turned off. When the hardware interprets electric signal as 1, the transistor must be turned on. Many more transistors will be used together to construct an electronics system.   Source : https://www.researchgate.net/figure/Transistor-symbol-and-binary-interpretation-as-ON-OFF-switch-The-digital-operation-of_fig1_340062372  ","version":"Next","tagName":"h3"},{"title":"Logic Gates​","type":1,"pageTitle":"Boolean Logic","url":"/cs-notes/computer-organization-and-architecture/boolean-logic#logic-gates","content":" When doing complex logic or computation, computer will rely on other component of digital circuits. The logic gates are the building blocks of digital circuits, it performs logical operations on one or more binary inputs to produce a binary output. Basically, logic gate is what makes bitwise operation possible in computer.   Source : https://www.explainthatstuff.com/logicgates.html  AND : Takes two input, only produces (1), if both input are (1).OR : Takes two input, only produces (1), if any of the input is (1).NOT (Inverter) : Takes one input, produces the inverse. Taking input (1) will produce (0) and vice versa.XOR (Exclusive OR) : Takes two input, only produces (1), if only the input is different. It is similar to OR, with difference, the two input must be different.NAND (NOT-AND) : Combination of AND followed by NOT gate. It produces the complement of AND gate.NOR (NOT-OR) : Combination of OR followed by NOT gate. It produces the complement of OR gate.  And many more...   Source : https://computerscienceigsce.wordpress.com/chapter-3/  Under the hood, each logic gates will have different kind of electrical construct. They consist of transistors, which act as switches to control the flow of current.  For example, the OR gate can be constructed using two transistors. The two transistors will be connected in parallel, with the common emitter or the same supply current. When any input produces 1s, transistor will be turned on, allowing current to flow. When the transistor is turned on, the current from emitter will go through it to the output current.   Source : https://www.pctechguide.com/cpu-architecture/principles-of-cpu-architecture-logic-gates-mosfets-and-voltage (with modification) ","version":"Next","tagName":"h3"},{"title":"ALU","type":0,"sectionRef":"#","url":"/cs-notes/computer-organization-and-architecture/alu","content":"","keywords":"","version":"Next"},{"title":"ALU​","type":1,"pageTitle":"ALU","url":"/cs-notes/computer-organization-and-architecture/alu#alu","content":" Overview​    note The V-shape of ALU is just for illustration purposes.  Input of ALU :  Operand : ALU takes two input called operand, which represent the input binaries for operation to be performed. Opcode : It is the instruction on what should ALU do based on the two operand. Status : Status provide information about the current status of ALU or the contextual information from the previous operations. The status is called a flag, it is obtained from a special registers called status registers. Some common flags are : Zero Flag (Z) : This flag is set when the result of an operation is zero. It is cleared when the result is nonzero. Carry Flag (C) : This flag is set when there is a carry or borrow in arithmetic operations such as addition or subtraction. Source : https://knowthecode.io/labs/basics-of-digitizing-data/episode-8 Overflow Flag (V) : This flag is set when there is a signed overflow in arithmetic operations, meaning the result is too large or too small to be accurately represented with the available number of bits. Sign Flag (S) : This flag reflects the sign of the result. It is set when the result is negative and cleared when the result is positive or zero.  Output of ALU :  Result : Represents the result of the arithmetic or logical operation performed based on the input.Status : The status flag that may be generated from the current operation.  Arithmetic​  Half Adder​  The example we have done earlier, which is adding two binary using an OR logic gates will not work for all binary inputs. For example, when we have input of &quot;01&quot; (1 in decimal) and &quot;11&quot; (3 in decimal), if we use OR gates, we will produce &quot;11&quot;, which still correspond to 3 in decimal. Notice that we didn't consider the carry that might have produced during the addition.  We will need to modify the behavior of the addition system. We will also consider the carry by using the carry status flag. So, performing addition on &quot;1&quot; with &quot;1&quot; should produce &quot;0&quot;. The logic gate that will produce result of &quot;0&quot; when given input &quot;1&quot; and &quot;1&quot;, is the XOR gate.  Here is the table of XOR gate possible input and output :  A B S C-out ----------------- 0 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1   Where A and B is the input, S is the result, C-out is the carry output. Now, how can we implement this? To produce S, we can easily use XOR gate, but how would we produce the C-out? One way is to perform AND operation using AND gate on input A and B, it will produce 1 for carry only if both input are 1.  So, we will combine the XOR gate with AND gate, this is called half adder.   Source : https://id.m.wikipedia.org/wiki/Berkas:Half-adder.svg (with modification)  Full Adder​  Notice that half adder doesn't consider the carry from the previous operation. To account for input carry, another circuit called full adder will be used. The full adder will be constructed by combining multiple half adder with additional OR gate.  The first half adder produces the sum S and carry C. The sum S will be inputted again to the next half adder, which takes a carry input C-in. It will produce another sum S, which is the actual sum produced. The C from first half adder will be combined with the C from the second half adder with an OR gate, producing the actual carry C-out.   Source : top, bottom  Summarizing the full adder, it will take 3 input, which is A, B, C-in, and produces output S and C-out :  A B C-in S C-out --------------------------- 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1   Ripple Carry Adder​  That full adder operates on a single bit binary numbers. To extend the concept to process a larger number of bits, we can combine multiple full adder.  For example, to create an 8-bit adder, we would need to combine eight full adders together.   Source : https://www.engineersgarage.com/vhdl-tutorial-21-designing-an-8-bit-full-adder-circuit-using-vhdl/  Each full adder still takes three input, which are A, B, and C-in, which can be the carry from the previous adder. The sum S from sum0 up to sum7 will be concatenated together at the end, producing the 8-bit binary number. The sum0 output represents the least significant bit (LSB) of the sum, while the sum7 output represents the most significant bit (MSB) of the sum.  This 8-bit adder is called ripple carry adder. The problem for this adder is, it is possible that the last adder produces a carry, which mean we will need to add that carry to the next column. However, we are using a limited 8-bit adder, we don't have that 9-bit column place. This problem is called overflow, which occur when we add or subtract number that are too large within our binary representation.  Other Operation​  In fact, other operation such as subtraction, multiplication, and division can be performed with only addition operation.  Subtraction : Subtraction, for example, (2 - 5), can be rewritten as (2 + (-5)), which is an addition operation. To represent the number (-5) in binary, we can use the two complement representation.Multiplication : Multiplication can be performed using iterative addition, 2 × 3 is essentially just 2 + 2 + 2.Division : Division can also be implemented using iterative subtraction, 4 / 2 is 4 - 2. We will repeatedly subtract the divisor from the dividend until the remainder is less than the divisor.  Logic Unit​  The logic unit in ALU performs various logical operations such as AND, OR, XOR, NOT on binary data.  For example, the logic unit is responsible for making logical decisions based on the results of the logical operations. We may use conditional statement (e.g., if input number is negative, then do A, else do B). We can use the logic unit to determine if input number is negative or not. To determine the sign of a number, we can check the sign bit if it represents 1 or 0, using the AND gate.  We will assume that we are using the two complement representation. The leftmost bit will be the sign bit, we will check the sign by doing the logical AND operation with some arbitrary binary number that has &quot;1&quot; as its leftmost bit, and &quot;0&quot; for the remaining bit. When the input is negative number &quot;1___&quot; (the latter bit can be anything), doing the logical AND with binary &quot;1000&quot; will result in &quot;1000&quot;, because the result is not 0, we can conclude that the sign bit is 1, indicating a negative number.  On the other hand, when the input is positive number &quot;0___&quot;, doing logical AND with binary &quot;1000&quot; will result in 0, which indicates a positive number. ","version":"Next","tagName":"h3"},{"title":"COA Fundamentals","type":0,"sectionRef":"#","url":"/cs-notes/computer-organization-and-architecture/coa-fundamentals","content":"","keywords":"","version":"Next"},{"title":"Computer Components​","type":1,"pageTitle":"COA Fundamentals","url":"/cs-notes/computer-organization-and-architecture/coa-fundamentals#computer-components","content":" CPU​  Central Processing Unit (CPU) is the most crucial component in a computer system, it is the &quot;brain&quot; of the computer that is responsible for executing program instructions. A CPU consists of other smaller components :  Control Unit (CU) : The control unit is responsible for managing and coordinating operations in CPU. It fetches instructions from memory, decodes them to determine their meaning, and controls the flow of data between different components of the CPU. Last but not least, it ensures instructions are executed in the correct sequence according to the program's instruction.Memory : Memory is the storage space where data and instructions are stored for the CPU to access. It holds program instructions, data, and variables during program execution. Memory can be organized into different levels : Main Memory : Main memory (RAM) is the primary storage used by computer to temporarily hold data and instructions that the CPU needs to access quickly during program execution. The characteristic of RAM is that it is volatile and requires power to operate. This means that any data present in RAM will be lost after the computer is turned off.Cache : Cache is a small, high-speed memory that are typically located closer to the CPU than main memory. Cache reduce the time required to fetch instructions and data from the main memory. Caches exist in multiple levels, such as L1, L2, and L3, with each level providing progressively larger storage capacity but at slightly slower access speeds.Secondary Memory : Secondary memory is the opposite of main memory, it is non-volatile (requires no power) and it provides long-term storage for programs, data, and files. They are devices like hard disk drives (HDDs) or solid-state drives (SSDs). Registers : Registers are small, high-speed memory units located within the CPU. They are used to store data that the CPU is currently using for its operations. Registers can store intermediate results, memory addresses, and control information. For example, a register can be used for program counter, which is a component to holds the memory address of the next instruction to be fetched and executed.Arithmetic Logic Unit (ALU) : ALU is the component that is responsible for performing arithmetic operations (addition, subtraction, multiplication, and division) and logical operations (such as comparisons, bitwise operations, and boolean operations). The ALU operates on data retrieved from registers and produces results that are stored back in registers.  Regarding instructions, they are made through the source code from the high-level programming language. These source code are translated to machine language which is recognized by the CPU as instruction. Those instructions will be stored in memory sequentially, along with the variables or any other data used in the program.   Source : https://www.learncomputerscienceonline.com/what-is-computer-memory/  Clock : Clock is a timing mechanism that synchronizes the operations of the CPU. It generates regular electrical pulses called clock cycles. Each clock cycle represents a fixed unit of time, and the CPU's operations are synchronized to the clock. The clock speed determines the number of clock cycles per second and is measured in hertz (Hz). A higher clock speed generally allows the CPU to execute instructions more quickly. For example, if the clock speed is 1 GHz, it will be able to execute 1 billion instructions per second. By instructions, it doesn't mean any specific calculation, it is the action the CPU can do, it can include fetching instructions, decoding instructions, executing arithmetic or logical operations, or accessing and updating data in registers or memory.Buses : Buses are the components that serves as the communication pathways that transfers data between different components of the computer system. They provide a means for the CPU, memory, and peripheral devices to exchange information. There are different types of buses, including the address bus (for specifying memory locations), the data bus (for transferring data), etc.Input/Output (I/O) Unit : The I/O unit handles communication between the CPU and external devices, such as keyboards, mice, displays, storage devices, and network interfaces. They convert signal from various device to the CPU and convert it back for output. For example, sound wave recorded by microphone will be converted to electrical signal, which will be converted to digital signal for CPU to understand, and then converted back to sound wave to the speaker.   Source : https://www.geeksforgeeks.org/computer-and-its-components/  RAM​  Random Access Memory (RAM) is a type of memory used for temporary data storage while a computer is running. The CPU can access data in RAM much faster than it can access data from other types of storage, such as hard drives or solid-state drives (SSDs).  RAM is organized into a large number of memory cells, each of which can store a fixed amount of binary data (bits). These cells are arranged in a grid-like structure, with each cell having a unique address. This allows the CPU to directly access and retrieve data from any specific memory location in RAM, hence the term &quot;random access.&quot;  A single memory cell in RAM stores a single bit of data, together will be combined to construct larger data size (8 bit = 1 byte).   Source : https://www.researchgate.net/figure/Fig3-internal-structure-of-RAM_fig2_328842382  ROM​  Read-Only Memory (ROM) is the type of memory which is used for storing data that can only be read, it is considered as non-volatile memory. ROM is used to store permanent instructions or data that do not change over time or during the course of normal system operation.  ROM can be used for :  Bootstrap Loader : It is the initial instruction for computer to boot or start up.Firmware : Firmware is software code embedded in hardware devices. It consists of basic machine instructions for hardware to do common task with other software in the device.Embedded Systems : Embedded systems are devices included with a dedicated computer systems to perform specific tasks or functions. They often rely on pre-programmed instructions stored in ROM to perform their intended functions.   Source : https://www.wawasanku.com/2023/01/RAM.html  Optical Drive​  Optical drive is a hardware device used to store data (non-volatile) that utilizes laser technology (e.g., CD, DVD, Blu-ray). The surface of the disk is flat, a laser beam will be emitted from the optical drive onto the surface of an optical disc. The laser beam will be used to create the pits and bumps on the disc's surface. The drive's optical sensor detects the variations in the reflected light, interpreting them as binary data (0s and 1s) to read the stored information.   Source : https://itec226pro.wordpress.com/component/cddvd/, https://commons.wikimedia.org/wiki/File:Afm_cd-rom.jpg  Flash Memory​  Flash memory is another non-volatile storage device that uses electrical circuits to store and retrieve data (e.g., USB, phone memory cards). Flash memory are built of many transistors called floating-gate transistors, the presence or absence of electrical charges on the floating gate represents binary data. Typically, a charged floating gate represents a &quot;0&quot; bit, while an uncharged floating gate represents a &quot;1&quot; bit.  We can write to the memory by applying voltage to charge the floating gate, or erase by applying higher voltage to the control gate, which causes the trapped charges to return to the source or drain region of the transistor.   Source : https://www.pinterest.com/pin/pen-drive-usb-component-diagram--366269382169975659/  HDD​  Hard Disk Drive (HDD) is a type of non-volatile storage device that uses magnetic storage to store and retrieve digital data. It consists of a stack of spinning disks called platters, and each platter has a read/write head that moves across its surface to read and write data.  Data is stored on the disks in the form of tiny magnetic areas. When data needs to be saved, the write head magnetically changes the magnetic orientation of the corresponding areas on the disk to represent the data. When data is read, the read head detects the magnetic patterns on the disk and converts them into digital information.   Source : https://eduinput.com/hard-disk-drive-hdd/  SSD​  Solid-State Drive (SSD) is the non-volatile storage device that uses flash memory instead of magnet to store digital data persistently. An SSD consists of multiple memory chips that store data electronically. These memory chips are made of flash memory cells, which can hold electrical charges to represent 0s and 1s, the basic building blocks of digital data.  The SSD controller act as the bridge between the memory chips and computer, it is the one that manages the storage and retrieval of data.   Source : https://www.storagereview.com/ssd-controller  Power Supply​  Power supply or Power Supply Unit (PSU), is a device that provides electrical power to a computer or electronic system. It converts the available input voltage from a power source (such as a wall outlet) into the appropriate voltage and current levels required by the components within the system.   Source : https://turbofuture.com/industrial/Basic-Elements-of-a-Power-Supply  RTC​  Real-Time Clock (RTC) is the component on computers or other electronic devices that keeps track of time, even when the device is powered off or restarted. Under the hood, RTC uses a quartz crystal oscillator, it is a material that have the property of vibrating at a very precise and consistent frequency when an electric current is applied. The stable vibration is used as a reference for counting time. It may also consist a battery backup to ensure continuous timekeeping operation.   Source : https://www.14core.com/starters-20-wiring-the-i2c-ds1307-real-time-clock-on-arduino/  Graphics Card​  Graphics card is the hardware component in a computer that is responsible for rendering and displaying visual information on a monitor or other display devices.  GPU is the specialized CPU designed to handle complex calculations related to rendering graphics and images. It performs tasks such as geometry processing, texture mapping, lighting effects, shading, and rasterization.  GPU is specifically designed for parallel processing, it is able to perform complex calculation simultaneously, but it is worse than CPU for procedural processing. It has a dedicated high-speed memory which is the video RAM (VRAM) closer to the GPU cores. GPU utilizes small yet many cores, in contrast, CPU has less core but each core is more powerful.   Source : https://www.researchgate.net/figure/CPU-vs-GPU-Architecture_fig1_270222593  Sound Card​  The sound card is a hardware component in computer that manages audio input and output. It is responsible for processing and producing sound, converting digital audio signals to analog signals for playback through speakers or headphones, and converting analog audio signals to digital format for recording or processing.   Source : https://www.escotal.com/soundcard.html  Network Interface Card​  Network Interface Card (NIC) or network adapter, is a hardware component that enables a computer to connect to a network. It provides the necessary physical interface and communication protocols for a computer to transmit and receive data over a network.  The NIC is physically connected with the computer through network port like Ethernet. When the computer needs to send data over the network, it passes the data to the NIC. The NIC converts the digital data into appropriate electrical signals or radio waves, depending on the type of network medium being used (wired or wireless). Before the transmission, NIC also handles the protocols or the rules for transmitting data. It then transmits these signals over the network medium.   Source : http://www.cs.uni.edu/~jacobson/secure/NICnacMAC.htm  Motherboard​  Motherboard is the circuit board that connects various hardware components, allowing them to communicate and work together to form a functional computer system. It has the RAM slots for installing RAM, ports for I/O devices, connectors to storage, etc.   Source : https://medium.com/@mitteam2021/computer-motherboard-and-its-components-8606f10dc08f  PCI​  Peripheral Component Interconnect (PCI) is a standard connection interface on a computer motherboard that allows for extra hardware components or expansion cards to computer system. These expansion cards can include devices like graphics cards, sound cards, network adapters, and other peripherals.   Source : https://en.wikipedia.org/wiki/PCI-X  ","version":"Next","tagName":"h3"},{"title":"Data Representation​","type":1,"pageTitle":"COA Fundamentals","url":"/cs-notes/computer-organization-and-architecture/coa-fundamentals#data-representation","content":" Computer represent data in various way, to know more about it, see below notes :  Binary number system, includes how can we store and represent number with binary.Binary representation, includes binary operation.Floating-number representation, includes how can computer represent real number.Data representation, includes how computers represent data like character (e.g., ASCII, UTF), color, database.  ","version":"Next","tagName":"h3"},{"title":"Terminology​","type":1,"pageTitle":"COA Fundamentals","url":"/cs-notes/computer-organization-and-architecture/coa-fundamentals#terminology","content":" Opcode &amp; Operand​  These two are related to the execution of program in CPU.  Opcode : Opcode or operation code, is a specific operation or instruction in machine language that the CPU needs to execute. Examples of opcode include instruction like &quot;add,&quot; which perform addition by taking value in a memory address or register.Operand : An operand is a value or a memory location that is operated upon by an instruction. It provides the data or the address on which the operation specified by the opcode will be performed. For example, in the instruction &quot;add R1, R2, R3,&quot; R1, R2, and R3 are the operands, representing registers that hold the values to be added together.  Word Size​  The CPU will execute instruction from memory, it will take some amount of data from the memory. The fixed amount of data that can be processed by CPU in one go is called word. The size of a word can vary, for example, word sizes can be 8 bits (1 byte), 16 bits (2 bytes), 32 bits (4 bytes), or 64 bits (8 bytes).  Endianness​  Endianness refers to the ordering of bytes in a multibyte data type, such as integers or floating-point numbers, in computer memory. It determines how the bytes of a larger data value are stored and retrieved.  Big endian : The most significant byte (the byte with the highest memory address) is stored first.Little endian : The least significant byte (the byte with the lowest memory address) is stored first.   Source : https://www.geeksforgeeks.org/little-and-big-endian-mystery/ (read from right to left)  Chipset​  Chipset refers to a set of integrated circuits that are designed to control and manage the flow of data and signals between various components of a computer. The chipset acts as a bridge or interface between the CPU, memory, and other devices.  The high-speed communication for CPU, memory, graphics card is handled by northbridge, while the lower-speed for I/O devices is handled by southbridge.   Source : https://www.baeldung.com/cs/chipset  Architecture​  Architecture is the overall design and organization of the hardware and software components that make up a computer system.  Processor Design : It involves the design of how CPU is structured, how one component communicate with each other.Memory Organization : The organization of various memory components (main memory RAM, hard drives, etc.) in computer systems.I/O System : Determines how input and output devices are connected to the computer system and how data is exchanged between them. It includes interfaces, drivers, and protocols for interacting with peripherals such as keyboards, mice, displays, printers, and network devices.  BIOS / UEFI​  Basic Input/Output System (BIOS) is a firmware stored in the ROM used to initialize hardware components during the boot process. Not only initializing, it also provides a text-based interface to interact with and configure basic settings of the computer's hardware components.  On the other hand, UEFI is a newer firmware interface that was introduced as a successor to BIOS. UEFI has more modern firmware architecture, it offers more advanced configuration, and uses a GUI for easier configuration.   Source : https://www.onlogic.com/company/io-hub/uefi-vs-bios-building-a-better-firmware/ (BIOS on the left) ","version":"Next","tagName":"h3"},{"title":"Assembly Language","type":0,"sectionRef":"#","url":"/cs-notes/computer-organization-and-architecture/assembly-language","content":"","keywords":"","version":"Next"},{"title":"Translatation to Machine Code​","type":1,"pageTitle":"Assembly Language","url":"/cs-notes/computer-organization-and-architecture/assembly-language#translatation-to-machine-code","content":" Assembly language uses mnemonic instructions, which are short and easy to remember symbol that represent a single instruction in the machine.  Once assembly language is written, it needs to be translated into machine code, this process is known as assembly or assembling. An assembler is used to convert the mnemonic instructions and symbolic names into the binary instructions understood by the target processor. The assembler looks up on the instruction table how the mnemonic instructions map to the binary codes. It will also perform other necesarry steps, such as, calculating memory addresses, combining the instruction with the operands, etc.  Assembly language also provide a way for programmers to define instruction that are not executed by the processor but provide instructions to the assembler, they are called directives. They help programmer to organize and control the assembly code. Also, in some cases, assembly programs may require multiple source files or external libraries that need to be linked together. This process is called linking, it is necesarry to resolve references to symbols and ensures that all required components are properly integrated.   Source : https://users.ece.utexas.edu/~valvano/assmbly/index.html  ","version":"Next","tagName":"h3"},{"title":"Instruction​","type":1,"pageTitle":"Assembly Language","url":"/cs-notes/computer-organization-and-architecture/assembly-language#instruction","content":" The commons types of instruction in assembly language :  Data Movement Instructions : These instructions move or copy data between registers, memory, and I/O devices.Arithmetic and Logic Instructions : These instructions perform arithmetic operations such as addition, subtraction, and multiplication; and logical operations such as bitwise AND, OR, NOT, and XOR on data.Control Flow Instructions : These instructions control the flow of execution within a program. Examples include changing program flow with or without a condition, or jumping to specific part of program after a function finishes its execution.Input/Output Instructions : These instructions facilitate communication between the processor and I/O devices.Stack and Memory Management Instructions : These instructions manipulate the stack (adding or removing values) and manage memory operations such as allocating and deallocating memory.  ","version":"Next","tagName":"h3"},{"title":"Syntax & Instructions​","type":1,"pageTitle":"Assembly Language","url":"/cs-notes/computer-organization-and-architecture/assembly-language#syntax--instructions","content":" Assembly code is typically written line by line, with each line representing a single instruction. Assembly code is divided into several section, there are section for code, which contains the actual logic of the program, section to declare variable and constants, and defining linker.  There are many instruction keyword in asm :  Instructions MOV (Move) : Copies the value from one location to another. It is used to transfer data between registers, memory locations, and immediate values.ADD (Addition) : Performs addition between two values and stores the result.SUB (Subtraction) : Performs subtraction between two values and stores the result.MUL (Multiplication) : Performs multiplication between two values and stores the result.DIV (Division) : Performs division between two values and stores the quotient.INC (Increment) : Increments the value of a register or memory location by 1.DEC (Decrement) : Decrements the value of a register or memory location by 1. Bitwise Operations AND (Bitwise AND) : Performs a bitwise AND operation between two values.OR (Bitwise OR) : Performs a bitwise OR operation between two values.XOR (Bitwise XOR) : Performs a bitwise XOR operation between two values. Registers AX, BX, CX, DX, EBX (general-purpose registers)AL (accumulator low)EAX (extended accumulator)SP (stack pointer)BP (base pointer)SI (source index)DI (destination index)IP (instruction pointer)FLAGS (flags register) Control Flow CMP (Compare) : Compares two values and sets the flags based on the result.JMP (Jump) : Transfers program control to a specified location in the code.JZ (Jump if Zero) : Jumps to a specified location if the zero flag is set.JNZ (Jump if Not Zero) : Jumps to a specified location if the zero flag is not - set.JE (Jump if Equal) : Jumps to a specified location if the equal flag is set.JNE (Jump if Not Equal) : Jumps to a specified location if the equal flag is not - set.CALL (Call) : Calls a subroutine or function at a specified location.RET (Return) : Returns from a subroutine to the calling code. Stack Operations PUSH (Push) : Pushes a value onto the top of the stack.POP (Pop) : Removes a value from the top of the stack. Directives DB (define byte)DW (define word)DD (define doubleword)DQ (define quadword) Data Types BYTE : A single byte of dataWORD : 16-bit value or two bytes of data.DWORD : 32-bit value or four bytes of data.QWORD : 64-bit value or eight bytes of data.  Example​  Example of asm code for adding two number  section .data num1 db 5 ; Define a variable called num1, which is a byte and has the value of 5 num2 db 3 ; Define a variable called num2, which is a byte and has the value of 3 section .text global _start _start: ; Load the first number into a register mov al, [num1] ; Add the second number to the first number add al, [num2] ; Store the result in another variable mov [result], al ; Exit the program mov eax, 1 ; System call number for exit xor ebx, ebx ; Exit status code (0 for success) int 0x80 ; Perform the system call section .data result db 0 ; Variable to store the result   The syntax is typically &lt;instruction&gt; &lt;operand1&gt; &lt;operand2&gt;, where operand can be a register, which is specified by its name, or immediate value.  The section .data is a section directive to define and initialize data such as variables, constants, and strings. The section .text is the section for program's executable instructions. _start is a label that marks the entry point of the program. ","version":"Next","tagName":"h3"},{"title":"Control Unit","type":0,"sectionRef":"#","url":"/cs-notes/computer-organization-and-architecture/control-unit","content":"","keywords":"","version":"Next"},{"title":"Instruction​","type":1,"pageTitle":"Control Unit","url":"/cs-notes/computer-organization-and-architecture/control-unit#instruction","content":" Everything is binary in computer, to instruct the CPU to do anything, you would need an instruction in binary format. Instruction are defined in structured way, an instruction should have the opcode, or operation code, which is a binary code that represent a specific operation or instruction to be executed by the CPU. You would also need operand fields, and specify memory address or register to identify the location of the data or the operand involved in the instruction's operation.  During execution, the CPU may use various type of registers, such as the instruction register to hold the current instruction being executed by the CPU.  The specification of an instruction for a CPU is typically defined in a table called instruction table. It contains opcode values, along with a description of the operation, the number and type of operands, and any specific addressing modes or flags associated with the instruction.   Source : https://youtu.be/FZGugFqdr60?si=Fz5CKRSen9_BXhDu&amp;t=116  Fetch-Decode-Execute Cycle​  The CPU executes instruction in a cycle, it goes through three main steps : fetch, decode, and executes. During execution, all the registers will be initialized as 0. The RAM will be used to store the program's instruction and data.  Fetch : The program counter (PC) (or instruction address register (IAR)) register will take an address from the RAM, it will be the memory address of the next instruction to be fetched.The CPU fetches the next instruction from PC.The control unit sends a request to the memory subsystem to retrieve the actual instruction at the address specified by the PC.The fetched instruction, stored in the instruction register (IR), is transferred from memory to the CPU. Source : https://youtu.be/FZGugFqdr60?si=toaAfVEe9Y676Thc&amp;t=175 Decode : The control unit decodes the fetched instruction in the IR to determine the operation or task specified by the opcode, along with the memory address or register address if it involve reading/writing to the RAM/registers.The control unit identifies the type of instruction, such as an arithmetic operation, data movement, control flow, or input/output.The specific circuit implemented will vary depending on the instruction, there will be so much logic gates just to determine if it's a specific instruction. Source : https://youtu.be/FZGugFqdr60?si=Tid_16t53px3sCvU&amp;t=212 Execute : The control unit generates control signals based on the decoded instruction. These signals coordinate the activities of various components within the CPU to execute the instruction.If the instruction involves memory access, the control unit coordinates the transfer of data between registers and memory, either for loading or storing data (e.g., enabling write and storing the input data).If the instruction involves data manipulation, the control unit directs the ALU (Arithmetic Logic Unit) to perform the specified arithmetic or logical operation.If the instruction involves control flow, such as branching or jumping, the control unit determines the next instruction to be fetched based on the outcome of the control flow instruction. Source : https://youtu.be/FZGugFqdr60?si=EaMWcLHwVHvK6s7R&amp;t=248 In the image above, the LOAD_A instruction tells us to read or load specific address from the RAM and write it to register A. The control unit can be abstracted as follows : Source : https://youtu.be/FZGugFqdr60?si=AYgrxk0nBWGUGVfP&amp;t=281 Source : https://youtu.be/FZGugFqdr60?si=po5Qm1YG2UPzX5Ur&amp;t=410 The interaction between control unit and ALU, it performs the arithmetic or logical operation according to the control signals and operand received from the control unit. Here's a common example : First, some binary data (number) is loaded into the register A, using the LOAD_A instruction.Another binary data is loaded into the register B, using the LOAD_B instruction.The two number form register A and register B will be added in the ALU, using the instruction ADD.The result will be stored in some RAM location, using the instruction STORE_A.  Clock Cycle​  During the fetch-decode-execute cycle, the control unit will utilize a clock. The clock will produces a regular series of electrical pulses, known as clock cycles, which represents a discrete unit of time. The purpose of timing between cycle is to ensures that operations within the CPU are executed in a controlled and synchronized manner (e.g., accessing register before we even write onto it).  The clock cycle or clock rate is measured in Hertz (Hz) per second. It determines the speed at which the CPU processes instructions and carries out operations, where higher clock rates generally result in faster execution.  A single fetch-decode-execute cycle is counted as one instruction cycle, which can execute in multiple clock cycle.   Source : https://www.learncomputerscienceonline.com/control-unit/  tip A term called overclocking, which mean we are configuring our CPU, GPU, or RAM at a higher clock speed than the default settings. The purpose is to achieve higher processing speed.  Interrupt &amp; Exception Handling​  Control unit is also responsible for handling interruption and exception during program's execution.  Interrupt is a signal that indicates the occurrence of an event that requires the CPU's immediate attention. Interrupts can be generated by various sources, such as devices (e.g., keyboard, timer), external signals, or software-generated events.  When an interrupt happen, the control unit will suspends the execution of current instruction and will transfers control to an interrupt handler routine, it is the component that is responsible for handling interrupt event. The control unit will also saves the execution context, including the relevant registers such as the PC. After the handler completes the handling, control unit will continue the execution of the program with the saved context.  Exceptions are unwanted event during program's execution, it will need special handling from exception handler. Similar to interruption, the control unit will save the execution context and transfers the control to the exception handler. Once the exception handling is completed, the control unit continue the program's execution.  More Instructions​  There are still many instructions that computer can execute, it is to handle more complex situation that occurs in a program. There are many more operations such as jump, halt, condition, looping, etc.   Source : https://youtu.be/zltgXvg6r3k?si=EXDRnkb43OL_tda1&amp;t=201  Halt : The halt instruction is used to explicitly terminate the execution of a program or a specific section of code. It is very important to halt when your program is done executing, to indicates a program has completed its tasks and ready to exit.Jump : Also known as goto statement, is an instruction that doesn't continue with the normal sequential flow. You can transfer control to a different part of program, such as &quot;jumping&quot; to specified memory address. Jumps are used to implement control flowCondition : Condition is anything that refer to expressions or logical statements that evaluate to either true or false. They are used to make decision within program, typically to implement the if-statement. Conditional statement will involve logical operation, such as checking if some number greater than other number using various logic gates.Looping : Looping is the act of repeating an instruction multiple times, it is used to implement loop statement like for-loop and while-loop. Instead of repeating certain task several times, loops allows us to automate it. Looping typically uses jump to enable non-sequential execution and simulate looping behavior.  Example​  LOAD_A 14 : Load the number 1 from the memory address 14 in RAM, store it in the register A.LOAD_B 15 : Load the number 1 from the memory address 15, store it in the register B.ADD B A : Add together register B and register A, resulting in 2, store the result in register A.STORE_A 13 : From register A, store the value to memory address 13.JUMP 2 : Jump to the memory address 2, the memory address 2 contains the instruction ADD B A, we are going to do it again the similar way.  This behavior may be repeated forever, because we keep jumping, adding, storing without any condition on when this will end.   Source : https://youtu.be/zltgXvg6r3k?si=WpIfH9GUWfWehoz2&amp;t=295  We will need to modify so that the jump statement should not be executed at some point. We can utilize a conditional jump statement, JUMP_NEG, which means we are only going to jump if the number on the registers is negative. Furthermore, we will also subtract the number on registers instead of adding them. This will effectively create a loop where we are going to subtract the number until they are negative.  LOAD_A 14, LOAD_B 15 : Load number from memory address 14 and 15 to register A and B, respectively.SUB B A : Subtract the number on register B with register A, store the result in register A.JUMP_NEG 5 : If the number on register A is negative, jump to address 5, else continue sequential execution.JUMP 2 : Because the number is not negative, we will continue to the next instruction, which is to jump to address 2.  This will be repeated until the number on register A is negative.  JUMP_NEG 5 : After register A is negative, we will jump to address 5.STORE_A 13 : Store the number from register A to the address 13.HALT : Stop the program.   Source : https://youtu.be/zltgXvg6r3k?si=1Nn9-95QfB123WYP&amp;t=335 ","version":"Next","tagName":"h3"},{"title":"CPU Design","type":0,"sectionRef":"#","url":"/cs-notes/computer-organization-and-architecture/cpu-design","content":"","keywords":"","version":"Next"},{"title":"Instruction Set​","type":1,"pageTitle":"CPU Design","url":"/cs-notes/computer-organization-and-architecture/cpu-design#instruction-set","content":" Instruction Set is a collection of instructions that CPU can execute. Instruction set is very important to implement the CPU, it will include the format of the instruction.   Source : top, bottom  An instruction may contain :  Opcode : The operation code or the type of instruction to be executed, such as adding number, subtracting, loading data from memory, etc.Operand : They are the data or values on which the instruction operates. They can be registers, or memory addresses.Addressing Modes : Specify how the operands are going to be accessed, we can provide a register that contains the operand, provide the operand from the memory, or even include a value directly (called immediate value).  ","version":"Next","tagName":"h3"},{"title":"Microarchitecture​","type":1,"pageTitle":"CPU Design","url":"/cs-notes/computer-organization-and-architecture/cpu-design#microarchitecture","content":" Microarchitecture is the implementation and design of the internal components and structures of the CPU. It focuses on how the CPU executes instructions, manages data, and performs computations at the hardware level.  Pipelining​  Pipeline is the technique used to improve the performance of processors by overlapping the execution of multiple instructions. It is known as achieving parallelism on the instruction-level. The execution of instruction is broke down into a series of smaller stages to enable the concurrent processing of multiple instructions.  In pipeline, stage is often divided by 5 :  Instruction Fetch (IF) : First stage is to fetch an instruction from memory, typically from the program counter (PC).Instruction Decode (ID) : Fetched instruction is decoded, also fetch the operand and register allocation if required.Execution (EX) : Stage that executes the instruction.Memory Access (MEM) : If the instruction requires memory access, this stage will handle the operation.Write Back (WB) : Final stage that will write the result of the instruction execution to the appropriate registers or memory locations.  The pipeline stages work in parallel, meaning while one instruction is progressing through a stage, the next instruction is entering the pipeline and moving to the subsequent stage. The overlap between instruction execution allows multiple instructions to be processed simultaneously.   Source : left, right  Pipelining can be likened to doing laundry. Instead of waiting for the washing machine to complete the entire wash cycle before using the dryer, we can start washing a portion of the clothes and transfer the already washed portion to the dryer. This approach enhances the efficiency of the laundry process by overlapping washing and drying tasks, similar to execution of tasks in CPU.  However, problems may happen during the pipeline process, these are called hazards.  Structural Hazards : Arise when multiple instructions require the same hardware resource simultaneously. It's when two instructions need to access the same functional unit or register simultaneously.Data Hazards : Occur when an instruction depends on the result of a previous instruction that is not yet available. Data hazards can be further classified into three types : read-after-write (RAW), write-after-read (WAR), and write-after-write (WAW).Control Hazards : Arise due to conditional branches or jumps that affect the sequential execution of instructions. Control hazards occur when the outcome of a branch or jump instruction is not known in the early stages of the pipeline, causing a potential misprediction. For example, we didn't know that some expression returns false, yet we keep executing the branch that only needs to be executed when the expression returns true.  Forwarding​  Forwarding is the technique to mitigate data hazards, it will bypass the normal data flow and directly forward the necessary data from the producing instruction to the dependent instruction, avoiding the need to wait for the data to be written to a register or memory.  Out-of-order Execution​  This technique dynamically order and execute instruction based on their availability of operands, rather than strictly following the sequential order of the program.  Prediction​  Prediction is the technique to make educated guesses or predictions about future events, such as the outcome of a branch instruction or the behavior of memory accesses.  Many techniques to predict which branch will likely to be executed, one of them is having a branch history, we can store the history of branch outcomes seen during program execution.  Another use of prediction is to predict the outcome of memory access instructions (loads and stores) in order to optimize the handling of memory dependencies. The speculation can be based on many principles, such as, temporal, which suggests that if a memory location is accessed once, it is likely to be accessed again in the near future. Another is spatial principle, which suggests that if a memory location is accessed, nearby memory locations are also likely to be accessed in the near future.   Source : https://en.wikipedia.org/wiki/Branch_predictor  Superscalar​  Superscalar is an instruction-level parallelism that allows the execution of multiple instructions in parallel within a single clock cycle. It can be employed with pipelining, where each stage of the pipeline can handle multiple instructions simultaneously.   Source : https://en.wikipedia.org/wiki/Superscalar_processor  Cache &amp; Memory​  Memory is the place for storing data and instructions that are actively used by the CPU during program execution. The commonly used memory for random access is the RAM.  Depending on the hardware architecture, the RAM placement can vary. Often times, accessing data from RAM can introduce delay. Cache is a small and fast memory component that sits between the CPU and main memory.  Cache typically organize the structure of their memory for more efficient retrieval of data. It may operate on the principle of locality, the temporal locality (recently accessed data is likely to be accessed again) and spatial locality (data located near recently accessed data is likely to be accessed soon).   Source : https://www.cloudeka.id/id/berita/web-dev/apa-itu-cache-memory/  tip More about caching, some may relate to backend development. It includes type of caching, cache invalidation, cache replacement.  ","version":"Next","tagName":"h3"},{"title":"Performance Evaluation​","type":1,"pageTitle":"CPU Design","url":"/cs-notes/computer-organization-and-architecture/cpu-design#performance-evaluation","content":" Many aspects and metrics are used to evaluate the performance in CPU design :  Instruction Execution Time : This metric measures the time taken to execute a single instruction or a sequence of instructions.Instruction Throughput : Refers to the number of instructions executed per unit of time (e.g., each clock cycle).Clock Frequency : Determines the rate at which instructions are executed. Higher clock frequencies generally lead to faster instruction execution, but they come with trade-offs such as increased power consumption and heat generation.Cycles Per Instruction (CPI) : CPI measures the average number of clock cycles required to execute one instruction. A lower CPI indicates better efficiency and shorter execution times.Cache Performance : Evaluating cache performance involve measuring how useful our cache is. For example, how many times have we used the cache (cache hit), how many times the cache doesn't have data we need (cache miss).Branch Prediction Accuracy : The accuracy of branch prediction, branch instructions are important because they significantly impact CPU performance.  ","version":"Next","tagName":"h3"},{"title":"Multiprocessing & Parallel Computing​","type":1,"pageTitle":"CPU Design","url":"/cs-notes/computer-organization-and-architecture/cpu-design#multiprocessing--parallel-computing","content":" Multiprocessing is the technique that utilize multiple processors or CPU cores to execute multiple tasks or processes concurrently. It involves dividing the workload among the processors or cores to achieve parallel execution, thereby improving overall system performance and efficiency.  Multicore is when a single CPU contains multiple cores or multiple processing unit. These cores are typically identical and share the same resources. Each core execute instructions independently and perform computational tasks concurrently.  While multiprocessor is when a system consists of multiple independent processors, each with its own instruction execution capabilities, registers, cache, and memory subsystem. These processors can be housed on a single chip or on separate chips, interconnected via a system interconnect, such as a shared bus or a high-speed interconnect.   Source : https://www.scaler.com/topics/operating-system/difference-between-multicore-and-multiprocessor-system/  Overall, the primary difference is about how they share resources.  Multiple unit of processing in hardware-level plays a role for concurrency in the software-level. Threading, is implemented in the hardware-level by having specific hardware that capable of processing concurrently, and utilized in the software-level by operating system to schedule their execution. ","version":"Next","tagName":"h3"},{"title":"GPU","type":0,"sectionRef":"#","url":"/cs-notes/computer-organization-and-architecture/gpu","content":"","keywords":"","version":"Next"},{"title":"GPU Components​","type":1,"pageTitle":"GPU","url":"/cs-notes/computer-organization-and-architecture/gpu#gpu-components","content":" GPU consists of several components :  Graphics and Compute Array (GCA) : GCA contains the core processing unit of a GPU. It consists of multiple streaming multiprocessors (SM) or compute units that perform parallel computations.Compression Unit : Some GPUs incorporate compression units that can compress and decompress data to reduce memory bandwidth requirements. This can help improve performance by reducing memory bottlenecks and improving memory access efficiency.Graphics Memory Controller (GMC) : GMC manages the interaction between the GPU and its dedicated graphics memory. It handles tasks such as memory allocation, data transfer, and memory bandwidth management.VGA BIOS : VGA BIOS is a firmware component stored on the GPU. It provides low-level initialization and configuration routines for the GPU during system boot-up. It is responsible for setting up the GPU's initial state and ensuring compatibility with the system's display capabilities.Bus Interface : Bus Interface is the component that connects the GPU to the computer's system bus. It enables data transfer between the GPU and other system components, such as the CPU and system memory.Power Management Unit (PMU) : The Power Management Unit is responsible for managing and regulating the power consumption of the GPU. It monitors power usage, adjusts clock frequencies, controls voltage levels, and implements power-saving features to optimize performance while minimizing power consumption.Video Processing Unit (VPU) : VPU is a specialized component within some GPUs that handles video-related tasks. It includes functions such as video decoding, encoding, and post-processing.Display Interface : GPUs include display interfaces, such as HDMI, DisplayPort, or DVI. These interfaces provide the necessary signals and protocols for connecting monitors or other display devices to the GPU. They transmit video and audio data from the GPU to the display.   Source : https://en.wikipedia.org/wiki/Graphics_processing_unit  Memory Hierarchy : GPUs have multiple levels of memory hierarchy : Global Memory : Global memory is the largest memory space in a GPU and is accessible by all threads. It is used to store data that needs to be shared across different threads.Shared Memory : Shared memory is a fast, low-latency memory space, but smaller than global memory, that is shared among threads within a thread block. It enables efficient data sharing and inter-thread communication.Texture Memory : Texture memory is a specialized read-only memory used for efficient texture mapping operations in graphics applications. It provides fast access to texture data for texture sampling.Constant Memory : Constant memory is a read-only memory space used for storing data that remains constant during kernel execution. It provides fast access to frequently used constants or configuration data. Rasterizer : The rasterizer is a component responsible for converting geometric primitives (such as triangles) into pixels for display. It performs operations like clipping, culling, and interpolation of vertex attributes. Texture Units : Texture units handle texture-related operations such as filtering, sampling, and interpolation. They are specialized hardware components designed for efficient texture mapping in graphics applications. Frame &amp; Pixel Buffer : These are temporary memory to store graphical data. The frame buffer represent the complete image on the screen, which is displayed currently, while the pixel buffer store pixel data temporarily during graphics processing tasks. The GPU continuously updates the buffer with new pixel data as it performs calculations and renders the scene.  tip More about computer graphics  note Not all GPU has all those specialized features. There are two types of GPU, integrated and dedicated. Integrated GPU is integrated with the CPU, they typically share the system memory. They are mostly used for basic graphics tasks. On the other hand, a dedicated GPU has its own dedicated memory called Video RAM (VRAM).  ","version":"Next","tagName":"h3"},{"title":"GPU Programming​","type":1,"pageTitle":"GPU","url":"/cs-notes/computer-organization-and-architecture/gpu#gpu-programming","content":" Traditional programming often rely on sequential processing, which utilizes a single processor core or a few cores. It focuses on optimizing the execution of instructions on a single thread or a limited number of threads.  The nature of parallelism in GPU architecture opens up a new field, GPU programming. Programmers can take advantage of the parallel architecture of GPU by breaking down tasks into smaller, independent units that can be executed in parallel, instead of sequential processing.  Compute Unified Device Architecture (CUDA), which is provided by NVIDIA, is an API that allow programmers to write code specifically for GPUs, these frameworks provide additional functionality and optimization techniques tailored for GPU architectures.  GPU programming typically uses less control flow construct like &quot;if-else&quot; statement. If-else statement is used to make a conditional branch based on some condition. GPU executes threads parallelly in a group called warps. When the processing encounter a branch, the threads may take different paths based on the condition. This leads to divergent execution, where threads within the same warp follow different code paths. Divergence can impact performance because threads that are not executing the same path are effectively serialized, leading to decreased parallelism.  Some branch may also take longer than the other branch, this will lead some thread idling to wait the other thread to finish, due to the parallelism of GPU. ","version":"Next","tagName":"h3"},{"title":"Harvard","type":0,"sectionRef":"#","url":"/cs-notes/computer-organization-and-architecture/harvard","content":"","keywords":"","version":"Next"},{"title":"Modified Harvard Architecture​","type":1,"pageTitle":"Harvard","url":"/cs-notes/computer-organization-and-architecture/harvard#modified-harvard-architecture","content":" The modified Harvard architecture combines the original Harvard architecture with some element of von Neumann architecture. The ability of treating data and instruction in the same way can be beneficial in some cases, such as the scenario where the data is from user input and that input will be used to generate code.  Modified Harvard architecture has less strict separation between the instructions and data. Some modifications are :  Split-cache : Modified Harvard architecture has a cache system for both instruction and data memory. The instructions and data caches are combined, which means they have a single address space. Although they may share the same physical cache memory, they are organized and accessed as separate caches.Data-Instruction Interchangeability : We can treat data in the data memory as instruction, and treat instruction in instruction memory as data. This allows for instruction generated from data, or instruction driven by data. ","version":"Next","tagName":"h3"},{"title":"Input/Output","type":0,"sectionRef":"#","url":"/cs-notes/computer-organization-and-architecture/input-output","content":"","keywords":"","version":"Next"},{"title":"I/O Modules​","type":1,"pageTitle":"Input/Output","url":"/cs-notes/computer-organization-and-architecture/input-output#io-modules","content":" An I/O modules typically consist of interface and controller.  Interfaces​  Interfaces are the electrical or physical connectors, such as USB ports for variety of I/O devices, Ethernet ports for networking, or audio jacks for I/O related to audio. The component in CPU talks to each other using system buses, specifically I/O bus for I/O interaction. It serves as a shared link that allows data to be exchanged between the CPU, memory, and I/O devices. I/O interfaces makes it possible for I/O devices to connect with the buses.   Source : https://edurev.in/t/97679/Input-Output-interface-Computer-Organization-and-A   Source : https://study.com/learn/lesson/nic-network-interface-card-types-function.html  I/O Controller​  I/O controller is the hardware component which will handle the necessary protocols and logic for data transfer. It acts as an intermediary between the CPU, memory, and the I/O devices, that handles data transfer and communication.  Function of I/O controller :  Data Buffering : I/O controller may use data buffers or temporary storage areas to handle the flow of data between the CPU/memory and the I/O devices systematically.Error Detection &amp; Interrupt Handling : Notify the CPU about interrupt events, data availability, or any error that happens in the I/O devices.   Source : https://witscad.com/course/computer-architecture/chapter/io-communication-io-controller  While I/O controller is the hardware level for handling communication, device driver, specifically I/O driver, is the component which enables communication in the software level between the operating system (OS) with the I/O controller.  Device driver provides a standardized interface to the operating system, allowing it to send commands, requests, status, or data to the I/O controller or device.   Source : https://repository.unikom.ac.id/50439/1/pertemuan6.pptx  ","version":"Next","tagName":"h3"},{"title":"I/O Technique​","type":1,"pageTitle":"Input/Output","url":"/cs-notes/computer-organization-and-architecture/input-output#io-technique","content":" There are various approaches and strategies to manage input and output operations between a computer system and its peripheral devices.  Interrupt-Driven I/O​  Interrupt is a signal that is used to indicate that the normal execution of the CPU should be diverted to handle a specific event or condition. It is a hardware or software-generated signal that notifies the CPU to suspend its current task and execute an interrupt handler routine, which will handles the event or condition that triggered the interrupt.  In interrupt-driven IO, the CPU initiates an I/O operation and then continues with other tasks. Once the I/O operation is completed, the I/O device sends an interrupt signal to the CPU, indicating that the data transfer is finished. The CPU then suspends its current task, services the interrupt, and transfers the data between the I/O device and memory. Interrupt-driven IO reduces CPU involvement in data transfers, allowing for multitasking and improved system performance.  Programmed I/O​  In this technique, the CPU directly controls the data transfer between the I/O device and the main memory. The CPU issues specific I/O instructions to transfer data between the device and memory and will wait for the operation to complete. Everything is done by CPU, I/O devices will not be able to interrupt. Programmed IO is simple to implement but can be inefficient since the CPU is involved in each data transfer, leading to slower overall system performance.  DMA​  DMA is a technique that allows data transfer between an I/O device and memory without direct CPU involvement. CPU will not be involved during the data transfer, the I/O device, which is managed by a DMA controller will transfer the data directly from the I/O device to the memory. This techinque can significantly reduces CPU overhead and improves I/O performance, especially for large data transfers.   Source : https://www.boardinfinity.com/blog/direct-memory-access/  Memory-mapped I/O​  Memory-mapped I/O is a technique where the CPU assign a specific range of memory addresses that it can use to send and receive data. The CPU can access the I/O device by reading from or writing to these memory addresses just like it would for regular memory operations.  When the CPU issues a read/write operation to a memory address, there will be an address decoder, which will routes the request to the appropriate I/O device instead of main memory.   Source : https://embeddedsystemvn.wordpress.com/2021/08/07/memory-mapped-i-o/ ","version":"Next","tagName":"h3"},{"title":"ISA","type":0,"sectionRef":"#","url":"/cs-notes/computer-organization-and-architecture/isa","content":"","keywords":"","version":"Next"},{"title":"Instructions​","type":1,"pageTitle":"ISA","url":"/cs-notes/computer-organization-and-architecture/isa#instructions","content":" Each instruction in an ISA has a specific format that defines how the instruction is encoded in binary. The format typically consist of two, the opcode, or operation code, which is the instruction to be performed, and some number operands, which are the value to be operated on. The operands can be registers, immediate value, or memory address if accessing data from memory.   Source : https://en.wikipedia.org/wiki/Instruction_set_architecture#Instruction_encoding  It's important to note that ISA is just an interface, different hardware may have different way to implement it, but it must suit the specification.  Instructions have specific types, each serving a specific purpose :  Arithmetic Instructions : Perform arithmetic operations such as addition, subtraction, multiplication, and division on data values.Logical Instructions : Perform logical operations such as bitwise AND, OR, XOR, and shift operations on data values.Data Transfer Instructions : Move data between registers and memory or between different memory locations.Load/Store Instructions : Load data from memory into registers or store data from registers into memory.Control Flow Instructions : Control the program flow, including branching and jumping to different parts of the program based on conditions or unconditional transfers.System Instructions : Access privileged operations and interact with the underlying operating system.Floating-Point Instructions : It is considered as a more complex instruction, it may take several steps on normal computers. This instruction performs a floating-point arithmetic operations on floating-point data.  Other ISA specification :  Registers : A set of registers that the processor uses to store and manipulate data during execution. This may include the number of registers, their sizes, and their specific purposes (e.g., general-purpose registers, special-purpose registers). Addressing Modes : This defines the different addressing modes to specify how memory operands are accessed. Common addressing modes include direct addressing, immediate addressing, register addressing, indirect addressing, and indexed addressing. The addressing mode determines how the memory addresses or offsets are calculated and how the data is fetched from or stored into memory. Direct Addressing : Operand of an instruction directly specifies the memory address where the data is located.Immediate Addressing : Operand of an instruction is a constant or immediate value.Register Addressing : Operand of the instruction is a register or a set of registers.Indirect Addressing : Operand of an instruction contains a memory address that points to the actual memory location where the data is stored. In other word, the operand is not the actual value, the operand is another memory location which contains the memory address of the actual value (similar to concept of pointers).Indexed Addressing : Operand of an instruction is a memory address that is calculated by adding an index value to a base address, where the index value can be a constant or stored in a register.  ","version":"Next","tagName":"h3"},{"title":"Classification​","type":1,"pageTitle":"ISA","url":"/cs-notes/computer-organization-and-architecture/isa#classification","content":" ISA can be classified in many ways, one of them is based on architecture complexity, which is CISC and RISC.  32-bit vs 64-bit Computing​  Before knowing the classification, it's important to know about 32-bit and 64-bit computing. When we say refer to the term 32-bit or 64-bit computer, it refers to how large is the data the computer processes in one go. The processor of a 32-bit computer can work with data in 32-bit chunks, meaning it can process and manipulate data in units of 32 bits at a time. Similarly, with 64-bit computer, it can process in a chunk of 64-bit.  The number of bit in the computer also determine the size of memory. Memory is accessed by referring its addresses, the length of its addresses depend on how large is the memory is. If the memory has a size of 4 GB or 2322^{32}232 bit, it must have 2322^{32}232 unique memory address as well. To be able to encode all different memory address ranging from 000 to 2322^{32}232, the memory address length must be the maximum amount, which is 32 length.  In conclusion, 32-bit computer can only have maximum of 2322^{32}232 bit memory or 4 GB, and 64-bit computer can only have maximum of 2642^{64}264 bit memory or 18.4 million TB.  CISC​  Complex Instruction Set Computer (CISC) is a computer architecture where a single instruction can execute multiple simple operation (e.g., loading from memory or arithmetic operation), or executing complex tasks in a single instruction.  CISC architectures aim to reduce the number of instructions required to perform a particular task, which can be advantageous in certain situations. CISC instructions typically have a variable length and can encompass several micro-operations within a single instruction.  So, instead of having a single instruction that only loads from memory or adding number between two registers, a single instruction can perform them both.  For example, consider an instruction called &quot;HIGH_LEVEL_ADD&quot; can be :  Load a value from memory into a register.Perform an arithmetic operation between two registers.Store the result back to memory.  By just using HIGH_LEVEL_ADD R1, [R2], R3, we can load a value from memory pointed by the address stored in register R2, into register R1, performs arithmetic operation between the values in registers R1 and R3 and storing the result back to memory, all in one instruction.  Composing multiple simple operation to a more complex operation provide a level of abstraction, which simplify programming for a particular task. However, the complexity of CISC instructions can make hardware design and instruction decoding more challenging.  CISC architectures often utilize microcode to implement complex instructions. Microcode is a low-level, machine-specific code that breaks down complex instructions into a sequence of simpler micro-operations. These micro-operations are executed by the processor's control unit to carry out the desired tasks.  CISC is said to be &quot;hardware-based&quot;, because they rely on specialized hardware components and microcode to execute complex instructions efficiently.   Source : https://binaryterms.com/cisc-processors.html  x86​  x86 refers to a family of computer processor architectures that are based on the Intel 8086 microprocessor. The x86 architecture defines a specific set of instructions and programming conventions that processors compatible with the x86 family must support.  x86 is based on CISC design philosophy, which is characterized by capable of doing multiple low-level operations or complex instructions in a single instruction. The number of bit for x86 architecture can vary, for example, the x86-16 is the 16-bit processor that implements the x86 architecture. IA-32 is the 32-bit version, and x86-64, which is also known as x64 AMD64, Intel64, is the 64-bit version.  There are many processors that implements the x86 architecture :  x86-16 : Originated from the Intel 8086 and 8088 processors, which were the first members of the x86 family. The x86-16 architecture features a 16-bit data bus and a 20-bit address bus, allowing it to address up to 1 MB of memory. This implementation was commonly used in early personal computers and operating systems, such as MS-DOS.IA-32 : It was introduced with the Intel 80386 processor (often referred to as the 386). IA-32 processors were widely used in desktop and server systems, and they supported various operating systems like Windows, Linux, and BSD.x86-64 : Popular processor such as AMD Ryzen and Intel Core i7. It introduced 64-bit registers, expanded memory addressing capabilities, and support for larger amounts of physical memory. They also maintained backward compatibility with 32-bit x86 software, allowing both 32-bit and 64-bit applications to run on x86-64 processors. x86-64 is currently the most prevalent implementation of the x86 architecture and is widely used in desktops, laptops, servers, and data centers.  RISC​  Reduced Instruction Set Computer (RISC) is a type of computer architecture that emphasizes simplicity and efficiency in the design of the instruction set. In contrast, CISC combine low-level operation into a single instruction, RISC might require multiple instruction just to execute a single instruction.  Characteristics of RISC :  Load-Store Architecture : This is an ISA that divides instruction into two categories, which are memory access and ALU operations. Memory access is when data is loaded or stored between memory and registers, while ALU operations consist of arithmetic and logical operations which is operated on data stored in registers.Simple Encoding &amp; Fixed-Length Instructions : RISC architectures often use fixed-length instruction formats, where each instruction is a uniform size. This simplifies the instruction-fetch and decode stages of the processor pipeline.Pipeline-Friendly Design : Because RISC architectures are typically simple, they are pipeline-friendly, meaning that instructions can be executed in a pipeline with multiple stages, allowing for efficient and overlapping execution of different instructions.Register-Rich Design : RISC architectures typically have many general-purpose registers available for storing intermediate results and operands. This reduces the need for memory accesses, also simpler addressing modes, and predictable instruction times.  RISC is typically hardware-friendly and more software-based. The simplification of instruction allows for easier decoding and execution of instructions in hardware, which reduce the complexity of the hardware. RISC often encourage on software-based implementation such as compiler optimizations. The simplified instruction set provide a clear guidelines for code generation, RISC architectures allow compilers to generate efficient code that can take advantage of the architecture's features.   Source : https://www.javatpoint.com/risc-vs-cisc  ARM​  Advanced RISC Machine (ARM) is a family of RISC architectures. Due to the design philosophy of RISC, ARM architectures are widely used in various embedded systems, mobile devices, and other low-power applications.  Example of ARM processors :  32-bit ARM Processors : ARM Cortex-A7 : Low-power and cost-sensitive applications.ARM Cortex-A9 : Mid-range performance for smartphones and embedded systems.ARM Cortex-A15 : High-performance processor for smartphones, tablets, and networking devices.ARM11 : Previous-generation processor used in smartphones, feature phones, and embedded systems. 64-bit ARM Processors: ARM Cortex-A53 : Power-efficient processor commonly used in smartphones and tablets.ARM Cortex-A57 : High-performance processor often paired with Cortex-A53 for a balance of power efficiency and performance.ARM Cortex-A72 : Advanced processor with high performance and efficiency for flagship devices.ARM Cortex-A73 : Successor to Cortex-A72, offering improved performance and power efficiency. ","version":"Next","tagName":"h3"},{"title":"Vector Processors & TPU","type":0,"sectionRef":"#","url":"/cs-notes/computer-organization-and-architecture/vector-processors-and-tpu","content":"","keywords":"","version":"Next"},{"title":"SIMD​","type":1,"pageTitle":"Vector Processors & TPU","url":"/cs-notes/computer-organization-and-architecture/vector-processors-and-tpu#simd","content":" Single Instruction, Multiple Data (SIMD) is parallel computing technique that allows a single instruction to be applied to multiple data elements simultaneously.  In SIMD processing, a single instruction is broadcasted to multiple processing units or execution lanes, each capable of processing a different data element from a vector or array.  Similar to GPU, SIMD is typically used for heavy computation tasks, but not for flow-control-heavy task that involves many conditional branches.  ","version":"Next","tagName":"h3"},{"title":"Vector Processor Architecture​","type":1,"pageTitle":"Vector Processors & TPU","url":"/cs-notes/computer-organization-and-architecture/vector-processors-and-tpu#vector-processor-architecture","content":" Vector processors implement an instruction set for handling large one-dimensional data. Some features of vector processor :  Vector Registers : Vector processors have dedicated vector registers that can hold multiple data elements in a single register. These registers are wider than traditional scalar registers to accommodate vector data. The number of elements that can be stored in a vector register is referred to as the vector length or vector width.Vector Instructions : Vector processors have specialized instructions, that can operate on the entire vector of data elements in a single instruction. These instructions define the operations to be performed on the vector data, such as arithmetic operations, logical operations, and data movement operations.Vector Functional Unit (VFU) : VPU is the component in a vector processor that is responsible for executing specific arithmetic or logical operations on vector data. The VFU is a specialized hardware components optimized for vector operations.Vector Pipelines : Vector processors often include vector pipelines, which are composed of multiple stages that can process vector instructions in parallel. The pipeline stages are designed to carry out different phases of instruction execution, such as fetching instructions, decoding them, executing the operations, and writing back the results.   Source : https://www.researchgate.net/figure/Simplified-view-of-a-vector-processor-with-one-functional-unit-for-arithmetic-operations_fig10_2985917  ","version":"Next","tagName":"h3"},{"title":"TPU​","type":1,"pageTitle":"Vector Processors & TPU","url":"/cs-notes/computer-organization-and-architecture/vector-processors-and-tpu#tpu","content":" Tensor Processing Unit (TPU) is a specialized hardware accelerator developed by Google for accelerating machine learning workloads, particularly those involving neural networks.  Data used in machine learning based task often represented in matrix, or higher-dimensional array, in conclusion :  Scalar processors : zero-dimensional data, such as &quot;5&quot;Vector processors : one-dimensional data, such as &quot;[5, 3, 2]&quot;TPU : n-dimensional data, such as &quot;[[1, 2], [3, 5], [7, 8]]&quot;, for two-dimensional data.  Some specialization TPU has :  Reduced Precision : TPU are designed to perform computation that doesn't require high precision, such as 8-bit or 16-bit floating-point formats, which is suitable for ML-based tasks. Using lower precision, we can reduce the memory bandwidth and storage requirements, which results in faster data transfer and processing times.Specialized Hardware &amp; Architecture : TPU has hardware specific for doing machine learning tasks, it supports operations like normalization and pooling, which are commonly used in neural networks. TPU has special memory for storing learned weights that may be accessed frequently. Another key component of the hardware is the Matrix Multiply Unit (MMU), it is highly optimized processing unit for matrix computations, including convolution operations.   Source : https://semiengineering.com/knowledge_centers/integrated-circuit/ic-types/processors/tensor-processing-unit-tpu/ ","version":"Next","tagName":"h3"},{"title":"Von Neumann","type":0,"sectionRef":"#","url":"/cs-notes/computer-organization-and-architecture/von-neumann","content":"","keywords":"","version":"Next"},{"title":"System Buses​","type":1,"pageTitle":"Von Neumann","url":"/cs-notes/computer-organization-and-architecture/von-neumann#system-buses","content":" System buses is an important component in computer system. Its primary function is to provide a communication pathway used to transfer data, instructions, and control signals between the components of a computer system. It serves as a physical connection that allows different hardware components to interact and exchange information.  The system bus combines several other buses :  Data Bus : The data bus carries the actual data being transferred between components. It is bidirectional, meaning it can transmit data in both directions. The width of the data bus determines the amount of data that can be transferred simultaneously.Address Bus : The address bus is responsible for transmitting memory addresses. It carries the location information of data or instructions in the system's memory. The width of the address bus determines the maximum addressable memory capacity.Control Bus : The control bus carries control signals that coordinate and synchronize the activities of various components in the system. It includes signals such as read/write signals, interrupt signals, clock signals, and bus control signals.   Source : https://www.learncomputerscienceonline.com/computer-bus/   Source : https://winstartechnologies.com/introduction-to-computer-bus/  ","version":"Next","tagName":"h3"},{"title":"Architecture​","type":1,"pageTitle":"Von Neumann","url":"/cs-notes/computer-organization-and-architecture/von-neumann#architecture","content":" The von Neumann architecture is a theoretical framework for designing and constructing digital computers. The architecture proposes a digital computer consists of :  Central Processing Unit (CPU) : The main processing unit, which includes the ALU for mathematical and logical operations and control unit that interprets and coordinates instructions. The CPU will also contain some registers such as instruction register and program counter register that will store necessary processing information. Memory : Main memory such as RAM that will store the data and instructions from the program. Larger Memory : Larger memory such as hard disk to store larger volume of data. Input/Output : Component that will handle the necessary input and output process between computer with I/O devices including keyboards, mice, displays, printers, disk drives, and network interfaces.   Source : https://en.wikipedia.org/wiki/Von_Neumann_architecture  The von Neumann architecture is based on the stored-program concept, which states that both instructions and data are stored in the same memory and treated the same way, allowing programs to be modified easily.  Prior to the stored-program concept, computers were typically designed to perform specific tasks and had their instructions and data hardwired into the machine. This limited their flexibility and made it difficult to modify or reprogram them for different tasks.  The stored-program concept allows for greater flexibility, CPU fetches instructions from memory, decode them, and execute them. To make a computer execute different task or programs, we can simply load different instructions into memory.  Bottleneck​  The data and instructions are stored in the same memory, the retrieval of data is done via a bus. The bus acts as a communication pathway between the CPU and memory for data transfer.  The problem come from the limitation of the bus, a bus can't be accessed multiple times simultaneously, due to some physical and electrical characteristics. The bottleneck occurs because the bus has limited bandwidth and can only transfer a certain amount of data at a time. This limitation can slow down the overall performance of the system. When the CPU needs to fetch data or instructions from memory, it has to wait for the bus to be available. Similarly, when the CPU wants to write data back to memory, it needs to wait for the bus to be free.  One way to mitigate the performance bottleneck is providing a cache memory between the CPU and the main memory. The cache memory is located closer to the CPU, it stores frequently accessed data and instructions. By having a faster cache memory, the CPU can access data and instructions more quickly, reducing the need to access the main memory via the bus. ","version":"Next","tagName":"h3"},{"title":"Registers & RAM","type":0,"sectionRef":"#","url":"/cs-notes/computer-organization-and-architecture/registers-and-ram","content":"","keywords":"","version":"Next"},{"title":"Latch​","type":1,"pageTitle":"Registers & RAM","url":"/cs-notes/computer-organization-and-architecture/registers-and-ram#latch","content":" Latch is the simplest form of memory in digital electronics that capable of storing a single bit of information. A latch uses a combination of logic gates that works by having a feedback loop, that is when the output of a gate is connected back to one of its own inputs.  We can construct a latch using self-looping OR and AND gates. We will have two input, A and B, the A is our input, while B is the input from the output.  OR : When input A is 0, the output will be 0.When input A is 1, the output will be 1. This output value will be fed back to input B, resulting in both inputs A and B being 1, and the overall output remaining 1.If input A is changed to 0, the output will still be 1 because the feedback from input B keeps it at 1.When we change input A to be 1, the output will still be 1, because we can't change the B. Therefore, the self-looping OR doesn't work, it stores the value 1 permanently, we can't use this as memory. AND : Similar to AND gate, when we start from input A and B as 1, the AND gate will produce 1. When we change the input A to 0, the logical AND between 0 and 1 will be 0, this will also change the input B. Now, whatever value we use as input for input A, it won't change anything, because the input B is permanently set to 0.   Source : https://youtu.be/fpnE6UAfbtU?si=jzfY47w-VWxCkJXr&amp;t=120  AND-OR Latch​  We can use the combination of OR and AND gate with a NOT gate to create a fully functional memory called the AND-OR latch. We will have two input, one is &quot;SET (S)&quot; and the other is &quot;RESET (R)&quot;.   Source : https://youtu.be/fpnE6UAfbtU?si=w3z481li5AWxMMpF&amp;t=163  When we have input 1 for SET, it will produce 1 from the OR gate. The OR gate will send the output to the AND gate. The AND get will take another bit from the inverse of RESET input. Because 0 negates to 1, the AND gate will produce 1 as output, the output will be fed back to the input of OR gate. In conclusion, setting the SET to 1 effectively make the output as 1.   Source : https://youtu.be/fpnE6UAfbtU?si=w3z481li5AWxMMpF&amp;t=163  When the RESET is 1, it will be negated to 0 by the NOT gate, making the AND gate produces 0. In conclusion, setting the RESET to 1 effectively resets the output to 0. Doesn't matter if we have 0 as the SET.   Source : https://youtu.be/fpnE6UAfbtU?si=oo9ZqdgbPOLxIYWp&amp;t=174  When the SET and RESET is 0, the output will be 0 or 1 depending on the last output. This is because the 0 in RESET will produce 1 from the NOT gate, therefore it all depends on the output of OR gate. If the last output is 1, the OR gate produces 1, and otherwise if the last output is 0.  The behavior of the output being the last output is what makes it possible to be used as memory. If we have 1 as the input for SET, this will effectively store value 1. As long as we don't set the RESET to 1, the output 1 remains in the latch. Only if we set the RESET to 1, the output will be reset.  Gated Latch​  This AND-OR latch can be simplified so that it requires only a single input bit, either 1 or 0, which will be stored immediately. We will also have another input which determine if we are allowed to write to the memory.   Source : https://youtu.be/fpnE6UAfbtU?si=WfSlrlDVk_xY01hB&amp;t=215  We will use two additional AND gates and another NOT gate. When we set the write to 0, this will effectively disable any write, because it will make the AND gate that takes input from DATA INPUT and WRITE ENABLE always produces 0.   Source : https://youtu.be/fpnE6UAfbtU?si=WfSlrlDVk_xY01hB&amp;t=215  Simplifying the gated latch :   Source : https://youtu.be/fpnE6UAfbtU?si=RUoSfgxY7fr0jQ_U&amp;t=255  The latch will store whatever the input is, only when the write is enabled. When the write is disabled, anything we do on the input data won't change the latch memory.  ","version":"Next","tagName":"h3"},{"title":"Registers​","type":1,"pageTitle":"Registers & RAM","url":"/cs-notes/computer-organization-and-architecture/registers-and-ram#registers","content":" A single latch can only store a single bit of information. A register is created by combining multiple latch. Registers have fixed size, it can range from 8 bits (1 byte) to 64 bits (8 bytes) or more, where each bit is stored by a single latch.   Source : https://youtu.be/fpnE6UAfbtU?si=H-WxLU6burkepXJv&amp;t=308  This is an example of 8-bit registers. The latches will be connected together within a single wire that will control whether the write is enabled or not. Each latch will take an input data and produce the output data, the concept is similar as gated latch. After inputting data to each latch, we successfully stored an 8-bit number.  Matrix Latch​  In the 8-bit register above, we need 8 wire for each latch to take the input data and another 8 wire to produce the output data. We also need a single additional wire for the enabling or disabling the write.  In certain digital circuit designs, the latches are arranged in a grid-like structure to reduce the wire required to construct a single registers. The wire that will enable/disable write will be within every intersection in the grid. To enable a certain latch in the grid, the corresponding wire on the specific row and column must be 1. The circuit also uses additional AND gate to connect the wire with the internal latch.   Source : https://youtu.be/fpnE6UAfbtU?si=eoDWiptQiBjjYiqQ&amp;t=349  In this structure, although we used additional wires just for enabling/disabling the write operation, we unified the wires for inputting data. The impact of enabling the write operation is limited to a specific row and column, leaving the other latches in the grid unaffected. So, instead of employing a single wire for enable/disable functionality, we opt for a single wire dedicated to data input.  Memory Address​  In the grid structure, we can select certain latch by its corresponding row and column. If each latch were to have an address, just like memory, their address would be their corresponding row and column.  But remember, the number of row and column will also be encoded in binary format. For example, a latch in 4th column and 7th row would have address of &quot;01000111&quot;, where 4 in binary is &quot;0100&quot; and 7 in binary is &quot;0111&quot;. The address length of the grid should match the number of latches in the grid. In a 16×16 grid (256-bit memory) with 16 latches per row and column, an 8-bit address is needed. This consists of 4 bits for the row address and 4 bits for the column address.  When we have certain address and we want to get the latch, we will need something that handles the conversion. A multiplexer is a component we can use, it is a component that is commonly used to select and route different data inputs to the desired output. So, whenever we want to enable write and write data for a specific latch, we just input the memory address in form of binary to the multiplexer.   Source : https://youtu.be/fpnE6UAfbtU?si=3eEYivxZm6Nfx-_C&amp;t=487  RAM​  The 16×16 grid memory can be simplified like below   Source : https://youtu.be/fpnE6UAfbtU?si=XDLNhZa1uqvkB82k&amp;t=519  It will have address lines that should accommodate the 256 possible memory locations, the data lines for writing to the memory, and the read/write enable/disable functionality.  We can combine multiple instance of this 256-bit memory to construct a storage with larger capacity. We can arrange them the similar way of arranging latches to create a register.   Source : https://youtu.be/fpnE6UAfbtU?si=THw2caJKKBD2tiSt&amp;t=550  Each of the eight individual 256-bit memories operates independently, with its own set of address lines, data lines, and read/write signals.  When accessing data from this 8 x 256-bit memory, the desired memory unit is selected, and the specific memory location within that unit is addressed using the address lines. Data can then be read from or written to the selected location through the corresponding data lines.  And then, together creating a unified memory system called Random Access Memory (RAM).   https://youtu.be/fpnE6UAfbtU?si=WUJg36L_x4mABGrz&amp;t=564  RAM can be constructed by combining each small registers, it can be thought as an array of registers. So, similar to registers, RAM will have sort of addressing, data lines, and read/write signals. They are constructed the same way, but they serve different purposes. RAM is a larger memory and typically used for more than just holding data temporarily during CPU operations.  For modern computers with larger memory, the number of addresses will increase as well. The number of available addresses determines the memory capacity of the RAM. For example, the A 32-bit RAM can address up to 2322^{32}232 (4,294,967,296) unique memory locations, which translates to a maximum RAM capacity of 4 gigabytes (GB). In contrast, a 64-bit RAM can address up to 2642^{64}264 (18,446,744,073,709,551,616) memory locations, allowing for a 16 exabytes (EB) storage or more.  When we say &quot;32-bit computer&quot;, we are referring to the architecture of the system, which is the size of the memory addresses used by the computer's processor. So, a 32-bit computer would have maximum of 4 GB RAM, the processor will be able to address up to 2322^{32}232 distinct memory locations.   Source : https://www.eecis.udel.edu/~davis/cpeg222/AssemblyTutorial/Chapter-10/ass10_2.html  note Sometimes memory address is denoted by hexadecimal number system. The prefix &quot;0x&quot; typically correspond to hexadecimal, while the prefix &quot;0b&quot; correspond to binary.  Type of RAM​  There are several types of RAM used in computer systems :  DRAM (Dynamic Random Access Memory) : DRAM is the most common type of RAM used in modern computer systems. It is a volatile memory technology that stores each bit of data in a separate capacitor within an integrated circuit. The data stored in DRAM needs to be refreshed periodically to retain its information, by refreshing, it means restoring the charge in the capacitor. DRAM is relatively slower compared to other types but offers higher storage density and lower cost. SRAM (Static Random Access Memory) : SRAM is another type of volatile RAM that stores data using latch circuits. Unlike DRAM, SRAM does not require periodic refreshing, which makes it faster and more reliable. However, SRAM is more expensive and has lower storage density compared to DRAM. SRAM is commonly used in cache memory and as registers in CPUs. SDRAM (Synchronous Dynamic Random Access Memory) : Typically traditional RAM can't keep up with the CPU's processing speed, this results in a performance bottleneck. SDRAM is a type of DRAM that synchronizes its operations with the CPU's clock, offering the potential for improved performance. SDRAM is used as the main memory in most computer systems. Different variations of SDRAM include DDR (Double Data Rate) SDRAM, DDR2, DDR3, DDR4, and DDR5, with each generation providing increased data transfer rates and improved performance. VRAM (Video Random Access Memory) : (not to be confused with virtual memory) VRAM is a specialized type of RAM that is specifically designed for graphics processing. It is used to store graphical data required by the graphics card. VRAM is optimized for high-speed read and write operations and is capable of simultaneously providing data to the graphics card while refreshing the display.   Source : https://quicklearncomputer.com/types-of-ram/  Type of Registers​  There are many types of registers :  General-Purpose : They are versatile register that can be used for various purposes. Examples are accumulator (AC), which is used to hold intermediate results during arithmetic and logical operations. Another is data register (DR), used to store the operands and results of arithmetic and logical operations.Specific-Purpose : Registers that are dedicated for specific tasks. Program Counter (PC) : Holds the memory address of the next instruction to be fetched and executed.Instruction Register (IR) : Holds the current instruction being executed by the CPU. It temporarily stores the fetched instruction until it is decoded and executed.Address Register (AR) : Hold memory addresses needed for data transfer between the CPU and memory. They are used to specify the source or destination memory addresses during data movements.Stack Pointer : During program's execution, a dedicated stack will be used to store function calls, local variables, intermediate data storage, etc. The stack pointer is the specific pointer that points to the stack. Simply the register contains memory address for the program's stack. Status Register/Flags Register : Contains individual bits that represent the status or condition of the CPU after an operation. These bits may provide information about the arithmetic operations in ALU, such as carry, overflow, zero, and sign indicators.  And many more…   Source : https://www.tutorialandexample.com/registers  ","version":"Next","tagName":"h3"},{"title":"Memory Hierarchy​","type":1,"pageTitle":"Registers & RAM","url":"/cs-notes/computer-organization-and-architecture/registers-and-ram#memory-hierarchy","content":" Memory Hierarchy is the arrangement and organization of different levels of memory in a computer system. The hierarchy consider factors such as cost, capacity, access speed, and latency.  Registers : These are the fastest and smallest storage units located directly within the CPU. Registers hold data that the CPU is currently processing. They have the fastest access times but the smallest capacity. Cache : Cache memory is a small and fast memory located between the CPU and main memory. It stores frequently accessed data and instructions to reduce the time taken to access them from the main memory. Cache memory is divided into multiple levels, such as L1, L2, and sometimes L3, with each level having larger capacity but slower access times than the previous level. Main Memory (RAM) : Main memory, typically is RAM modules, is the primary storage used by programs and data during execution. It has larger capacity compared to cache memory but slower access times. Secondary Storage : Secondary storage devices, such as hard disk drives (HDDs) and solid-state drives (SSDs), provide non-volatile storage for long-term data storage. They have larger capacities than main memory but longer access times. Tertiary Storage : Tertiary storage refers to external storage devices, such as magnetic tapes or optical disks. They offer even larger capacities, but slower access times compared to secondary storage.   Source : https://www.geeksforgeeks.org/memory-hierarchy-design-and-its-characteristics/ ","version":"Next","tagName":"h3"},{"title":"AES","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/aes","content":"","keywords":"","version":"Next"},{"title":"Algorithm​","type":1,"pageTitle":"AES","url":"/cs-notes/computer-security/aes#algorithm","content":" AES operates on fixed block size of 128 bits and operates in a series of transformations applied in multiple rounds, 10, 12, and 14 rounds for 128-bit, 192-bit, 256-bit keys, respectively.  The 128 bits block is structured in a 4×4 matrix from b0tob15b_{0} to b_{15}b0​tob15​, typically called the state.   Source : https://en.wikipedia.org/wiki/Advanced_Encryption_Standard#Description_of_the_ciphers  AES is based on substitution–permutation network (SPN), it is a way of mixing up and rearranging information. It involves substitution, where certain letters or characters in the block are replaced with different ones, and permutation, where the order of the remaining letters is shuffled or rearranged. This process is done in the rounds of transformations, each round applies the same operations but with different keys that determine the specific substitutions and permutations.  Here is the high-level overview of AES :  Key Expansion : The chosen encryption key is divided into a set of round keys, which will be used in the SPN process. Each word being 32 bits long in AES, 4 words for 128-bit keys, 6 words for 192-bit keys, and 8 words for 256-bit keys. Initial Round : The input data (plaintext) is combined with the first round key using a bitwise XOR operation. Rounds : Each round consists of four sub-steps: SubBytes, ShiftRows, MixColumns, and AddRoundKey. SubBytes : Each byte of the input is substituted with a corresponding byte from a substitution box (S-box), which provides nonlinear substitution. S-box is a specific 16×16 substitution table, it takes an 8-bit input value (byte) and outputs an 8-bit value. The S-box is initialized with specific values based on mathematical operations over the finite field of GF(28)GF(2^8)GF(28). Source : https://en.wikipedia.org/wiki/Advanced_Encryption_Standard#The_SubBytes_step ShiftRows : The bytes in each row of the state (the current block being processed) are cyclically shifted. Specifically, the first row is left unchanged. Each byte of the second row is shifted one to the left. Similarly, the third and fourth rows are shifted by offsets of two and three respectively Source : https://en.wikipedia.org/wiki/Advanced_Encryption_Standard#The_ShiftRows_step MixColumns : Each column of the state is multiplied with a fixed polynomial. This step introduces diffusion and non-linearity into the encryption process. Mathematically, the multiplication is performed in the finite field of GF(28)GF(2^8)GF(28), using the irreducible polynomial x8+x4+x3+x+1x^8 + x^4 + x^3 + x + 1x8+x4+x3+x+1 as the modulus. Source : https://en.wikipedia.org/wiki/Advanced_Encryption_Standard#The_MixColumns_step AddRoundKey : The round key for the current round is combined with the state using a bitwise XOR operation. Source : https://en.wikipedia.org/wiki/Advanced_Encryption_Standard#The_AddRoundKey Final Round : The final round omits the MixColumns step and only performs the SubBytes, ShiftRows, and AddRoundKey operations.  The decrypt process is basically the reverse of the encryption process.  In summary, AES structure input data into the state matrix, mix the data which includes permutation and substitution operation, altering the data again with the multiplication of matrix and polynomial arithmetic based on various mathematical properties, and also the bitwise XOR operation. All the process is controlled by the chosen key.   Source : https://github.com/PitCoder/Cryptography ","version":"Next","tagName":"h3"},{"title":"Antivirus & Antimalware","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/antivirus-antimalware","content":"","keywords":"","version":"Next"},{"title":"How Malware Works​","type":1,"pageTitle":"Antivirus & Antimalware","url":"/cs-notes/computer-security/antivirus-antimalware#how-malware-works","content":" Malware can do various thing to cause problems, for example, they may execute instruction without authority. The instruction can be deleting user's data, leaking information, doing a useless computation to slow down computers, etc.  Malware exploit vulnerabilities of a computer system. It can exploit weaknesses in software security, outdated patches, or misconfigurations to bypass defenses and gain elevated privileges.   Source : https://www.wallarm.com/what/explained-ransomware-attack  ","version":"Next","tagName":"h3"},{"title":"Type of Malware​","type":1,"pageTitle":"Antivirus & Antimalware","url":"/cs-notes/computer-security/antivirus-antimalware#type-of-malware","content":" Malware is a broad term for malicious software, here are some types of malware :  Viruses : Viruses is analogous to biological viruses, when they are executed, they will infect other programs by injecting malicious code.Worms : Worms is a self-replicating malware that starts from an infected computer and replicates to other device in a network. Worms can consume network bandwidth and degrade system performance.Trojans : Trojans are software that imitate other software to trick users into downloading and executing them, often by hiding in seemingly harmless files or applications.Ransomware : Ransomware is a type of malware that encrypts files on a victim's computer or network, making them inaccessible. The attackers then demand a ransom payment in exchange for the decryption key.Adware : Adware is a form of malware that displays unwanted advertisements. It may come bundled with legitimate software or be installed unintentionally by the user. While it is not very dangerous compared to other, it can be annoying to users.Spyware : Spyware is designed to covertly gather information about a user's activities, such as browsing habits, keystrokes, or sensitive data.Rootkits : Rootkits are stealthy malware that provides unauthorized access to a computer or network.Keyloggers : Keyloggers record keystrokes made by a user on a compromised system. They can capture sensitive information such as passwords, credit card details, or other confidential data.  ","version":"Next","tagName":"h3"},{"title":"How Antivirus & Antimalware Works​","type":1,"pageTitle":"Antivirus & Antimalware","url":"/cs-notes/computer-security/antivirus-antimalware#how-antivirus--antimalware-works","content":" Antivirus and antimalware help protecting computer systems, many techniques are used for this.  Database of Virus : Antivirus software often have a huge database of known malware, they will scan through user's file and check if those file contains a common pattern or characteristics.Analysis : Antivirus can analyze software by having a heuristic analysis. Heuristic is an approach to solve problem that involves using rules or strategy that may evolve over time. For example, if a program often access user's information and send it over network, it may be considered as malicious.  An antivirus software regularly updates its signature database and program components to stay up-to-date with the latest threats. It is a very crucial to keep up with recently known vulnerabilities as fast as possible.   Source : https://info.support.huawei.com/info-finder/encyclopedia/en/Antivirus.html ","version":"Next","tagName":"h3"},{"title":"Computer Security","type":0,"sectionRef":"#","url":"/cs-notes/computer-security","content":"","keywords":"","version":"Next"},{"title":"All pages​","type":1,"pageTitle":"Computer Security","url":"/cs-notes/computer-security#all-pages","content":" Computer Security FundamentalsCryptography Math ConceptsHashing Hash FunctionMD5SHA EncryptionSymmetric Encryption DESAESBlowfish Asymmetric Encryption &amp; Key Exchange Diffie-HellmanRSAElliptic Curve CryptographyDSA Lattice-Based CryptographyBlockchain Antivirus &amp; AntimalwareReverse EngineeringNetwork SecurityWeb SecurityMobile SecurityBackend &amp; Server SecurityOther Attack &amp; Exploit ","version":"Next","tagName":"h3"},{"title":"Backend & Server Security","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/backend-server-security","content":"","keywords":"","version":"Next"},{"title":"Injection Attack​","type":1,"pageTitle":"Backend & Server Security","url":"/cs-notes/computer-security/backend-server-security#injection-attack","content":" There are many kinds of attacks, one of the common type of attack is an injection attack. Injection attack is when the attacker insert malicious code or commands into an application's backend system. Example for injection attack are :  SQL Injection : Attacker manipulates user input to execute unauthorized SQL queries, potentially gaining access to or modifying the database.Cross-site scripting (XSS) : Injecting malicious scripts into web pages viewed by users in some input field, might steal user's sensitive information.Cross-site request forgery : Trick a user into performing an action on a website without their knowledge or consent.DDoS attack : Flooding a backend server with by overwhelming it with a flood of traffic, making it unable to process users request.  ","version":"Next","tagName":"h3"},{"title":"API Security​","type":1,"pageTitle":"Backend & Server Security","url":"/cs-notes/computer-security/backend-server-security#api-security","content":" APIs are very important in software development to enable communication and data exchange between various systems, applications, or services. APIs often expose valuable data, and they are heavily relied on by the application, attacking them might be a good starting point.  API Brute forcing : Attacker attempts all possible combinations of authentication credentials to gain unauthorized access to an API. This is often used to bypass weak or improperly implemented authentication mechanisms.Cookie Hijacking : When someone that uses an API has been successfully authenticated using token or session, some attackers may try to steal them and use it to gain unauthorized access.Excessive Request : This is related to DDoS attack, when a backend service receives flood of request from an attacker within a short period. It will consume server's resources such as CPU, memory, network, and the server may experience performance degradation, unresponsiveness, or even the server might down.  ","version":"Next","tagName":"h3"},{"title":"Authentication & Authorization Security​","type":1,"pageTitle":"Backend & Server Security","url":"/cs-notes/computer-security/backend-server-security#authentication--authorization-security","content":" Authentication is a crucial aspect in application, developers must ensure that only legitimate users and authorized individuals are able to authenticate to a system.  Authentication system can be upgraded to be more secure by having multiple factor consideration, various other identity of user (e.g., uses fingerprint or face to authenticate rather than password that can be cracked).  Always validate and sanitize input from user to prevent common vulnerabilities such as SQL injection, cross-site scripting  Encrypt communication between user and server, also store user credentials like passwords using strong encryption algorithm like AES, avoid of storing it in plaintext.  ","version":"Next","tagName":"h3"},{"title":"Security Mitigation​","type":1,"pageTitle":"Backend & Server Security","url":"/cs-notes/computer-security/backend-server-security#security-mitigation","content":" Some strategy used to protect backend system :  Rate Limiting : A technique which will limit API request per time interval, this can help prevent DDoS attack. Sanitizing Input : Make sure to always validate the input field in your application, make sure it doesn't contain exploitable character to be safe from injection attack like XSS or SQL injection. Data Encryption : Encrypt sensitive data stored in database, communication may also use secure protocol such as HTTPS. Port Blocking : When connecting to a specific computer, we typically connect through a designated port number. Similar to physical ports on electronic devices serving as entry points for external devices, connecting to a specific port number allows us to select the desired service. Port blocking involve blocking specific port numbers, thus preventing outgoing network traffic to those ports. Content Security Policy (CSP) : A security mechanism implemented by web browsers to mitigate the risk of certain types of web-based attacks. CSP works by specifying a whitelist of trusted sources for various types of content, such as scripts, stylesheets, images, fonts, and frames. For example : default-src Specifies the default source for all types of content if a specific directive is not specified.script-src : Specifies the allowed sources for JavaScript code.style-src : Specifies the allowed sources for CSS stylesheets.img-src : Specifies the allowed sources for images.font-src : Specifies the allowed sources for fonts.  ","version":"Next","tagName":"h3"},{"title":"CORS​","type":1,"pageTitle":"Backend & Server Security","url":"/cs-notes/computer-security/backend-server-security#cors","content":" Web browser enforce the Same-Origin Policy, which restricts web pages from making requests to resources on different domains. For example, we have a website at https://www.example1.com and another one at https://www.example2.com. When our application at https://www.example1.com has a script to access data or make request to the https://www.example2.com, this is called cross-origin request and it is not allowed according to the Same-Origin Policy.  Cross-Origin Resource Sharing (CORS) is a security mechanism that relaxes the Same-Origin Policy allowing for cross-origin request. CORS provides a way for servers to explicitly allow cross-origin requests from specific domains (can be our own server that is located in different domain) while still protecting against unauthorized access. It works by adding exception for a specific request :  Client-Side Request : When the browser makes a cross-origin request, the browser sends an HTTP request with an &quot;Origin&quot; header that indicates the page's origin (e.g., https://example.com). Server-Side Response : The server that receives the request can respond with specific HTTP headers to indicate whether the requested resource should be accessible to the client-side script. These headers include : Access-Control-Allow-Origin : Specifies the domains that are allowed to make cross-origin requests. For example, &quot;Access-Control-Allow-Origin: https://example.com&quot; would allow requests from &quot;https://example.com&quot; but reject requests from other origins. The server can also use wildcard values with * to allow requests from any origin. Access-Control-Allow-Methods : Specifies the HTTP methods (e.g., GET, POST, PUT, DELETE) allowed for cross-origin requests. Access-Control-Allow-Headers : Specifies the headers that are allowed to be included in the cross-origin request. Header can contain content-type, which specify the format data in request or response body, or authentication/authorization credentials. Access-Control-Allow-Credentials : Indicates whether the request can include credentials such as cookies or HTTP authentication information. Preflight Request : The browser may send a preflight request also known as HTTP OPTIONS request. An HTTP OPTIONS request is used to retrieve information about the communication options available in a server. For example this is an example of an HTTP OPTIONS request and response.  OPTIONS /api/resource HTTP/1.1 Host: example.com Origin: https://client.com Access-Control-Request-Method: POST Access-Control-Request-Headers: Content-Type, Authorization   HTTP/1.1 200 OK Allow: GET, POST, HEAD, OPTIONS Access-Control-Allow-Origin: https://client.com Access-Control-Allow-Methods: GET, POST, PUT, DELETE Access-Control-Allow-Headers: Content-Type, Authorization  ","version":"Next","tagName":"h3"},{"title":"Blockchain","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/blockchain","content":"","keywords":"","version":"Next"},{"title":"Explanation​","type":1,"pageTitle":"Blockchain","url":"/cs-notes/computer-security/blockchain#explanation","content":" A single record of transaction is called a block, each block has a reference to other block, these blocks are chained together to create a chain of block. The chaining mechanism in blockchain is similar to linked list data structure.  In linked list, each element is considered as a node, each node has memory reference to the next node. However, blockchain is more advanced than that, blocks are chained together using cryptographic hash functions.  Block Structure​  A block consists of header and body. Header contains important information about the block, and the body, contains the transaction data.  Header :  Hash of Previous Block : The hash value of the previous block in the blockchain, creating the chain.Timestamp : The time when the block was created or mined.Version : This field indicates the version of the blockchain protocol being used.Merkle Root : Merkle root is a hash value that represents a summary of all the transactions in the block. Merkle's tree, also known as hash tree, is a data structure used to verify the integrity and consistency large sets of data. A Merkle tree is constructed by recursively hashing pairs of data until a single root hash, known as the Merkle root, is obtained. This mean the hashes of data will be hashed again to create the parent, up to the root node.  When a block is hashed, the entire block is converted into specific format that can be processed by hash function. The resulting hash serve as a unique identifier for a block in the chain.  The chaining mechanism in blockchain ensure the immutability of the data stored in a blockchain. If any data in a block is modified, it will result in a change in the block's hash value (due to avalanche effect). As a result, the subsequent blocks in the chain will have invalid references to the altered block, indicating that the blockchain has been tampered with. Another thing to note is blockchain is a decentralized platform, the ledger is distributed among multiple participants who maintain their own copies of the blockchain. This distributed nature ensures that any changes made to a particular block or transaction would need to be replicated across the entire network to maintain consensus and consistency.  Body :  Transactions : The block body contains a list of transactions that are being added to the blockchain.   Source : Chain of blocks, Block structure, Merkle tree  Mining​  Blockchain mining is the process of adding new blocks to a blockchain by solving complex mathematical puzzles or cryptographic problems. Some blockchain implementation requires you a proof when you add blockchain.  Consensus​  Consensus is the mechanism that involves asking participants whether blockchain can be modified or not. These participants are individuals or organizations that are actively involved in the operation and maintenance of the blockchain network. It is the process of all participants to verify if a transaction is valid or not, to ensure the validity of transactions and the integrity of the ledger.  One of the popular consensus mechanism is the Proof of Work (PoW). This mechanism require network participants (known as miners) to solve complex mathematical puzzles. In a PoW system, the puzzle typically involves finding something called nonce that, when combined with the other block data, produces a hash value that meets certain criteria. The nonce is a random value that also exist in the header of a block.  Miners attempt to find the solution to the puzzle through a trial-and-error process. They repeatedly modify the nonce value, calculate the hash of the block, and check if it meets the required criteria. Since the output of a cryptographic hash function is essentially unpredictable, miners must make many attempts until they find a suitable nonce that satisfies the puzzle's difficulty criteria. The difficulty depends on the blockchain system, the system may make it to be easy to increase the block creation rate or hard to decrease it.  For example, the puzzle may require miners to find a nonce value that, when combined with the block data, produces a hash value that starts with three zeros (000) when hashed using a cryptographic hash function like SHA-256. So we will need to find hash such that: Hash = SHA-256(Block Data + Nonce) = 000.... After a miner found a solution, the other participants will verify the solution by doing the same calculation with the given nonce solution. If it's valid, the nonce will be used for creating new block, the miner that found the solution will also be rewarded with something like cryptographic currency or tokens.  This also applies when you modify certain block, when you modify the block, you will also change the hash of the block. Let's say you are changing certain block, the previous block that refer to it will need to be changed as well. When you want to change that other block, you will need to complete the PoW again to satisfy the block's difficulty criteria.   Source : https://www.ledger.com/academy/blockchain/what-is-proof-of-work ","version":"Next","tagName":"h3"},{"title":"Blowfish","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/blowfish","content":"","keywords":"","version":"Next"},{"title":"Algorithm​","type":1,"pageTitle":"Blowfish","url":"/cs-notes/computer-security/blowfish#algorithm","content":" Blowfish was designed to be an alternative of DES. They share some similarities in terms of the rounds of encryption and decryption, that is to use Feistel structure. Another similarity is the fixed block size of 64 bits. The difference between them is the choice of key size, blowfish variable key sizes between 32 and 448 bits.  Consist of 16 rounds of encryption or decryption  Key Setup : Key is divided into 32-bit words. An 18-entry array called P-array (P, or denoted as K), which is used to generate subkeys in the encryption or decryption rounds. It is initialized from the hexadecimal digits of pi (π≈3.14159265358979323846\\pi \\approx 3.14159265358979323846π≈3.14159265358979323846). Another setup is initializing the four 256-entry of S-boxes (S0, S1, S2, S3), it will be used to perform substitutions. The initial value will be derived from the fraction part of pi. Encryption Process : Operates on 64-bit blocks of input plaintext, which is divided into two 32-bit halves, referred to as left and right. Each round consist of 4 actions : Action 1 : The left half is XORed with a subkey derived from the P-array. Action 2 : The XORed data is fed into the Blowfish's F function. The F function involves splitting the 32-bit input into four 8-bit segments, substituting each segment with values from the S-boxes, perform a series of XOR and modular addition operations on the segments. Action 3 : The output of F function is XORed with the right half of the data. Action 4 : Left and right halves are swapped. Key Expansion : The key expansion process generates subkeys used in the encryption and decryption rounds. The chosen key is cycled through the P-array and XORed with the previous subkey to generate a new subkey. This process is repeated until all the subkeys for the rounds are generated. After 16 rounds, the left and right halves are swapped one final time. The final block is XORed with the last two subkeys derived from the P-array, which results in encrypted form of the plaintext block.  The decryption process is similar to encryption but with the subkeys used in reverse order (i.e., instead of starting from 0, 1, 2,..., it starts from 17, 16, 15,...)  ","version":"Next","tagName":"h3"},{"title":"Bcrypt​","type":1,"pageTitle":"Blowfish","url":"/cs-notes/computer-security/blowfish#bcrypt","content":" Bcrypt is a password-hashing function, based on the Blowfish cipher.  Initialization : Bcrypt is used to hash a password, a random salt of 128 bits (16 bytes) is generated. Bcrypt generates key based on the Blowfish key cipher, we will need to initialize P-array and S-boxes.Key Expansion : Password and salt are combined and goes through the key expansion algorithm based on the Blowfish cipher. The password and salt will be shuffled based on the P-array (for subkey generation) and S-boxes (for substitution). The number of iteration is derived from number called cost, it can be from 4 to 31 (in total 2cost2^{\\text{cost}}2cost iteration).Hashing : The hashing process doesn't involve converting the password directly. Password and salt are used to modify the state of P-array and S-boxes. The modified state will be used to encrypt the fixed string &quot;OrpheanBeholderScryDoubt&quot;. The encryption is run for 64 times using the standard Blowfish algorithm. The output of that is the resulting password hash, which will be concatenated with the selected cost and salt. ","version":"Next","tagName":"h3"},{"title":"Computer Security Fundamentals","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/computer-security-fundamentals","content":"","keywords":"","version":"Next"},{"title":"Essence of Computer Security​","type":1,"pageTitle":"Computer Security Fundamentals","url":"/cs-notes/computer-security/computer-security-fundamentals#essence-of-computer-security","content":" Vulnerability &amp; Threats​  Vulnerability refers to the weakness of a computer system, including network, software, or configuration which can be exploited. It can be user's fault such as weak password, or the programmer's fault, in the case of vulnerability in an application. These potential dangers or harmful events are called threats.  Malicious Actors​  Malicious actors are any individuals, groups, or organizations that intent to cause harm and exploit vulnerability in a computer system. There are various motivation for this, including financial reasons, unhealthy competition, political, ideological, or personal reasons, etc.  Goal of Computer Security​  In computer security, a common model to understand goals and principles of computer security is the CIA triad, which stands for Confidentiality, Integrity, and Availability.  Confidentiality : Ensures that sensitive data is accessible only to authorized individuals or entities. Access preventation such as encryption, access controls, and secure communication protocols are implemented. Integrity : Ensures data remains accurate, reliable, and unaltered throughout its lifecycle. It involves protecting data from unauthorized modification, deletion, or tampering. Techniques such as data validation, checksums, digital signatures, and secure storage mechanisms are used to maintain data integrity and prevent unauthorized changes. Availability : Ensures that computer systems are accessible and operational when needed. It involves preventing disruptions or unauthorized denial of service (DoS). Various technique such as backup systems, fault tolerance, and distance recovery plans are implemented to maximize system availability and minimize downtime.  To ensure computer systems are not disrupted by malicious actors, authentication and authorization techniques are employed.  Authentication : Authentication is the process of verifying identity of users attempting to access a computer system. A simple authentication mechanism used in our daily life is the password mechanism.Authorization : Authorization is the process of giving appropriate level of access to users. We need to ensure authenticated users are only allowed to perform specific actions. For example, in a blog website, a regular user may not have the permission to delete blog posts or delete other user's comments.  ","version":"Next","tagName":"h3"},{"title":"Cryptography​","type":1,"pageTitle":"Computer Security Fundamentals","url":"/cs-notes/computer-security/computer-security-fundamentals#cryptography","content":" Cryptography is the practice of securing communication and data transmission by converting it into a form that is not understandable by unauthorized individuals. The main goal is to protect sensitive information such as user's data while communicating over insecure networks in various applications, including secure messaging, online transactions, etc.  The process of transforming readable data (plaintext) into unreadable data (ciphertext) is called encryption. In encryption, the plaintext is processed through mathematical algorithm along with a &quot;key&quot;, resulting in ciphertext. A key is like extra information that sender and receiver has to transform plaintext into a ciphertext or vice versa. This key is a crucial secret, as the data encryption depends on it.  Cryptography is heavily based on mathematical concepts, mathematical proofs and techniques are used to analyze the strength of encryption algorithms. Besides mathematics, cryptography includes concepts from other topics like information security, electrical engineering, signal processing, physics, etc.  One-way Function​  A one-way function is a mathematical function that is relatively easy to compute in one direction but computationally difficult to reverse. In other words, given an input, it is easy to compute the output, but given the output, it is difficult to determine the original input.  One-way functions are heavily used in cryptography, it makes it extremely difficult for attackers to determine the original input from the output. For example, when storing a user's password, it will go through a one-way function before being stored. Even if attackers gain access to the storage, they cannot easily retrieve the original password. During login, to actually verify the password correctness, the input password is processed again to the function and compared to the one stored in the storage.   Source : https://www.twilio.com/blog/what-is-public-key-cryptography  ","version":"Next","tagName":"h3"},{"title":"Terminology​","type":1,"pageTitle":"Computer Security Fundamentals","url":"/cs-notes/computer-security/computer-security-fundamentals#terminology","content":" Penetration Testing​  Penetration testing is the practice of simulating real-worlds attacks toward a computer system. It is done to evaluate the effectiveness of the security system. Penetration testing is done systematically with defined methodology and involves a series of steps.  Ethical Hacking​  Ethical hacking is a more general term for a cybersecurity practice that identifies vulnerabilities and weaknesses in computer systems. Ethical hackers are &quot;good&quot; hacker, they use the same methodologies as malicious hackers, but with the explicit permission and legal authority to conduct their activities. They will try to identify security flaws and provide recommendations for improving the overall security.  Blue, Red, Purple Team​  They are different roles in cybersecurity :  Blue Team : Defensive side of security, they analyze, implement, monitor security systems.Red Team : Offensive side of security, they conduct activities such as penetration testing, vulnerability assessments, and targeted attacks to test a computer system.Purple Team : The combination of both red and blue elements. They collaborate and shares knowledge between the defensive and offensive teams.   Source : https://www.iansresearch.com/resources/all-blogs/post/security-blog/2022/04/19/understand-the-roles-of-red-blue-and-purple-teams  Zero-Day Attack​  Many new attacks and exploits are discovered all the time, the attacks that are done on the same day they are discovered is called a zero-day attack. Zero-day attack is indeed very dangerous, because it leaves no time for the affected party to develop a protection implementation. Attackers have a significant advantage at the moment when security professionals and software developers are not yet aware of them.  Breach​  Breach refers to an unauthorized or unintended access to, use of, or disclosure of sensitive information. It can be a security breach, which is when unauthorized party gains access to some computer systems, bypassing security measures. Another one is data breach, which is the unauthorized access to sensitive or confidential data such as financial records, health records, intellectual property, etc.  Sandboxing​  When accessing specific systems, often times they are potentially harmful. For example, downloading specific file from untrusted website, it may contain malicious software inside. Sandboxing is the process of mitigating potential impact of malicious or vulnerable software.  It isolates an application or process from the rest of the system or network environment. It creates a controlled and restricted environment where an application can run, limiting its access to system resources and potentially harmful actions. Sandboxing can be implemented using virtualization or containerization.  Obfuscation​  Obfuscation is the process of making code or data difficult to understand or analyze. It prevents an application to be reverse engineered., or intellectual property theft by making the code or data less comprehensible to human readers or automated analysis tools.  Technique can include changing variable, function, class name to some arbitrary or non-descriptive name, encrypting a section of code, changing a structure of code without changing its functionality.  Digital Signature​  A digital signature is basically a signature which is used to provide authentication and integrity for digital documents. It uses the concept of encryption, specifically the private and public key.  A digital signature is unique to the sender. It is created using special mathematical algorithms and a secret code known as a private key. This private key is kept secure and known only to the signer. To verify the content, another key called public key will be used. The public key will also produce some code, the resulting code will be the same as the resulting code produced by private key.  Certificate​  Certificate is a digital document issued by a trusted third-party organization known as a Certificate Authority (CA). It is used to verify the authenticity and integrity of digital entities such as websites, individuals, organizations, or software. It contains subject, issuer, validity period, and digital signature.  Steganography​  Steganography is the practice of hiding information in a media such as images or videos. An image contains a sequence of RGB pixels, which is encoded in bits (e.g., 8-bit for each color). Hiding information involve changing one of the bits which has the lowest significance (called the least significant bits (LSB)) to the hidden information. Changing the least significant bits make the embedded information doesn't affect the overall data significantly.  The inserted information will need to be encoded in number as well, the point is, it must fit within the color. For example, we can transform a hidden message in plain text to number using the ASCII encoding, which is 8-bit, making it suitable for RGB color representation (e.g., in ASCII decimal form, &quot;goodbye&quot; can be encoded to 01100111). We will then insert each binary bit into the image by modifying each LSB of the color. To further ensure no significant change are visible, we can solely use one color channel. To signal the conclusion of the message, we can easily designate the succeeding pixel following the message's end as the ending sequence.   Source : https://www.quora.com/What-is-steganography-and-what-is-it-used-for ","version":"Next","tagName":"h3"},{"title":"DES","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/des","content":"","keywords":"","version":"Next"},{"title":"Algorithm​","type":1,"pageTitle":"DES","url":"/cs-notes/computer-security/des#algorithm","content":" The DES algorithm processes plaintext inputs in fixed-size blocks of 64 bits. The key used for encryption and decryption is 56 bits in length, although the actual key size is 64 bits, with 8 bits reserved for parity checking purposes. The relatively short key length used in DES makes it vulnerable.  Here is the high-level overview of DES :  Padding : Padding is added to the message if required, the message should be multiple of 64 bits. Key Schedule : The original 64-bit key undergoes an initial permutation called PC-1. Only 56-bit key is taken, the remaining 8-bit is discarded or used as parity check. The remaining 56-bit key is then split into two 28-bit halves, known as the left half (C0) and the right half (D0). During encryption, the key is rotated left by one or two bits, depending on the specific round of encryption. After rotation, a 48-bit subkey will be derived from the combined halves by another permutation process called PC-2. Source : https://en.wikipedia.org/wiki/File:DES-key-schedule.png Encryption Rounds : The main encryption process is divided into rounds of transformations. The transformation process is called Feistel cipher, or Feistel (F) function, it takes half size of block (32-bit) and a key. It involves key mixing, substitution, permutation to introduce confusion and diffusion. note Feistel structure is the general design principle for cipher that divides the input data into two equal halves and applies a series of rounds to these halves. On the other hand, Feistel cipher is a specific type of symmetric encryption algorithm that employs the Feistel structure. Initial Permutation (IP) : The 64-bit plaintext block goes through an initial permutation (IP) step. The permuted plaintext block is divided into two 32-bit halves, known as the left half and the right half. In each round, the left half will be swapped with the right half from the previous round. Expansion : The 32-bit half-block is expanded to 48 bits using an expansion permutation table (E-box). The table contains bits which will determine the specific order in which the bits of the input and output blocks are arranged. Key Mixing : The expanded half is combined with the 48-bit key for that round using XOR operation, derived from the original 56-bit key in the key scheduling process. Substitution : The combined result is then passed through eight S-boxes (substitution boxes), which substitute 6-bit values with 4-bit values based on predefined tables. Permutation : The outputs of the S-boxes are rearranged by permutation operation according to a fixed permutation called P-box (P). The result of the P-box permutation is then XOR-ed with the left half from the previous round to produce the new right half. Also, The new left half (Li) is set equal to the previous right half (Ri-1). Source : https://en.wikipedia.org/wiki/File:Data_Encription_Standard_Flow_Diagram.svg Final Steps : After the 16 rounds of encryption, the left and right halves are concatenated. It then goes through a final permutation (FP), which is the inverse of the initial permutation. The resulting 64-bit block is the ciphertext.  In summary, DES follows the Feistel cipher that operates on blocks of data by dividing them into two halves and applying a series of transformations. In each round, the right half of the data undergoes expansion, combined with key (which is also permutated), substitution, and permutation operations, while the left half remains unchanged. The output of these operations is then XOR-ed with the left half, and the resulting values become the new right and left halves for the next round. This process repeats for the specified number of rounds.  The alternating nature of processing the left and right halves in each round provide cryptographic security and diffusion throughout the encryption process.   Source : https://medium.com/@murshedsk135/cryptographic-evolution-from-feistel-cipher-to-triple-des-and-beyond-3aa0f8e08541 ","version":"Next","tagName":"h3"},{"title":"Diffie-Hellman","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/diffie-hellman","content":"","keywords":"","version":"Next"},{"title":"Explanation​","type":1,"pageTitle":"Diffie-Hellman","url":"/cs-notes/computer-security/diffie-hellman#explanation","content":" The Diffie-Hellman key exchange is based on mathematical concepts including the modular exponentiation and the difficulty of the discrete logarithm problem. The protocol relies on mathematical operations performed in a finite field.  Public Key Agreement : Each party, often denoted as Alice and Bob, agree on two number : ppp, which is a prime number and ggg, which is the base value.Key Generation : Both parties choose their own private key (an integer), let's say Alice's private key is aaa and Bob's private key is bbb.Public Key Calculation : Alice's public key (denoted as AAA) is obtained by A=ga mod pA = g^{a} \\text{ mod } pA=ga mod p, and for Bob (denoted as BBB) is similar : B=gb mod pB = g^{b} \\text{ mod } pB=gb mod p. After the calculation, they will share the public key to each other.Compute Secret Key : The shared secret key, denoted as sss is computed as follows: For Alice, s=Ba mod ps = B^a \\text{ mod } ps=Ba mod p. Similarly, for Bob, s=Ab mod ps = A^b \\text{ mod } ps=Ab mod p.  That's it, they will have the same secret key. The secret key is not shared in public, they will derive the secret key on their own. Although they chose a different number (their own private key), they will still arrive at the same secret key.  The most important math properties of why this works is described as the following formula :  (ga mod p)b mod p=(gb mod p)a mod p(g^a \\text{ mod } p)^b \\text{ mod } p = (g^b \\text{ mod } p)^a \\text{ mod } p(ga mod p)b mod p=(gb mod p)a mod p or Ab mod p=Ba mod pA^b \\text{ mod } p = B^a \\text{ mod } pAb mod p=Ba mod p  This equation demonstrates that if Party A raises the result of ga mod pg^a \\text{ mod } pga mod p to the power of bbb and takes the modulus ppp, it will yield the same result as Party B raising the result of gb mod pg^b \\text{ mod } pgb mod p to the power of aaa and taking the modulus ppp. This property allows both parties to compute the same shared secret key without directly exchanging their private keys.   Source : https://www.pcmag.com/encyclopedia/term/diffie-hellman  Here is an analogy of Diffie-Hellman key exchange in terms color combination.   Source : https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange#/media/File:Diffie-Hellman_Key_Exchange.svg  Security​  If an attacker were to brute force the secret key, the attacker would need to compute the secret key formula, which is (ga mod p)b mod p(g^a \\text{ mod } p)^b \\text{ mod } p(ga mod p)b mod p or (gb mod p)a mod p(g^b \\text{ mod } p)^a \\text{ mod } p(gb mod p)a mod p. Note that only aaa and bbb are kept secret, all the other values such as ppp, ggg, ga mod pg^a \\text{ mod } pga mod p, and gb mod pg^b \\text{ mod } pgb mod p are sent in clear.  The hacker would need to solve for gab mod p=gba mod pg^{ab} \\text{ mod } p = g^{ba} \\text{ mod } pgab mod p=gba mod p, which is equal to the secret key formula. As aaa, bbb, ppp grow larger, the complexity grows exponentially, making it computationally infeasible for large values. Therefore, it is crucial to select larger values to ensure the security of the Diffie-Hellman key exchange. ","version":"Next","tagName":"h3"},{"title":"DSA","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/dsa","content":"","keywords":"","version":"Next"},{"title":"Explanation​","type":1,"pageTitle":"DSA","url":"/cs-notes/computer-security/dsa#explanation","content":" The discrete logarithm problem is finding the exponent number in the equation gx≡y (mod p)g^x \\equiv y \\space (\\text{mod } p)gx≡y (mod p), where ggg is base value, yyy is the result of gx mod pg^x \\text{ mod } pgx mod p, and ppp is a prime number. The xxx is the private key which is kept secret, the remaining, ggg, yyy, and ppp are the public keys.  Here is a more detailed explanation of DSA :  Key Generation : Choose parameters : Choose a hash function HHH with output length of ∣H∣|H|∣H∣ bits (e.g., SHA-1 or SHA-2).Choose key length LLL, it should be multiple of 64 between 512 bits and 1024 bits inclusive (or even larger).Choose modulus length NNN, should be lower than key length and hash function's output. For example, LLL and NNN can be (1024, 160), (2048, 224).Choose NNN-bit prime qqq and LLL-bit prime ppp, such that p−1p - 1p−1 is a multiple of qqqChoose a random integer hhh from {2..p−2}\\{2..p - 2\\}{2..p−2}.Compute generator ggg: g:=h(p−1)/q mod pg := h^{(p - 1) / q} \\text{ mod } pg:=h(p−1)/q mod p.The ppp, qqq, and ggg can be shared publicly. Compute keys : Choose random integer xxx as private key from {1..q−1}\\{1..q - 1\\}{1..q−1}Compute public key yyy: y:=gx mod py := g^x \\text{ mod } py:=gx mod p Key Distribution : The (signer) of the digital data should public the public key yyy, whereas the xxx is kept secret. Signing : Choose a random integer kkk from {1..q−1}\\{1..q - 1\\}{1..q−1}.Compute r:=(gk mod p) mod qr := (g^k \\text{ mod } p) \\text{ mod } qr:=(gk mod p) mod q.Compute s:=(k−1(H(m))+xr) mod qs := (k^{-1} (H(m)) + xr) \\text { mod } qs:=(k−1(H(m))+xr) mod q, where H(m)H(m)H(m) is the output of hash function when inputting message mmm.The resulting signature is the pair (r,s)(r, s)(r,s), which will be attached to the document. Signature Verification : Given a pair of signature (r,s)(r, s)(r,s), to determine if it's valid for message mmm : Verify that 0&lt;r&lt;q0 &lt; r &lt; q0&lt;r&lt;q and 0&lt;s&lt;q0 &lt; s &lt; q0&lt;s&lt;q.Compute w:=s−1 mod qw := s^{−1} \\text{ mod } qw:=s−1 mod q.Compute u1:=H(m)⋅w mod qu_1 := H (m) \\cdot w \\text{ mod } qu1​:=H(m)⋅w mod q.Compute u2:=r⋅w mod qu_2 := r \\cdot w \\text{ mod } qu2​:=r⋅w mod q.Compute v:=(gu1yu2 mod p) mod qv := ( g^{u_1}y^{u_2} \\text{ mod } p ) \\text{ mod } qv:=(gu1​yu2​ mod p) mod q. If vvv is equal to rrr, then the signature is valid.  In conclusion, the public key yyy is used to verify the signature, while the private key xxx (which is hard to find) is used to sign the digital data.  When an attacker tries to break DSA, their objective is to impersonate the original owner. They will try to find the private key so they can modify the content of digitally signed messages without invalidating the signature. This can lead to the manipulation or corruption of data, potentially causing harm or deception.  If we say the document is aaa, the hashed document is bbb, and the hashed document encrypted with private key is ccc, then, the attacker must find the original private key owned by the actual signer, so that the decryption of digital signature or converting back from ccc to bbb will match the hashing of document from aaa to bbb. The verification process doesn't check if document is modified or not, the point is, the resulting hash value from the document must match with the decrypted digital signatures attached in the document.   Source : https://medium.com/@21_000_000/digital-signature-algorithm-60c8318cf9b6 ","version":"Next","tagName":"h3"},{"title":"Encryption","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/encryption","content":"","keywords":"","version":"Next"},{"title":"Cipher​","type":1,"pageTitle":"Encryption","url":"/cs-notes/computer-security/encryption#cipher","content":" Cipher is a method or algorithm used to perform encryption and decryption. It is a set of rules or steps that determine the transformation of plaintext into ciphertext or vice versa.  Caesar Cipher​  The Caesar cipher is one of the simplest and oldest known encryption techniques. The method is very simple, it simply transforms the input plaintext by shifting each letter in the plaintext a certain number of positions down or up the alphabet.  A positive shift value corresponds to shifting the letters down the alphabet (to the right), while a negative shift value corresponds to shifting the letters up the alphabet (to the left).  For example, &quot;A&quot; is letter 1 on alphabet, shifting it down by 3 would change it to alphabet &quot;D&quot;, 'B' as 'E', 'C' as 'F', and so on. When the end of alphabet is reached, it repeats from the beginning. Here is all the transformation :  Plaintext: ABCDEFGHIJKLMNOPQRSTUVWXYZ Ciphertext: DEFGHIJKLMNOPQRSTUVWXYZABC  Here is an example of transforming a real message into the cipher text :  Plaintext: THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG Ciphertext: WKH TXLFN EURZQ IRA MXPSV RYHU WKH ODCB GRJ  To decrypt it, we will do the reverse process, which is shifting each of the letter to the left by 3.   Source : https://forum.arduino.cc/t/caesar-cipher-with-arduino-pt-1/189683  Decipher vs Decrypt vs Decode​  They are commonly used interchangeable, but they have different context.  Decipher : Converting back ciphertext to plaintext.Decrypt : Converting encrypted data to original form, it is used in the context of various encryption schemes such as symmetric and asymmetric encryption.Decode : Converting back encoded data into readable format, it doesn't have to be encrypted data. For example, it can include converting ASCII encoded text (in binary) to plaintext.  ","version":"Next","tagName":"h3"},{"title":"Cryptographic Encryption​","type":1,"pageTitle":"Encryption","url":"/cs-notes/computer-security/encryption#cryptographic-encryption","content":" This types of algorithms are more advanced as it involves cryptographic algorithms and keys. Key is a very crucial component, it is a piece of information or a parameter that is inputted into the encryption algorithm to control the transformation of plaintext into ciphertext and vice versa. In Caesar cipher, the shifting value can be thought as the key.  The key is literally the &quot;key&quot; of decrypting an encrypted data, other than knowing how the algorithm encrypt the data. The encryption algorithm alone is not sufficient for decryption because there are typically multiple possible keys that could have been used with a given algorithm.  In the case of Caesar cipher, there are total of 25 possible key, shifting by 1 up to 25. However, as we are doing cryptographic encryption, brute forcing every single key may not be possible.  Public &amp; Private Key​  In cryptography, the terms public and private key is often encountered.  Public key : The public key is the key that is intended to be freely distributed and shared with others. The public key is typically used for encrypting the data.Private key : The private key is kept secret and only known to the owner. It is used for decrypting encrypted data.  As we know, cryptography involve a lot of math. Mathematically, public and private key are related, the public key is derived from the private key. The fundamental concept of cryptography is that anyone can encrypt the data, but only the holder of the private key, which is the target recipient that can decrypt it.  Symmetric &amp; Asymmetric Encryption​  These are two types of encryption :  Symmetric encryption : Also known as private key encryption or private key cryptography, it is a form of cryptography where the same key is used for both encryption and decryption. The key must be kept secret for secure communication and is shared between the sender and recipient. However, this increases the risk of the key being stolen by attacker during transmission. Symmetric encryption algorithms are generally faster and more efficient than asymmetric encryption algorithms.Asymmetric encryption : Also known as public key encryption or public key cryptography, it is a form of cryptography where both public and private key is used. It enables secure communication between parties without requiring a shared secret key, in exchange for more intensive computation.   Source : https://www.clickssl.net/blog/symmetric-encryption-vs-asymmetric-encryption  Stream vs Block Cipher​  Stream and block cipher are the two types of how plaintext is converted into ciphertext. They are considered as two different categories or classes in the symmetric encryption (uses single key to encrypt and decrypt).  Stream cipher : Operates on single bit or byte.Block cipher : Convert the plaintext in fixed-size block (e.g., 64 bits, 128 bits, etc). The plaintext is divided into these fixed-size blocks, and each block is encrypted or decrypted independently.   Source : https://medium.com/networks-security/private-key-public-key-encryption-91f1a90c905f   Source : https://medium.com/networks-security/private-key-public-key-encryption-91f1a90c905f  Key Exchange​  Key exchange is the process of exchanging secret key between parties securely. It involves cryptographic protocols and techniques to exchange keys without revealing them to potential eavesdroppers.  Symmetric encryption : It can be used in symmetric encryption to securely share the shared private key.Asymmetric encryption : It can be used to share the public key of both parties. While the public key is intended to be public, securely exchanging it would ensure both parties receive the legitimate key.   Source : https://open.oregonstate.education/defenddissent/chapter/exchanging-keys-for-encryption/  The key exchange protocol involve various mathematical properties, to analogy, it can be thought as combining color. Colors can be combined to create new colors by adding the intensity of each color component. Even if we add different color, because of the additive properties of color, they will end up with the same color.  Similar to the actual key exchange protocol, both parties pick some number and use mathematical properties to derive the key at the end. ","version":"Next","tagName":"h3"},{"title":"Hash Function","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/hash-function","content":"","keywords":"","version":"Next"},{"title":"Properties & Example​","type":1,"pageTitle":"Hash Function","url":"/cs-notes/computer-security/hash-function#properties--example","content":" Hash function have some properties :  Deterministic : A hash function must be deterministic, in other word, the same input should always produce the same output. Fixed Output, Variable Input : While there are some hash function that produces variable-length output, a hash function typically produces fixed output. The thing is, the hash function should be able to take variable input length. Irreversibility : Hash functions are designed to be one-way functions, meaning it is computationally infeasible to get the original input from the output. Uniformity : Hash function should uniformly distribute hash values across the output space. This reduces the chance of collisions, where different inputs produce the same hash value. Avalanche Effect : A small change in the input should result in a significantly different hash value. Efficiency : A good hash function should be fast and efficient, capable of processing large amounts of data quickly. Source : https://signmycode.com/resources/best-hashing-algorithms  A hash function can range from a simple algorithm like division hash function (non-cryptographic hash functions) to a complex algorithm like SHA (cryptographic hash functions) that can guarantee a high level of security, in terms of irreversibility and collision resistance.  XOR Hash Function​  The XOR hash function uses the bitwise XOR on the individual bits of the input data with some specific key. The idea is, bitwise operation operates on the bit level of the data, the same data which mean they have the same binary representation will produce the same result. Besides that, distinct data will at least have some difference in their binary representation.   Source : https://intezer.com/blog/research/unraveling-malware-encryption-secrets/  ","version":"Next","tagName":"h3"},{"title":"Salting​","type":1,"pageTitle":"Hash Function","url":"/cs-notes/computer-security/hash-function#salting","content":" Salting is a hash preprocessing technique that involves adding a random or unique value to the input data before it is hashed. The purpose of salting is to provide an additional layer of security.  According to the hash function properties, hash function should have the avalanche effect, a small change in the input should result in a significantly different hash value. However, in some hash function, there is still a chance for collision to occur. By adding a random value to the input data, we can reduce the chance of collision.  Another benefit of salting is to mitigate the rainbow table attack. Rainbow tables are precomputed lookup tables that map hash values to their original input. Attackers generate these tables in advance to quickly find the original password corresponding to a hash value. However, rainbow tables are only effective for unsalted hashes.   Source : https://cyberhoot.com/cybrary/password-salting/  ","version":"Next","tagName":"h3"},{"title":"Checksums​","type":1,"pageTitle":"Hash Function","url":"/cs-notes/computer-security/hash-function#checksums","content":" Checksums are a form of error detection used to verify the integrity or correctness of data. A checksum is a value that is computed from input data using a specific algorithm. This value is then compared with a previously computed or expected checksum to determine if the data has been modified or corrupted.   Source : https://en.wikipedia.org/wiki/Checksum#/media/File:Checksum.svg ","version":"Next","tagName":"h3"},{"title":"Lattice-Based Cryptography","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/lattice-based-cryptography","content":"","keywords":"","version":"Next"},{"title":"Lattice​","type":1,"pageTitle":"Lattice-Based Cryptography","url":"/cs-notes/computer-security/lattice-based-cryptography#lattice","content":" In math, lattice is an infinitely many points that extends infinitely in all directions. Each point in the lattice, called lattice point, is associated with a set of coordinates that specify its position in space.   Source : https://en.wikipedia.org/wiki/Lattice_(group)  A lattice can be represented as vector, vector can be thought as an arrow that points to some direction and has a magnitude or the length of the vector.  A vector can be made by a bunch of other vectors, the smallest vector you can use to build another vector is called the basis vector. To construct a vector, you would add together the basis vector several times (or multiply).    The basis vector serves as the building block for making vector, which will determine the lattice's structure. Operation in lattice is closed, which means if you add or subtract a point with another lattice point, it will always result in another point which exists in the lattice itself.  Basis vector can even be more complex, for example, the basis vector iii may point to the x-axis as well as the y-axis. In other word, it is not necessary to only point to one direction, it can point diagonally.   Source : https://medium.com/cryptoblog/what-is-lattice-based-cryptography-why-should-you-care-dbf9957ab717  ","version":"Next","tagName":"h3"},{"title":"Cryptography Applications​","type":1,"pageTitle":"Lattice-Based Cryptography","url":"/cs-notes/computer-security/lattice-based-cryptography#cryptography-applications","content":" Unlike many other cryptographic schemes that rely on the difficulty of factoring large numbers or solving the discrete logarithm problem, lattice-based cryptography is believed to be resistant to attacks by quantum computers.  tip Quantum computer is a computer that takes advantage of quantum mechanism phenomena, it is able to do computation faster than classical computer, which mean it can break cryptography algorithm faster by brute forcing faster.  The security of lattice-based cryptography is based on the hardness of certain problems related to lattices, such as the Closest Vector Problem (CVP).  CVP​  The CVP problem states that we are given some lattice and two vectors, we are told to construct a vector that starts from some point to the target lattice point. The problem is hard because the vector we are looking for is the shortest vector.  The problem become harder as we extend the lattice's dimension. It may not only have x and y coordinate, it may have 10000 coordinates. Going to some direction in a dimension may not be the best choice, it may result in further distance in another dimension.  The given vector may be peculiar, or the given lattice structure is more complex (e.g., we can't form a straight path, we may need to go diagonally). The more complex the constraint, the more combination would exist, which mean will be harder to solve.    There is no known algorithm that can solve CVP in efficient time for arbitrary lattices. There are some algorithms that approximate the closest vector, but still, it may not be the closest.  Usage​  Both sender and recipient will have a pair of public and private key. The private key will be a &quot;good&quot; basis vector that defines the lattice structure. The public key is the lattice structure, which will be used by the recipient.  To securely share the lattice structure, the private key (the &quot;good&quot; basis vector) is not used. Instead, a set of &quot;bad&quot; vectors is used to define the lattice structure.  When sending a message, it is encoded as a lattice point. The recipient, equipped with the private key (the &quot;good&quot; vector), can easily locate and retrieve that specific lattice point. However, an attacker, knowing only the lattice structure but lacking the private key, would face difficulties in reaching the desired lattice point. ","version":"Next","tagName":"h3"},{"title":"Elliptic Curve Cryptography","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/elliptic-curve-cryptography","content":"","keywords":"","version":"Next"},{"title":"Elliptic Curve​","type":1,"pageTitle":"Elliptic Curve Cryptography","url":"/cs-notes/computer-security/elliptic-curve-cryptography#elliptic-curve","content":" An elliptic curve is a mathematical curve defined by y2=x3+ax+by^2 = x^3 + ax + by2=x3+ax+b.   Source : https://www.globalsign.com/en/blog/elliptic-curve-cryptography  aaa and bbb are the constants that defines the curve, and xxx and yyy are the coordinates of points on the curve. The equation represents the set of points (x,y)(x, y)(x,y) that satisfy the equation.  The unique properties of elliptic curve is that they are symmetric around the x-axis. When you draw a line from two point, the line will always intersect another point somewhere.   Source : https://www.globalsign.com/en/blog/elliptic-curve-cryptography (with modification)  The operation that involves taking two point and obtaining the intersection point is called the &quot;dot&quot; operation. We can keep repeating this operation, the intersection point will be reflected over the x-axis, and we will draw another line from the first point to the new reflected point.   Source : https://blog.cloudflare.com/a-relatively-easy-to-understand-primer-on-elliptic-curve-cryptography/  After doing a lot of dot operation, it is said to be hard to arrive again at the initial point.  ECDLP​  The Elliptic Curve Discrete Logarithm Problem (ECDLP) is the problem of finding the scalar value kkk in the equation kP=QkP = QkP=Q. In this equation, PPP is a known point on the elliptic curve, QQQ is another point on the curve, and kkk is the scalar value that needs to be determined.  To relate this with the graphical illustration, we need to find how many times we need to do the dot operation in order to arrive at the same initial point. There is also a limit of how many dot operation we can do, it is denoted as nnn. The nnn should be a prime number, prime number is said to have unique properties that makes the problem becomes harder to solve.  So, when we have n=1000n = 1000n=1000, this mean there are 1000 possible addition or 1000 possible dot operation. Out of all 1000 operation, we will need to find the one number kkk inside the range of nnn that makes the intersection point same as the initial point.    As of now, there are no efficient algorithms that can find the value of kkk, while the brute force algorithm is computationally infeasible for large nnn. For example, if nnn is a 256-bit prime, there are approximately 22562^{256}2256 possible values of kkk to check.  Relating the ECDLP problem to cryptography, the nnn is the key size of the encryption, the value of kkk is the private key that is kept secret and hard to be found, and the public key is all the multiple of initial point, it is the P, 2P, 3P, 4P or all the intersection in the graphical illustration.  ","version":"Next","tagName":"h3"},{"title":"Elliptic Curve Diffie-Hellman​","type":1,"pageTitle":"Elliptic Curve Cryptography","url":"/cs-notes/computer-security/elliptic-curve-cryptography#elliptic-curve-diffie-hellman","content":" The Diffie-Hellman key exchange is a traditional key exchange method. Elliptic Curve Diffie-Hellman (ECDH) is a key exchange algorithm based on the Diffie-Hellman (DH) protocol that utilizes elliptic curve cryptography (ECC).  Explanation​  Just like the traditional Diffie-Hellman, two parties are going to share the same secret key.  Key Generation : Each party, let's say Alice and Bob, generates their own elliptic curve key pair, which consist of a private key and a corresponding public key.The private key is denoted as aaa for Alice and bbb for Bob, and it's a randomly generated scalar value, which is an integer.The public key is a specific point on the curve, it is obtained by scalar multiplication of private key and a chosen base point GGG on the elliptic curve.For Alice, it will be A=aGA = aGA=aG, for Bob, it will be B=bGB = bGB=bG. Key Exchange : Alice sends her public key AAA to Bob, and Bob sends his public key BBB to Alice. Shared Secret Key Computation : Alice computes the shared secret key S=aB=a(bG)=(ab)GS = aB = a(bG) = (ab)GS=aB=a(bG)=(ab)G.Bob computes the shared secret key S=bA=b(aG)=(ab)GS = bA = b(aG) = (ab)GS=bA=b(aG)=(ab)G. Source : https://asecuritysite.com/ecdh  Security​  The security of ECDH relates to the ECDLP problem. Even if the attacker intercepts the base point GGG and the public key AAA or BBB, which is equal to aGaGaG or bGbGbG, it will be hard to find the value of aaa or bbb. Finding aaa or bbb is the same as finding the value of integer kkk. ","version":"Next","tagName":"h3"},{"title":"MD5","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/md5","content":"","keywords":"","version":"Next"},{"title":"Algorithm​","type":1,"pageTitle":"MD5","url":"/cs-notes/computer-security/md5#algorithm","content":" The high level description of MD5 algorithm :  Padding &amp; Chunking : The input message is padded to ensure its length is a multiple of 512 bits. The padding includes a 1-bit followed by zeros and a 64-bit representation of the original message length. The message will be broken up into chunk of 512-bit blocks (sixteen 32-bit words). Initialization : MD5 uses four 32-bit registers (A, B, C, and D) as its internal state. These registers are used to store intermediate values during the processing of the input message blocks, they are initialized with predefined values. Processing : For each block, a series of rounds are performed, which involve bitwise logical operations, modular addition, and non-linear functions. The rounds modify the internal state (registers) of the algorithm. Source : https://en.wikipedia.org/wiki/MD5#algorithm Finalization : After processing all blocks, the final state of the registers is concatenated to produce the 128-bit hash value. The order of the registers' values is typically little-endian, meaning the least significant byte comes first. ","version":"Next","tagName":"h3"},{"title":"Mobile Security","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/mobile-security","content":"","keywords":"","version":"Next"},{"title":"Permissions Exploit​","type":1,"pageTitle":"Mobile Security","url":"/cs-notes/computer-security/mobile-security#permissions-exploit","content":" In mobile operating system like Android, malicious apps can tricks user into granting excessive permissions to an app, allowing it to access sensitive data or perform unwanted actions. This can happen by tricking user through social engineering.  ","version":"Next","tagName":"h3"},{"title":"Clickjacking​","type":1,"pageTitle":"Mobile Security","url":"/cs-notes/computer-security/mobile-security#clickjacking","content":" Clickjacking is the technique where attacker hide elements on a mobile app's user interface to deceive users into clicking on unintended buttons or links. This can lead to unintended actions, such as granting permissions, making unwanted purchases, or revealing sensitive information.   Source : https://9to5google.com/2017/05/26/cloak-dagger-android-vulnerability/  In the image above, the malicious app cover up the system window that ask for permission.  ","version":"Next","tagName":"h3"},{"title":"Biometric Spoofing​","type":1,"pageTitle":"Mobile Security","url":"/cs-notes/computer-security/mobile-security#biometric-spoofing","content":" Biometrics is the measurement of unique physical or behavioral characteristics of individuals. Biometrics are commonly used for identification or authentication, as it is unique to each person, and it will always available. Example of biometrics measurement are fingerprint, facial recognition, iris, retina, voice.  Biometric spoofing is a technique to trick the biometric authentication system by various methods such as using high-resolution photos, artificial fingerprints, 3D models, or identify the weakness of system that recognize them. ","version":"Next","tagName":"h3"},{"title":"Reverse Engineering","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/reverse-engineering","content":"","keywords":"","version":"Next"},{"title":"Reverse Engineering Technique​","type":1,"pageTitle":"Reverse Engineering","url":"/cs-notes/computer-security/reverse-engineering#reverse-engineering-technique","content":" Trial &amp; Error​  Trial and error involves systematically experimenting with different inputs, configurations, or parameters of a software or system to understand its behavior. Reverse engineers observe the outcomes of these experiments, after many experiments, they will come up in conclusion of how it works.  For example, we may have a program that connect to server, suppose we want to know how the server works. We can try each setting and input to see how will the server respond, including the invalid request. Seeing the response, we will get the idea of how it works a little by little.  Decompilation​  Decompilation is a technique to translate compiled binary code back into a higher-level programming language. When we program in higher-level language, the source code will be converted down into machine language (e.g., assembly language), which will be executed by computers. These codes are hard to understand by human, by decompiling it back to higher-level language, we can understand it easier.  Decompiling can recreate (or at least approximate) the original source code from executable code to higher-level language. This will help reverse engineering process a lot, because we are able to see the readable code. However, decompilation tools are not perfect. The resulting decompiled code may not be an exact replica of the original source code. Decompilers can face challenge when reconstructing the code due to compilers optimizations that results in loss of information during the compilation process, and other factors.  Disassembly​  Disassembly is a technique to convert machine code or binary instructions back into assembly language. It doesn't convert back to higher-level language, but it converts it into readable machine code.  Disassembly provides more detail about the low-level instruction, it doesn't need to approximate high-level abstraction. It can reveal specific instructions executed by the processor, including memory accesses, register operations, and control flow.   Source : https://blog.ret2.io/2017/11/16/dangers-of-the-decompiler/ ","version":"Next","tagName":"h3"},{"title":"Network Security","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/network-security","content":"","keywords":"","version":"Next"},{"title":"Eavesdropping​","type":1,"pageTitle":"Network Security","url":"/cs-notes/computer-security/network-security#eavesdropping","content":" Eavesdropping is an unauthorized activity that intercept or monitor network communication between devices. It involves capturing and inspecting network traffic without the knowledge or consent of the parties involved in the communication.  Eavesdropping can be very dangerous in an insecure network, hacker could take your personal information that you intend to send to the server.  There are many ways to eavesdrop, it is possible that local communication devices (e.g., Wi-Fi or network cables) is modified so that it can send incoming and outgoing network packet to the hacker's device.   Source : https://securitypilgrim.com/what-is-an-eavesdropping-attack-5-tips-to-prevent-it/  Packet Sniffing​  Packet sniffing is the practice of collecting and gathering network traffic by capturing the incoming or outgoing packets in the network. Malicious actor could analyze the packets and extract the contents of data packets flowing across a network.   Source : https://upload.wikimedia.org/wikipedia/commons/c/cf/Wireshark_3.6_screenshot.png  Man in The Middle Attack​  Man in The Middle Attack (MitM attack) is the practice of an attacker that positions themselves between two parties engaged in communication. The attacker might intercepts and modify the communication between the two parties without their knowledge.  Let's say there are two parties, A and B. A wants to connect to B, the attacker will inserts themselves in the communication between them. This can be achieved by various means, such as hacking into a router, or executing attacks on the underlying protocols. When the attacker successfully places themselves, the attacker will try to impersonate party B, to make party A thinks they are communicating with party B. By successfully impersonating Party B, the attacker can intercept, read, modify, steal data, or inject malicious content into the communication.   Source : https://www.imperva.com/learn/application-security/man-in-the-middle-attack-mitm/  ","version":"Next","tagName":"h3"},{"title":"DDoS Attack​","type":1,"pageTitle":"Network Security","url":"/cs-notes/computer-security/network-security#ddos-attack","content":" Distributed Denial of Service (DDoS) attack is a type of cyberattack in which many computers or devices, often infected with malware, are used to flood a website or network with traffic. This makes the server overwhelmed and cause it to become unavailable to legitimate users. A network of computers that does a DDoS attack is called botnet.  An attack where the requests typically originate from a single source is just Denial of Service (DoS). On the other hand, a DoS attack is an attack which run on multiple system connecting to server is called Distributed Dos (DDoS), this causes the server to floods and be overwhelmed.  Another type is Yo-Yo DDoS attack which typically targets server that has limited capacity for processing incoming traffic. In a back-and-forth manner, the attacker first attack the server and wait for it to stop the operation. After the server continue the normal operation, the hacker will attack again causing the server to constantly switch between normal operation and overload.  There are several ways to anticipate a DDoS attack :  Increase Network Bandwidth : If possible, increasing the available bandwidth will helps absorb the impact of a DDoS attack by allowing the network to handle a larger volume of traffic. Load Balancing &amp; Content Delivery Network (CDN) : If the DDoS attackers are around the world, using CDN might help. Or we can also distribute the incoming network request using load balancing technique. This technique distribute network request from client evenly across all the server, this will ensure there will be no server that becomes overwhelmed or underwhelmed. Traffic Filtering : Filtering traffic means we are blocking specific client from making request to the server. There are several technique including blacklisting IP addresses, detect and block suspicious traffic patterns, and by using access control lists (ACLs).   Source : https://www.onelogin.com/learn/ddos-attack  ","version":"Next","tagName":"h3"},{"title":"Spoofing​","type":1,"pageTitle":"Network Security","url":"/cs-notes/computer-security/network-security#spoofing","content":" Spoofing is the act of impersonating information or identities to manipulate others to gain advantage. It can be presenting false data that looks legitimate, with the intention of gaining unauthorized access or tricking individuals.  IP Spoofing​  IP spoofing is when an attacker modify the source IP address in network packets to make it appear as if they are originating from a different IP address. The attacker might impersonate themselves as an innocent user by modifying the IP in network packets so that it looks like it was sent by legitimate user.  MAC Spoofing​  MAC spoofing, on the other hand, impersonate MAC address instead. MAC address is a unique identifier assigned to network interface cards. An attacker can deceive network switches or routers into associating their device with the MAC address of a trusted device.  DNS Spoofing​  When we enter an address of a website, it will be sent to DNS server, the server will give us the IP address of the website for us to load in our browser. DNS spoofing is the technique where an attacker manipulates the DNS server, so it returns malicious websites instead of a legitimate websites.   Source : https://socradar.io/what-is-spoofing-attack-and-how-to-prevent-it/  ","version":"Next","tagName":"h3"},{"title":"Firewall​","type":1,"pageTitle":"Network Security","url":"/cs-notes/computer-security/network-security#firewall","content":" Firewall is a network security device that acts as a barrier between a trusted internal network and an untrusted external network, such as the Internet. Its primary purpose is to control incoming and outgoing network traffic.   Source : https://en.wikipedia.org/wiki/Firewall_(computing)  Firewall examines the packets of data passing through them and make decisions about whether to allow or block the traffic based on the defined criteria. Common criteria include source and destination IP addresses, ports, protocols, and packet contents. By analyzing this information, firewalls can determine whether a packet should be allowed to pass through or if it poses a potential security risk and should be blocked.   Source : https://youtu.be/kDEX1HXybrU?si=iRlxiRyRBPYrG90I&amp;t=137  A firewall can be implemented in both hardware and software forms. A hardware firewall protects the entire network, typically in a standalone form or built-in to the router. A software firewall, also known as host-based firewall, is implemented on individual computers or servers. For example, Windows operating system has a built-in firewall called Microsoft Defender Firewall.   Source : https://shop3213.sosoutremer.org/category?name=hardware%20vs%20software%20firewall  ","version":"Next","tagName":"h3"},{"title":"Network Encryption​","type":1,"pageTitle":"Network Security","url":"/cs-notes/computer-security/network-security#network-encryption","content":" See network encryption ","version":"Next","tagName":"h3"},{"title":"Other Attack & Exploit","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/other-attack-and-exploit","content":"","keywords":"","version":"Next"},{"title":"Brute Forcing​","type":1,"pageTitle":"Other Attack & Exploit","url":"/cs-notes/computer-security/other-attack-and-exploit#brute-forcing","content":" Systematically trying out possible combination without strategy to find a correct solution. See brute force for example.  ","version":"Next","tagName":"h3"},{"title":"Rainbow Table Attack​","type":1,"pageTitle":"Other Attack & Exploit","url":"/cs-notes/computer-security/other-attack-and-exploit#rainbow-table-attack","content":" A rainbow table attack is a precomputed table-based method used to crack hashed passwords or other cryptographic keys. Before actually attacking a system, the attacker compute the hash of plaintext passwords and store the plaintext along with the hash codes in some table called rainbow table.  In the actual attack, when the attacker has the hashed value of a password but don't know what the plaintext password is, the attacker can perform a reverse lookup in the rainbow table. They compare the hashed value with the values stored in the table to find a match. If a match is found, the corresponding plaintext value is retrieved, giving the attacker the original password or key.  ","version":"Next","tagName":"h3"},{"title":"Memory Leak​","type":1,"pageTitle":"Other Attack & Exploit","url":"/cs-notes/computer-security/other-attack-and-exploit#memory-leak","content":" Memory leak is a type of software bugs that occurs when the program fails to release memory that it no longer needs or fails to return memory to the operating system after it has finished using it.  When a program uses memory, it will be allocated by the OS. Memory leaks occur when you don't deallocate the memory after using it, leading to a gradual accumulation of unused memory over time. The program's memory usage increases, potentially leading to performance degradation and eventually causing the program to crash or become unresponsive.  ","version":"Next","tagName":"h3"},{"title":"Buffer Overflow​","type":1,"pageTitle":"Other Attack & Exploit","url":"/cs-notes/computer-security/other-attack-and-exploit#buffer-overflow","content":" Buffer overflow is software vulnerability when a program writes data beyond the boundaries of a buffer, or a fixed-size memory storage area. For example, a program require you to write a fixed 5-length character. Under the hood, the program only allocates memory with the size of 5 character. When you enter a longer character, the program doesn't have any more memory to store, causing it to write into adjacent memory locations. Reserved memory might be overwritten, making the program experience undefined behaviors (UB).   Source : https://www.imperva.com/learn/application-security/buffer-overflow/  ","version":"Next","tagName":"h3"},{"title":"Integer Overflow​","type":1,"pageTitle":"Other Attack & Exploit","url":"/cs-notes/computer-security/other-attack-and-exploit#integer-overflow","content":" Integer overflow is another software bug that occurs when you do an arithmetic operation that results in a value that exceeds the maximum representable value for a given integer type.  For example, the maximum number you can represent using 8-bit binary representation is 11111111, which represent the number 255.  When you add 255 with the number 1, you will obviously end up with 256, however, computer can't represent this, because it has limited bits. When adding 11111111 with 00000001, you will end up back to 00000000, because the most significant bit is discarded, and the result wraps around to 00000000. This is known as &quot;wrapping&quot; or &quot;overflow&quot; behavior, where the value resets back to the minimum representable value.   Source : https://www.sdsolutionsllc.com/forcedentry-and-integer-overflows/  ","version":"Next","tagName":"h3"},{"title":"Birthday Attack​","type":1,"pageTitle":"Other Attack & Exploit","url":"/cs-notes/computer-security/other-attack-and-exploit#birthday-attack","content":" Birthday attack is a cryptographic attack that exploits the mathematical probability of collisions in hash functions. It is named after the &quot;birthday paradox,&quot; which states that in a group of a relatively small number of people, the probability of two individuals having the same birthday is higher than one might intuitively expect.  Even a strong hash function has the potential to generate identical outputs for distinct inputs. Birthday attack takes advantage of this collision probability to find collisions more efficiently than by brute force. Instead of trying to find a collision between two specific inputs, which would require a large number of computations, a birthday attack aims to find any two inputs that produce the same hash value.  The attack works by hashing a large number of randomly generated inputs and storing their resulting hash values. As more inputs are hashed, the probability of finding a collision increases.   Source : https://www.slideshare.net/RkskEkanayaka/birthday-paradox-explained  ","version":"Next","tagName":"h3"},{"title":"Social Engineering​","type":1,"pageTitle":"Other Attack & Exploit","url":"/cs-notes/computer-security/other-attack-and-exploit#social-engineering","content":" Social engineering is the technique of manipulating individuals or groups to deceive them into performing unintended actions. It is a technique used by malicious actors to exploit human psychology and trust in order to gain unauthorized access to systems, sensitive data, or to carry out fraudulent activities.  As an example, consider a situation where a friend asks to exchange social media accounts. They might provide you with an incorrect password for their account, leading to a scenario where the account you share with them could be exploited.  Phishing​  Phishing is the technique where an attacker tries to impersonate well-known organization or individuals with the goal to trick individuals into clicking on malicious links, downloading malicious attachments, or providing their confidential information directly to the attacker.  The attacker may replicate an email from bank and ask you for your banking information. On a fake website, you may be prompted to enter sensitive credentials. In a mobile app context, an attacker may create an app that closely resembles a legitimate one, aiming to trick users into believing it is the legitimate application.  Typosquatting​  Typosquatting is the technique that take advantage of human's error when typing for something. It includes variations such as misspelled words, additional characters, or different top-level domains (TLDs).  For example, attacker can reserve the domain googlr.com incase people mispelled the domain google.com. In another scenario, the attacker could add specific characters, such as a hyphen -, to a website address, creating a domain like paypal-support.com. This alteration might mislead users into believing that it is the authentic website associated with PayPal's customer support.  The domain name is the important thing to check while you are surfing the internet. Domain names comes after the protocol (e.g., https://) and after the subdomain (e.g., www.), also before the top-level domain (e.g., .com).  tip More about URL syntax  ","version":"Next","tagName":"h3"},{"title":"Cryptojacking​","type":1,"pageTitle":"Other Attack & Exploit","url":"/cs-notes/computer-security/other-attack-and-exploit#cryptojacking","content":" Cryptojacking is the practice of using someone's computer or computing resources to mine cryptocurrencies. Cryptojacking can happen silently in the background, idenfity it by checking if the application you are using consume unreasonable amount of resources.   Source : https://www.imperva.com/learn/application-security/cryptojacking/  ","version":"Next","tagName":"h3"},{"title":"Tampering​","type":1,"pageTitle":"Other Attack & Exploit","url":"/cs-notes/computer-security/other-attack-and-exploit#tampering","content":" Tampering is the act of intentionally modifying something in an unauthorized or improper manner, which might happen when an attacker have access to a system by exploiting system's vulnerabilities. This can includes modifying data, documents, or physical objects with the intent to deceive, manipulate, or gain an unfair advantage.  ","version":"Next","tagName":"h3"},{"title":"Backdoor​","type":1,"pageTitle":"Other Attack & Exploit","url":"/cs-notes/computer-security/other-attack-and-exploit#backdoor","content":" Backdoor is a hidden or secret method of bypassing regular authentication or security measures in a computer system, software, or network. Software bugs or vulnerabilities can be a contributing factor to the existence of backdoors.  Because they are hidden, they are more difficult to detect than typical method. Backdoors can be intentionally created by developers, system administrators, or individuals with access to a system. These individuals may design and implement hidden access points or bypass mechanisms to gain unauthorized access to a system or network. ","version":"Next","tagName":"h3"},{"title":"SHA","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/sha","content":"","keywords":"","version":"Next"},{"title":"SHA-1​","type":1,"pageTitle":"SHA","url":"/cs-notes/computer-security/sha#sha-1","content":" SHA-0 is the first SHA algorithm, many collisions were found, and it is considered not secure, therefore SHA-1 is developed.  Here is the high-level overview of the SHA-1 algorithm :  Preprocessing : The input message is padded to a multiple of the block size. The padding includes adding a 1-bit followed by a series of 0-bits until the desired length is reached (it should be congruent to −64≡448 (mod 512)−64 ≡ 448 \\space (\\text{mod } 512)−64≡448 (mod 512)). After that, append the original length of the message (in bits) as a 64-bit integer in big-endian format. This means representing the length using 8 bytes (64 bits) and placing the most significant byte first. Initialization : Initialize five 32-bit variables (A, B, C, D, E) with predetermined constants (initial hash values). These variables will be updated during the hashing process. Chunk Processing : Message are broken up into 512-bit chunks, each chunk goes through a series of rounds that involve logical functions, bitwise operations, and modular arithmetic. Each of the operations is divided within different rounds, rounds 0-19, rounds 20-39, rounds 40-59, rounds 60-79. For example, rounds 0-19 involve the bitwise operation and shifting, rounds 40-59 involve calculating the parity (odd or even) of the inputs. Update Variables : The internal state, which was initialized with some initial hash values, is updated after each block processing round. Final Hash : After processing all the blocks, the final hash value is derived from the updated internal state. It is produced by concatenating the final values of A, B, C, D, and E in big-endian to get the 160-bit hash value. Source: https://en.wikipedia.org/wiki/SHA-1  ","version":"Next","tagName":"h3"},{"title":"SHA-2​","type":1,"pageTitle":"SHA","url":"/cs-notes/computer-security/sha#sha-2","content":" SHA-2 changes significantly from the previous version, SHA-1. It has 6 variant : SHA-224, SHA-256, SHA-384, SHA-512, SHA-512/224, SHA-512/256, they differ in their output size and the number of internal rounds they perform.  Here is the algorithm for SHA-256 :  Padding : Append padding bits to the message to ensure its length is a multiple of 512 bits. The padding includes a single '1' bit followed by a series of '0' bits, and the length of the original message is appended to the end. Specifically, the padding needs to be (L + 1 + K + 64), where L is the message length and K is the number of '0' bits needs to be added. Initialize Variables : Initialize eight 32-bit variables (A, B, C, D, E, F, G, H) with the initial hash values, they are the first 32 bits of the fractional parts of the square roots of the first 8 primes 2 to 19. Process Chunk: Message Schedule : Message is broken up into 512-bit chunks. An array called message schedule (typically denoted as W) will be created, it ranges from W[0] to W[63], and it contains a set of words derived from the input messages, which will be used for later steps. The original message chunk is copied to the first 16 words of the message schedule array. The remaining 48 words from 16 to 63 will be populated by extending the initial 16 words. The populating process includes bitwise process such as rotate, shifting, and XOR logical operation. Initialize : Using the current hash value (the initial hash values if it's the first time), initializes variables A, B, C, D, E, F, G, H. Compression Function : The compression function is the main step of the algorithm, it involves updating the current hash value with the message schedule array. It consists of a series of logical and arithmetic operations. Update Variables : Update the variables A, B, C, D, E, F, G, H with the results of the compression function. Final Hash : After processing all chunks, concatenate the final values of A, B, C, D, E, F, G, H to obtain the hash value (big-endian). Source : https://upload.wikimedia.org/wikipedia/commons/thumb/7/7d/SHA-2.svg/400px-SHA-2.svg.png  ","version":"Next","tagName":"h3"},{"title":"SHA-3​","type":1,"pageTitle":"SHA","url":"/cs-notes/computer-security/sha#sha-3","content":" SHA-3 is the latest member of SHA cryptographic hash function family, the variants are SHA3-224, SHA3-256, SHA3-384, SHA3-512. The design of SHA-3 differs with SHA-1 and SHA-2, it is based on the Keccak sponge construction, in contrast SHA-1 and 2 uses the Merkle-Damgård construction.  The Merkle-Damgård construction works by processing the input message in fixed-size chunks, each chunk is sequentially hashed and combined with the previous output (or the initial hash), the final step combines all the hash state together.  The Keccak sponge construction uses a &quot;sponge function&quot; that absorbs and squeezes. It begins by padding the input and breaking it down into fixed-size chunk. The sponge construction has some internal state, by XOR-ing the input and the internal state, the chunk will be absorbed to the portion of the state.  The absorbed data undergoes a permutation operation to introduce more complexity, this includes series of nonlinear transformations and bitwise operations. After the permutation, a portion of the state is extracted for the hash output, this is the squeezing phase.   Source : https://codesigningstore.com/hash-algorithm-comparison ","version":"Next","tagName":"h3"},{"title":"Web Security","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/web-security","content":"","keywords":"","version":"Next"},{"title":"Type of Attacks​","type":1,"pageTitle":"Web Security","url":"/cs-notes/computer-security/web-security#type-of-attacks","content":" There are many types of attacks which includes :  ","version":"Next","tagName":"h2"},{"title":"Cross-Site Scripting (XSS)​","type":1,"pageTitle":"Web Security","url":"/cs-notes/computer-security/web-security#cross-site-scripting-xss","content":" This is a type of attack where an attacker injects malicious code into a web page, which is then executed by the victim's browser. This can be used to steal sensitive information or take control of the user's session.  Attacker could inject the code from an input field where the website asks user for information. XSS attacks typically occur when a website does not properly validate or sanitize user input, the code will be executed by the browser, this allows the attacker to steal their login credentials or perform other malicious actions.  In the image below, user try to enter malicious JavaScript code through a forum.   Source : https://www.acunetix.com/blog/articles/persistent-xss/  ","version":"Next","tagName":"h3"},{"title":"SQL Injection​","type":1,"pageTitle":"Web Security","url":"/cs-notes/computer-security/web-security#sql-injection","content":" This is a type of attack where an attacker exploits vulnerabilities in a web application's database to execute malicious SQL statements. This can be used to steal or modify sensitive information.  SQL injection attacks work by taking advantage of the fact that many web applications use user input to construct SQL queries that are then executed on the database.  For example, suppose a web application uses the following SQL query to authenticate users : SELECT * FROM users WHERE username='[username]' AND password='[password]'  The web application asks for username and password which will be directly plugged in into the SQL field. This can be dangerous if the web application does not properly validate or sanitize the input for the username and password fields, an attacker can insert their own SQL statements into these fields.  For example, the attacker could enter the following as the username : ' OR 1=1; -- This would result in the following SQL query being executed : SELECT * FROM users WHERE username='' OR 1=1; --' AND password='[password]'  The -- at the end of the query is a SQL comment, which causes the rest of the query (including the password check) to be ignored. This means that the attacker can bypass the password check and log in with any password.  And many more SQL Injection such as deleting database or table.  ","version":"Next","tagName":"h3"},{"title":"Cross-Site Request Forgery​","type":1,"pageTitle":"Web Security","url":"/cs-notes/computer-security/web-security#cross-site-request-forgery","content":" Cross-Site Request Forgery (CSRF) is a type of attack where an attacker tricks a user into performing an action on a website without their knowledge or consent. The attack works by exploiting the user's existing session on the website, allowing the attacker to perform actions as if they were the user.  There are many ways to tricks user such as :  Social Engineering : An attacker can use social engineering techniques to trick the user into performing an action on the target website, such as clicking on a link or submitting a form.Phishing Emails : An attacker can send a phishing email that includes a link to a malicious website or a fake login page. When the user enters their login credentials, the attacker can use their session to perform CSRF attacks.Malicious Websites : An attacker can create a malicious website that includes a hidden form or link that submits a request to the target website. If the user is logged into the target website and visits the malicious website, the request will be executed using the user's existing session.   Source : https://www.okta.com/identity-101/csrf-attack/  ","version":"Next","tagName":"h3"},{"title":"Cookie Hijacking​","type":1,"pageTitle":"Web Security","url":"/cs-notes/computer-security/web-security#cookie-hijacking","content":" Also known as session hijacking or cookie stealing, is a type of cyberattack in which an attacker steals a user's cookie. Cookie is a small piece of data containing user's information, session, or login credentials stored on the user's computer by a website.  Example of methods to steal the cookies includes :  Session Fixation : In session fixation, the attacker sets a session ID for the user, either by guessing or by providing a fake URL or link, before the user logs in. A session ID is a unique identifier for someone who connect to server to exchange a series of related message. Once the user logs in, their session are associated with the fixed session ID set by the attacker. The attacker can then use the fixed session ID to hijack the user's session and gain access to their account or other sensitive information.Packet Sniffing : Packet-sniffing is a technique used by attackers to intercept and analyze network traffic in order to extract sensitive information, such as login credentials, credit card numbers, or cookies.   Source : https://www.geeksforgeeks.org/what-is-cookie-hijacking/  ","version":"Next","tagName":"h3"},{"title":"URL Hijacking​","type":1,"pageTitle":"Web Security","url":"/cs-notes/computer-security/web-security#url-hijacking","content":" URL Hijacking or domain hijacking is the act of taking control of a website's domain or URL in order to redirect visitors to a different website or to gain unauthorized access to sensitive information. Many techniques such as MitM, other spoofing techniques, and social engineering can be employed to achieves this.  Attackers may exploit vulnerabilites in some website in domain registrar systems to gain control over a legitimate domain. Once they have control, they can modify DNS settings or transfer the domain to a different registrar, effectively hijacking the URL. ","version":"Next","tagName":"h3"},{"title":"Data Structures & Algorithms","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms","content":"","keywords":"","version":"Next"},{"title":"All pages​","type":1,"pageTitle":"Data Structures & Algorithms","url":"/cs-notes/data-structures-and-algorithms#all-pages","content":" Analysis of AlgorithmsData Structures ArrayLinked-ListStackQueueHash TableSetTreeHeapGraphTrie Algorithms Common Types SortingSearchRecursionDivide And ConquerTraversalBacktrackingGreedyDynamic Programming Other Algorithms Two PointerSliding WindowPrefix SumGraph Algorithms Cycle DetectionShortest PathUnion FindTopological SortMinimum Spanning Tree Complexity Theory ","version":"Next","tagName":"h3"},{"title":"RSA","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/rsa","content":"","keywords":"","version":"Next"},{"title":"Explanation​","type":1,"pageTitle":"RSA","url":"/cs-notes/computer-security/rsa#explanation","content":" The RSA is an actual asymmetric encryption algorithm (e.g., used for exchanging data), it is not used solely for exchanging key like the Diffie-Hellman key exchange.  It consists of four steps :  Key Generation : Randomly select two large prime numbers which has large different, ppp and qqq. These prime numbers should be kept secret. Compute n=pqn = pqn=pq, where nnn is the modulus used for both the public and private keys. nnn is also publicly available as part of the public key. Calculate λ(n)\\lambda(n)λ(n), where λ\\lambdaλ is Carmichael's totient function, and it is the smallest positive integer mmm such that am≡1(modn)a^{m}\\equiv 1{\\pmod {n}}am≡1(modn), where nnn is a positive integer, for all integers aaa coprime to nnn. Since n=pqn = pqn=pq, λ(n)=lcm(λ(p),λ(q))\\lambda(n) = \\text{lcm}(\\lambda(p), \\lambda(q))λ(n)=lcm(λ(p),λ(q)), and since ppp and qqq are prime, λ(p)=ϕ(p)=p−1\\lambda(p) = \\phi(p) = p − 1λ(p)=ϕ(p)=p−1, and likewise λ(q)=q−1\\lambda(q) = q − 1λ(q)=q−1. Hence, λ(n)=lcm(p−1,q−1)\\lambda(n) = \\text{lcm}(p − 1, q − 1)λ(n)=lcm(p−1,q−1), where ϕ\\phiϕ is the Euler's totient function. The LCM itself can be calculated using Euclidean algorithm through the LCM formula, also λ(n)\\lambda(n)λ(n) is kept secret. Choose a public exponent, eee, such that 2&lt;e&lt;λ(n)2 &lt; e &lt; \\lambda(n)2&lt;e&lt;λ(n) and gcd(e,λ(n))=1\\text{gcd}(e, \\lambda(n)) = 1gcd(e,λ(n))=1; that is, eee is coprime to λ(n)\\lambda(n)λ(n). eee is also publicly available as part of the public key. Compute ddd as d≡e−1((modλ)(n))d \\equiv e^{-1} (\\pmod \\lambda(n))d≡e−1((modλ)(n)) to obtain ddd. ddd is modular multiplicative inverse of eee modulo λ(n)\\lambda(n)λ(n). ddd can be obtained by solving de≡1((modλ)(n))de \\equiv 1 (\\pmod \\lambda(n))de≡1((modλ)(n)) using the extended Euclidean algorithm, using the properties of Bézout's identity. This ddd is kept secret as the private key exponent. In conclusion, modulus nnn and exponent eee is the public key. While exponent ddd is obtained from ppp, qqq, and λ(n)\\lambda(n)λ(n), these must be kept secret. note In the original RSA paper, the Euler's totient function, ϕ(n)=(p−1)×(q−1)\\phi(n) = (p - 1) \\times (q - 1)ϕ(n)=(p−1)×(q−1) is used instead of λ(n)\\lambda(n)λ(n). Key Distribution : The public key is shared with anyone who wants to send an encrypted message to the owner of the private key.The private key must be kept secret by the owner and should not be shared. Sender must know recipient's public key to encrypt message, and the recipient must use its own private key to decrypt the message. Similar when the scenario is reversed, if the recipient want to send message to the sender instead, then the recipient needs sender's public key and sender will use their private key to decrypt the message. Encryption : The sender must convert the plaintext message into a numerical representation, the plaintext will also be added with random padding, becoming mmm.The sender retrieves the recipient's public key eee and the modulus nnn.The ciphertext is computed as c≡me mod nc \\equiv m^e \\text{ mod } nc≡me mod n. Decryption : The recipient uses their private key ddd to decrypt the encrypted message.The numerical representation of plaintext mmm is calculated by cd≡(me)d≡m mod nc^d \\equiv (\\text{m}^e)^d \\equiv m \\text{ mod } ncd≡(me)d≡m mod n.The original message MMM is recovered by reversing the random padding scheme in the beginning of encryption scheme. Source : https://www.practicalnetworking.net/series/cryptography/using-asymmetric-keys/  ","version":"Next","tagName":"h3"},{"title":"Security​","type":1,"pageTitle":"RSA","url":"/cs-notes/computer-security/rsa#security","content":" RSA is used to encrypt message that is transmitted over insecure network. Attacker would intercept that encrypted message and decrypt it. In order to decrypt it, the attacker would need the recipient's private key.  To obtain the recipient's private key, the modulus nnn, which is available in public needs to be prime factorized. Modulus nnn is obtained by the product of two prime number ppp and qqq, we would need to find these numbers. After that, the ppp and qqq will be used to calculate the Carmichael totient's function to obtain the private key ddd, which is used to decrypt the message.  The difficulty of breaking RSA comes when factoring large composite numbers into their prime factors. Modulus nnn is obtained by the product of two prime number (product of two prime number is always a composite number), however, factoring large numbers is computationally challenging. As long as the key sizes used in RSA are sufficiently large, it is considered secure against brute force attacks.  tip See factorization as reference about factoring number.  The level of RSA security can be chosen by how many bits used for prime numbers ppp and qqq :  1024-bit RSA : The modulus nnn is 1024 bits, each prime factor ppp and qqq would be roughly 512 bits.2048-bit RSA : nnn is 2048 bits, ppp and qqq is 1024 bits.3072-bit RSA : nnn is 3072 bits, ppp and qqq is 1536 bits.4096-bit RSA : nnn is 4096 bits, ppp and qqq is 2048 bits.  By having 2048 bits for ppp, it means ppp can vary from 000 to 22048−12^{2048} - 122048−1, which is approximately equal to 1.1×10771.1 \\times 10^{77}1.1×1077. Consider the table below, the time grows exponentially. For reference, 2577907×35324489=910632474645232577907 \\times 35324489 = 910632474645232577907×35324489=91063247464523 is approximately 47 bits.   Source : https://www.semanticscholar.org/paper/Using-Random-Search-and-Brute-Force-Algorithm-in-Budiman-Rachmawati/c54d03d38e7b1e34efb712fb6ec6f23e8673d559 ","version":"Next","tagName":"h3"},{"title":"Array","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/array","content":"","keywords":"","version":"Next"},{"title":"Dynamic Array​","type":1,"pageTitle":"Array","url":"/cs-notes/data-structures-and-algorithms/array#dynamic-array","content":" A dynamic array, also known to as an array list, is an array that has the ability to resize and expand automatically as required during program execution. It expands the functionality of a static array to a variable-sized array.  Dynamic array starts with an initial size, it will automatically resize itself as the number of element present exceeds some threshold. The resizing operation involve allocating a new and larger block of memory in a new memory location. The new capacity is determined by some growths factor (e.g., we may adjust the size of the array to 1.5x of its current size). Each element in the old array will be copied to the new memory location, after moving all the element, the old array will be deallocated.  ","version":"Next","tagName":"h3"},{"title":"Multidimensional Array​","type":1,"pageTitle":"Array","url":"/cs-notes/data-structures-and-algorithms/array#multidimensional-array","content":" An array contains a collection of elements with the same type, an array can also contain another array, which is called multidimensional array. An array is called one-dimensional if it only contains a linear sequence of elements, without additional array inside it. A two-dimensional array is where an array has another array inside it, making the array has matrix-like structure.  For instance, here are comparison between the dimension of an array :  [1, 2, 3, 4, 5] : one-dimensional array, contain only linear sequence of element [[1, 2], [3, 4], [5, 6]] : two-dimensional array, each element in the array contains another array The same two-dimensional array can be organized like : [ [1, 2], [3, 4], [5, 6] ] Has 3 rows and 2 columns   Accessing a multidimensional array is similar to one-dimensional array. We will need to provide multiple index to access a single element. Consider the [[1, 2], [3, 4], [5, 6]] array, we want to access number 4. Before accessing the number 4, we will need to access the second array first, which can be accessed using index 1. After accessing the array inside, we can access number 4 directly by index 1. Therefore, accessing 4 would be: arr[1][1]. ","version":"Next","tagName":"h3"},{"title":"Math Concepts","type":0,"sectionRef":"#","url":"/cs-notes/computer-security/math-concepts","content":"","keywords":"","version":"Next"},{"title":"Factorization​","type":1,"pageTitle":"Math Concepts","url":"/cs-notes/computer-security/math-concepts#factorization","content":" Factorization is the process of decomposing a number into several numbers that, when multiplied together, result in the original number. In other words, it is a way to represent a number as a product of several numbers.  Integer Factorization​  Integer factorization is the factorization of a number into their integer product. For example, we can represent the number 24 as the multiply of :  1×241 \\times 241×242×122 \\times 122×123×83 \\times 83×84×64 \\times 64×6  When we ask, what is the factors of 24? They are going to be 1, 2, 3, 4, 6, 12, 24. When we can represent a number with the multiply of two or more number smaller than the number itself, they are called composite number.  Prime Factorization​  To review, prime number is a natural number greater than 1 that is divisible only by 1 and itself, with no other positive divisors. Prime number is the opposite of composite number, we can only represent them in two number, which is 1 and itself.   Source : https://en.wikipedia.org/wiki/Prime_number  Prime factorization is the factorization using prime numbers, in other word, we represent a number in a product of prime numbers. We can get a prime factorization of a number by continuing their integer factorization. For example, number 24, we can continue with one of the factor : 2×122 \\times 122×12. 222 is already a prime, we can still break down 121212 further into 2×62 \\times 62×6, further again into 2×2×32 \\times 2 \\times 32×2×3. This result in 24=2×2×2×324 = 2 \\times 2 \\times 2 \\times 324=2×2×2×3, therefore, the prime factorization of 242424 is 23×32^3 \\times 323×3.  One of the way to find the prime factorization is to decompose a number into some number multiplied by a prime number. The prime number will start from 222, increasing to 333, 555, and so on. While the technique is simple, it can be quite inefficient for large number like 770770770. First, we can factor it by 222, resulting in 385385385, then we will try dividing 385385385 by 222, which is not possible. Divide it by 333, also not possible, until 555, which result in 777777.  In the worst case, the technique will be more inefficient when we encounter a prime number, this mean we will need to brute force every single prime number smaller than the number itself.   Source : https://thirdspacelearning.com/gcse-maths/number/factor-tree/  A better way would to decompose the number into a multiply of larger number first, just like the image above.  ","version":"Next","tagName":"h3"},{"title":"Greatest Common Divisor (GCD)​","type":1,"pageTitle":"Math Concepts","url":"/cs-notes/computer-security/math-concepts#greatest-common-divisor-gcd","content":" Greatest Common Divisor (GCD), also known as Greatest Common Factor (GCF), is the largest positive integer that divides two or more integers, exactly without remainder, where the two or more integers are not zero.  When we ask the GCD of 121212 and 181818, we are asking what is the largest number that divides both number. Number 222 can indeed divide both of them, but there is an integer larger than 222. The GCD of 121212 and 181818 is 666, 121212 divided by 666 is 222 and 181818 divided by 666 is 333. Further division cannot be performed on both 222 and 333, indicating that 666 is the final answer.  There are several methods to find the GCD :  Listing Factors : To use this method, we will find the factor of each number. For example, the factor of 121212 is [111, 222, 333, 444, 666, 121212] and the factor of 181818 is [111, 222, 333, 666, 999, 181818]. To actually find the GCD, we will take the largest common factors shared by both numbers, which is 666. Prime Factorization : Find the prime factorization of each number. The prime factorization of 121212 is 22×32^2 \\times 322×3, prime factorization of 181818 is 2×322 \\times 3^22×32. To find the GCD, we will multiply the common factors shared by both numbers. When there are duplicate (e.g., there are 222^222 and 222, also 323^232 and 333), we will take the lowest one. The common factors are 222 and 333, we are ignoring the 222^222 and 323^232. Multiplying 222 and 333 will result in 666, which is the GCD. Euclidean Algorithm : The Euclidean algorithm is a recursive formula to find the GCD. The formula is GCD(a,b)=GCD(b,a mod b)\\text{GCD}(a, b) = \\text{GCD}(b, a \\text{ mod } b)GCD(a,b)=GCD(b,a mod b), where &quot;mod&quot; represents the modulus operation, which gives the remainder when aaa is divided by bbb. 666 divided by 444 will have remainder of 222, therefore, 6 mod 4=26 \\text{ mod } 4 = 26 mod 4=2. This algorithm is recursive because it will repeatedly apply the formula until it reaches the base case, which is when any of the number becomes zero. At that point, the algorithm terminates, and the GCD is found to be the non-zero value.  Relative Prime​  Also known as coprime, when two numbers are relatively prime, it means they have no common positive integer divisors other than 1. In other words, the greatest common divisor (GCD) of the two numbers is 1.  For example, consider the numbers 15 and 28. The GCD of 15 and 28 is 1, which means they are relatively prime. There is no positive integer greater than 1 that can divide both 15 and 28 without leaving a remainder.  ","version":"Next","tagName":"h3"},{"title":"Least Common Multiply (LCM)​","type":1,"pageTitle":"Math Concepts","url":"/cs-notes/computer-security/math-concepts#least-common-multiply-lcm","content":" Least Common Multiply (LCM) is the smallest positive integer that is divisible by two or more given numbers without leaving a remainder. When we ask for LCM of 121212 and 181818, we are asking what is the smallest number that can be divided by both 121212 and 181818 exactly, the answer is 363636.  Finding LCM :  Prime Factorization : This method is very similar to the prime factorization method to find GCD. We will take the common factor shared by both numbers, the difference is, instead of taking the lowest number, we will instead take the highest number. To actually find the LCM, we will multiply that number.With GCD : This method uses a formula : LCM(a,b)=a×bGCD(a,b)\\text{LCM}(a,b)=\\dfrac{a\\times b}{\\text{GCD}(a,b)}LCM(a,b)=GCD(a,b)a×b​, it needs the GCD of two numbers.    ","version":"Next","tagName":"h3"},{"title":"Modular Arithmetic​","type":1,"pageTitle":"Math Concepts","url":"/cs-notes/computer-security/math-concepts#modular-arithmetic","content":" Modular arithmetic is a branch of mathematics that deals with the arithmetic operations performed on remainders. It involves working with numbers in a specific modulus or modulo. Modulo, as we know before, is a remainder of a division. Modular arithmetic focuses on the operations that can be performed within a specific modulus or modulo.  Congruence​  The key concept is the congruence, two numbers are said to be congruent modulo of a given modulus if they have the same remainder when divided by that modulus. This is denoted by the symbol ≡\\equiv≡. For example, 10≡3 (mod 7)10 \\equiv 3 \\space (\\text{mod } 7)10≡3 (mod 7) means that 101010 and 333 have the same remainder when divided by 777.  In other word, when we say two numbers are congruent within some modulo, it means they will have the same remainder when divided by the modulo number. 101010 divided by 777 left out 333, which is the same remainder as 333 divided by 777 (we can't divide 333 by 777, therefore the remainder is 3 itself).  One way to think about modular arithmetic is to visualize it in terms of clock. A clock starts from 1 to 12, then in 13:00 PM, it will repeat to 1 again up to 12 in 00:00 PM. The repeating behavior of clock can be thought as the modulo operator. For example, in a clock with a 12-hour format, if it is currently 8 o'clock, and we add 5 hours, we can use the modulo operator (%\\%%) to determine the resulting hour : (8+5) % 12=1(8 + 5) \\space \\% \\space 12 = 1(8+5) % 12=1.  In the context of a clock analogy, being congruent of modulo 12 would mean that two numbers would occupy the same position on the clock face (e.g., 14≡2 (mod 12)14 \\equiv 2 \\space (\\text{mod } 12)14≡2 (mod 12)).   Source : https://mathsbyagirl.wordpress.com/2016/06/20/modular-arithmetic-whats-the-point/  In modular arithmetic, it can be thought that the numbers &quot;wrap around&quot; a fixed modulus. The modulus defines the range of values that can be represented within the arithmetic system.  Quotient Remainder Theorem​  The theorem states that, given two integers aaa and bbb, where bbb is not zero, there exist unique integers qqq (the quotient) and rrr (the remainder) such that:  a=b×q+ra = b \\times q + ra=b×q+r  When you divide one integer (a) by another integer (b), you can express the dividend (a) as the product of the divisor (b) and the quotient (q), plus a remainder (r). This theorem is closely related to the modulo operator.  The above theorem can be expressed in terms of modulo and division :  a=b×(a // b)+(a % b)a = b \\times (a \\space // \\space b) + (a \\space \\% \\space b)a=b×(a // b)+(a % b)  Where a // ba \\space // \\space ba // b is integer division or the quotient obtained when dividing aaa with bbb. Integer division is an operation that returns the largest whole number that is less than or equal to the quotient obtained when dividing one integer by another. For example, 5 // 25 \\space // \\space 25 // 2 would be just 222 instead of 2.52.52.5.  Modular Addition &amp; Subtraction​  These are the addition and subtraction performed under modular arithmetic system.  Modular addition involves adding two numbers and then taking the remainder when divided by the modulus. Mathematically, given two numbers aaa and bbb, the modular sum (a+b) mod n(a + b) \\text{ mod } n(a+b) mod n is computed by performing the addition of aaa and bbb, and then taking the remainder when divided by nnn.  For example, let's consider modular addition with a modulus of 555. If we want to calculate (3+4) mod 5(3 + 4) \\text{ mod } 5(3+4) mod 5, we simply add 333 and 444, which gives us 777. Then, when we divide 777 by 555, the remainder is 222. Therefore, (3+4) mod 5(3 + 4) \\text{ mod } 5(3+4) mod 5 is equal to 222.  It is similar for subtraction, basically we subtract instead of adding the number. When we are taking the modulo of negative number, we can simply ignore the minus sign. For example, −7 mod 3=1-7 \\text{ mod } 3 = 1−7 mod 3=1 and −10 mod 4=2-10 \\text{ mod } 4 = 2−10 mod 4=2.  Here are two properties of modular addition and subtraction :  (A+B) mod C=(A mod C+B mod C) mod C(A + B) \\text{ mod } C = (A \\text{ mod } C + B \\text{ mod } C) \\text{ mod } C(A+B) mod C=(A mod C+B mod C) mod C(A−B) mod C=(A mod C−B mod C) mod C(A - B) \\text{ mod } C = (A \\text{ mod } C - B \\text{ mod } C) \\text{ mod } C(A−B) mod C=(A mod C−B mod C) mod C  Modular Multiplication​  Similar to addition and subtraction, doing modulo operator that involve multiplication, we are going to do the multiplication first.  For example, consider modular multiplication with a modulus of 777. If we want to calculate (3×4) mod 7(3 \\times 4) \\text{ mod } 7(3×4) mod 7, we multiply 333 and 444, which gives us 121212. Then, when we divide 121212 by 777, the remainder is 555. Therefore, (3×4) mod 7(3 \\times 4) \\text{ mod } 7(3×4) mod 7 is equal to 555.  The properties of modular multiplication :  (A×B) mod C=(A mod C×B mod C) mod C(A \\times B) \\text{ mod } C = (A \\text{ mod } C \\times B \\text{ mod } C) \\text{ mod } C(A×B) mod C=(A mod C×B mod C) mod C  Modular Exponentiation​  Modular exponentiation is another operation similar to addition, subtraction, and multiplication. We will complete the number raised to the power first before taking the modulo. However, often times the power become very large, therefore, making it hard to calculate. One way to tackle this is to use exponent properties and use the modular multiplication.   Source : https://www.khanacademy.org/computing/computer-science/cryptography/modarithmetic/a/modular-exponentiation  We break down the large power into smaller power and then calculate it independently, at the end, we will multiply them together. This technique is called divide and conquer, that is, breaking down a big problem into smaller problem, solving them independently, and then combining the result to solve the overall problem.  There are also more efficient algorithm to calculate the modulo of exponentiation.  Properties of modular exponentiation :  AB mod C=((A mod C)B) mod CA^B \\text{ mod } C = ( (A \\text{ mod } C)^B ) \\text{ mod } CAB mod C=((A mod C)B) mod C  Modular Inverse​  Inverse of a number is 111 divided by that number. For example, the inverse of number 555 is 15\\dfrac{1}{5}51​, or in exponent, it is the negative power of the number itself (e.g., 23=82^3 = 823=8, inverse is 2−3=182^{-3} = \\dfrac{1}{8}2−3=81​). The properties of inverse number is the inverse of a number multiplied by the number itself will always equal to 111.  Modular arithmetic doesn't have division operator, but we can make it possible by using the exponent properties. Instead of direct division, we can multiply number together with the other number being negative exponent. For example, dividing 444 by 222 can be represented as : 41×2−14^1 \\times 2^{-1}41×2−1, which will result in 22×2−1=212^2 \\times 2^{-1} = 2^122×2−1=21.  In modular arithmetic, the modular inverse of a number is another number that, when multiplied with the original number, yields a remainder of 111 when divided by a given modulus.  Mathematically written as :  a×a−1≡1 (mod m)a \\times a^{-1} \\equiv 1 \\space (\\text{mod } m)a×a−1≡1 (mod m)  Finding the modular inverse involves solving a congruence equation. For example, to find the modular inverse of aaa modulo mmm, you would need to find a number bbb such that:  a×b≡1 (mod m)a \\times b \\equiv 1 \\space (\\text{mod } m)a×b≡1 (mod m)  Finding it using a naive method, trying out every single number :   Source : https://www.khanacademy.org/computing/computer-science/cryptography/modarithmetic/a/modular-inverses  The modular inverse of A mod CA \\text{ mod } CA mod C is the BBB value that makes A×B mod C=1A \\times B \\text{ mod } C = 1A×B mod C=1.  Extended Euclidean Algorithm​  Using the Euclidean algorithm used to find GCD, we can also use the algorithm to find modular inverse more efficiently than the brute force way.  The properties of Euclidean Algorithm :  GCD(A,0)=A\\text{GCD}(A, 0) = AGCD(A,0)=AGCD(0,B)=B\\text{GCD}(0, B) = BGCD(0,B)=BIf A=B×Q+RA = B \\times Q + RA=B×Q+R and B≠0B ≠ 0B=0 then GCD(A,B)=GCD(B,R)\\text{GCD}(A, B) = \\text{GCD}(B, R)GCD(A,B)=GCD(B,R) where QQQ is an integer, RRR is an integer between 000 and B−1B - 1B−1.  For review, the first and second properties is the base case of Euclidean algorithm, the third property is the GCD algorithm formula : GCD(a,b)=GCD(b,a mod b)\\text{GCD}(a,b) = \\text{GCD}(b,a\\text{ mod }b)GCD(a,b)=GCD(b,a mod b) combined with the quotient remainder theorem.  Bézout's Identity​  Bézout's Identity states that for any two integers aaa and mmm such that their GCD is equal to 1 (i.e., they are relatively prime), there exist integers xxx and yyy such that :  ax+my=1ax + my = 1ax+my=1  In the context of modular arithmetic, this equation can be written as :  ax≡1 (mod m)ax \\equiv 1 \\space (\\text{mod } m)ax≡1 (mod m)  Using Extended Euclidean Algorithm​  Using the Bézout's identity, the extended Euclidean algorithm follows a recursive process to calculate the GCD and the Bézout's identity coefficients xxx and yyy. It starts with the base case where mmm is 000, in which case the GCD is aaa and the coefficients are (1, 0). If mmm is not 000, it recursively calculates the GCD and the coefficients based on the equation : ax+my=1ax + my = 1ax+my=1.  The formula for extended Euclidean algorithm in code like :  extended_gcd(a, m): if m = 0: return a, 1, 0 else: gcd, x, y = extended_gcd(m, a % m) return gcd, y, x - (a // m) * y   Euler's Totient Function​  The Euler's Totient function, denoted as ϕ(n)\\phi(n)ϕ(n), is a mathematical function that counts the number of positive integers less than or equal to nnn that are relatively prime to nnn. In other words, it calculates the count of numbers between 111 and nnn (inclusive) that do not share any common factors with nnn except for 111.  For example, consider the number 101010. The positive integers less than or equal to 101010 are [1,2,3,4,5,6,7,8,9,10][1, 2, 3, 4, 5, 6, 7, 8, 9, 10][1,2,3,4,5,6,7,8,9,10]. Among these numbers, the coprime numbers to 101010 are [1,3,7,9][1, 3, 7, 9][1,3,7,9], as they do not share any common factors with 101010 except for 111, therefore, ϕ(10)=4\\phi(10) = 4ϕ(10)=4.   Source : https://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/EulerPhi.svg/800px-EulerPhi.svg.png  ","version":"Next","tagName":"h3"},{"title":"Primality Test​","type":1,"pageTitle":"Math Concepts","url":"/cs-notes/computer-security/math-concepts#primality-test","content":" Primality test is a method or algorithm used to determine whether a given number is prime or composite (not prime). The naive way to determine is to try to divide the number with every single number smaller than the number itself, this method is called trial division. However, the method will become inefficient for large number.  Fermat's Primality Test​  One of the more efficient method is the Fermat's Primality Test, it is based on Fermat's Little Theorem. It states that if ppp is a prime number and aaa is any positive integer less than ppp, then a(p−1)≡1 (mod p)a^{(p-1)} \\equiv 1 \\space (\\text{mod } p)a(p−1)≡1 (mod p).  The test randomly selects values of aaa and checks if the equality holds. If it fails for any value, the number is composite; otherwise, it is likely prime. This method is a probabilistic primality test, it offers a good balance between efficiency and accuracy when determining the primality of large numbers. ","version":"Next","tagName":"h3"},{"title":"Analysis of Algorithms","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/analysis-of-algorithms","content":"","keywords":"","version":"Next"},{"title":"Time & Space Complexity​","type":1,"pageTitle":"Analysis of Algorithms","url":"/cs-notes/data-structures-and-algorithms/analysis-of-algorithms#time--space-complexity","content":" An algorithm performance typically depends on the characteristics and the size of its input. The time and memory requirements to execute an algorithm can vary depending on the nature of the input. For instance, algorithms may take different amounts of time and memory to process large amounts of input compared to small amounts.  Best, Average, and Worst Scenario​  There are three different scenarios we can consider in order to evaluate the performance of an algorithm.  Best-case Scenario : The best-case scenario represents the optimal conditions under which an algorithm performs at its best. The scenario has input that leads to the most efficient execution or the lowest time or space complexity. However, this is not realistic and may not happen frequently. Average-case Scenario : The average-case scenario takes into account the expected or average performance of an algorithm over a range of inputs. The average-case analysis provides a more realistic estimate of an algorithm's performance when dealing with typical inputs. Worst-case Scenario : The worst-case scenario represents the input that leads to the least efficient execution or the highest time and space complexity for an algorithm. It represents the most unfavorable conditions the algorithm may encounter in terms of input data. The worst-case analysis is commonly used to measure the algorithm's performance as it helps identify any potential limitations.  For example, the linear search algorithm is an algorithm that search for specific element in some collection. The algorithm works by checking each element one by one and decide if it is the element we are looking for. This algorithm typically starts from the left until the rightmost element.   Source : https://sushrutkuchik.wordpress.com/2020/05/24/searching-algorithms-visualized/  In the best-case scenario, the target element is found as the first element in the collection. The average scenario typically occurs when the element is located somewhere in the middle of the collection, which is probably the more common situation. The worst-case scenario occurs when the target element is the rightmost element, encountered as the last element during the search process.  ","version":"Next","tagName":"h3"},{"title":"Big O Notation​","type":1,"pageTitle":"Analysis of Algorithms","url":"/cs-notes/data-structures-and-algorithms/analysis-of-algorithms#big-o-notation","content":" Big O notation is a mathematical notation used to describe the performance characteristics (time and space complexity) of an algorithm. As explained before, algorithm performance may depend on the size of the input, the big O notation represent the algorithm growth toward the size of input using a mathematical function.  The notation is typically written as O(f(n))O(f(n))O(f(n)), where f(n)f(n)f(n) represents the growth rate function and nnn denotes the input size.  Using the similar example as before, the linear search algorithm, as we can see, the algorithm checks each element one by one, starting from the left and moving towards the rightmost element. As the input size increases, the algorithm needs to check a greater number of elements.  Notice that there is a direct correlation between the number of elements in the collection and the number of comparisons required. If there are n elements in the collection, the algorithm may need to check each element until it finds the target or reaches the end of the collection (the worst case scenario). This means that as the input size increases, the time taken by the linear search algorithm to find the target element increases linearly with the input size. In other words, if the input size doubles, the algorithm's execution time will also roughly double.  The appropriate growth rate function for linear search algorithm is a linear function, therefore, the big O notation for linear search algorithm is O(n)O(n)O(n).  There are many more growth rate function commonly encountered in algorithms :  O(1)O(1)O(1) - Constant Complexity : The algorithm's time or memory usage remains constant regardless of the input size. By constant, it doesn't mean the algorithm require no time or memory, it describes that whatever the input size is, the algorithm will always perform the same.O(log⁡n)O(\\log n)O(logn) - Logarithmic Complexity : The growth rate of time or memory increases logarithmically (typically log base-10) with the input size.O(n)O(n)O(n) - Linear Complexity : The growth rate is directly proportional to the input size.O(nlog⁡n)O(n \\log n)O(nlogn) : Linearithmic Complexity : The growth rate is a multiplication of linear and logarithmic factors.O(n2)O(n^2)O(n2) - Quadratic Complexity : The growth rate is proportional to the square of the input size.O(2n)O(2^n)O(2n) - Exponential Complexity : The growth rate doubles with each increase in input size.O(n!)O(n!)O(n!) - Factorial Complexity : The growth rate increase factorially with input size.  For instance, if we denote this notation as the number of operation done in an algorithm, when we have input size of n = 1000 and the algorithm we are using has a logarithmic complexity O(log⁡n)O(\\log n)O(logn), then the number of operation is roughly 3 times (log base 10 of 1000 is 3).   Source : https://www.freecodecamp.org/news/all-you-need-to-know-about-big-o-notation-to-crack-your-next-coding-interview-9d575e7eec4/  Big O as Asymptotic Notation​  Big O notation describe the upper bound of the growth rate of an algorithm's time or space complexity. We are interested in how algorithm's performance scales with increasing input sizes. In the actual analysis, we do not typically measure the number of operation directly. Instead, Big O notation provides an abstract representation of the growth rate of an algorithm's time or space complexity.  In Big O analysis, the primary focus is on the dominant term or the most influential factor that determines an algorithm's growth rate. For instance, if an algorithm has a time complexity of O(2n2+n+1)O(2n^2 + n + 1)O(2n2+n+1), we concentrate on function in the dominant term, which is n2n^2n2, while disregarding the coefficient 222, lower-order term nnn, and the constant term 111. This simplification is based on the assumption that the input size, denoted as nnn, approaches infinity (asymptotically). As a result, the lower-order term becomes insignificant and does not significantly impact the algorithm's growth rate.  Big Omega &amp; Theta Notation​  There are many variants of big O notation, another common notation is the Big Omega Notation and the Big Theta Notation.  Big O Notation : The big O notation measure the upper bound or the worst case scenario of an algorithm.Big Omega Notation (Ω)(\\Omega)(Ω) : The big omega notation measure the lower bound or the best case scenario of an algorithm.Big Theta Notation (Θ)(\\Theta)(Θ) : The big theta notation provides both an upper bound and a lower bound on the growth rate of an algorithm's time or space complexity. It represents the tightest bound or the exact rate of growth.   Source : https://tarunjain07.medium.com/complexity-asymptotic-analysis-e9cd6d4b766e  ","version":"Next","tagName":"h3"},{"title":"Runtime Complexity​","type":1,"pageTitle":"Analysis of Algorithms","url":"/cs-notes/data-structures-and-algorithms/analysis-of-algorithms#runtime-complexity","content":" While the big O notation provides a way to evaluate an algorithm time and space complexity, it does a lot of simplification such as removing coefficients, lower-order term, and constant terms. Theoretically with big O notation, an algorithm with time complexity of O(100n)O(100n)O(100n), which will be simplified to O(n)O(n)O(n) will have the same time complexity with an algorithm that has time complexity of just O(n)O(n)O(n).  While they have the same performance analytically, in the real computation, they probably take different amount of time and memory in the execution. Runtime complexity is the more precise measurement of an algorithm's performance. The runtime or actual execution time can be achieved through technique like profiling and benchmarking. Profiling involves collecting data on how much time is spent executing different parts of the program. On the other hand, benchmarking involves running the program or algorithm on different inputs and measuring the execution time. ","version":"Next","tagName":"h3"},{"title":"Backtracking","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/backtracking","content":"","keywords":"","version":"Next"},{"title":"Backtracking vs Brute Force​","type":1,"pageTitle":"Backtracking","url":"/cs-notes/data-structures-and-algorithms/backtracking#backtracking-vs-brute-force","content":" Backtracking approach in solving problem differ with the brute force approach. In brute force, we mindlessly try all possible combination. In the case of solving Sudoku, we would fill out all the cell with number 1, and then try again with one cell being number 2, and so on.  On the other hand, backtracking takes into account problem constraints and uses them to efficiently explore the solution space. In the case of solving Sudoku, we would add a constraint such as, not using numbers that are already used in the same grid. Backtracking also involve undoing some steps, instead of starting from zero, we would undo the invalid step and continue from there. This additional characteristics make backtracking faster than a typical brute force solution.  ","version":"Next","tagName":"h3"},{"title":"Solving Maze​","type":1,"pageTitle":"Backtracking","url":"/cs-notes/data-structures-and-algorithms/backtracking#solving-maze","content":" Consider a maze with some wall obstacle in it.   Source : https://brilliant.org/wiki/depth-first-search-dfs/  A pseudocode to solve it using backtracking :  function solve(row, col): if (row, col) is the goal: return true if (row, col) is a valid position in the maze and not a wall: Mark (row, col) as part of the solution // Try moving in all possible directions: up, down, left, right if solveMaze(row - 1, col) is true: // Try moving up return true if solveMaze(row + 1, col) is true: // Try moving down return true if solveMaze(row, col - 1) is true: // Try moving left return true if solveMaze(row, col + 1) is true: // Try moving right return true // If none of the directions lead to the goal, backtrack and mark the cell as invalid Mark (row, col) as not part of the solution return false return false // Start solving the maze from the top-left corner (0, 0) solveMaze(0, 0)   This backtracking solution uses depth-first search (DFS) to traverse the maze. The constraint is quite simple, basically we must navigate to the exit while avoiding wall obstacles. ","version":"Next","tagName":"h3"},{"title":"Complexity Theory","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/complexity-theory","content":"","keywords":"","version":"Next"},{"title":"Complexity Class​","type":1,"pageTitle":"Complexity Theory","url":"/cs-notes/data-structures-and-algorithms/complexity-theory#complexity-class","content":" Complexity class is a classification of a set of problem with common level of computational complexity. To analyze the behavior and the complexity of algorithms, a theoretical mathematical model of a general computing machine called Turing machine is used.  Terminology :  Deterministic Turing machine : The Turing machine follows a fixed set of rules to process input and produce output, the behavior will depend on current state and the current input. In other word, the Turing machine behave in predictable manner and always return the same output, given the same input.Nondeterministic Turing machine : Turing machine which can have multiple choices for the next transition, it can have multiple possible outcomes for a given state and symbol, which can explored simultaneously.Decision problems : Problem that requires yes-or-no answer. For example, whether number 7 is prime or not.Polynomial time : Mathematical expression that involves one or more variables raised to non-negative integer powers, multiplied by coefficients. For example, n1n^1n1 or simply nnn, n2n^2n2, n3n^3n3, 3n2+33n^2 + 33n2+3. When we say a problem can be run in polynomial time, it means the running time of the algorithm for the problem grows no faster than a polynomial function of the input size.Reduction : Reduction is a concept where we transform one problem into other problem. The purpose of this is to use the solution of another solved problem. For example, we have solved problem B, but haven't solved problem A. If we can somehow reduce problem A to problem B, then we can use the same solution of problem B to solve problem A.  Time Complexity​  P​  P : Polynomial TimeP is the complexity class that consists of decision problems that can be solved by a deterministic Turing machine in polynomial time. In other word, it can be solved quickly.P problem's solution are also easy to find.Example of P algorithms includes sorting algorithm, it can be solved quickly and can be checked easily.  NP​  NP : Nondeterministic Polynomial TimeNP is the complexity class that consists of decision problems that can be solved by a nondeterministic Turing machine in polynomial time.The &quot;yes&quot; solution of an NP problem can be verified efficiently in polynomial time using deterministic Turing machine.  Example of NP problem is the Subset Sum.  Given a set of integers and a target value, determine if there exists a subset of the integers that sums up to the target value.  For example, let's say we have the set of integers 9 and the target value is 13. The problem is to find out if there is a subset of these numbers that adds up to 13. In this case, the subset 9 sums up to 13, so the answer would be &quot;yes.&quot;  To verify a potential solution, we can simply sum up the numbers in the subset and check if the sum equals the target value. This verification process can be done in polynomial time. However, finding the subset itself is the challenging part, as the set of integers increase, it becomes more difficult.  Co-NP​  Co-NP : Complement of NPIt consists of decision problems for which the &quot;no&quot; instances can be verified in polynomial time.While NP focuses on verifying or checking if a solution is valid, Co-NP, on the other hand, does the opposite by verifying or checking if a solution is invalid or incorrect.  It is not known whether NP and Co-NP are the same or different classes. If they belong to the same classes (NP = Co-NP), it would imply that every problem for which a &quot;yes&quot; instance can be verified in polynomial time also has a polynomial-time verification for &quot;no&quot; instances. In other words, if a solution can be quickly checked for correctness, then its invalidity can also be quickly determined.  An example of a problem in Co-NP is the problem of determining if a given number is not prime. The check can be done efficiently by dividing the number with any integer between 2 and the square root of the number without a remainder.  NP-Hard​  NP-Hard : Nondeterministic Polynomial-Time HardNP-Hard is a class of decision problems that are at least as hard as the hardest problems in NP.Do not necessarily need to be in NP themselves.NP-Hard problems are believed to be computationally intractable (can be solved in theory but not in practice) and do not necessarily have a polynomial-time algorithm for their solution (again, because they may not be in NP).Solving an NP-Hard problem does not imply solving all problems in NP, but it indicates that the problem is at least as difficult as the hardest problems in NP.  Example of an NP-hard problem is the traveling salesman problem. The problem is to find the shortest possible route that visits a set of cities once and returns to the starting city. This problem is considered NP-Hard because we wouldn't know if some answer is really the shortest route. To verify this, we would need to check every possible route, which takes a long amount of time.   Source : https://makeagif.com/gif/traveling-salesman-problem-visualization-GZeMvl  NP-Complete​  NP-Complete : Nondeterministic Polynomial-Time CompleteAn NP-Complete problem is one that is believed to be among the most difficult problems in NP, it is the subset of NP and NP-hard.If a polynomial-time algorithm exists for any NP-Complete problem, it implies that a polynomial-time algorithm exists for all problems in NP.NP-Complete problems are considered to be of equal difficulty, meaning that solving one NP-Complete problem would effectively solve them all. This property is known as &quot;completeness&quot; because the complexity of all problems in NP is &quot;complete&quot; with respect to the complexity of NP-Complete problems.  An example of an NP-Complete problem is the Circuit satisfiability problem (CSAT). This problem ask whether a circuit that consist of boolean gates can produce output 1, while ensuring that the inputs consistently remain as either 0 or 1.  To know if CSAT is NP-complete, we would need to proof it belongs to NP and NP-hard.  CSAT is indeed in NP, we know that we can verify the solution efficiently in polynomial time.To know if it's NP-hard, we can find another problem that is already known to be NP-hard and reduce that problem to CSAT. The known NP-hard problem that can be reduced to CSAT is the Boolean satisfiability problem (SAT) problem.   Source : https://www.researchgate.net/figure/An-example-of-circuit-satisfiability-problem-The-answer-should-be-yes-here-as-an-input_fig2_354459093  The complexity classes diagram (P != NP) and comparison table :   Source : diagram, table  Space Complexity​  These are the complexity classes for space complexity :  L (Logarithmic Space) : L is the class of decision problems that can be solved using a logarithmic amount of space on a deterministic Turing machine. It represents problems that can be solved efficiently with a very limited amount of memory. NL (Nondeterministic Logarithmic Space) : NL is the class of decision problems that can be solved using a logarithmic amount of space on a nondeterministic Turing machine. It represents problems that can be efficiently solved in logarithmic space with the help of nondeterministic choices. PSPACE (Polynomial Space) : PSPACE is the class of decision problems that can be solved using a polynomial amount of space on a deterministic Turing machine. It represents problems that can be solved efficiently with a reasonable amount of memory. NPSPACE (Nondeterministic Polynomial Space) : NPSPACE is the class of decision problems that can be solved using a polynomial amount of space on a nondeterministic Turing machine. It represents problems that can be efficiently solved with polynomial space when nondeterministic choices are allowed. EXPSPACE (Exponential Space) : EXPSPACE is the class of decision problems that can be solved using an exponential amount of space on a deterministic Turing machine. It represents problems that require a significant amount of memory to solve and grow exponentially in space complexity. NEXPSPACE (Nondeterministic Exponential Space) : NEXPSPACE is the class of decision problems that can be solved using an exponential amount of space on a nondeterministic Turing machine. It represents problems that can be efficiently solved with exponential space when nondeterministic choices are allowed.  ","version":"Next","tagName":"h3"},{"title":"P vs NP Problem​","type":1,"pageTitle":"Complexity Theory","url":"/cs-notes/data-structures-and-algorithms/complexity-theory#p-vs-np-problem","content":" P vs NP is a very famous question in computer science and mathematics. It asks whether every problem for which a solution can be verified in polynomial time (belonging to the class NP) also has a polynomial-time algorithm to find the solution (belonging to the class P).  If the answer for the question is true, meaning P problems belong to the same class to NP, or P = NP, it would mean that there is a polynomial-time algorithm for solving NP problems, indicating that difficult problems could be efficiently solved.  If the answer for the question is false, meaning P problems does not belong to the same class to NP, or P != NP, it would confirm that solving NP problems are indeed computationally difficult and that finding efficient algorithms for them is unlikely.  Below are the diagram of complexity classes if P = NP and P != NP :   Source : https://upload.wikimedia.org/wikipedia/commons/a/a0/P_np_np-complete_np-hard.svg ","version":"Next","tagName":"h3"},{"title":"Cycle Detection","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/cycle-detection","content":"","keywords":"","version":"Next"},{"title":"Floyd's Tortoise & Hare Algorithm​","type":1,"pageTitle":"Cycle Detection","url":"/cs-notes/data-structures-and-algorithms/cycle-detection#floyds-tortoise--hare-algorithm","content":" Floyd's Tortoise and Hare Algorithm is a popular algorithm to detect cycle, it can be applied to sequences and linked structures such as linked list and graph. This algorithm make us the two pointer technique to traverse the data structure and detect the cycle.  We will use two pointers, one pointer is referred as &quot;slow&quot; (or tortoise) pointer, which is used for traversing the data structure gradually, visiting each element one by one, and the &quot;fast&quot; (or hare) pointer for traversing the data structure at a faster pace, advancing two elements at a time.  The idea of this algorithm is that if a cycle exist in a sequence, both pointer will eventually meet at some point. The fast pointer, will eventually catch up with the slow pointer. If there are no sequence, then the fast pointer will finish first before even meeting slow pointer.  Here is the pseudocode for the algorithm to detect linked list :  function hasCycle(head): slow = head fast = head while fast is not null and fast.next is not null: slow = slow.next fast = fast.next.next if slow == fast: return true return false   When the pointer becomes null, it indicates the end of the list. If the loop completes, which mean the fast pointer reach the end of the linked list, then there is no cycle.   Source : https://turboyourcode.com/algorithm/floyd-s-cycle-finding  This algorithm can also be modified to find the start of the cycle. The start of the cycle can be found by resetting the slow pointer to the head and keep the fast pointer as it is. Both pointer will traverse again normally, in addition with the same pace, one step at a time. When they meet again, that meeting point is the starting point of the cycle. ","version":"Next","tagName":"h3"},{"title":"Divide And Conquer","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/divide-and-conquer","content":"","keywords":"","version":"Next"},{"title":"Merge Sort​","type":1,"pageTitle":"Divide And Conquer","url":"/cs-notes/data-structures-and-algorithms/divide-and-conquer#merge-sort","content":" Merge sort is a sorting algorithm that follows divide and conquer strategy to sort a collection of elements.  The idea is, sorting a large array must involve a lot of comparison, which result in a harder problem. Merge sort divides the array into smaller subarrays, sorting them individually, and then merging the sorted subarrays, which effectively reduces the number of comparisons needed.  Here is the pseudocode :  MergeSort(arr): if length of arr &lt;= 1: return arr mid = length of arr // 2 left = arr[0:mid] right = arr[mid:end] left = MergeSort(left) right = MergeSort(right) return Merge(left, right) Merge(left, right): result = empty array while left is not empty and right is not empty: if left[0] &lt;= right[0]: append left[0] to result remove left[0] from left else: append right[0] to result remove right[0] from right append remaining elements of left to result append remaining elements of right to result return result   The algorithm starts by dividing the input array into two equal-sized subarrays (or approximately equal-sized if the array size is odd). This process continues until the array contains only one element or empty. Once the subarrays are small enough, the algorithm starts sorting them by comparing each other and placing them in correct order in the temporary array (result array), this process also merge the two subarray.   Source : https://en.wikipedia.org/wiki/Merge_sort (with speed modification)  Merge sort results in best, average, and worst-case of O(nlog⁡n)O(n \\log n)O(nlogn) time complexity. This is because the algorithm divides the array into halves recursively (log⁡n\\log nlogn term), and the merging step takes linear time (nnn term) proportional to the size of the subarrays being merged.  The memory required for merge sort is O(n)O(n)O(n), this is because it requires additional space to store the temporary subarrays during the merging process.  ","version":"Next","tagName":"h3"},{"title":"Quick Sort​","type":1,"pageTitle":"Divide And Conquer","url":"/cs-notes/data-structures-and-algorithms/divide-and-conquer#quick-sort","content":" Quick sort is another sorting algorithm that follows the divide and conquer strategy. Quick sort selects an element (called pivot), and then it partitions or reorders the array based on that element. The reordering involves dividing the array into two subarrays, one containing elements smaller than or equal to the pivot, and the other containing elements greater than the pivot.  QuickSort(arr, low, high): if low &lt; high: pivotIndex = Partition(arr, low, high) QuickSort(arr, low, pivotIndex - 1) QuickSort(arr, pivotIndex + 1, high) Partition(arr, low, high): pivot = arr[high] i = low - 1 for j = low to high - 1: if arr[j] &lt;= pivot: i = i + 1 Swap(arr, i, j) Swap(arr, i+1, high) return i + 1 Swap(arr, i, j): temp = arr[i] arr[i] = arr[j] arr[j] = temp   The low and high represent the range of the array. The Partition function takes the range of the array and returns the pivot index that represents the final position of the pivot element after partitioning. Partition function selects a pivot element (in this case the last element) and rearranges the subarray such that elements smaller than or equal to the pivot are on the left, and elements greater than the pivot are on the right. After pivotIndex is obtained, the algorithm recursively applies QuickSort function on the subarrays to the left and right of the pivot until the entire array is sorted (when low &lt; high).   Source : https://en.m.wikipedia.org/wiki/File:Quicksort-example.gif (with speed modification)  Quick sort achieves the same in the best and average scenario with merge sort, which is O(nlog⁡n)O(n \\log n)O(nlogn). The worst-case scenario results in O(n2)O(n^2)O(n2) time complexity, when the pivot is consistently chosen poorly (e.g., already sorted array or sorted in reverse order). Quick sort has an advantage in terms of space complexity, requiring only O(log⁡n)O(\\log n)O(logn) memory, as the recursive calls are made on smaller subarrays. ","version":"Next","tagName":"h3"},{"title":"Greedy","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/greedy","content":"","keywords":"","version":"Next"},{"title":"Coin Change​","type":1,"pageTitle":"Greedy","url":"/cs-notes/data-structures-and-algorithms/greedy#coin-change","content":" One of a simple problem that can be solved using a greedy approach is the coin change problem. The problem wants us to find the minimum number of coins needed to make a specific amount of money.  Given a set of coin and a target amount of change to make, the goal is to determine the minimum number of coins needed to make exactly that amount.  For example, if we have a set of coin of: 1 cent, 5 cents, 10 cents, 25 cents, and the target is 79, we may use 3 x 25 cents and 4 x 1 cents, resulting in 7 coins.  Here is the pseudocode for it :  function coinChange(coins, amount): sort coins in descending order currentAmount = 0 numCoins = 0 for each coin in coins while currentAmount + coin &lt;= amount: currentAmount += coin numCoins += 1 return numCoins   The greedy approach always select the largest worth of coin available (as long as it doesn't exceed the amount) and use that coin to make the change. While this approach seems to be working all the time, there are some case where this solution doesn't give us the most optimal solution.  In the case of having a coin set of [1, 6, 10] and a target amount of 12, the greedy solution would choose the largest coin, which is 10, and then add 2 coins of 1 cent, resulting in a total of 3 coins. However, it's worth noting that by using two 6-cent coins, we can achieve the target amount of 12 with only 2 coins. ","version":"Next","tagName":"h3"},{"title":"Dynamic Programming","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/dynamic-programming","content":"","keywords":"","version":"Next"},{"title":"Dynamic Programming vs Divide and Conquer​","type":1,"pageTitle":"Dynamic Programming","url":"/cs-notes/data-structures-and-algorithms/dynamic-programming#dynamic-programming-vs-divide-and-conquer","content":" Both are very similar in terms of the approach. However, both are unique approach and may not be applicable to all problems interchangeably.  The difference is, divide and conquer approach divides the problem into non-overlapping subproblems that are solved independently. In other word, each subproblem doesn't depend on each other, we can't apply the solution of a subproblem into another subproblem.  On the other hand, dynamic programming may contain overlapping subproblem that depends on each other. The idea is, if the problem exhibits overlapping subproblems, instead of solving each of the subproblem, we just need to solve it once and store the solution for another subproblem. This characteristics of dynamic programming makes it efficient than just brute forcing.  ","version":"Next","tagName":"h3"},{"title":"Top-Down​","type":1,"pageTitle":"Dynamic Programming","url":"/cs-notes/data-structures-and-algorithms/dynamic-programming#top-down","content":" Dynamic programming can be done in two approach, top-down and bottom-up.  In the top-down approach, we will start with the original problem and breaks it down into smaller subproblems. We will then solve these subproblems recursively.  Fibonacci​  A common example of dynamic programming is calculating the Fibonacci sequence. Fibonacci's sequence, is a series of numbers in which each number is the sum of the two preceding ones. It starts with 0 and 1, and each subsequent number is the sum of the two numbers that precede it.  The Fibonacci sequence typically starts with 0 and 1, and the subsequent numbers are generated by summing the two previous numbers. The sequence continues indefinitely, and it follows this pattern:  0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ...  Mathematically, it can be defined as :  F(0)=0F(0) = 0F(0)=0, F(1)=1F(1) = 1F(1)=1, F(n)=F(n−1)+F(n−2) for n ≥2F(n) = F(n-1) + F(n-2) \\text{ for n } \\ge 2F(n)=F(n−1)+F(n−2) for n ≥2.  Naive Fibonacci's Sequence Solution​  Here is the pseudocode to compute the Fibonacci's sequence :  function fibonacci(n): if n &lt;= 1: return 1 else: return fibonacci(n - 1) + fibonacci(n - 2)   The Fibonacci sequence exhibits overlapping subproblems because each number in the sequence is derived from adding the two preceding numbers. Each of the two preceding numbers are also derived from the two preceding numbers again up to 1 and 0.  Here is an image to visualize the dependency, we can see that calculating F5 involve the dependency of F2 up to 3 times.   Source : https://avikdas.com/2019/04/15/a-graphical-introduction-to-dynamic-programming.html  Memoization​  The pseudocode above is a naive solution that recursively computes the sequence multiple times. For example, when n = 5, we will calculate F(4) and F(3). Calculating F(4) involve calculating F(3) again. We are doing a lot of repeated work and it can be very inefficient for large n.  The pseudocode above results in O(2n)O(2^n)O(2n) time complexity, this is because, for each n we will always calculate the two preceding Fibonacci numbers, n - 1 and n - 2. Each of those calls, in turn, calls the function for their preceding Fibonacci numbers, and so on. As a result, the number of function calls grows exponentially with the input value n. Specifically, for each Fibonacci number, two additional function calls are made, resulting in a branching factor of 2. Therefore, the total number of function calls follows a recursive pattern where the number of calls doubles with each increase in n.  Memoization is a technique used in dynamic programming, specifically in the top-down approach to improve the algorithm efficiency. Memoization optimize the execution by storing (or caching) the result of function calls for specific input values and reusing them when the same inputs occur again.  We know that when we input n = 2, we will always obtain 1. If we store the result of F(2), we can eliminate some repeated work, therefore improving the overall efficiency.   Source : https://avikdas.com/2019/04/15/a-graphical-introduction-to-dynamic-programming.html (with modification)  The memoization technique stores all the result of every Fibonacci's sequence in some data structure that can be random accessed efficiently like array or hash map.  Here is the improved version of the previous pseudocode :  memo = array of size n function fibonacci(n): if memo[n] is defined: return memo[n] if n &lt;= 1: return 1 else: result = fibonacci(n - 1) + fibonacci(n - 2) memo[n] = result return memo[n]   We will first check if the n or the current Fibonacci's sequence is present in the memo array. If yes, it means we have already computed it before and we can just use that result. If not, we will compute the sequence normally (also don't forget to store the result).  Here is the visualization, we only need to compute each Fibonacci sequence once. This results in O(n)O(n)O(n) time complexity, where nnn is the Fibonacci sequence we want to calculate.   Source : Source : https://avikdas.com/2019/04/15/a-graphical-introduction-to-dynamic-programming.html (with modification)  ","version":"Next","tagName":"h3"},{"title":"Bottom-Up​","type":1,"pageTitle":"Dynamic Programming","url":"/cs-notes/data-structures-and-algorithms/dynamic-programming#bottom-up","content":" In top-down approach, we start with the original problem, breaking it down into smaller problem until we can solve it directly. In the case of Fibonacci's sequence, we will need to break it down until n is at least 1. Top-down approach also uses recursion, therefore requiring additional memory for the call stack (space complexity for Fibonacci's sequence with the top-down solution is O(n)O(n)O(n)).  Bottom-up approach, on the other hand, solves the problem starting from the smallest subproblems and gradually builds up the solutions to larger subproblems until the original problem is solved. While at the end we still solve the smaller subproblems first, in bottom-up approach, we immediately start from the bottom iteratively, eliminating the need of recursion.  In some case, bottom-up approach has no difference in terms of space complexity, because we still need some place to store the previously computed result. However, there is scenario where solving it bottom-up can improve the solution, which is in the case of Fibonacci's sequence.  Here is the pseudocode for calculating Fibonacci's sequence in bottom-up approach :  function fibonacci(n): if n &lt;= 1: return n fib = array of size (n + 1) fib[0] = 0 fib[1] = 1 for i from 2 to n: fib[i] = fib[i - 1] + fib[i - 2] return fib[n]   Similar to top-down approach, we used an array to store the previous solution. The difference is, instead of using recursion, we do it iteratively.  Improved Bottom-Up​  This code can be further improved so we don't need additional space to store the previous solution :  function fibonacci(n): if n &lt;= 1: return n fib1 = 0 fib2 = 1 for i from 2 to n: nextFib = fib1 + fib2 fib1 = fib2 fib2 = nextFib return fib2   Using the bottom-up approach, it's worth noting that we don't need to store the whole sequence result. Whenever we calculate the sequence, we always use the last two result which are fib[i - 1] and fib[i - 2], therefore, we can eliminate the need of array and instead store it in some variable. This result an improvement of space complexity from O(n)O(n)O(n) to a constant O(1)O(1)O(1).  Tabulation​  The Fibonacci's sequence problem is simple enough that we can use a simple one-dimensional array to store the previous result. We can even use two variable to store the previous result in the improved bottom-up approach. In conclusion, we can say that the problem is one-dimensionally dependent (also called 1-D dynamic programming).  In a more complex problem, the problem may depend multidimensionally (2-D dynamic programming or higher). Instead of creating a one-dimensional array, we may need to create multidimensional array such as table or matrix. Each cell in the table represents the solution to a specific subproblem.  Tabulation is a technique used in dynamic programming to solve problems by iteratively filling up a table or a matrix of precomputed values.   ","version":"Next","tagName":"h3"},{"title":"Hash Table","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/hash-table","content":"","keywords":"","version":"Next"},{"title":"Implementation​","type":1,"pageTitle":"Hash Table","url":"/cs-notes/data-structures-and-algorithms/hash-table#implementation","content":" Hash table is a very efficient data structure, it is similar to array, it provides an efficient retrieval of an element in constant O(1) time. While array uses index to retrieve specific element, it can change anytime when we modify the array. Hash table uses a unique identifier and this identifier will always map to the same value.  Under the hood, hash table is implemented using an array. Array with a pre-determined size will be allocated in the memory. Every value associated with a key will be stored in that array. The key act as a determinant that will determine in which position in the array should we store the value. This is why the key should be unique, to ensure the hash table doesn't store different value in the same position in the array.  However, we don't use key directly to decide the position of value in the array, we will use something called hash function.  Hash Function​  Hash function is a mathematical function that takes input and output a hash code, which is an integer value that can be used as the index of the element in the array. Depending on the hash function logic, the key used as the input doesn't have to be number. The hash function's implementation handles the conversion to a numerical hash code.  Hash function used doesn't have to be an advanced cryptographic function, the point is, it must be able to produce a good distribution of hash codes, minimizing the chance of different key producing same hash code. When different key produced same hash code, therefore having same index in the array, this is called collision.  ","version":"Next","tagName":"h3"},{"title":"Collision​","type":1,"pageTitle":"Hash Table","url":"/cs-notes/data-structures-and-algorithms/hash-table#collision","content":" There are many techniques to mitigate collision in hash table, one of the technique is chaining. Chaining is a technique where in the array inside the hash table, a linked list is used. Linked list provide a way to chain together collided element, so that when an element collide, it will be appended to the list.  Another technique to mitigate collision, which is a trivial one is just increasing the array size. Hash function generates hash code which will be used to represent index in the array, the hash function typically consider the size of array, this is to ensure the hash function doesn't produce out of bound index. By increasing the array size, we can lower the chance for collision.  ","version":"Next","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"Hash Table","url":"/cs-notes/data-structures-and-algorithms/hash-table#example","content":" One of a simple hash function for hash table is the division hash function. The formula is : hash code=key % array size\\text{hash code} = \\text{key } \\% \\text{ array size}hash code=key % array size The hash code for some key will be obtained by calculating the remainder (% modulo) of dividing the key by the array size.  For example, when we have key = 10 and the array size = 1000, the hash code will be 10 % 1000=310 \\space \\% \\space 1000 = 310 % 1000=3, which mean the value associated with the key will be stored in the index 3 of the array.    Lastly, we may also put the key inside the array, so that we can identify which key does a value belong to. ","version":"Next","tagName":"h3"},{"title":"Graph","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/graph","content":"","keywords":"","version":"Next"},{"title":"Terminology​","type":1,"pageTitle":"Graph","url":"/cs-notes/data-structures-and-algorithms/graph#terminology","content":" Vertex / Vertices : Also called node, it is a fundamental element in graph, it is a single object or entity which a graph has. Each vertex can connect to each other.Edges : The connections or relationships between vertices in a graph is called edge. They represent the associations or interactions between the entities represented by the vertices.Adjacent : Two nodes are considered adjacent to each other, when there is an edge connecting them directly.Neighbors : All the nodes that are directly connected to a given node by an edge.    Degree : The number of edges incident to a vertex.Path : A sequence of vertices connected by edges, where each vertex in the path is adjacent to the next vertex. The length of a path is the number of edges it contains.Cycle/Loop : A closed path in a graph that starts and ends at the same vertex. It consists of at least three vertices and at least three edges.Circuit : A circuit is a closed path in a graph that may or may not repeat vertices or edges. It starts and ends at the same vertex, but it may traverse some edges multiple times.Euler Path/Circuit : An Euler path is a path in a graph that visits every edge exactly once. An Euler circuit is a closed path that visits every edge exactly once and starts and ends at the same vertex. In other words, an Euler circuit is an Euler path that covers all the vertices of the graph.  ","version":"Next","tagName":"h3"},{"title":"Types of Graph​","type":1,"pageTitle":"Graph","url":"/cs-notes/data-structures-and-algorithms/graph#types-of-graph","content":" Graph can be classified based on various characteristics :  Directed​  A graph where its edges have a specific direction, indicating a one-way relationship between nodes. The edges can be traversed only in the direction specified by the edge.  In a directed graph, the degree is divided into the indegree (number of incoming edges) and the outdegree (number of outgoing edges) of the vertex.  Undirected​  The opposite of directed, its edges have no specific direction and represent a two-way relationship between nodes. The edges can be traversed in both directions.    In an undirected graph, the degree is simply the count of edges connected to the vertex.  Weighted​  Each edge in the graph is assigned a weight or a cost, representing the strength, distance, or any other quantitative value associated with the relationship between nodes.  Unweighted​  On the other hand, unweighted graph's edges have equal importance and do not carry any additional information or weight.    Cyclic​  A cyclic graph contains cycles or loops, meaning that it is possible to traverse a path in the graph that starts from a node and returns to the same node without repeating any edges.  Acyclic​  An acyclic graph has no cycles or loops. It is not possible to traverse a path in the graph that starts from a node and returns to the same node without repeating any edges.    Connected​  A connected graph is one in which there is a path between every pair of nodes. In other words, there are no isolated nodes or disconnected components in the graph.  Disconnected​  A disconnected graph is a type of graph where there are at least two vertices that are not connected by any path within the graph.    Bipartite​  A bipartite graph is a graph whose nodes can be divided into two disjoint sets such that no two nodes within the same set are connected by an edge. In other words, the graph can be colored using only two colors without any adjacent nodes having the same color.   Source : https://rohithv63.medium.com/graph-algorithm-bipartite-graph-dfs-f7f6a4afed4c  Complete​  A complete graph is one in which there is an edge between every pair of distinct nodes. In other words, all nodes in the graph are connected to each other.   Source : https://www.tutorialscan.com/datastructure/complete-graph/  Depending on the problem, a graph may have multiple characteristics together. For example, a tree is a special type of graph where it is undirected and acyclic. We can traverse from top to bottom or vice versa and it has no cycle in it. A graph is a more generalized tree, it is more flexible, and it can have arbitrary connections and structures.  ","version":"Next","tagName":"h3"},{"title":"Graph Representation​","type":1,"pageTitle":"Graph","url":"/cs-notes/data-structures-and-algorithms/graph#graph-representation","content":" To represent graph, we will need a way to represent their edge and the vertices which stores values.  Adjacency Matrix​  An adjacency matrix is a two-dimensional matrix where rows and columns represent the nodes of the graph. The value at position (i, j) in the matrix indicates whether there is an edge between nodes i and j (typically indicated by 1 if they are connected and 0 if they are not connected). This representation is useful for dense graphs (graphs with many edges) and allows for efficient lookup of edge existence (multidimensional array indexing). However, it requires O(V2)O(V^2)O(V2) space, where VVV is the number of vertices.   Source : https://www.geeksforgeeks.org/add-and-remove-edge-in-adjacency-matrix-representation-of-a-graph/  Adjacency List​  An adjacency list represents a graph as an array of lists or arrays. Each element of the array corresponds to a node, and the list at each index contains the neighbors or adjacent nodes of that node. This representation is efficient for sparse graphs (graphs with fewer edges) as it requires space proportional to the number of edges. It allows for efficient traversal of neighbors for each node.   Source : https://www.lavivienpost.com/implement-weighted-graph-as-adjacency-list/  For example, we have an array of nodes [[2, 3], [1, 3], [1, 2, 4], [3]], where node i correspond to index i - 1 in the array. The node 1 should correspond to index 0, which has 2 and 3.  Here is the comparison of the time complexity of adjacency matrix and list, where EEE and VVV is the number of edges and the number of vertices, respectively.   Source : https://en.wikipedia.org/wiki/Graph_(abstract_data_type) ","version":"Next","tagName":"h3"},{"title":"Heap","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/heap","content":"","keywords":"","version":"Next"},{"title":"Binary Heap​","type":1,"pageTitle":"Heap","url":"/cs-notes/data-structures-and-algorithms/heap#binary-heap","content":" The heap implementation using binary tree is called binary heap. Binary tree used for binary heap is a complete binary tree, which is a binary tree in which all levels except possibly the last level are completely filled, and all nodes are as left as possible. This means that the elements in a binary heap are inserted from left to right across each level of the tree.   Source : https://www.andrew.cmu.edu/course/15-121/lectures/Trees/trees.html  Heap data structures maintain collection based priority, the common priority is the maximum or minimum value. This mean we can prioritize larger value first or smaller value first. The heap in which it prioritizes larger value first is called max-heap, while the one that prioritizes smaller value first is called min-heap.  A heap data structures must satisfy the heap property, which states that for a max heap, every parent node has a value greater than or equal to its children, and for a min heap, every parent node has a value less than or equal to its children.  This property implies that the most prioritize element will be on the top of the tree, this is because every parent node must have larger priority than its children. This implication is what makes heap is very efficient for getting the most prioritized value in some collection, to get the value, we will just extract the root of the tree, which should run in constant O(1) time.   Source : https://www.geeksforgeeks.org/heap-data-structure/  Restoring Heap Property​  When we add element to the heap, it is initially placed in the next available position at the bottom-rightmost spot of the tree. Then, it is compared with its parent node, and if the heap property is violated, the element is swapped with its parent until the heap property is satisfied (can even be swapped up to the root). This process is called up-heap or bubble-up operation.  Similarly, when the root element is extracted from the heap, which is the highest priority element, the last element in the heap is moved to the root position. Then, this element is compared with its children, and if the heap property is violated, it is swapped with the larger (in a max heap) or smaller (in a min heap) child until the heap property is satisfied. This process is called down-heap or bubble-down operation.  In conclusion, heap will always operate on the complete binary tree and heap property. After doing any operation on heap, we will always reorganize the tree to make it a complete binary tree. After that, we will restructure the heap so that the highest priority element is on the root, to adhere the heap property.   Source : https://www.tutorialspoint.com/data_structures_algorithms/heap_data_structure.htm  note Another commonly used term in heap data structure is the heapify, which mean building a heap from an array of elements or restoring the heap property in a heap that has been partially or completely violated.  ","version":"Next","tagName":"h3"},{"title":"Binary Heap Implementation​","type":1,"pageTitle":"Heap","url":"/cs-notes/data-structures-and-algorithms/heap#binary-heap-implementation","content":" Binary heap uses array to store the element, however, the array uses some indexing formula to access an element, making it resembles a parent-child relationship in binary tree.  For any given element at index i in the array, its left child can be found at index 2i + 1, and its right child can be found at index 2i + 2. Similarly, the parent of an element at index i can be found at index ⌊(i − 1) / 2⌋ (floor operation).   Source : https://en.wikipedia.org/wiki/Binary_heap  The time complexity for binary heap :  Heapify : Building a heap from an unsorted array can result in worst-case scenario of O(n), while just storing the heap property can result in O(n).Peek : An operation that returns the most prioritized element, which happen in O(1) time. We are just extracting the root of the binary tree without changing anything.Poll : An operation that remove and returns the most prioritized element, also happen in O(1) time, however, we will need to heapify after to restore the heap property.Search : Average and worst-case happens in O(n) time, this is because heap doesn't guarantee element in sorted order.Insertion : Average of O(1) time and the worst-case of O(log n), which happen while heapifying up to the root.Deletion : Deleting non-root element involve searching it first (O(n) time), removing the element, and then heapifying after (O(log n) time).  ","version":"Next","tagName":"h3"},{"title":"Min-Max Heap​","type":1,"pageTitle":"Heap","url":"/cs-notes/data-structures-and-algorithms/heap#min-max-heap","content":" Min-max heap is a variation of the binary heap data structure that supports both minimum and maximum operations efficiently. Unlike a standard binary heap that focuses on either the minimum or maximum element, a min-max heap allows for quick access to both the minimum and maximum elements.  Similar to binary heap, the elements are organized in complete binary tree. The heap property is extended to two levels : the even levels store the minimum elements, while the odd levels store the maximum elements.  At even levels (0, 2, 4, etc.), each node must be smaller than or equal to its parent and its children.At odd levels (1, 3, 5, etc.), each node must be larger than or equal to its parent and its children.  Heapify operations after deleting or inserting in a min-max heap are more complex than in a standard binary heap. When inserting an element, it is initially placed at the next available position in the tree and then compared with its parent and ancestors at even and odd levels. The element is swapped with its ancestors if the heap property is violated. Similarly, when deleting an element, it is replaced with the last element in the heap and then moved up or down the tree based on the heap property.   Source : https://stackoverflow.com/questions/53888694/try-to-understand-delete-min-of-min-max-heap ","version":"Next","tagName":"h3"},{"title":"Minimum Spanning Tree","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/minimum-spanning-tree","content":"","keywords":"","version":"Next"},{"title":"Prim's Algorithm​","type":1,"pageTitle":"Minimum Spanning Tree","url":"/cs-notes/data-structures-and-algorithms/minimum-spanning-tree#prims-algorithm","content":" Prim's algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph.  function prim(graph, start): MST = set() PQ = PriorityQueue() visited = set() PQ.add(start, 0) while PQ is not empty: current = PQ.extractMin() if current is not in visited: visited.add(current) MST.add(current) for each neighbor in current's neighbors: if neighbor is not in visited: PQ.add(neighbor, weight(current, neighbor)) return MST   Similar to Dijkstra's algorithm, it uses priority queue to help to prioritize the minimum weight and a visited set to keep track the visited vertex.  It begins from the start node, which is initially added to the PQ and extracted immediately.Ensure that we only visit the node which hasn't been visited.If it hasn't been visited, add it to the visited set and MST set.Iterate over each neighbor of the current vertex, make sure that only neighbors that have not been visited yet are added.   Source : https://en.wikipedia.org/wiki/Prim%27s_algorithm  The complexity depends on the graph representation (e.g., adjacency list or adjacency matrix) and the priority queue implementation (e.g, binary heap or Fibonacci heap). Using adjacency list and binary heap, we can obtain O(Elog⁡V)O(E \\log V)O(ElogV) time complexity. Updating a key in the heap takes O(log⁡V)O(\\log V)O(logV), updating it for each edge takes O(Elog⁡V)O(E \\log V)O(ElogV). The space complexity would be O(V)O(V)O(V), which comes from the visited set storing the visited vertices and the heap itself. ","version":"Next","tagName":"h3"},{"title":"Linked List","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/linked-list","content":"","keywords":"","version":"Next"},{"title":"Linked List Operation​","type":1,"pageTitle":"Linked List","url":"/cs-notes/data-structures-and-algorithms/linked-list#linked-list-operation","content":" The first element in a linked list is often referred as the head of list, while the last element is called tail.  Access &amp; Search / Traversal : Accessing or searching an element involve going in the linked list from the head to the tail, we can access it by position by counting how many elements we have encountered in the list. Searching an element in linked list is basically similar to linear search, it involves checking each node in the list one by one.Insertion : There are three different insertion in linked list. The first is inserting an element at the beginning of the list, we can do this by making a new head and make it points to the old head. The second is inserting an element at the end of the list, we can do it easily by making the tail or the last element point to the new element. The third is inserting an element at a specific position in the middle, this involves traversing the list to the specific position, make the node in the position to point to the new element instead, and we will make the new element points to the originally next node.Deletion : We can either delete at the beginning, at the end, or at specific position in the middle. The operation is quite similar to insertion, the difference is we remove element instead of adding new element.  In the worst case scenario, all the linked list operation has the time complexity of O(n)O(n)O(n). The worst scenario is when the element we are looking for is at the end of the linked list.   Source : Insertion, deletion  ","version":"Next","tagName":"h3"},{"title":"Doubly Linked List​","type":1,"pageTitle":"Linked List","url":"/cs-notes/data-structures-and-algorithms/linked-list#doubly-linked-list","content":" The linked list we talked about previously is a singly linked list, a node can only point to the next node. In contrast, a doubly linked list, is a type of linked list where each node can point to the next and the previous node. This allows for traversal in both directions, from the head to the tail and from the tail to the head.  Being able to traverse in reverse direction allows us for efficient operation. For example, if we are going to search element somewhere at the end of the list, we will need to traverse from the head all the way up to the latter element. Using doubly linked list, we can access an element from the tail or behind and use the previous reference to traverse.  The presence of tail and previous reference in doubly linked list allows for time complexity reduction from O(n)O(n)O(n) to O(n/2)O(n / 2)O(n/2).   Source : https://www.boardinfinity.com/blog/a-detailed-walkthrough-of-doubly-linked-list/ (with modification)  ","version":"Next","tagName":"h3"},{"title":"Circular Linked List​","type":1,"pageTitle":"Linked List","url":"/cs-notes/data-structures-and-algorithms/linked-list#circular-linked-list","content":" A circular linked list is a type of linked list where the last node in the list never points to a null reference, it always points back to the first node, forming a circular loop structure. This type of linked list is typically used for specific application where we need a circular data structure.   Source : https://study.com/academy/lesson/circularly-linked-lists-in-java-creation-uses.html  ","version":"Next","tagName":"h3"},{"title":"Skip List​","type":1,"pageTitle":"Linked List","url":"/cs-notes/data-structures-and-algorithms/linked-list#skip-list","content":" A singly linked list can be very inefficient when the element we are looking for is at last node of the list. We know that linked list is a linear-like data structure, all the operation, at the worst, happen in linear time complexity. Adding a reference to the tail like what doubly linked list is doing, help us improve the performance.  A skip list is a data structure based on linked list which is included with additional reference or pointer. A skip list is different with the traditional linked list. Skip list maintain a collection of sorted data, it is a data structure that provides an efficient data structure for searching, inserting, and deleting elements in a sorted collection.  In a skip list, the elements are stored in linked lists at different levels or layers. The bottom level contains all the elements in sorted order, while the higher levels contain a subset of the elements, skipping over some elements. Each element in the list has a tower of pointers, linking it to the same element in higher levels.  The skip pointers enable efficient searching by allowing the algorithm to &quot;skip&quot; over large portions of the list, reducing the number of comparisons needed. When searching for an element, the algorithm starts from the top level and moves forward until it finds an element that is greater than or equal to the target. It then moves down to the next level and continues the search within the sublist until it either finds the target or reaches the bottom level.  The choice of which element we choose as the skip pointer can happen in probabilistic or deterministic manner. Overall, the average time complexity of skip list is O(log⁡n)O(\\log n)O(logn).   Source : https://en.m.wikipedia.org/wiki/File:Skip_list_add_element-en.gif ","version":"Next","tagName":"h3"},{"title":"Queue","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/queue","content":"","keywords":"","version":"Next"},{"title":"Queue Operation​","type":1,"pageTitle":"Queue","url":"/cs-notes/data-structures-and-algorithms/queue#queue-operation","content":" Following the FIFO principles, we can remove element in the front (also called head) and add element in the rear (also called back or tail) of the queue.  There are two main operation in queue data structure :  Enqueue : Adds an element to the rear of the queue.Dequeue : Removes the element from the front of the queue.   Source : https://www.geeksforgeeks.org/queue-data-structure/  A queue data structure can be implemented efficiently using doubly linked list, this is because doubly linked list behavior aligns with queue operation. Doubly linked list has a head and a tail, enqueuing an element, or adding element to the rear of the queue is just inserting element in the tail. Dequeuing an element, or removing an element from the front of the queue is just a linked list deletion in the head. Inserting or deleting element in the head or tail of a linked list can be done in constant time.  Using doubly linked list, we can even create double ended queue, which is a queue data structure that supports enqueue and dequeue in the front or the rear of the queue.   Source : https://www.geeksforgeeks.org/implementation-deque-using-doubly-linked-list/  ","version":"Next","tagName":"h3"},{"title":"Circular Queue​","type":1,"pageTitle":"Queue","url":"/cs-notes/data-structures-and-algorithms/queue#circular-queue","content":" Circular Queue, also known as a circular buffer, is a variation of the queue data structure where the element are stored circularly, the last element is connected to the first element, forming a loop. Similar to traditional queue, it follows the FIFO principle, we can only enqueue in the rear and dequeue in the front.   Source : https://www.geeksforgeeks.org/introduction-to-circular-queue/  The circular nature of circular queue enables continuous enqueue and dequeue operations. When we ran out of space in the queue (when rear pointer touch front pointer), the enqueue operation will overwrite anything in the front of the queue. This is useful for scenario where we need to store data in a queue-like structure, but we don't want it to take up to many spaces, so that we will overwrite the oldest data.  ","version":"Next","tagName":"h3"},{"title":"Priority Queue​","type":1,"pageTitle":"Queue","url":"/cs-notes/data-structures-and-algorithms/queue#priority-queue","content":" Priority Queue is a data structure that stores a collection of elements, each associated with a priority value (e.g., larger number have larger priority). Unlike a regular queue, where elements are processed in a first-in-first-out (FIFO) manner, a priority queue retrieves elements based on their priority.   Source : https://www.javatpoint.com/ds-priority-queue  note Priority queue is conceptually different with heap, a priority queue is just a concept where each element has some priority. Heap is often used to implement a priority queue, it is a specific implementation of priority queue using binary tree. ","version":"Next","tagName":"h3"},{"title":"Prefix Sum","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/prefix-sum","content":"","keywords":"","version":"Next"},{"title":"Sum of K First Elements​","type":1,"pageTitle":"Prefix Sum","url":"/cs-notes/data-structures-and-algorithms/prefix-sum#sum-of-k-first-elements","content":" The objective of this problem is to calculate the sum of the first K elements in an array. Given an array [1, 4, 5, 7], if we have a value of K equal to 2, we need to return the sum of the first two elements. In this case, the first two elements are 1 and 4, and their sum is 5. Hence, when K is 2, the expected result is 5.  The complexity of this problem increases when we introduce multiple values of K. In addition to the original array, we are now given an array of K that contains varying numbers.  Input : arr = [1, 4, 5, 7] k = [0, 2, 3, 1]  Output : result = [0, 5, 10, 1]  Explanation:  For k = 0, the sum is 0 (no elements considered).For k = 2, the sum is 1 + 4 = 5 (considering the first two elements).For k = 3, the sum is 1 + 4 + 5 = 10 (considering the first three elements).For k = 1, the sum is 1 = 1 (considering the first one elements).  Naive Solution​  The pseudocode for this :  function sumKElements(arrayOfNumber, arrayOfK) result = [] sum = 0 for each k in arrayOfK: for the first k element in arrayOfNumber: sum = sum + element append sum to result sum = 0 return result   We would sum up to K for each K in the arrayOfK. While this is a correct solution, it is very inefficient. As we can see in the explanation above, we did a lot of repeated work. For example, when we calculate k = 3, we would need to add 1 + 4 + 5. When we calculate k = 2, we would need to add 1 + 4.  The worst case scenario occurs when we got an array of K, where each K is a large number, making us need to sum the whole array again at each K. The worst-case scenario time complexity for this algorithm would be O(n∗k)O(n * k)O(n∗k), where nnn is the length of the array and kkk is the length of the array of K.  Prefix Sum Approach​  This problem can be optimized using prefix sum approach. We will first generate the prefix sum array, and then we can just return the prefix sum array at index k - 1 (we subtract with 1 because array index starts from 0) for each K in the arrayOfK easily.  function sumKElements(arrayOfNumber, arrayOfK) result = [] prefixSum = [0] for each number in arrayOfNumber: lastSum = get last element of prefixSum currentSum = lastSum + number add currentSum to prefixSum for each k in arrayOfK: add prefixSum[k] to result return result   Here is the illustration :    To generate a prefix sum, you iterate through the array of numbers and at each iteration, add the current element to the sum accumulated so far. We will initially put 0, indicating we have accumulated 0 so far. The result of each sum is appended to the prefixSum array, where each element represents the cumulative sum of the arrayOfNumber up to that specific element's index.  Generating prefix sum basically iterate the whole array, it takes O(n)O(n)O(n) time, where nnn is the length of the arrayOfNumber. The iteration of arrayOfK will result in O(k)O(k)O(k), where kkk is the length of the arrayOfK. With both iteration together, this results in O(n+k)O(n + k)O(n+k) time complexity ","version":"Next","tagName":"h3"},{"title":"Search","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/search","content":"","keywords":"","version":"Next"},{"title":"Linear Search​","type":1,"pageTitle":"Search","url":"/cs-notes/data-structures-and-algorithms/search#linear-search","content":" Linear search, also known as sequential search, is the simple and intuitive search algorithm that sequentially checks each element in a collection of data until a match is found or the entire collection has been traversed. It is applicable to both sorted and unsorted data.  function linearSearch(array, target): for i from 0 to length(array) - 1 do: if array[i] equals target then: return i // Return the index where the target is found end if end for return -1 // Return -1 if the target is not found end function   Linear search iterates from the beginning of the array/list, then check each element if it is the element we are looking for. If so, we will return the element and complete the search, else continue with the iteration. If we don't find the target element, we typically return -1 to indicate the target is not found.  Linear search results in best O(1)O(1)O(1) time, when the target element is at the beginning, average of O(n)O(n)O(n) time, and the worst-case scenario in O(n)O(n)O(n) time, which occurs when the target element is at the end of the collection. Linear search does nothing other than checking if the element in the array is equal to the target, it requires constant extra memory.  ","version":"Next","tagName":"h3"},{"title":"Binary Search​","type":1,"pageTitle":"Search","url":"/cs-notes/data-structures-and-algorithms/search#binary-search","content":" Binary search is a searching algorithm, used for searching an element in a sorted collection.  Binary search assumes that the collection is in sorted order, the idea is that, we don't need to look for the target element in the range of elements where they are smaller than the target element. Binary search will simply eliminate the portion of element that we think doesn't make sense to look in for.  function binarySearch(array, target): low := 0 high := length(array) - 1 while low &lt;= high do: mid := (low + high) / 2 if array[mid] equals target then: return mid // Return the index where the target is found else if array[mid] &lt; target then: low := mid + 1 else: high := mid - 1 return -1 // Return -1 if the target is not found end function   Binary search starts checking in the middle, it checks if the element in the middle is larger or smaller than the target element. If it is smaller, then it will be impossible for our target element to be present in the smaller element range. We will eliminate that portion and search the other range of elements. This will effectively divide the search space by half until the target is found, or the target does not exist in the collection.  Binary search efficiently reduces the search space by half in each iteration, resulting in a logarithmic O(log⁡n)O(\\log n)O(logn) time complexity, where nnn is the number of elements in the sorted array. It is significantly faster than linear search for large datasets. However, it requires a sorted collection, sorting an unsorted collection first would result longer than linear search, since a typical sorting algorithm requires O(nlog⁡n)O(n \\log n)O(nlogn) time.  Similar to linear search, binary search require constant memory space. No matter how large the collection is, the low, mid, and high pointer will always store an integer index.   Source : https://tenor.com/view/binary-search-sequence-search-gif-20595028 ","version":"Next","tagName":"h3"},{"title":"Set","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/set","content":"","keywords":"","version":"Next"},{"title":"Set Implementation​","type":1,"pageTitle":"Set","url":"/cs-notes/data-structures-and-algorithms/set#set-implementation","content":" Set data structure can be implemented in many ways. The simplest way to implement set is just to use a simple array or list. When an element is added and its already present, we will ignore the operation. However, this implementation is inefficient, assuming the array is not sorted, we would need to perform search algorithm like linear search, which has a time complexity of O(n), to determine if an element exists in the collection.  A simple yet efficient implementation of set is the hash set. Hash set is implementation of set data structure using hash table. To determine if an element already present or not, we will use hash function that produces hash code. This allows for efficient checking, addition, and removal operations with a constant time complexity of O(1). ","version":"Next","tagName":"h3"},{"title":"Recursion","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/recursion","content":"","keywords":"","version":"Next"},{"title":"Example​","type":1,"pageTitle":"Recursion","url":"/cs-notes/data-structures-and-algorithms/recursion#example","content":" In math, factorial (denoted as n!n!n!) is an operation that give us the product of all positive integers from the number nnn up to 111. If we ask, what is the factorial of 555 ? Then it will be 5×4×3×2×15 \\times 4 \\times 3 \\times 2 \\times 15×4×3×2×1, which is equal to 120120120.  Factorial can be seen as a big problem that requires a smaller problem. For example, to calculate factorial of 555, we will also need to calculate factorial of 444. The factorial of 555 which is the 5×4×3×2×15 \\times 4 \\times 3 \\times 2 \\times 15×4×3×2×1 can be rewritten as 5×4!5 \\times 4!5×4!. Expanding further, the factorial of 4, which is 4×3×2×14 \\times 3 \\times 2 \\times 14×3×2×1 is basically just 4×3!4 \\times 3!4×3!. If we keep expanding, what is factorial 333, what is factorial 222, up to factorial 111, then the problem will be the same as the original factorial 555 question.    In other word, we can solve the factorial 555 problem by breaking it down into smaller subproblem, each calculating its own factorial and we will combine all the result to solve the original problem.  In recursion, we keep breaking down the problem into smaller subproblems, the simplest case or the most fundamental condition of the problem which can be directly solved without further recursion is called the base case.  The factorial problem, the last question we will ask is the factorial of 111, this is because the factorial of 111 require no additional subproblem. The factorial of 111 will just be 111, no other subproblem.  Here is an example of a recursive function in Python programming language. It is a recursive function that calculates factorial.  def factorial(n): # Base case if n == 1: return 1 # Recursive call else: return n * factorial(n - 1) factorial(5) # should be equal to 120   In the code, when we call factorial(5), it will first check if the input or n is equal to 1 (the base case). If yes, it will just return 1, if no, it will return that input times whatever the result of factorial(n - 1) is. Because n is 5, it will be conditioned to the else statement, which will return 5 * factorial(4). The factorial(4) will also return 4 * factorial(3). This will go on until we reach n == 1.  ","version":"Next","tagName":"h3"},{"title":"Call Stack​","type":1,"pageTitle":"Recursion","url":"/cs-notes/data-structures-and-algorithms/recursion#call-stack","content":" When we call a function or methods, the program's runtime environment will keep track the function calls and execution contexts. The data structure used to store the information is a stack-like data structure called call stack.  The purpose of using stack data structure is to ensure that the most recently called function is the first one to complete its execution and be removed from the stack (LIFO principles). This order is crucial for maintaining the correct flow and sequence of function calls, such as the case when we expect a function to return something.  While recursion seems like an elegant way to solve big problem by breaking it down into subproblem, it can be quite expensive. Recursion is a function that call itself, because it is a function, every time a recursion happens, the function call will be push into the call stack. In addition, each function call may also contain local variables and function parameters. This is what makes recursion memory-intensive and potentially expensive in terms of computational resources.  Call stack size is typically fixed, when we keep doing recursive call and the call stack exceed its allocated size, then an error called stack overflow will occur. This error indicates that there is insufficient space on the call stack to accommodate additional function calls and their associated stack frames. Stack overflow error can also occur in a recursive function where we do not have base case or a valid one. In such a situation, the recursive calls continue indefinitely, similar to an infinite while loop.   Source : https://dev.to/muirujackson/call-stack-recursion-40eh  ","version":"Next","tagName":"h3"},{"title":"Linked List Traversal​","type":1,"pageTitle":"Recursion","url":"/cs-notes/data-structures-and-algorithms/recursion#linked-list-traversal","content":" Traversing a linked list from the head to the tail can be implemented using a recursive function. Linked list, contains a node that has reference to the next node, the next node then have another reference to the next node again. This recursive nature allows us to traverse the linked list by recursively visiting each node until we reach the end of the list.  To make a recursive function, we will need a base case, or the problem where another recursive call is not needed. When it comes to traversing a linked list until the tail, the base case occurs when we reach the tail itself, indicating the conclusion of the problem and eliminating the need for another recursive call.  Here is a Python code for traversing linked list  def recursive_traversal(node): # Base case: check if the current node is None (end of the list) if node is None: return # Recursively call the traversal function on the next node recursive_traversal(node.next) recursive_traversal(some_linked_list_head)   In this code, we do not implement the linked list. Assume that the linked list has attributes data which contains the value of the node, and next, which is the reference to the next node.  The code checks if the given linked list node is null (known as None in Python), else we will call the function itself passing the reference of the next node. When we encounter a null node, we will complete the function.    tip The recursive nature of linked list is similar to the Matryoshka doll. ","version":"Next","tagName":"h3"},{"title":"Sliding Window","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/sliding-window","content":"","keywords":"","version":"Next"},{"title":"Example Algorithm​","type":1,"pageTitle":"Sliding Window","url":"/cs-notes/data-structures-and-algorithms/sliding-window#example-algorithm","content":" Here is a pseudocode example of sliding window algorithm that obtain a window of given size in an array.  function slidingWindow(arr: Array, windowSize: Int) leftPointer = 0 rightPointer = leftPointer + windowSize - 1 while (rightPointer &lt; size of arr): leftPointer = leftPointer + 1 rightPointer = rightPointer + 1    ","version":"Next","tagName":"h3"},{"title":"Shortest Path","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/shortest-path","content":"","keywords":"","version":"Next"},{"title":"Dijkstra's Algorithm​","type":1,"pageTitle":"Shortest Path","url":"/cs-notes/data-structures-and-algorithms/shortest-path#dijkstras-algorithm","content":" Dijkstra's Algorithm is a popular algorithm to find the shortest path between a source vertex to all the other vertex in a weighted graph. Dijkstra's algorithm works by exploring the graph from the source to each target, the cost of each traversal between nodes is kept track.  Dijkstra's algorithm explores the path systematically. When there are many possible paths, the algorithm prioritize the path with the currently minimum cost. It continues along this path before moving on to others. At the end, the algorithm will explore all the possible paths.  The systematic behavior of Dijkstra's algorithm is similar to BFS traversal, it uses a queue data structure, specifically the priority queue, which is often implemented using min-heap to keep track of the path with the currently minimum cost. It is also considered as greedy algorithm, because it traverses the optimal path first.  Here is the pseudocode :  function Dijkstra(graph, source): visited = set() distance = [] previous = [] heap = minHeap() for each vertex v in graph: distance[v] = infinity previous[v] = undefined distance[source] = 0 heap.add(source, 0) while heap is not empty: currentVertex = heap.poll() if currentVertex is visited: continue visited.add(currentVertex) for each neighbor of currentVertex: if neighbor is visited: continue cost = distance[currentVertex] + weight(currentVertex, neighbor) if cost &lt; distance[neighbor]: distance[neighbor] = cost previous[neighbor] = currentVertex heap.update(neighbor, cost) return distance, previous    Source : https://blog.aos.sh/2018/02/24/understanding-dijkstras-algorithm/  The distance is an array to store the shortest distance from the source vertex to each vertex. The previous is another array used to store the previous vertex that leads to the currently known shortest path from the source vertex to each vertex.We initially make all the distance from the source to each vertex in the graph to infinity, indicating they are yet to be determined. Except for the source vertex, we will make it 0, because that is where we are going to start.We add the source to the min-heap and start the loop by polling it (remove and return the minimum element). Whatever vertex is returned, if that vertex has been visited before (present in visited set), then we will skip it, else we will continue the loop and add it to the visited set.We will iterate through every neighbor of that vertex. Next, we will add the vertex to the heap for next visit, but we will only add the one that hasn't been visited before.We will also calculate the cost by adding the previous tracked distance with the new cost from the currentVertex to the target neighbor. Also update the distance and previous array with the currentVertex and cost and update the neighbor's key with the new cost in the min-heap.When the heap is empty, indicating there are no more paths to traverse, we will return the distance and previous array. The distance array contains the shortest path from source vertex to all other vertex, and the previous array contains the path that correspond to it.  Using binary heap, we will obtain (V+E)log⁡V(V + E) \\log V(V+E)logV time complexity, where VVV is the number of vertices and EEE is the number of edges.  Initializing the distance and previous arrays is O(V)O(V)O(V).The main part of the algorithm is the while loop, which runs until the priority queue becomes empty. Adding vertex to the heap takes O(log⁡V)O(\\log V)O(logV) time, if we add a total of VVV number of vertices, then it will be O(Vlog⁡V)O(V \\log V)O(VlogV)Within the while loop, we iterate over the neighbors of the current vertex. In the worst case, each edge is considered once, resulting in a total of O(E)O(E)O(E) iterations.Performing heap update takes O(log⁡V)O(\\log V)O(logV). Since the total number of vertex-neighbor pairs in the graph is EEE, the time complexity of these operations is O(Elog⁡V)O(E \\log V)O(ElogV).  We can also optimize this using Fibonacci heap instead of binary heap to obtain O(E+Vlog⁡V)O(E + V \\log V)O(E+VlogV) time complexity.  ","version":"Next","tagName":"h3"},{"title":"Pathfinding​","type":1,"pageTitle":"Shortest Path","url":"/cs-notes/data-structures-and-algorithms/shortest-path#pathfinding","content":" Pathfinding is a similar problem to shortest path, it finds the most efficient path from a starting point (source) to a destination point (target). The efficiency of a path is typically measured based on certain criteria.  Dijkstra's algorithm can be used for pathfinding, it works for non-negative edge weights. It explores the graph in a breadth-first manner, iteratively expanding the vertices with the smallest known distance.  A* Search​  A* search algorithm is a popular pathfinding algorithm, which is an extension of Dijkstra's algorithm. While Dijkstra solely considers the weight of edges, a* search algorithm adds another consideration and prioritize that seem more likely to lead to the goal.  The priority value is calculated by :  f(v)=g(v)+h(v)f(v) = g(v) + h(v)f(v)=g(v)+h(v)  Where :  f(v)f(v)f(v) : referred as f-score or function scoreg(v)g(v)g(v) : the cost accumulated so farh(v)h(v)h(v) : heuristic estimate of the remaining cost from the current vertex to the target vertex.  The key point of a* is the h(v)h(v)h(v), Dijkstra's algorithm is the case where the h(v)h(v)h(v) is 0, or it doesn't consider the heuristic estimate and solely depends on the weights.  The heuristic estimate is an approximation of the distance or cost from some vertex to the goal. The specific implementation of the function depends on the problem. For example, in a map navigation problem, it can be a straight line distance.  The heuristic estimate helps us to only focus on the path that might lead to the target. A* search algorithm doesn't visit every node, it stops when it reached the goal. This is a reason of why a* is more suited for goal-directed search problems, where we don't need to know the distance to all the vertex, unlike Dijkstra's algorithm.  Here is the pseudocode :  function AStarSearch(start, goal): openSet = PriorityQueue() openSet.add(start) cameFrom = empty map gScore = map with default value of infinity gScore[start] = 0 fScore = map with default value of infinity fScore[start] = heuristicCostEstimate(start, goal) while openSet is not empty: current = openSet.removeMin() if current is goal: return reconstructPath(cameFrom, current) for each neighbor in neighbors(current): tentativeGScore = gScore[current] + distance(current, neighbor) if tentativeGScore &lt; gScore[neighbor]: cameFrom[neighbor] = current gScore[neighbor] = tentativeGScore fScore[neighbor] = gScore[neighbor] + heuristicCostEstimate(neighbor, goal) if neighbor not in openSet: openSet.add(neighbor) return failure function reconstructPath(cameFrom, current): totalPath = [current] while current in cameFrom: current = cameFrom[current] totalPath.appendAtFirst(current) return totalPath    Source : https://github.com/vittin/A-Star  We can see that the path frequently approach the wall that closely covers the target due to the heuristic function. The algorithm may perceive those paths as potentially shorter or more promising.  We initialized some data structures, the openSet is a data structure, which is a priority queue that keeps track of the vertices that have been discovered but have not yet been fully explored. Similar to Dijkstra's algorithm, we will initialize the distance as infinity. The heuristic function takes source and target vertex, the implementation depends on the specific problem.The algorithm runs while the openSet is not empty. We will always start from the highest priority, or the one that has the lowest f-score.If the current vertex is not the goal, the algorithm explores its neighbors. The tentative g-score represents the cost of reaching a target vertex from the starting vertex via the current path being explored. We will compare each g-score of the neighbor, if we found a lower g-score than the current path we are exploring, we will continue in that path, else we will continue with the current path.If we found the better path, we will update the cameFrom, gScore, and fScore. We will also update that neighbor if it is not already in the open set.If the open set becomes empty and the goal vertex has not been reached, the algorithm returns &quot;failure&quot; to indicate that no path was found.The successful search returns the path constructed by the reconstructedPath function. The reconstructedPath construct the path that we traversed in reverse manner. It starts from the goal and it finds the source from the cameFrom map.  The complexity of a* search algorithm, again, it depends on the heuristic function. In the terms of vertices and edges in the graph, a* search algorithm results in the worst-case performance of O(Elog⁡V)O(E \\log V)O(ElogV), with the space complexity being O(V)O(V)O(V).  The priority queue operation (e.g., using heap) requires O(log⁡V)O(\\log V)O(logV) time to remove an element. In the worst case, the number of elements in the priority queue can be proportional to the number of edges, resulting in O(Elog⁡V)O(E \\log V)O(ElogV) time complexity. The space required is O(V)O(V)O(V), the algorithm needs to store information in the openSet, and other data structures such as the cameFrom, gScore, and fScore maps.  Another way to represent the complexity of a* algorithm is using branching factor (b) and depth of the solution (d). (b) represents the average number of neighbors that a node has in a graph. (d) is the length or number of steps required to reach the goal or desired solution from the starting point. Using these, the overall worst-case time complexity becomes O(bd)O(b^d)O(bd). ","version":"Next","tagName":"h3"},{"title":"Stack","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/stack","content":"","keywords":"","version":"Next"},{"title":"Stack Operation​","type":1,"pageTitle":"Stack","url":"/cs-notes/data-structures-and-algorithms/stack#stack-operation","content":" Stack is a fairly simple data structure, the general implementation typically only includes adding and removing element.  Push : Push is when we add an element to the stack, which is to the top.Pop : Pop is when we remove an element from the stack, which is the topmost element.  Stack data structure can be implemented using an array or linked list, the important thing is we need to follow the LIFO principle. Even if array allow random access and linked list allow insertion in the middle, we can't use these.  Stack operation is done in O(1) or constant time, no matter what the element is, no matter how large the stack is, removing or adding element always performs the same.   Source : https://www.programiz.com/dsa/stack  ","version":"Next","tagName":"h3"},{"title":"Monotonic Stack​","type":1,"pageTitle":"Stack","url":"/cs-notes/data-structures-and-algorithms/stack#monotonic-stack","content":" Monotonic Stack is a variant of stack data structure that maintains a specific order of elements while still adhering to LIFO principles. Monotonic stack enforce a particular order of element, either non-decreasing or non-increasing, from the bottom to the top of the stack.  When we have a non-decreasing or an increasing monotonic stack, this mean from bottom to the top, we must store element in increasing order. When an element doesn't satisfy this condition, we will need to remove them. The process of adding or removing an element is the same as traditional stack.  Here is an example of monotonic stack :   Source : https://itnext.io/monotonic-stack-identify-pattern-3da2d491a61e (with modification)  We have an array of numbers, we want to turn it into a non-decreasing monotonic stack. We will push the element from the array to the stack as long as the element we encounter keep increasing. When we encounter an element smaller than the topmost element in the stack, we will remove every element in the stack that is larger than it. ","version":"Next","tagName":"h3"},{"title":"Topological Sort","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/topological-sort","content":"","keywords":"","version":"Next"},{"title":"Algorithm​","type":1,"pageTitle":"Topological Sort","url":"/cs-notes/data-structures-and-algorithms/topological-sort#algorithm","content":" The algorithm can be implemented using DFS or BFS traversal.  function topologicalSort(graph): mark all vertices as unvisited initialize an empty stack for each vertex in the graph: if the vertex is unvisited: visit(vertex, stack) return reversed(stack) function visit(vertex, stack): mark the vertex as visited for each neighbor of vertex: if the neighbor is unvisited: visit(neighbor, stack) push vertex onto stack   In this code, it is implemented using DFS, in the visit function. For each neighbor of the vertex, if the neighbor is unvisited, the visit function is called recursively on that neighbor. After visiting all the neighbors of a vertex, that vertex is pushed onto the stack. The stack is used to maintain the ordering of the vertices based on the topological sort property.  We will also need to return the reverse order of the stack, to ensure that the top element of the stack represents the first vertex in the topological ordering.  The time complexity for topological sort is O(V+E)O(V + E)O(V+E), where VVV and EEE is the number of vertices and edges, respectively. We will need to traverse the whole graph in order to obtain the vertices ordering. The space complexity is O(V)O(V)O(V) to store the visited vertices. ","version":"Next","tagName":"h3"},{"title":"Sorting","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/sorting","content":"","keywords":"","version":"Next"},{"title":"Bubble Sort​","type":1,"pageTitle":"Sorting","url":"/cs-notes/data-structures-and-algorithms/sorting#bubble-sort","content":" Bubble sort is a very simple and intuitive sorting algorithm, it sorts elements by comparing each of them and swap them if they are in the wrong order.  Here is the pseudocode for bubble sort :  procedure BubbleSort(A: list) n := length(A) for i from 0 to n-2 do for j from 0 to n-i-2 do if A[j] &gt; A[j+1] then swap A[j] and A[j+1] end if end for end for end procedure   We will have outer and inner loop, inside the inner loop, we will compare each adjacent element. Bubble sort results in best of O(n)O(n)O(n) time, average in O(n2)O(n^2)O(n2), and worst in O(n2)O(n^2)O(n2), with the space complexity being O(1)O(1)O(1). The worst-case scenario occurs when we compare and swaps for every pair of elements in the input list.   Source : https://www.doabledanny.com/bubble-sort-in-javascript  ","version":"Next","tagName":"h3"},{"title":"Selection Sort​","type":1,"pageTitle":"Sorting","url":"/cs-notes/data-structures-and-algorithms/sorting#selection-sort","content":" Selection sort divides the input list into two parts: the sorted portion at the beginning and the unsorted portion at the end. The algorithm repeatedly find and select the smallest (or largest) element from the unsorted portion and swaps it with the element at the beginning of the unsorted portion. This process continues until the entire list is sorted.  procedure SelectionSort(A: list) n := length(A) for i from 0 to n-2 do minIndex := i for j from i+1 to n-1 do if A[j] &lt; A[minIndex] then minIndex := j end if end for swap A[i] and A[minIndex] end for end procedure   In the pseudocode, we will find the index of the minimum (or maximum) element. After going to the entire list, we will swap the element at index i (outer loop) with the minimum element. This will be repeated for each element in the list. The algorithm always performs in O(n2)O(n^2)O(n2) time complexity for the best, average, and worst-case scenario, and uses constant O(1)O(1)O(1) memory, this is because, we will always need to swap each element in the list. While it isn't the fastest, it has the advantage of doing fewer swaps compared to bubble sort.   Source : https://matcha.fyi/selection-sort-javascript/  ","version":"Next","tagName":"h3"},{"title":"Insertion Sort​","type":1,"pageTitle":"Sorting","url":"/cs-notes/data-structures-and-algorithms/sorting#insertion-sort","content":" Insertion sort builds the sorted portion of the list incrementally. It iterates through the input list, considering one element at a time and inserting it into its correct position within the already sorted portion of the list. The algorithm repeats this process until the entire list is sorted.  procedure InsertionSort(A: list) n := length(A) for i from 1 to n-1 do key := A[i] j := i - 1 while j &gt;= 0 and A[j] &gt; key do A[j + 1] := A[j] j := j - 1 end while A[j + 1] := key end for end procedure   It will iterate starting from index 1 to the last index of the list. We will also have extra index j, which is set as an index before i. If we encounter an element at index j which is greater than element at current index i (called the key), we will swap them, and then swap it again continuously backward until we achieved the correct order. After that, we will continue iterating the outer for loop.  Insertion sort results in best of O(n)O(n)O(n) time, average and worst in O(n2)O(n^2)O(n2) time, with the space complexity being O(1)O(1)O(1). The worst-case scenario occurs when the input list is in reverse order. In this case, the algorithm has to perform the maximum number of comparisons and shifts for each element in order to insert it into its correct position within the sorted portion of the list.   Source : Mark Bowman Chapter_15.01 ","version":"Next","tagName":"h3"},{"title":"Traversal","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/traversal","content":"","keywords":"","version":"Next"},{"title":"Graph Traversal​","type":1,"pageTitle":"Traversal","url":"/cs-notes/data-structures-and-algorithms/traversal#graph-traversal","content":" Graph traversal refers to the process of exploring nodes or vertices in a graph data structure. There are two commonly used methods for graph traversal: depth-first search (DFS) and breadth-first search (BFS).  Depth-First Search (DFS)​  DFS is a graph traversal algorithm that explores vertices as deeply as possible. It starts at a given vertex, visits one of its neighbors, and continues to explore that neighbor's neighbors until it reaches a dead end. After reaching dead end, it will come back to the previous vertex and continue to visit that vertex neighbor until it reaches a dead end again.  Here is the pseudocode :  DFS(graph, vertex, visited): visited add vertex // mark as visited process(vertex) // process it as needed for neighbor in the neighbors of graph(vertex): // visit each of the neighbor if neighbor is not in visited: // only visit the unvisited DFS(graph, neighbor, visited) // recursive call   In DFS, we will visit a vertex as deep as possible, each vertex have a neighbor, and that neighbor will have another vertex. After reaching dead end, we will continue exploring the neighbors of the previous vertex. This behavior aligns closely with stack and recursion, therefore we can use recursion and take advantage of its call stack to implement DFS.  It is possible that the graph we are traversing consists of a loop (cyclic graph). If we keep traversing it mindlessly, we may never finish our recursion function, which will result in stack overflow error. To mitigate this, we will have another data structure called visited, which will keep track the visited vertex. We will only traverse to unvisited vertex. The data structure of visited can be a set data structure, to ensure fast retrieval of element and to store only unique element.   Source : https://en.m.wikipedia.org/wiki/File:Depth-First-Search.gif  Breadth-First Search (BFS)​  BFS, also called level-order search, is a graph traversal algorithm that explores all vertices of a graph at the same level before moving to the next level. It starts at a given vertex and explores all its neighboring vertices before moving on to their neighbors.  BFS(graph, start): visited = set() // Keep track of visited vertices queue = Queue() // Initialize an empty queue queue.enqueue(start) // Enqueue the starting vertex while queue is not empty: vertex = queue.dequeue() // Dequeue a vertex from the front of the queue if vertex is not in visited: visited.add(vertex) // Mark the vertex as visited process(vertex) // Process the vertex as needed // Enqueue unvisited neighbors for neighbor in graph.adjacentVertices(vertex): if neighbor is not in visited: queue.enqueue(neighbor)   BFS doesn't use recursion to traverse, it instead uses a queue data structure to keep track all the neighbors in the current level.  The algorithm starts by enqueuing the starting vertex into the queue and marking it as visited. It then enters a loop where it repeatedly dequeues a vertex from the front of the queue, marks it as visited, process it depending on the task, and enqueues its unvisited neighbors. This process continues until the queue becomes empty, indicating that all vertices have been visited.  tip The queue act like a &quot;to-do list&quot; to keep track all the node for us to visit later.   Source : https://commons.wikimedia.org/wiki/File:Breadth-First-Search-Algorithm.gif  ","version":"Next","tagName":"h3"},{"title":"Tree Traversal​","type":1,"pageTitle":"Traversal","url":"/cs-notes/data-structures-and-algorithms/traversal#tree-traversal","content":" As we know, a tree data structure is a special case of graph where it is undirected and acyclic. We can also use DFS and BFS to traverse a tree (the previous GIF is literally a graph organized tree structure).  Both BFS and DFS are a general graph traversal algorithm that can be applied on arbitrary graph and tree. Depending on the structure, the algorithm should be similar.  In tree traversal using DFS, there are specific orders of visiting and processing the nodes in a tree, which are inorder, preorder, and postorder. Let's assume we are traversing a binary tree.  Inorder Traversal​  In inorder traversal of a binary tree, we will visit the left subtree, then the current node, and finally the right subtree. By visiting the left subtree, it means we are going to visit all the child in the left subtree in recursive manner (DFS).  function DFS(node): if node is not null: DFS(node.left) visit(node) DFS(node.right)    Source : https://commons.wikimedia.org/wiki/File:Inorder-traversal.gif (red means all the child has been visited)  Preorder Traversal​  In preorder traversal, we visit the current node first, then the left subtree, and finally the right subtree.  function DFS(node): if node is not null: visit(node) DFS(node.left) DFS(node.right)    Source : https://commons.wikimedia.org/wiki/File:Preorder-traversal.gif  Postorder Traversal​  In postorder traversal, we visit the left subtree first, then the right subtree, and finally the current node.  function DFS(node): if node is not null: DFS(node.left) DFS(node.right) visit(node)    Source : https://commons.wikimedia.org/wiki/File:Postorder-traversal.gif  Depending on the task, the order of visiting nodes matters. For example, inorder traversal can be used to visits the nodes in ascending order in the case of a binary search tree. ","version":"Next","tagName":"h3"},{"title":"Trie","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/trie","content":"","keywords":"","version":"Next"},{"title":"Trie Implementation​","type":1,"pageTitle":"Trie","url":"/cs-notes/data-structures-and-algorithms/trie#trie-implementation","content":" We can implement trie with a tree data structure, which can be implemented using linked list under the hood.  Each node will represent either a character or a partial string, except the root node, containing an empty string.The root node should have reference to its child, which can vary depending on the use case. If we only store character, we would only need a maximum 26 children nodes, assuming we are storing the unique alphabets.We will also include a flag or boolean value in each node to indicate if that node represents the end of a complete string or not. For example, when we insert string &quot;tea&quot;, we don't want to treat &quot;te&quot; as the complete word, so we will set the flag to false in the &quot;t&quot; and &quot;e&quot; node, while setting the flag to true in &quot;a&quot; node.  Searching : Traverse the trie from the root node, following the path dictated by the characters of the target string. If at any point a character doesn't exist, it indicates that the string is not present in the trie. If we found all the prefix of the string, we will also check if it is a complete string.Insertion : Similar to search, we will traverse from the root node, when a character exist, we will continue going down there. When a character doesn't exist, we will make a new node for it.Deletion : Traverse the trie from the root node, once the string is found, we will simply mark the last node as not a complete string. We will not remove the node, to ensure we don't accidentally remove other prefix.  Here is an trie traversal illustration for matching strings.   ","version":"Next","tagName":"h3"},{"title":"Tree","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/tree","content":"","keywords":"","version":"Next"},{"title":"Binary Tree​","type":1,"pageTitle":"Tree","url":"/cs-notes/data-structures-and-algorithms/tree#binary-tree","content":" A binary tree is a type of tree data structure in which each node has at most two child nodes, commonly referred to as the left child and right child. Binary tree is commonly used, it has many other variants.  Binary tree operation are typically done in average of O(log n) time complexity and the worst being O(n), this is because the nature of binary tree. For instance, when we are searching for some node in the tree, we effectively divide the search space in half at each level of the tree, we will always eliminate one of the child subtrees based on a comparison with the current node (we can either choose to go to the left or right child).   Source : https://en.wikipedia.org/wiki/Binary_tree  Binary Search Tree​  Binary search tree (BST) is a binary tree in which the values of nodes in the left subtree are less than the value of the node, and the values in the right subtree are greater. BSTs enable efficient searching, insertion, and deletion operations with an average time complexity of O(log n) for balanced trees.  BST can be thought as the representation of the binary search algorithm in a tree structure.   Source : https://en.wikipedia.org/wiki/Binary_search_tree  Balanced Tree​  In binary trees, some node of the same parent may have more child node than the sibling. When the height of the left and right subtree of node differ more than two, we can say the tree is unbalanced. In other word, a balanced binary tree is one in which the heights of the left and right subtrees of every node differ by at most one.  A balanced tree is important in some tree operation, this ensures we can effectively divide the tree roughly by half, so that choosing left or choosing the right node results in the same elimination.   Source : https://www.techiedelight.com/check-given-binary-tree-is-height-balanced-not/  note Balanced tree doesn't only apply to binary tree.  AVL Tree​  Adelson-Velsky and Landis (AVL) Tree is a self-balancing binary search tree. AVL tree achieves self-balance by ensuring that the heights of the left and right subtrees of any node differ by at most one.  In AVL tree, each node is associated with a balance factor, it is the difference between the heights of the left and right subtrees of a node. The balance factor can be -1, 0, or 1 for a node to be considered balanced.  Balance Factor of 1 : Have extra node anywhere in the right childBalance Factor of 0 : Have the equal number of nodesBalance Factor of -1 : Have extra node anywhere in the left child   Source : https://www.programiz.com/dsa/avl-tree  Rotation​  When the balance factor exceed -1 or 1 (e.g., -2 or 2), a rotation will be performed. Rotation is an operation that modifies the structure of the tree.  Left Rotation : A left rotation is performed on a node to rotate it and its right child. It changes the structure of the tree such that the former right child becomes the new root, and the original node becomes the left child of the new root. Right Rotation : A right rotation is performed on a node to rotate it and its left child. It rearranges the tree structure so that the former left child becomes the new root, and the original node becomes the right child of the new root.  There are also double rotations, which involve performing a sequence of two rotations to restore balance.  Left-Right Rotation : This involves performing a left rotation on the left child, followed by a right rotation on the original node.Right-Left Rotation : This involves performing a right rotation on the right child, followed by a left rotation on the original node.   Source : https://en.wikipedia.org/wiki/AVL_tree#/media/File:AVL_Tree_Example.gif  AVL tree able to achieve a worst-case time complexity of O(log n) for search, delete, and insert operation. In contrast to the O(n) worst-case time complexity of a typical binary tree.  Red-Black Tree​  Red-Black tree is a self-balancing binary search tree that ensures the tree remains balanced even after insertions or deletions. In red-black tree, we will color a node either as red or black. The color in red-black tree is a mechanism used to enforce certain properties and maintain balance within the tree structure.  Properties​  In order for a red-black tree to be considered as valid, it must adhere to these properties :  Every node is either red or black.All NIL (also known as null nodes and leaf nodes) are considered black.A red node does not have a red child (called red property).Every path from a given node to any of its descendant NIL nodes goes through the same number of black nodes. In other words, the number of black nodes encountered when traversing any path in the tree remains constant (called black depth property).(Conclusion) If a node N has exactly one child, it must be a red child, because if it were black, its NIL descendants would sit at a different black depth than N's NIL child, violating requirement 4.  Source : https://en.wikipedia.org/wiki/Red%E2%80%93black_tree (with modification)  Similar to AVL tree, red-black tree can achieve the worst-case scenario time complexity of O(log n).   Source : https://www.javatpoint.com/red-black-tree-java  Repainting &amp; Rotation​  When performing operations on a red-black tree, such as inserting a new node or deleting an existing node, the tree may violate one or more of the red-black tree properties. To restore these properties and maintain the balance of the tree, repainting is performed.  Insertion Repainting : When a new node is inserted, it is initially colored red.If the parent of the newly inserted node is also red, repainting is required to restore the red property (red node can't have red child).Repainting involves changing the colors of certain nodes and performing rotations to maintain the red-black properties. Deletion Repainting : When a node is deleted, the tree may violate the red-black tree properties, particularly the black depth property and the red property.Repainting involves adjusting the color of nodes and performing rotations to restore the properties.The specific rules for repainting after deletion depend on various cases, such as whether the deleted node is red or black, whether the sibling of the deleted node is red or black, and so on.  Rotation :  Left Rotation : During a left rotation, the right child of the node becomes the new root of the subtree, and the node becomes the left child of its original right child. Right Rotation : During a right rotation, the left child of the node becomes the new root of the subtree, and the node becomes the right child of its original left child. Source : https://en.wikipedia.org/wiki/Red%E2%80%93black_tree  ","version":"Next","tagName":"h3"},{"title":"B-Tree​","type":1,"pageTitle":"Tree","url":"/cs-notes/data-structures-and-algorithms/tree#b-tree","content":" B-Tree is another self-balancing tree, similar to AVL and red-black trees, but it is not a binary tree. It is a normal tree that can have more than two child nodes per parent, designed to address the limitation of self-balancing binary tree which can only store data in the left and right child.  In b-tree, often times we will hear the word &quot;key&quot;. This term probably comes from database system, as b-tree is often used for implementing database index. B-tree node consist multiple keys, these keys are the actual value within a node. For example, the image below is a b-tree where the root node contains 2 keys, which are 20 and 40. The root node then has 3 child node, each node has key(s) : 10, 30 and 33, 50 and 60.   Source : https://www.programiz.com/dsa/b-tree  Rules &amp; Balancing​  B-tree maintains a sorted order of data from left to right, just like binary search tree. This property allows b-tree to be &quot;binary searchable&quot;. To make an efficient search, the tree needs to be balanced, b-tree have some rules to maintain the sorted order. B-tree limits the number of node each node can have.  When we say a b-tree with order m, it means m is the maximum number of children a node can have, more specifically :  Every node has at most m children.Every internal node has at least ⌈m/2⌉ (ceil) children.The root node has at least two children unless it is a leaf.All leaves appear on the same level.A non-leaf node with k children contains k − 1 keys.  Source : https://en.wikipedia.org/wiki/B-tree  B-tree can be unbalanced after insertion or deletion operation, the balance is achieved by splitting or merging the trees.  Splitting : Splitting occurs when a node becomes overfull, meaning it exceeds its maximum capacity of keys. In this case, the overfull node is divided into two separate nodes, and a median key is chosen to become the separator key that will be moved to the parent node. Merging : Merging occurs when a node becomes underfull, meaning it has fewer keys than the minimum required. In this case, the underfull node is merged with a sibling node, effectively reducing the number of nodes in the tree.  Inserting a new key involves finding the appropriate position in the tree to insert. Deleting a key from a B-tree involves finding the key and removing it. ","version":"Next","tagName":"h3"},{"title":"Union Find","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/union-find","content":"","keywords":"","version":"Next"},{"title":"Operation​","type":1,"pageTitle":"Union Find","url":"/cs-notes/data-structures-and-algorithms/union-find#operation","content":" Apart from being data structure, it also provides a way to determine whether two elements belong to the same set and allows us to merge two sets containing some elements into a single set.  Union find organizes its sets in a tree-like structure, where each node represents an element, the root is called the representative element, which serves as the identifier or representative for the entire set. This identifier will help us to determine whether two elements belong to the same set easily. The idea is, if two elements belong to the same set, their root node must be the same.    Terminology :  Parent : The root node, which is the representative element of the set.Size : The number of elements or nodes in the set represented by a particular node.Rank : The height of a node or the longest path from the node to a leaf.  The size and rank is used to which one should be the representative, as the representative should be the one which has the most information.  Find​  Given a node, Find is a function that should return the representative. The Find function is recursive, it will find the parent of some node, the parent of that node again, up to the root node.  function Find(x): if x.parent != x: x.parent = Find(x.parent) return x.parent else: return x   In union find initialization, each element's representative is itself. When we merge element together, one of the element will become the representative, but the representative itself will still have its own representative as itself, only the other element's representative will change. The characteristics of representative's parent being itself serve as the base case for the Find recursive function.    Union​  The Union function takes two element and merge them together.  function Union(x, y): rootOfX = Find(x) rootOfY = Find(y) if rootOfX == rootOfY: return if rootOfX.rank &lt; rootOfY.rank: rootOfX.parent = rootOfY rootOfY.parent = rootOfX if rootOfX.rank == rootOfY.rank: rootOfX.rank = rootOfX.rank + 1   We will first find the root of x and y. If their root or representative are the same, it means they belong to the same set and no further operation is needed.  If they are in different set, we will need to decide how should we attach the tree. Should we attach x to the y or the y to the x? The decision should be attaching the smaller tree to the larger tree. How large a tree is can be determined using size or rank.  If they have equal rank, one root is chosen arbitrarily, and its rank is increased by one. We don't increase rank if we attach x to y or y to x, because by attaching it, the tree depth won't be increased (remember the definition of rank and the rules of attached tree should be smaller).   Source : https://youtu.be/ayW5B2W9hfo?si=76W-GO9H-hEy-Xk1&amp;t=205  The union doesn't do much apart from assigning parent, its complexity come from the Find function. The Find function itself has the time complexity of O(log⁡n)O(\\log n)O(logn), where nnn is the number of element exist in the tree. The union find data structure doesn't use extra memory for its operation, its space complexity is simply O(n)O(n)O(n) for storing all the nnn elements. ","version":"Next","tagName":"h3"},{"title":"Two Pointer","type":0,"sectionRef":"#","url":"/cs-notes/data-structures-and-algorithms/two-pointer","content":"","keywords":"","version":"Next"},{"title":"Palindrome Checking​","type":1,"pageTitle":"Two Pointer","url":"/cs-notes/data-structures-and-algorithms/two-pointer#palindrome-checking","content":" Palindrome is a word, number, sequence, or phrase that reads the same as forwards and backwards. Example of a palindrome word is &quot;racecar&quot;, reading it forward from left to right or backward from right to left will be the same.  Naive Solution​  How would we design an algorithm that checks if a given string is palindrome or not? The naive solution would be reading it forward first, then reading it backward after, and then compare it if they are the same or not.  Here is the pseudocode for it :  function(word: String): Boolean readForward = &quot;&quot; readBackward = &quot;&quot; for each character in word: append character to last position of readForward append character to first position of readBackward return readForward == readBackward   Here's how will it look like :    In this code, we created two empty string. We will iterate the string and append each character to last and first position of readForward and readBackward, respectively. We will need to iterate the whole string, therefore this algorithm results in O(n)O(n)O(n) time complexity, where nnn is the length of the string. The space complexity is O(2n)O(2n)O(2n) = O(n)O(n)O(n), because we used two string to store the read.  Two Pointer Approach​  Notice that we did a repeated work in the naive solution, we can optimize this using two pointer approach. If a string is palindrome, it will be read the same from forward and backward. The idea is, instead of storing the read somewhere, why don't we compare it on the fly while iterating? Now the question is, how would we compare it on the fly?  The solution is to have two pointer, with one pointing to the first character of the string or index 0, and the other pointer points to the last character. The first pointer (or we call left pointer because it starts from left) will be incremented while the second pointer (right pointer) will be decremented in each iteration. We will check if the character at the index of left pointer is the same as the character at index of right pointer. If they are not the same, we will return false, indicating they are not palindrome.  function(word: String): Boolean leftPointer = 0 rightPointer = last index of word while leftPointer &lt; rightPointer: if word[leftPointer] != word[rightPointer]: return false leftPointer = leftPointer + 1 rightPointer = rightPointer + 1 return true     Using this two pointer approach, we managed to achieve a constant O(1)O(1)O(1) space complexity, we don't need to store the string read anywhere. This algorithm also have better time complexity, the worst-time complexity would be the same as the naive solution, however, the best and average may differ in some scenario. In the case of &quot;abcde&quot;, we can immediately return false in the first iteration, because &quot;a&quot; is indeed not equal to &quot;e&quot;, it immediately break the palindrome definition. ","version":"Next","tagName":"h3"},{"title":"Database System","type":0,"sectionRef":"#","url":"/cs-notes/database-system","content":"","keywords":"","version":"Next"},{"title":"Introduction to Database​","type":1,"pageTitle":"Database System","url":"/cs-notes/database-system#introduction-to-database","content":" Database is an organized collection of structured data that is stored and managed electronically. It is designed to store, retrieve, update, and manage large amounts of information.  Database is managed by the database management system (DBMS). The DBMS is a software system responsible for creating, modifying, and retrieving the database. The underlying component that power DBMS is called a database engine or database server.  Database on its own is just a collection of data, there are many choices for storing the data. Small databases can be stored on a file system, such as disk storage on our device. Larger databases are stored in a cloud storage, which is a specialized storage on remote network.  In addition, the place where we store the data also affect the performance of retrieving the data. For instance, choosing to store data in the main memory (RAM) allows for quicker retrieval, but it comes at the expense of storage capacity since RAM is typically smaller than a disk storage.  Database is typically classified to relational and non-relational. Relational database is when the data is organized into tables consisting columns and rows, and a unique identifier (called key) within each row.  A format in which database is structured and organized is called database model. The relational database is an example of a database model.  The purpose of having specific model is to suit with the application's requirement. For example, in a social network, we may use database that is modeled in a graph structure. This way, we can easily represent complex relationship between users, such as their common preferences or friendship connection, which we can use to make recommendation system.   Source : https://medium.com/@saad.jameel1992/graph-vs-relational-databases-62ecc5902eb2  ","version":"Next","tagName":"h3"},{"title":"All pages​","type":1,"pageTitle":"Database System","url":"/cs-notes/database-system#all-pages","content":" Relational DataQuery LanguageDatabase Management Database Design Database ModelNormalizationTransactionsConcurrency ControlTrigger &amp; Constraints Logging &amp; RecoveryDatabase IndexDatabase Optimization NoSQLDatabase Implementation Storage ManagementIndex ImplementationQuery ProcessingQuery Compiler ","version":"Next","tagName":"h3"},{"title":"Database Optimization","type":0,"sectionRef":"#","url":"/cs-notes/database-system/database-optimization","content":"","keywords":"","version":"Next"},{"title":"Query Optimization​","type":1,"pageTitle":"Database Optimization","url":"/cs-notes/database-system/database-optimization#query-optimization","content":" Query Plan​  It is possible for single query to have multiple valid execution plans, with different performance. For example, consider the following query :  SELECT * FROM table1 JOIN table2 ON table1.id = table2.id WHERE table1.column = 'value'   There are three ways to join both table :  Nested Loop Join : The optimizer scans each row from table1 and looks up matching rows in table2. This plan is suitable when one table is significantly smaller, or when the join condition is selective, meaning it filters out a relatively small portion.Hash Join : The optimizer builds hash tables for both table1 and table2. The hash table serves as a quick lookup for matching rows between two tables. Hash join can be more efficient when both tables are large and the join condition is not selective.Merge Join : Merge join efficiently merge the sorted data to perform the join. It scans the sorted tables simultaneously, comparing the values of the join columns. Merge join can be efficient when the data is already sorted and the join condition is not selective.  Join Ordering​  Join Ordering is the process of determining the order in which tables are joined in a database query. The goal of join ordering is to find the most efficient sequence of join operations that minimizes the overall cost of executing the query.  The query plan is represented as tree.   Source : https://en.wikipedia.org/wiki/Query_optimization  Join order is determined using dynamic programming algorithm pioneered by IBM's System R database. This algorithm works in two stages :  Enumeration : The optimizer generates all possible join order for the given tables.Cost-Based Selection : The algorithm estimates the cost of each generated join order and choose the best way.  Cost Estimation​  A cost of query plan is estimated by several factors such as disk I/O operations, CPU time, connectivity (in the case of distributed systems), and selectivity and cardinality.  Selectivity : refers to the proportion of rows in a table that satisfy a particular condition or predicate. Selectivity information helps the optimizer to plan and optimize query execution by estimating the number of rows that will be returned by a given query or predicate. For example, if a predicate has a high selectivity, meaning it filters out a large portion of the rows, it might be advantageous to push the predicate closer to the data source to reduce the amount of data that needs to be processed. Cardinality : Cardinality is the number of distinct values or rows in a table or a column. Cardinality is important for estimating the number of rows that will be returned by a query or the number of distinct values that will be involved in a join.  The information and metrics collected about the data and structure of a database is stored in a database statistics. These can be table, column, index statistics, which stores data types, size, cardinality, or any other metadata. Statistical summaries such as frequency and distribution of values can also be stored in a histogram.  tip More about query processing and query compiler.  ","version":"Next","tagName":"h3"},{"title":"Caching​","type":1,"pageTitle":"Database Optimization","url":"/cs-notes/database-system/database-optimization#caching","content":" DBMS can introduce caching mechanism to improve database queries.  Buffer caching : Buffer pool is a region of memory that is allocated by the buffer manager as the place to transfer disk blocks. It is basically the &quot;transit&quot; area between the disk storage and the CPU. When query happens, the DBMS may check the buffer pool if the required data is available already.Query caching : When a query is executed, the DBMS checks if the same query with the same parameters has been executed before and if the result is already present in the query cache. If the result is found in the cache, it can be directly returned without the need for re-executing the query and accessing the disk, resulting in performance improvement.  ","version":"Next","tagName":"h3"},{"title":"Partition​","type":1,"pageTitle":"Database Optimization","url":"/cs-notes/database-system/database-optimization#partition","content":" Database partitioning is a technique to logically divide a large database into smaller, more manageable partitions. Partitioning is particularly useful when dealing with large datasets or when performance bottlenecks occur due to the size and complexity of the database.  Partitioning also improves scalability and availability, it ensures that a failure of one partition does not affect the availability of the other partitions. Those partitions can be distributed across multiple servers.  For example, a global company with customers in multiple regions could partition its customer data by region. This would allow the company to store the customer data for each region on a server in that region, which would improve data locality and reduce latency.  Type of database partitioning :  Range Partitioning : Data is divided based on a specific range of values from a chosen attribute. For example, a date attribute could be partitioned into monthly or yearly ranges. Each partition contains data that falls within that specific range.List Partitioning : Divides data based on specific values or a list of values from a chosen attribute. For instance, a database could be partitioned based on region attribute, where each partition contains data related to a specific region.Composite Partitioning : Composite partitioning combines multiple partitioning techniques to create more complex partitioning strategies.Round-robin Partitioning : Round-robin partitioning evenly distributes data across partitions in a circular fashion. Each new record is inserted into the next partition cyclically. This technique can be useful when the data distribution is expected to be uniform and there is no specific criterion for partitioning.Hash Partitioning : Hash partitioning distributes the data across partitions based on a hash function applied to a chosen attribute. The hash function ensures an even distribution of data across partitions, making it useful when there is no natural range or list criterion for partitioning.   Source : https://www.enjoyalgorithms.com/blog/data-partitioning-system-design-concept ","version":"Next","tagName":"h3"},{"title":"Database Index","type":0,"sectionRef":"#","url":"/cs-notes/database-system/database-index","content":"Database Index Database index - WikipediaHow much do B-trees reduce disk accesses? - stackoverflow In database, index is a data structure that helps to improve performance in database queries, at the cost of extra space to store the data structure and additional maintenance. The idea of index is, rather than searching the whole table for particular value, we will instead create a special column that will narrow down the search space and help us locate the relevant data more quickly. An index is a copy of a column, it is associated with a key and pointer. A key is a value that identify a data, it's going to be a value that will be searched for. Each key will be associated with a pointer, which is the reference or address that points to the physical location of the corresponding data within the table. Naive Example​ Let's say we have a table of employee, it consists of name, ID, and department. For the example, we are going to find the department associated with a particular employee name. Data from : https://www.linkedin.com/pulse/database-index-selvamani-govindaraj Suppose we are going to find the department for employee Derek. The naive approach is to check each row of Employee_Name column one by one from top to bottom. We will have to go through the table until we find someone with the name of Derek. In this case, we have to go through 4 row before we found Derek. In a small table, the impact on query performance may not be significant. However, as the table size increases and the query becomes more frequent, it can have a noticeable effect on overall performance. Now obviously we can improve the performance by sorting the table, so we can perform binary search. However, it will be harder in a scenario where we have to search based on multiple values, not just the name. Binary search still requires accessing specific disk blocks/pages where the data resides. In a big sorted table, the data may not be stored contiguously, meaning that multiple disk blocks/pages may need to be read to complete the binary search. This can result in multiple I/O operations, especially if the desired data is distributed across different disk blocks. Another downside of flat sorted table is the performance of insertion and deletion, as we need to maintain the sorted order of the table. If we insert a new record, we may need to shift all the subsequent records to make room for the new entry. This process can be time-consuming and resource-intensive. B-Tree Index Example​ There are many ways to implement database indexes. One way is to maintain a column of key and pointer, arranged in a tree structure. The column will be sorted based on the key. This tree structure is called B-tree. Source : https://www.linkedin.com/pulse/database-index-selvamani-govindaraj (with modification) The tree will consist of many nodes, each node has different column of key and pointer. As said earlier, the key will be the terms that is being searched, such as name. Each column entry is associated with a pointer that refers to the child nodes, eventually referring to the leaf nodes. Let's say we are searching for name &quot;Derek&quot;. Start from the root node.Compare the search key with the keys stored in the root node.If the search key is found in the root node, the search is complete, and the corresponding data can be retrieved.Because Derek is higher than David but lower than Greg, we will follow the pointer associated with David.This comparison will keep being repeated until we find the data or arrived at the leaf nodes.We followed the pointer which led us to the disk location of 2459.Upon searching the node, we found Derek and its department. In this case, we found Derek by doing 5 search, checking all the key on the root node and 2 key on the middle leaf nodes. So why is this index structure better than binary search? One advantage is the balanced structure which results in reduced I/O operation. The binary search can only narrow down the search by half. In other word, it only divides the search space by 2. If we have 1 million data, we would need 20 comparison in average (log⁡2(1,000,000)≈19.93\\log_{2} (1,000,000) \\approx 19.93log2​(1,000,000)≈19.93). By 20 comparison, this mean we need to do the disk operation 20 times. On the other hand, b-tree is more flexible in terms of the node maintained in each level. In contrast, a binary search can be represented in a tree like below. Source : https://en.wikipedia.org/wiki/Binary_search_tree A b-tree node does not have to be 2, the number of node can be within a certain range. A node in a b-tree is sized to match the disk page size. Each node can contain many keys which all held in a single page. This is possible because an index entry consist only a key and a pointer, whereas binary search performed on sorted table requires all rows and column. The compact representation of index entries in b-tree indexing enables a significant amount of data (the index keys) to fit within a single page. We can check a large amount of key in just a single I/O operations that retrieves the specific page. Also, we don't need to reorganize the node and all the disk pages every time insertion or deletion happens, this is because b-tree by itself is a self-balancing tree. tip The terms page and block is sometimes used interchangeably. Index Architecture &amp; Methods​ Indexes can be organized differently : Clustered : A clustered index stores their index in the same order as how data rows are stored on disk. Clustered index can determine the physical order of the data rows in a table. Each table can have only one clustered index.Non-clustered : A non-clustered index contains copy of the indexed column(s) along with a pointer to the corresponding data row. Non-clustered index does not determine the physical order of the table rows. Instead, it provides a quick lookup mechanism to locate specific rows based on the indexed columns. There can be more than one non-clustered index on a table. info The b-tree example before is an example of non-clustered index. Source : https://josipmisko.com/posts/clustered-vs-non-clustered-index Clustered index can significantly improve the performance of queries that involve range-based searches or sorting operations on the clustered index key. However, we need to maintain the sorted order of the data, possibly rearranging the data every insertion or deletion operation. Non-clustered index can be efficient for query of data based on the indexed column(s) that involve filtering, sorting, or joining. However, it requires additional disk I/O operations to access the actual data rows after locating them through the index. Types of Indexes​ Bitmap Index : A bitmap index stores data in a bit array (bitmap) and answer query by performing bitwise operations. Each bit in the bitmap represents the yes or no of a particular value in the indexed column. Bitmap indexes are efficient for low cardinality columns (columns with a few distinct values) and can provide fast lookup and efficient boolean operations.Dense Index : Dense index contains an entry for every record in the indexed data structure. In other words, there is an index entry (containing key and pointer) for each data record in the table.Sparse Index : Sparse index contains an entry for some subset of record in the indexed data structure. It skips some records in the data structure, resulting in a smaller index size.Inverted Index : Inverted index maps a value to the records that contain them. Inverted indexes are commonly used in search engines which uses keyword as the value and the document or website as their records.Primary Index : Primary index is an index structure that is based on the primary key of a table. It determines the physical location of records in a data file. Data file is a specific file that is used to store the actual data records of a database, and may contain primary index data.Secondary Index : A secondary index is an index structure that is based on a non-primary key attribute of a table. Unlike the primary index, a secondary index does not determine the physical order of the records on disk. Instead, it provides an alternate way to access the data by creating an index on a specific attribute or combination of attributes. Secondary indexes are always dense, meaning they contain entries for all records in the data file. This is in contrast to primary indexes, which can be sparse, representing only a subset of the records.Hash Index : Hash index uses a hash function to map the indexed values to specific locations in the index structure. Hash indexes provide fast equality searches, but they are not well-suited for range queries or partial matches. Source : Bitmap index, Dense index, Sparse index, Inverted index, Primary and secondary index, Hash index tip See index implementation for more detail about index and their implementation.","keywords":"","version":"Next"},{"title":"Database Model","type":0,"sectionRef":"#","url":"/cs-notes/database-system/database-model","content":"","keywords":"","version":"Next"},{"title":"Representation​","type":1,"pageTitle":"Database Model","url":"/cs-notes/database-system/database-model#representation","content":" E/R Model​  Entity-Relationship (E/R) Model is a representation of database structure in a diagram called ER diagram. ER consists of three elements :  Entity sets : Entity is a distinct and identifiable object, concept, or thing in the real world or in the domain being modeled. An entity sets is a collection of similar entities. In a student dataset, a student is an entity, some set of students is an entity sets. Attributes : Attributes are the properties or characteristics that is associated within an entity set. Student entity set might include name, major, or date of birth. Relationships : Relationships describe the associations between two or more entity sets. They represent how entities from different sets are related to each other. Relationships can have cardinality constraints that indicate the number of instances of one entity set that are associated with instances of another entity set. Cardinality constraints include one-to-one (1:1), one-to-many (1:N), and many-to-many (N:M) relationships.  ER Diagram​   Source : Book page 128  In the diagram :  Entity sets are represented by rectangles.Attributes are represented by ovals.Relationships are represented by diamonds.  The relationship in E/R diagram can also be categorized into to, stars-in and owns relationship. Stars-in represent a many-to-many relationship, one entity sets can have multiple entities from the other sets and vice versa. While owns represent a one-to-one relationship, one entity sets can only have one entity from the other sets.  For example in a movie database, movie have stars-in relationship with stars. This is because multiple stars can play in multiple movie and multiple movie can have multiple stars. On the other hand, movie have owns relationship with studio, because a movie can only be owned by a single studio.  For more complex relationship, there can be multiway relationship, which consist of three or more entity sets are connected through a single relationship.   Source : Book page 131  In the movie database again, movies can have multiple stars and stars can have multiple movie. A contract specifies a star involvement in a particular movie and is associated with a specific studio.  Constraints​  We can enforce constraints on a database, they are specific rules or conditions that must be satisfied by the data in the database.   Source : Book page 149  In the diagram, we indicate an attribute as the unique identifier of an entity set by underlining it. In this case, a studio is uniquely identified by its primary key, which is the name.   Source : Book page 150, 151  Other constraints include referential integrity, which is indicated by arrow head. It enforces that values in foreign key attributes match the values in the corresponding primary key attributes of the related entity.  The other constraint is called degree constraint, which limits the number of relationship between entity sets. Setting a constraint of stars to be less than 10 with a movie means that the movie can only have less than 10 relationship with stars.  Weak Entity Sets​  A weak entity set is an entity set that does not have sufficient attributes to form a primary key uniquely on its own. Instead, it relies on a related strong entity set to provide part of its primary key.   Source : Book page 153  Such entity set is indicated by double rectangle box. A crew can't be identified by its number only, but it requires dependency on the name of studio it belongs to.  UML​  Unified Modeling Language (UML) is a standardized visual modeling language used to model, design, and document software systems and other systems with a graphical notation, in an object-oriented style. While UML is typically used for software systems, it can also be used to model database.  UML diagram is represented in three parts of box. The top part is the name of the class, or the entity set in the case of similarity with E/R model. The middle part consists of the attributes and its data types. The bottom part is the methods of the class.   PK stands for primary key Source : Book page 172  Association​   Source : Book page 173  Classes can have relationship between them, it is indicated with a straight line and also provided with the relationship type and its constraints. The &quot;0..1&quot; means zero or one relationship, the &quot;0..*&quot; means zero or more, &quot;1..1&quot; means exactly one.  Subclasses​  A subclass is a class that inherits properties, attributes, and operations from another class, known as its superclass or parent class. Subclass is indicated by another rectangle with an arrow pointing to its parent class.   Source : Book page 177  ODL​  Object Definition Language (ODL) is a language used for defining object-oriented database schemas.  class Movie { attribute string title; attribute integer year; attribute integer length; attribute enum Genres {drama, comedy, sciFi, teen} genre; relationship Set&lt;Star&gt; stars inverse Star::starredln; relationship Studio ownedBy inverse Studio::owns; }; class Star { attribute string name; attribute Struct Addr {string street, string city} address; relationship Set&lt;Movie&gt; starredln inverse Movie:: stairs; }; class Studio { attribute string name; attribute Star::Addr address; relationship Set&lt;Movie&gt; owns inverse Movie::ownedBy; };   An entity set is defined as a class.The class contains attribute name and its data types. It is possible for the attribute to be a more complex type such as list, array, or dictionary.An attribute can be an enum that can take values from some set (e.g., Genres) or a struct (e.g., Addr)Relationship is modeled in a set along with the specified types (e.g., starredIn, owns). The inverse keyword is used to specify the inverse relationship on the other end of the association.  class Cartoon extends Movie { relationship Set&lt;Star&gt; voices; };   In the case of subclass, we would add the extends keyword along with the parent class. The subclass will inherit the parents attributes.  A key is declared as follows :  class Movie (key (title, year))  This mean title and year is the primary key of Movie.  info The two code example above is taken from the book chapter 4.9.  ","version":"Next","tagName":"h3"},{"title":"Data Model​","type":1,"pageTitle":"Database Model","url":"/cs-notes/database-system/database-model#data-model","content":" Hierarchical​  Hierarchical data model organizes data in a tree-like structure. Data is represented as a collection of records that are linked together in a parent-child relationship, forming a hierarchy. A record is a unit of data that represents a single entity.   Source : https://en.wikipedia.org/wiki/Database_model#/media/File:Hierarchical_Model.svg  This model can represent one-to-many relationship, the parent record can have multiple child records, but each child record can have only one parent.   Source : https://en.wikipedia.org/wiki/Hierarchical_database_model  The image above is an example of relational model, it is an employee and computer table. The computer table has a column User EmpNo, which is a foreign key to the primary key of employee table. If this relational model were organized in hierarchical model, then the employee would be the parent of computer.  XML​  eXtensible Markup Language (XML) is a markup language used for structuring and representing data in a human-readable and machine-readable format. XML documents are structured as a hierarchy of elements. Each element is enclosed by opening and closing tags, and elements can be nested to form a tree-like structure.  An example of XML :  &lt;person id=&quot;1&quot;&gt; &lt;name&gt;John Doe&lt;/name&gt; &lt;age&gt;25&lt;/age&gt; &lt;/person&gt;   The example defines a record of person entity with an attributes id equal to one. The name and age is the actual data associated with the entity.  Tags : XML uses tags to define elements, they are enclosed in angle brackets &lt; &gt;. Opening tags indicate the beginning of an element, while closing tags indicate the end of an element. For example, &lt;person&gt; and &lt;/person&gt; are the opening and closing tags for the &quot;person&quot; element.Attributes : XML elements can have attributes which provide additional information about the element. Attribute is defined as name-value pairs, in the example above, the person element have attribute named id equal to value of 1.Parent &amp; Child : XML is a hierarchical model, the person element is the root and also the parent element of its child, name and age. Child elements represent actual data while attribute is just additional details.  info More about XML.  Network​  Network model is the expansion of hierarchical model, it extends from a tree-like structure into a graph-like structure (because essentially a tree is a graph with constraints).   Source : https://en.wikipedia.org/wiki/Database_model#/media/File:Network_Model.svg  The graph structure of network model allows for many-to-many relationship, a single parent is not a requirement anymore. There can be more than one path from an ancestor node to a descendant.  Records and relationships are organized into sets. The sets are represented in a circular linked list, where each circle represents a set and consists of records of a specific record type.   Source : https://byjus.com/gate/network-model-in-dbms-notes/  The circular set establishes a directed graph, where the direction is determined by the ownership relationship. We can access related records by following the links within the circular linked lists.  Relational​  Relational model is what we have discussed in the previous topics.  Object​  The object model follows the object-oriented programming (OOP) paradigms. In object model, an entity is defined as a class.  Objects encapsulate both data (attributes or properties) and behavior (methods or operations) within a single entity. OOP concepts like inheritance are supported in object databases, this allows specific entity to inherit attributes and behavior from their parent classes.  Object model can be combined with relational model, becoming object-relational database. This model help to bridge between database and object-oriented modeling used in programming language to build a software system.  The connection between database and OOP languages uses a technique called object-relational mapping (ORM). Developers define object-oriented classes that represent database tables or entities. The ORM framework will handle the mapping and synchronization of data between these classes and the corresponding database tables. It abstracts away the complexities of writing raw SQL queries, instead developer can call database operation in object-oriented manner.   Source : https://en.wikipedia.org/wiki/Object%E2%80%93relational_mapping  Document​  Document model stores and organizes data in a flexible, semi-structured format known as documents. In this database model, data is stored in self-contained documents, typically in formats such as JSON (JavaScript Object Notation).  tip XML is considered as a document model as well.  Document model has a standard format in how they structure the data. For example, XML uses tags and element, JSON relies on key-value pairs, other document such as YAML relies on key-value pairs as well, but the structure is designed to be simple.  An example of YAML :  employee: - name: John Smith age: 30 position: Software Engineer skills: - Java - Python - SQL - name: Jane Doe age: 28 position: UX Designer skills: - User Experience Design - Wireframing - Prototyping  ","version":"Next","tagName":"h3"},{"title":"Concurrency Control","type":0,"sectionRef":"#","url":"/cs-notes/database-system/concurrency-control","content":"","keywords":"","version":"Next"},{"title":"Serial & Serializable Schedules​","type":1,"pageTitle":"Concurrency Control","url":"/cs-notes/database-system/concurrency-control#serial--serializable-schedules","content":" Schedule is a sequence of actions performed by one or more transactions in a database.  Serial : Schedule can be serial, meaning transactions execute one at a time, without any interleaving. In the example below, T1 completes first before T2 do anything. T2 can also be started first, and T2 after. The point is, no mixing between transaction is allowed. Source : Book page 885, 886 Serializable : In serializable schedule, actions from different transactions can be interleaved. A schedule is said to be serializable, if there exists a serial schedule, and interleaving between them produce the same result. The figure 18.5 is an example of serializable schedule, while figure 18.6 is not, because the result differ from the serial schedule above. Source : Book page 886, 887  ","version":"Next","tagName":"h3"},{"title":"Transaction Notation​","type":1,"pageTitle":"Concurrency Control","url":"/cs-notes/database-system/concurrency-control#transaction-notation","content":" Transaction is denoted as follows :   Source : Book page 889 and previous images  ri(X)r_i(X)ri​(X) means that a value XXX is being read by transaction iii. In the case of www, it means the transaction is writing data. When doing transaction, the scheduler is not bothered by what the transaction actually doing.  ","version":"Next","tagName":"h3"},{"title":"Conflict Serializability​","type":1,"pageTitle":"Concurrency Control","url":"/cs-notes/database-system/concurrency-control#conflict-serializability","content":" Conflict is a situation where changing the order of two or more transactions leads to data inconsistencies or incorrect results. Conflict serializability is a concept that extends serializable. It ensures that a schedule is serializable and avoid conflict. Conflict serializability is a desirable property as it guarantees the correctness and consistency of the database state.  Conflict will not occur when :  ri(X);rj(Y)r_i(X);r_j(Y)ri​(X);rj​(Y) : Two different transaction reading the same or different value.ri(X);wj(Y)r_i(X);w_j(Y)ri​(X);wj​(Y) or wi(X);rj(Y)w_i(X);r_j(Y)wi​(X);rj​(Y) : As long as the two transactions access different data (i.e., X≠YX \\ne YX=Y), then conflict won't occur.wi(X);wj(Y)w_i(X);w_j(Y)wi​(X);wj​(Y) : Similar to before, as long as they are distinct data.  Conflict may occur when :  Two actions of the same transactions, e.g., ri(X);wi(Y)r_i(X); w_i(Y)ri​(X);wi​(Y). Changing their order would lead to different results.Two writes of the same database element by different transactions, wi(X);wj(X)w_i(X);w_j(X)wi​(X);wj​(X).Read and write of the same database element by different transactions, ri(X);wj(X)r_i(X);w_j(X)ri​(X);wj​(X).  The general scenario of conflict is when two or more transaction access same data, and at least one of them is writing. If we swap schedule and conflict doesn't occur, this is called non-conflicting swap.  Two schedules are considered conflict-equivalent if one schedule can be transformed into the other by performing a sequence of non-conflicting swaps of adjacent actions. A schedule is considered conflict-serializable if it is conflict-equivalent to a serial schedule. In other word, if we can rearrange the order of operations in the schedule without changing the final result, then the schedule is conflict-serializable.   Source : Book page 892  The above is example of a schedule that is conflict-serializable. A sequence of swap is performed indicated by the underline.  ","version":"Next","tagName":"h3"},{"title":"Common Problem​","type":1,"pageTitle":"Concurrency Control","url":"/cs-notes/database-system/concurrency-control#common-problem","content":" Some common problem that occurs in concurrent transactions :  Deadlock : Deadlock is a situation where two or more transactions are waiting indefinitely for resources that are held by other transactions.Dirty Reads : Dirty read occurs when one transaction reads data that has been modified by another transaction that has not yet been committed. In other words, a transaction reads uncommitted data that may be rolled back later, leading to data inconsistency.Non-repeatable Reads : Non-repeatable reads occur when a transaction reads the same data multiple times during its execution, but the values of the data change between each read. This inconsistency can happen when another transaction modifies the data that the first transaction is reading.Phantom Reads : Phantom reads occur when a transaction reads a set of rows that satisfy a certain condition, but when it repeats the same read, additional rows are found that meet the condition. This can occur when another transaction inserts or deletes rows that match the condition.  ","version":"Next","tagName":"h3"},{"title":"Methods​","type":1,"pageTitle":"Concurrency Control","url":"/cs-notes/database-system/concurrency-control#methods","content":" Concurrency control mechanism are categorized into three :  Optimistic : Optimistic concurrency control assumes that conflicts between transactions are rare. In this approach, transactions are allowed to execute concurrently without blocking each other. When a transaction is ready to commit, it checks if any conflicts have occurred with other concurrently executing transactions. If conflicts are detected, the transaction is rolled back and can be retried with a new copy of the data. This approach can be efficient if conflict happened less.Pessimistic : Pessimistic concurrency control assumes that conflicts between transactions are likely to occur. In this approach, transactions must wait for each other until possibility of violation disappear (e.g., it's fine to let multiple transactions read the same data). This approach can lead to performance overhead during blocking.Semi-optimistic : Semi-optimistic concurrency control combines both optimistic and pessimistic approaches. It allows transactions to execute concurrently, similar to optimistic mode. However, at certain points in the transaction, it may check for conflicts and switch to a pessimistic mode if conflicts are detected.  Serializability Checking​  The schedule of transactions execution can be represented by a graph called the precedence graph. The method to check serializability involve checking the graph if a conflict is present. A conflict occurs when the order of two actions of different transactions cannot be swapped without affecting the final result of the schedule.   Source : https://www.geeksforgeeks.org/equivalent-serial-schedule-of-conflict-serializable-schedule-in-dbms/  In the precedence graph, each transaction is represented by a node, and there is a directed edge from one transaction to another if the former transaction must precede the latter transaction in the schedule. The edges in the graph represent the dependencies between transactions based on their read and write operations.  Locks​  Lock is a mechanism to prevent multiple transaction accessing the same database element. A lock can be acquired by a transaction, meaning that particular transaction has the access to the data and the others can't interfere it until the lock is released.  tip See also mutex.  The scheduler keep track a lock table, which contains the mapping that associates database elements with locking information specific to each element. Based on the lock table information, a transaction request will be delayed if another transaction is currently holding the lock.   Source : Book page 898  Notation for lock is :  li(X)l_i(X)li​(X) : Transaction iii requests a lock on database element XXX.ui(X)u_i(X)ui​(X) : Transaction iii releases or unlock its lock on database element XXX.  Below is an example of lock notation for transactions.   Source : Book page 899  Transaction 1 acquire lock and read the value database element AAA. It increases the value of it by 100 and write the result. After that, it releases the lock and transaction 2 continue. It's a valid schedule of transaction, but not serializable.  Two-Phase Locking​  Two-phase locking (2PL) is a locking mechanism that enforces a specific order of actions within a transaction that guarantees serializability.  It divides a schedule into two distinct phases : the lock acquisition phase and the lock release phase. In the lock acquisition phase, a transaction acquires all the necessary locks it needs to read or modify database elements before proceeding with its operations. In other word, all lock actions precede all unlock actions.  Once the transaction enters the lock release phase, it releases the locks it acquired during the lock acquisition phase, allowing other transactions to acquire them. Any transaction requesting the lock before it is released will be denied and delayed further.   Source : Book page 901 (with modification)  In this example, the transaction 1 acquire the lock for database element B before even unlocking A. This way, even if the schedule of transaction 2 precede it in instantaneous time, it won't be able to acquire B.  Timestamp Ordering (TO)​  This method assigns a unique timestamp to each transaction when it begins execution. The timestamps are used to order the transactions and determine their relative precedence. The system validates that transactions are executed in timestamp order, which means that a transaction with a higher timestamp is executed after a transaction with a lower timestamp.  There are two approaches to generating timestamps in this method :  One approach is to use the system clock as the timestamp, as long as the scheduler does not operate so fast that it could assign the same timestamp to two transactions.Another approach is to maintain a counter that is incremented by 1 each time a transaction starts, and the new value becomes the timestamp.  Each database element is associated with two timestamps and a commit bit. The two timestamps are read time (RT) and write time (WT), which are the highest timestamp of a transaction that has read and write the element, respectively. The commit bit is used to track whether the most recent write has already committed or not.  The commit bit gives an information to prevent a situation where one transaction reads data written by another transaction that subsequently aborts (dirty read).  In the case of conflict, we can allow the transaction with the earlier timestamp to proceed, while the transaction with the later timestamp may be rolled back and restarted or delayed. ","version":"Next","tagName":"h3"},{"title":"Index Implementation","type":0,"sectionRef":"#","url":"/cs-notes/database-system/index-implementation","content":"","keywords":"","version":"Next"},{"title":"B-Tree​","type":1,"pageTitle":"Index Implementation","url":"/cs-notes/database-system/index-implementation#b-tree","content":" Structure​  B-tree is a common data structure used to implement database index and the specific variant used is the b+ tree.   Source : https://stackoverflow.com/questions/870218/what-are-the-differences-between-b-trees-and-b-trees  B+-tree has linked structure on the leaf nodes, making it possible to traverse the full tree structure in a linear scan. In contrast, original b-tree require us to traverse every level in the tree.  Some key features of b-tree in indexing :  Balanced tree and also self-balancing, all the path from root to leaf have the same length.Index level is automatically adjusted based on the size of file being indexed.  B-tree organize index (key and pointer) as nodes (also called blocks) into tree.  In the block of b-tree :  Each block is maintained between half used and completely full. This means that there is a balance between utilizing the available space efficiently and reducing the need for frequent block splits and mergesB-tree copies the column used as the keys, and it will be distributed among the leaf nodes in sorted order.Each leaf node point to next leaf node on the right.Interior nodes, or any nodes that has child, will point to next b-tree block at the lower level. These nodes effectively splits the range of the key, similar to binary search.   Source : Book page 635, 636  B-trees can be used for indexing in the scenario of :  Data file is sorted or unsorted by the primary key, which act as the search key. This will result in a dense index, there will be key-pointer pair for every record in the data file.Data file is sorted by its primary key, then the b-tree serves as a sparse index with one key-pointer pair at a leaf for each block of the data file.Data file is sorted based on non-key attributes, and is the search key. Each key value will have one key-pointer pair at a leaf node, which will point to the first record with that key value.  info The difference between dense and sparse b-tree lies upon the pointer at the leaf nodes. A leaf node in dense b-tree will point to a record, while in sparse b-tree, it will point to a block.  Modification​  Insertion : When inserting a key to the node of a b-tree, we will need to find an empty room. If there is, simply just insert there. If not, we will need to split the node into two, and the new key is then inserted into the appropriate new leaf node. After the leaf node has been split and the new key has been inserted, the parent node of the leaf node needs to be updated. If there is enough room in the parent node, the new key-pointer pair is simply added to it. If there is not enough room, the parent node may need to be split as well, and the process continues recursively until the appropriate level is reached. If the root node needs to be split, then a new root will be created and the old root becomes its child. Source : https://levelup.gitconnected.com/building-a-b-tree-in-javascript-4482dee083cb Deletion : We first locate the key and its associated record in a leaf node. We then delete the record from the data file and remove the key-pointer pair from the b-tree. If the leaf node falls below the minimum occupancy, a rebalancing process occurs by borrowing from a sibling node or merging nodes. The process continues recursively until the tree remains balanced. Source : https://levelup.gitconnected.com/building-a-b-tree-in-javascript-4482dee083cb  ","version":"Next","tagName":"h3"},{"title":"Hash Table​","type":1,"pageTitle":"Index Implementation","url":"/cs-notes/database-system/index-implementation#hash-table","content":" Hash table is another option for creating index.  In the case of main memory index, hash table is very straightforward. We will create an array (called bucket array) and each would hold a pointer to linked list, which contains the actual records. A request with specific search key will be hashed, and it will fall to some bucket within the array. We will search the corresponding key in the list of the bucket and find the value.  Secondary Storage​  For secondary storage, the bucket array will store blocks that contain records. If the blocks can't hold more records, an overflow block. Overflow block holds a pointer that points to the next block, linking them all together and form a chain.   Source : Book page 650  The image above is an example of hash table in secondary storage, where each block contains maximum of two records.  Insertion : To insert a record, we will get the hash code by applying the hash function into the key. We will then insert the record in the corresponding block of the produced bucket. If there is no room, we will create additional block in the overflow block.Deletion : We will search for the corresponding key in the bucket produced by the hash function and delete them all. We may also move record to the previous block if needed to reduce the number of blocks.  ","version":"Next","tagName":"h3"},{"title":"Hash Table & B-Tree Comparison​","type":1,"pageTitle":"Index Implementation","url":"/cs-notes/database-system/index-implementation#hash-table--b-tree-comparison","content":" I/O Operation : In b-tree, a large number of keys per node will reduce the I/O operation by a lot. Insertion and deletion will rarely happen, therefore reducing the I/O cost for reorganization. For query, the number of I/O operation will be based on the depth of the tree, this is because a single node is a block which require a single I/O operation. A well distributed bucket will reduce I/O operation for query, insertion, and deletion. In the case of bucket holding tons of block, we will need to traverse a long list of blocks, increasing the I/O cost significantly. One may to mitigate is to make the hash table to grow or shrink based on the number of records. Range Queries : The sorted order of records in a B-tree allows for easy traversal of consecutive keys, making it good for range-based queries. On the other hand, hash table is not sorted at all, and is not ideal for range-based queries.  ","version":"Next","tagName":"h3"},{"title":"Dynamic Hash Table​","type":1,"pageTitle":"Index Implementation","url":"/cs-notes/database-system/index-implementation#dynamic-hash-table","content":" To enable dynamic hashing, or the method to remove bucket on demand, there are two methods.  Extensible Hash Tables​  In extensible hashing, the bucket array in the hash table is organized as a directory of pointers that points to the actual data blocks. Each bucket can hold multiple records, and the number of buckets can dynamically grow or shrink as needed (typically in power of two). The directory provides indirection to the appropriate bucket based on the hash value of the key. This method minimizes the need for data reorganization when the size of the hash table increases.  The hashing is kinda different with traditional hash table. The hash function will produce a sequence of kkk bits. The number of bucket will be based on 2i2^i2i, where iii is taken from the last iii bits of kkk.   Source : Book page 653  The above is an example of extensible hash table where k=4k = 4k=4 and i=1i = 1i=1. The number associated with the bucket determine which block should it holds. The bucket 0 will hold all the block that store with the bit 0, similar to bucket 1. We still use linked list and overflow block in extensible hash table.  For searching, we will hash the search key to find the location of which bucket does the search key fall into. We will then extract iii subset of bits from the produced hash.   Source : http://www.cs.emory.edu/~cheung/Courses/554/Syllabus/3-index/extensible-hashing-new2.html  In some case, we may not need the entire iii bits to identify the data blocks. For example, bit 01 is enough to distinguish between 0000 and 0100. The minimum number of bits used in the hash value to locate the keys in the block is denoted by jjj.  Insertion : We do the similar thing as search and insert the record if there is room in the block. If not, there are two possibilities, when j&lt;ij &lt; ij&lt;i and j=ij = ij=i. When insertion happens, we do not insert directly, we instead check whether j&lt;ij &lt; ij&lt;i or j=ij = ij=i first. First possibility, we can locate the block with less than the number of bits used in the overall bucket size, thus we don't need to change the bucket array. We will need to split the block into two, and distribute the records inside it to the two blocks. The distribution will be like, if the key has 0 in it, then stay in the old block, else move it to the new block. Finally, we will need to adjust the pointer in the bucket array so the record that formerly points to old block is pointed into the new block. Second possibility, we will need to increase the number of bucket, so we will increment iii by 1. In this case, when we want to insert, we must take the number of bits equal to iii. In other word, the sequence of bits produced by hash function, denoted as www, must equal to iii bits. After incrementing iii, the bucket array will be extended, and the entries indexed by www will be extended by 0 and 1, becoming w0w0w0 and w1w1w1. Now they will point to the same block, and the block doesn't change. After that, we will now do the actual insertion, and we will get the first possibility, which will split the block. Source : Book page 653, 654  Linear Hash Tables​  Extensible hash table has some drawbacks. One disadvantage arises during insertion when there is a need to expand the number of buckets. This expansion process can involve splitting a significant number of blocks, particularly when the level iii becomes substantial. In some cases, doubling the array size may result in the hash table exceeding the available main memory capacity, requiring additional disk I/O operations to relocate the hash table.  Linear hash tables has different strategy :  The number of buckets is chosen to ensure that the average number of records per bucket remains fixed, e.g., 80% of the number of records that can fill one block.Overflow block is allowed to hold additional records. The average number of overflow blocks per bucket is typically less than 1.The number of bits used to number the entries of the bucket array is equal to log⁡2(n)\\log_2(n)log2​(n), where nnn represents the current number of buckets. These bits are always taken from the right (low-order) end of the bit sequence generated by the hash function.   Source : Book page 657  Example of linear hash table with i=1i = 1i=1 (number of bits of the hash function currently are used), n=2n = 2n=2 (current number of buckets), and r=3r = 3r=3 (current number of records). The rate of average number of records per bucket is represented as ratio r/nr/nr/n.  Insertion : When a new record needs to be inserted, the hash function is applied to its key to compute the hash value. Next, a number of bits iii, are extracted from the end of the hash value to determine the bucket number, mmm. If mmm is less than the total number of buckets, denoted as nnn, the record is placed in bucket mmm. However, if mmm is greater than nnn, the record is placed in bucket m−2(i−1)m - 2^{(i-1)}m−2(i−1). If there is no room, then create an overflow and put it there. Source : Book page 658 After insertion, if the r/nr/nr/n exceed our limit, we will add another bucket, and all the previous bucket get additional 0 bit.  ","version":"Next","tagName":"h3"},{"title":"Multidimensional Index​","type":1,"pageTitle":"Index Implementation","url":"/cs-notes/database-system/index-implementation#multidimensional-index","content":" Multidimensional Index is an index that is based on multiple search key (two or more). One of the implementation of multidimensional index is the grid files.  Grid files divide the multidimensional space (represented as coordinate) into a grid of cells, where each cell corresponds to a bucket that contains the data points falling within that cell. The grid structure allows for efficient lookup by determining the cell in which a data point falls, and then searching within that cell.   Source : Book page 666  The above is an example of grid files in 2-dimensional space with 12 points. The dotted lines represent range queries taking point that range between 40 and 55, and the salary between 90K and 225K.  Each of the regions into which a space is partitioned is considered as a bucket. The points that fall in the region has its record placed in a block belonging to that bucket.  To determine the appropriate bucket for a point, we need to know the list of values at which the grid lines occur in each dimension. These values help in identifying the specific region or grid cell where the point belongs.   Source : Book page 667  For insertion, we can either add overflow blocks to the bucket, or reorganize the structure by adding or moving the grid lines.  Performance &amp; Application​  Multidimensional index is useful for queries like :  Partial-match queries : Partial match queries are the type of query where we specify values for one or more dimensions and search for all points that partially match those specified values in those dimensions. The ability to slice regions using grid lines is useful for partial match queries.Range queries : Grid files partition the space of points into regions using grid lines. This partitioning enables efficient range queries by identifying the relevant regions and retrieving the data within those regions.Nearest-neighbor queries : Nearest-neighbor queries involve finding the data points that are closest to a given reference point in multiple dimensions. Grid files can efficiently support nearest-neighbor queries by identifying and retrieving data points within the closest regions or buckets.Where-am-I queries : Grid files partition the space of points using grid lines, with each point falling into the stripe for which the corresponding grid line is the lower boundary. Analyzing the grid lines would efficiently answer where-am-I queries.  As the dimension get larger, the number of bucket grows exponentially, and may not be well distributed. The number of disk I/O operation depends on the dimensions and the type of query.  For example, in partial-match query, such as &quot;find all customers aged 50&quot;, we will need to look either at the row or column of the grid. The number of disk I/O can be quite high if there are many buckets in a row or column, but only a small fraction of all the buckets will be accessed. ","version":"Next","tagName":"h3"},{"title":"Logging & Recovery","type":0,"sectionRef":"#","url":"/cs-notes/database-system/logging-and-recovery","content":"","keywords":"","version":"Next"},{"title":"Logging​","type":1,"pageTitle":"Logging & Recovery","url":"/cs-notes/database-system/logging-and-recovery#logging","content":" Logging is the process of recording activities and changes that occur within a computer system. A log can be a problem, error, or just information on current operations.  Transaction Log​  Database operations are grouped into a unit of transaction. Transaction log is the log file that contains all the transactions and changes made to the database over time.  Keeping track of database operation can be helpful to maintain data integrity. In the case of system failures, the DBMS can review the log and rolls back the state of database to a consistent state. By replaying the logged transactions, the system can reapply the changes made by committed transactions or undo the changes made by uncommitted or rolled-back transactions.  Database logs are linked together, forming a linked list. A log record is made up of :  Log Sequence Number (LSN) : LSN is a unique ID of a log record, it is assigned to each transaction record.Prev LSN : A reference to the previous log record in the linked list structure of the log.Transaction ID : The associated transaction of a log record.Type : Type of database log record. Update : It can be an update or change to database, it will include a reference to the modified page, length and offset of the page, and size of the page before and after the update.Compensation : A log about rollback of particular changes to the database. Each record is associated with the update log record, it also includes the next log record that needs to be undone for the transaction that wrote the last update log record.Commit : A decision to commit a transaction, which is a successful completion of a transaction.Abort : The decision to abort and rollback a transactionCheckpoint Changes made in the memory is called dirty pages, a checkpoint is when all the dirty pages has been flushed or written to disk. The checkpoint record contains : redo LSN : Reference for first update after checkpoint, it is the starting point of redo on recovery.undo LSN : Reference to starting point for undoing the changes made by that transaction. Completion : Information about all work that has been done, either aborted or committed, for a particular transaction. Actual Information : Contains the necessary information to describe the changes made by the transaction that causes the log to be written.   Source : https://sqlbak.com/academy/transaction-log/  During logging transaction, it can be implemented using the write-ahead logging (WAL) technique. This technique write changes to the database to a log file, along with undo and redo information, before they are applied to the actual data pages on disk.  The transaction and logging usually happens periodically. Transaction are written to the log, after some point, the system will perform a checkpoint, writing all the changes to the database and clear the logs.  ","version":"Next","tagName":"h3"},{"title":"Replication​","type":1,"pageTitle":"Logging & Recovery","url":"/cs-notes/database-system/logging-and-recovery#replication","content":" Replication is the process of maintaining a copy of data for fault-tolerance.  Key concept of data replication :  Replicated data can be stored in the same device or distributed in multiple device.Replicated data can be all or some subset of the data.Replica is typically updated frequently.To synchronize with modification, we can either modify the primary and replicas simultaneously, or modify the primary data first and all the replicas after.  Some techniques of replication :  Master-Slave Replication : In this approach, there is a single master database that handles write operations, while one or more slave databases replicate the data from the master. The slaves are read-only replicas, they can only serve read queries from clients. The master propagates the changes to the slaves asynchronously or synchronously in some amount of time.Multi-Master Replication : In this approach, multiple database instances function as both masters and slaves. Each master can handle write operations independently, and changes made on one master are replicated to the other masters.Multi-Level Replication : This approach creates a hierarchy of replicas, where changes are propagated through multiple levels of replication. For example, changes made to a primary database are replicated to secondary replicas, which in turn replicate the data to tertiary replicas.  It is common for conflict to occurs in multi-master replication. One way to resolve conflict is to abort and rollback based on timestamp of when the transaction was done.  RAID​  RAID is a fault-tolerance technique that involve creating redundancy in computer storage by distributing data across multiple disks.  See RAID.  ","version":"Next","tagName":"h3"},{"title":"Migration​","type":1,"pageTitle":"Logging & Recovery","url":"/cs-notes/database-system/logging-and-recovery#migration","content":" Migration is the process of moving or transferring a database from one system or environment to another. Database migration transfer data, schema, and associated objects from the source system to the target system while ensuring data integrity and compatibility.  A migration is needed when a specific changes applied to a database alters its structure. Some scenarios are :  Schema Changes : When modifying the structure of a database, such as adding, modifying, or removing tables, columns, indexes, or constraints.Data Transformations: This may include data conversions, normalization, denormalization, etc.Version Upgrades : When upgrading the DBMS to a newer version, a migration is often required to ensure compatibility with the new database software.System Upgrades : Scenario of system upgrades, or when moving data to a different platform, such as migrating from an on-premises database to a cloud-based solution. ","version":"Next","tagName":"h3"},{"title":"Normalization","type":0,"sectionRef":"#","url":"/cs-notes/database-system/normalization","content":"","keywords":"","version":"Next"},{"title":"Functional Dependency​","type":1,"pageTitle":"Normalization","url":"/cs-notes/database-system/normalization#functional-dependency","content":" Functional Dependency is a concept that describes the relationship between column or sets of attributes within a table. It specifies how the values of one or more attributes determine the values of other attributes. Functional dependency is an important concept for normalization.  One attribute, known as the determinant, uniquely determines the value of another attribute, known as the dependent. This means that for a given value of the determinant, there can be only one corresponding value for the dependent attribute.  Functional dependencies are denoted using arrow notation (→). For example, if A and B are attributes in a table, A → B represents a functional dependency where the value of attribute A uniquely determines the value of attribute B. So, if we need a value of B, we can find it by knowing the value of A.  note In actual application, functional dependency is the one that you find on primary key and foreign key relationship.  There are different types of functional dependencies :  Full functional dependency : A functional dependency A → B is considered full if no proper subset of A determines B. In other words, removing any attribute from A would result in the dependency no longer holding.Partial functional dependency : A functional dependency A → B is considered partial if there is a proper subset of A that also determines B. In this case, removing any attribute from A would still preserve the dependency.Transitive dependency : A transitive dependency occurs when there is a functional dependency A → B and B → C, which implies a transitively dependent relationship between A and C. In other words, A indirectly determines C through B.  Heath's Theorem​  Given a relation or table RRR with attributes X,Y,X, Y,X,Y, and ZZZ, where X→YX → YX→Y is a functional dependency, and ZZZ is defined as the set difference between UUU, which is some set of RRR and XYXYXY, then R=∏XY(R)⋈∏XZ(R)R = \\prod_{XY}(R) \\bowtie \\prod_{XZ}(R)R=∏XY​(R)⋈∏XZ​(R).  Where ∏\\prod∏ is a symbol for projection, and ⋈\\bowtie⋈ is a symbol for natural joins.  info Natural join : a join that combines two relations based on matching values in their common attributes, without duplicate columns for the common attributes.  In simpler term, the expression says that a relation can be decomposed into two smaller relations to eliminate redundancy while still maintaining the original functional dependency, which is used to reconstruct the decomposed relation.  The expression describes the decomposition, it is achieved by projecting the attributes XXX and YYY from RRR and joining this projection with the projection of attributes XXX and ZZZ from RRR.  ","version":"Next","tagName":"h3"},{"title":"1NF​","type":1,"pageTitle":"Normalization","url":"/cs-notes/database-system/normalization#1nf","content":" In First Normal Form (1NF), a table must satisfy atomicity.  Atomic Values : Each column in a table should contain only atomic values, meaning that each value is indivisible. There should be no repeating groups or arrays of values within a single column.  info Example here are taken from the Wikipedia page.   Source : https://en.wikipedia.org/wiki/Database_normalization#Satisfying_1NF  A book record containing an array in the subject column can be separated into two tables. The first table holds the book data without the subject column, and the second table contains the removed subject data, with a foreign key of ISBN that refers to the primary key in the book table.  ","version":"Next","tagName":"h3"},{"title":"2NF​","type":1,"pageTitle":"Normalization","url":"/cs-notes/database-system/normalization#2nf","content":" In Second Normal Form (2NF), the table must meet 1NF and has no partial dependency.   Source : https://en.wikipedia.org/wiki/Database_normalization#Satisfying_2NF  In the book data above, the composite key is title and format. Partial dependency is when one of the column depend on one of the composite key, but not all. For example, the Price column depends on format, but not title.  To conform 2NF, all non-key attributes (attributes that are not part of the primary key) must be fully functionally dependent on the entire primary key.  The solution is separating the book and price table, so the other column on book table depends only on title, and the price in the price table also depend on title.  ","version":"Next","tagName":"h3"},{"title":"3NF​","type":1,"pageTitle":"Normalization","url":"/cs-notes/database-system/normalization#3nf","content":" In Third Normal Form (3NF), the table must meet 2NF and has no transitive dependency.  A transitive dependency occurs when a non-key attribute in a table depends on another non-key attribute, rather than directly on the primary key. In other words, there is an indirect relationship between non-key attributes through other non-key attributes.  For example, in the 2NF book table, the Author Nationality is dependent on Author, which is dependent on Title. To eliminate transitive dependency, we would separate all the column that violates it.   Source : https://en.wikipedia.org/wiki/Database_normalization#Satisfying_3NF  ","version":"Next","tagName":"h3"},{"title":"4NF​","type":1,"pageTitle":"Normalization","url":"/cs-notes/database-system/normalization#4nf","content":" In Fourth Normal Form (4NF), the table must meet 3NF and has no non-trivial multivalued dependencies.  A multivalued dependency occurs when there is a relationship between two sets of attributes such that for every instance of a particular value of one set of attributes, there can be multiple instances of the other set of attributes. For example, consider attributes A, B, and C, where A determines B and B determines C. In this case, we can have multiple instances of C for each instance of B, indicating a multivalued dependency between B and C.  A non-trivial multivalued dependency is one that is not trivially implied by the existing dependencies in the schema.  To eliminate multivalued dependencies, we would need to decompose the columns into separate tables.   Source : https://en.wikipedia.org/wiki/Database_normalization#Satisfying_4NF  In this table, Title depend on Franchisee ID, and Location depend on title. In the normalized form, now Location can depend on Franchisee ID directly.  ","version":"Next","tagName":"h3"},{"title":"5NF​","type":1,"pageTitle":"Normalization","url":"/cs-notes/database-system/normalization#5nf","content":" In Fifth Normal Form (5NF), the table must meet 4NF and has no join dependencies. Join dependency occurs when a table can be logically reconstructed by joining two or more smaller tables together based on their common attributes.  So, to meet 5NF, a table must not be able to be reconstructed from join query. To examine if a table meet 5NF or not, we would try decomposing it into several tables and see if joining them together reconstructed the original table.  Typically, a 4NF table already meets 5NF.   Source : https://en.wikipedia.org/wiki/Database_normalization#Satisfying_5NF  We will first decompose the first table into two separate tables. We won't be able to fully reconstructed the table if we join them. We will then decompose the table again into three separate tables. At this point, the three table can't be joined anymore, therefore the original table meet 5NF.  ","version":"Next","tagName":"h3"},{"title":"Denormalization​","type":1,"pageTitle":"Normalization","url":"/cs-notes/database-system/normalization#denormalization","content":" Normalization reduce data redundancy by eliminating duplicate data by separating them into multiple table and use a foreign key to link between them. While normalization can be good to reduce redundancy, query can be overhead. This is because we will need to perform join operation to retrieve the data separated into several tables.  Denormalization is the opposite of normalization, we will instead introduce duplicate data or combine related data into single table to reduce joins operation. We can also create a summary table to store pre-aggregated data to reduce the need for complex calculations during query execution.  tip Denormalization is different with unnormalized form. Unnormalized form means that the table is not normalized yet. ","version":"Next","tagName":"h3"},{"title":"NoSQL","type":0,"sectionRef":"#","url":"/cs-notes/database-system/nosql","content":"","keywords":"","version":"Next"},{"title":"Key-Value​","type":1,"pageTitle":"NoSQL","url":"/cs-notes/database-system/nosql#key-value","content":" A key-value NoSQL database stores data in a key-value pair. A key is a unique identifier for the data item, and the value is the data item itself.  One way to implement key-value database is to use hash table. Hash table is a data structure that stores data in an array, using a hash function to map keys to array indices. This allows for very fast lookups and insertions, which is suitable for storing random or unstructured data.   Source : https://www.michalbialecki.com/2018/03/18/azure-cosmos-db-key-value-database-cloud/  The above is an example of key-value database that stores different data types as its values, one key may correspond to an integer, string, or another key-value pairing.  Advantages &amp; Disadvantages :  Simple, flexible, and easy to use.Very quick for read, insert, delete, and update.Limited query abilities, one key can only map to single data.Because it's not relational, we can't connect data together.  ","version":"Next","tagName":"h3"},{"title":"Document​","type":1,"pageTitle":"NoSQL","url":"/cs-notes/database-system/nosql#document","content":" Document database is a subclass of key-value database. It extends the simple key-value mapping by allowing specific format or encoding to represent a more complex structure.  Document database may organize or groups document by :  Collections : Collections are analogous to tables in relational databases. A collection consists of a group of documents (analogous to records) that share a similar structure or purpose.Tags : Tags are keyword or label within a document, which provide information about the content or characteristics of the document.Non-visible metadata : Non-visible metadata include information such as timestamps, versioning, access control, or any additional attributes related to the documents that doesn't impact the visual content.Directory hierarchies : Document database such as XML organizes its data into a hierarchical, directory-like structure through the use of nested tags.  Document database includes JSON, XML, and YAML.   Source : https://thedeveloperstory.com/2021/10/03/everything-you-need-to-know-about-yaml-files/  Advantages &amp; Disadvantages :  Same as key-value database, they are flexible. They allow documents of varying structures and fields to be stored within the same collection.A more advanced way to store data, we can provide additional metadata or label which helps us for complex queries.Unlike relational databases, document databases have limited support for transactions. This can make it challenging to maintain data consistency and integrity in certain scenarios.  ","version":"Next","tagName":"h3"},{"title":"Graph​","type":1,"pageTitle":"NoSQL","url":"/cs-notes/database-system/nosql#graph","content":" A graph NoSQL database uses a graph structures to represent and store data.  Graph structure consist of :  Nodes : An entity or object that exist within the graph structure, analogous to record in relational database.Properties : Information associated with nodes.Edges : Edges are connection between two nodes, they are lines that connects nodes together. Edges can be directed, which mean they have a one-way relationship, or undirected, which mean a symmetric relationship. Edges can also be labeled to indicate their relationship.   Source : https://en.wikipedia.org/wiki/Graph_database#/media/File:GraphDatabase_PropertyGraph.svg  One example of graph database is Neo4j, the query language used to access the graph structure is called Cypher, below is an example :   Source : https://neo4j.com/product/cypher-graph-query-language/  A two node of Person, named Dan and Ann, respectively is matched together in a LOVES relationship.  Under the hood, node is represented as a fixed-size record or object on the disk. An edge relationship is represented as doubly linked list, it has pointer to the start and end nodes. Node's properties are organized in a linked-list, with each node being a key-value pair.  The query engine traverse the graph, following the graph relationship. It decides whether to continue the traversal or include the node and properties in the result set based on the given query.  Advantages &amp; Disadvantages :  Can represent connection between entities, this allows for complex queries and analysis of complex relationships.Flexibility in storing different types of data.The graph database ecosystem is relatively diverse, with multiple database systems and query languages available. This lack of standardization can lead to concerns regarding vendor lock-in and interoperability.Can be harder to learn compared to relational database. This involves learning specific query language and understanding graph modeling principles.  ","version":"Next","tagName":"h3"},{"title":"Vector​","type":1,"pageTitle":"NoSQL","url":"/cs-notes/database-system/nosql#vector","content":" Vector database stores data in a mathematical object called vectors. Vectors are mathematical representations of objects or data points in a multidimensional space. Each dimension of the vector corresponds to a specific attribute of the object (called features). The location of vector in the multidimensional space represent their characteristics.  The process of turning data, such as words, phrases, documents, images, audio, or any other data, is called embedding. Each data is transformed into a numerical representation. Embedding involves mapping all numerical representation of the data into a common representation in the single multidimensional space. The embedding process typically uses machine or deep learning techniques.   Source : https://www.elastic.co/what-is/vector-database  Vector databases excel at similarity search, where the goal is to retrieve similar data from the given data. Searching for similar data involves turning the given data into another vector and finding the closest vector (also known as nearest neighbor) that is available in the multidimensional space with the query vector. The method to find distance can use metrics like Euclidean distance or cosine similarity.  Example​  Consider the scenario where we are constructing a vector database to store words. The primary objective of this database is to facilitate the identification of similar words.  We have the following words :  Word: &quot;cat&quot; - Embedding: [0.2, 0.3, 0.1, 0.5]Word: &quot;dog&quot; - Embedding: [0.1, 0.4, 0.2, 0.6]Word: &quot;elephant&quot; - Embedding: [0.4, 0.1, 0.6, 0.5]Word: &quot;lion&quot; - Embedding: [0.7, 0.6, 0.8, 0.1]Word: &quot;tiger&quot; - Embedding: [0.6, 0.5, 0.3, 0.7]  A machine model should be trained on the word dataset, and it should produce some numerical representation. For simplicity, let's say the embedding is like above. The word &quot;cat&quot; with embedding [0.2, 0.3, 0.1, 0.5] means it will be located on coordinate (0.2, 0.3, 0.1, 0.5) in the multidimensional space.  Let's say we have the word &quot;kitten&quot; as the query, and the embedding is [0.25, 0.35, 0.15, 0.55].  Similarity with &quot;cat&quot; ≈ 0.997798Similarity with &quot;dog&quot; ≈ 0.973726Similarity with &quot;elephant&quot; ≈ 0.792752Similarity with &quot;lion&quot; ≈ 0.640261Similarity with &quot;tiger&quot; ≈ 0.969144  ","version":"Next","tagName":"h3"},{"title":"BASE Properties​","type":1,"pageTitle":"NoSQL","url":"/cs-notes/database-system/nosql#base-properties","content":" NoSQL typically lacks strong consistency and true transactions compared to relational databases. BASE properties are in contrast to ACID properties in relational database.  Basically Available (BA) : The importance of providing high availability for read and write operations, even in the presence of failures or network partitions. A system should strive to remain operational and responsive, even under challenging conditions.Soft State (S) : NoSQL tolerate temporary inconsistencies, but the state across system should eventually be consistent.Eventually consistent (E) : Eventually consistency means that the system will eventually reach a consistent state across all system or replicas. While updates may take some time to propagate and synchronize, the system guarantees that, given enough time and absence of further updates, all replicas will converge to a consistent state.  We can infer that NoSQL tends to prioritize availability over consistency. ","version":"Next","tagName":"h3"},{"title":"Query Compiler","type":0,"sectionRef":"#","url":"/cs-notes/database-system/query-compiler","content":"","keywords":"","version":"Next"},{"title":"Parsing & Preprocessing​","type":1,"pageTitle":"Query Compiler","url":"/cs-notes/database-system/query-compiler#parsing--preprocessing","content":" Syntax &amp; Parse Tree​  The query written in language like SQL is parsed and converted into a parse tree. In the parse tree, a node can either be an atom or syntactic categories.  Atoms are lexical elements like built-in keywords (e.g., SELECT), names of attributes or relations, constants, parentheses, and operators.  Syntactic categories are category of syntax, they are typically represented as descriptive name enclosed with triangular brackets. For example, a &lt;Condition&gt; means an expression that represents a condition. A syntax category have children, which describe how the expression is constructed. The children must follow the grammar or the rules of the language.  For example, the rules for query is &lt;Query&gt; ::= SELECT &lt;SelList&gt; FROM &lt;FromList&gt; WHERE &lt;Condition&gt;. This mean that a query expression &quot;can be expressed as&quot; (denoted by ::= symbol) the right hand side syntax. &lt;SelList&gt; and &lt;FromList&gt; represent another syntax category that can be used with SELECT and FROM statement.  The rules for &lt;SelList&gt; is &lt;SelList&gt; ::= &lt;Attribute&gt;, &lt;SelList&gt; or &lt;SelList&gt; ::= &lt;Attribute&gt;. This mean &lt;SelList&gt; can be represented with a single attribute, or another &lt;SelList&gt;, which can be another attribute (recursive).  Other syntax category :   Source : Book page 761, 762  An example of a query and its subquery; and it's parse tree :   Source : Book page 763  Preprocessor​  After a parse tree is constructed, the preprocessor is responsible for checking the semantic rules. For example, the preprocessor may check if the relation used in the FROM clause is really a relation. It also checks if attributes and data types is valid and conform to the database schema.  Once the semantic checks are complete, the preprocessor performs some tree transformations to convert the parse tree into a tree of algebraic operators. This transformed tree represents the initial or logical query plan. The nodes of parse tree are replaced with algebraic operators that represent the operations of relational algebra that need to be performed.  For example, the previous parse tree example can be turned into this expression tree :   Source : Book page 782  The produced expression tree now does not have subqueries, they are all replaced with relational algebra operators.  πmovieTitle\\pi_{\\text{movieTitle}}πmovieTitle​ : Projection operator or the SELECT movieTitle clauses.σstarName = name AND birthdate LIKE ’%1960’\\sigma_{\\text{starName = name AND birthdate LIKE '\\%1960'}}σstarName = name AND birthdate LIKE ’%1960’​ : selection operator or the WHERE ... clauses.  Another example of expression tree, where the query contains another subquery in the condition :   Source : Book page 784  After constructing the expression tree, the preprocessor may improve the logical query plans based on some relational algebra laws.  In some cases, when we have a projection or selection query with one or multiple conditions, we can split and push each condition down the expression tree separately. By doing this, we can filter out unnecessary data as early as possible, which reduces the amount of data that needs to be processed. Some grouping operators such as joins are associative or commutative. By rearranging the order and grouping of these operators, we can minimize the number of operations needed to perform the join. ⋈\\bowtie⋈ and ⋃\\bigcup⋃ are symbol for natural join and union, respectively Source : Book page 791  ","version":"Next","tagName":"h3"},{"title":"Cost Estimation​","type":1,"pageTitle":"Query Compiler","url":"/cs-notes/database-system/query-compiler#cost-estimation","content":" To turn logical query plan into physical query plan, we will need to know what physical plans are possible. After that, we will choose the physical query plan that will be executed by the engine based on the cost of operations. The least estimated cost will be chosen, this method is called cost-based enumeration.  Depending on the physical plan, we may choose different method to process the data.  The order and grouping for associative-and-commutative operators such as joins, unions, and intersections.Choosing which algorithm should we use to implement the operator, for example, deciding whether a nested-loop join or a hash-join should be used.Operator such as scanning or sorting to support the query.Method to pass data after each operator, for example, storing results in disk, processing one tuple, or one main-memory buffer at a time.  Some example to estimate operations :  Projection : The projection operation cannot be exactly computed. The only certainty is that the size will always be less than or equal the size of the relation by some factor. Selection : Depending on the condition, the estimation for selection may differ. In simple case where we select the attributes equal to some value, then it would be T(S)=T(R)/V(R,A)T(S) = T(R) / V(R, A)T(S)=T(R)/V(R,A). T(S)T(S)T(S) represent the number of tuple, T(R)T(R)T(R) represent the total number of tuple in relation RRR, and V(R,A)V(R, A)V(R,A) represent the number of distinct values in relation RRR for attribute AAA. Join : Join involve matching together same attributes of tuple from two relations. Join estimation would consider how relate are the two relations. The join estimation formula is : T(R⋈S)=T(R)T(S)/max(V(R,Y),V(S,Y))T(R \\bowtie S) = T(R)T(S)/\\text{max}(V(R, Y), V(S, Y))T(R⋈S)=T(R)T(S)/max(V(R,Y),V(S,Y)). The intuition behind this formula is that the size of the join result depends on the number of possible combinations between tuples from R and S that share a common value in Y. If the number of distinct values in Y is large, the probability of finding matching tuples between R and S decreases, resulting in a smaller join size. On the other hand, if the number of distinct values in Y is small, the probability of finding matching tuples increases, leading to a larger join size.  ","version":"Next","tagName":"h3"},{"title":"Plan Enumeration & Selection​","type":1,"pageTitle":"Query Compiler","url":"/cs-notes/database-system/query-compiler#plan-enumeration--selection","content":" After estimation of operators are determined, the optimizer needs to estimate the cost of evaluating certain expressions. They are based on the number of disk I/O operations performed, which can be influenced by similar factors as before :  Choice of logical operators : The selection of logical query plans includes decisions such as join methods, sorting, and filtering.Sizes of intermediate resultsPhysical operators used : The choice of physical operators to implement logical operators. For example, the decision to use a one-pass or two-pass join, or whether to sort a relation or not.Ordering of similar operations, which join on which relations to be performed first.Method of passing arguments  There are several approaches to enumerate or explore all possible physical plans :  Top-down approach : Start from the root of the logical query plan down to bottom of the tree. For each possible implementation, we enumerate the physical plans and evaluate their costs.Bottom-up approach : Keeps track of the plan of the least cost for each subexpression. By considering the cost of each subexpression, the overall cost of the query plan can be obtained by combining them all.Heuristic Selection : Heuristic plans follows rules or guidelines that are likely to produce good results. For example, we can start by joining the pair of relations with the smallest estimated size and then repeating the process for the result of that join and the remaining relations. This heuristic aims to minimize the intermediate result sizes and reduce computational overhead by joining smaller relations first.Hill climbing approach : This approach involves searching for a &quot;valley&quot; in the space of physical plans and their costs. It starts with a heuristically selected physical plan and makes small changes to the plan, such as replacing one execution method with another or reordering joins using associative and commutative laws. The goal is to find nearby plans that have lower costs. When no further modifications yield a plan with a lower cost, the current plan is chosen as the physical query plan.  ","version":"Next","tagName":"h3"},{"title":"Completing the Physical-Query-Plan​","type":1,"pageTitle":"Query Compiler","url":"/cs-notes/database-system/query-compiler#completing-the-physical-query-plan","content":" The last step to turn logical query plan into physical query plan are :  Algorithm Selection : The algorithms to implement each operation are selected based on factors such as available indexes, statistical information about relation sizes, and other metadata.Materialization and Pipelining : Decision of whether to materialize intermediate results or pipeline them depends on factors such as available memory, disk space, and the size of the intermediate results. Materialization involves creating the entire intermediate result and storing it on disk, which can be useful when the result is large or needs to be reused later.Pipelining involves creating the intermediate result only in main memory, without necessarily keeping it in its entirety at any given time. Physical Plan Implementation : The specific details regarding access methods for stored relations and algorithms for implementing relational-algebra operators. Access methods refer to the techniques used to retrieve tuples from stored relations, such as table scans, index scans, or sort scans. ","version":"Next","tagName":"h3"},{"title":"Query Processing","type":0,"sectionRef":"#","url":"/cs-notes/database-system/query-processing","content":"","keywords":"","version":"Next"},{"title":"Physical-Query-Plan Operators​","type":1,"pageTitle":"Query Processing","url":"/cs-notes/database-system/query-processing#physical-query-plan-operators","content":" Physical-Query-Plan Operators are the component of query planning that does a specific step of a plan, which often correspond to an operation in relational algebra.  The physical query plan is generated after the query has been compiled into a logical query plan, which is similar to expressions of relational algebra. The generation of physical query plan involve these operators.  An example of physical-query-plan operator is scanning table. Scanning table operators read table and its record, they are useful for us to know the table's data. There are three approach of scanning table :  Table-scan : Read record in the block one by one.Index-scan : Uses index to read the blocks and records.Sort-scan : If the table can fit in main-memory, we may sort it and read the records.  ","version":"Next","tagName":"h3"},{"title":"One-Pass Algorithms​","type":1,"pageTitle":"Query Processing","url":"/cs-notes/database-system/query-processing#one-pass-algorithms","content":" In the case of database, one-pass algorithms are category of algorithms that reads data from the disk, process it, and write the results back in the disk, all in one pass.  There are three types of one-pass algorithms :  Tuple-at-a-time, unary operations : Read and process one block at a time, require a small amount memory. This type of algorithms is used in operation like projection and selection. These kinds of operations are implemented by loading records from the disk into the input buffer and the operation is performed on them (e.g., checking for WHERE condition). The result of the operation goes to the output buffer. Source : Book page 711 Full-relation, unary operations : Require most or all record, the amount of record processed at once is limited to the memory buffers available, this includes grouping and eliminating duplicate operation. The process of eliminating duplicate is similar to the previous operation. Before sending the result to the output buffer, we will first check if the record has been seen before. The previously seen record is stored at another buffer, we may use hash table to speed up the search. Source : Book page 713 Full-relation, binary operations : Require most or all record and performed on two table or records, such as union and intersection. For example, set intersection can be done by storing the record of one set (table) into some buffer, and read the other set and see if the record is present in the buffer. If so, we can copy to the output buffer.  info Unary operations are operation performed on single input, while binary is performed on two inputs.  ","version":"Next","tagName":"h3"},{"title":"Nested-Loop Join​","type":1,"pageTitle":"Query Processing","url":"/cs-notes/database-system/query-processing#nested-loop-join","content":" Nested-Loop Join is a family of algorithms that joins two tables. There are some variants of nested-loop join :  Tuple-Based : A straightforward algorithm that loops over two sets (tables) and produce another tuple that satisfy certain join condition. Source : Book page 719 Block-Based : The block-based algorithm is an improvement over tuple-based algorithm. It involves considering a block of tuples instead of individual tuple, to reduce the disk I/O operation. Furthermore, we will store as many tuple from the outer set in main memory.  ","version":"Next","tagName":"h3"},{"title":"Two-Pass Algorithm​","type":1,"pageTitle":"Query Processing","url":"/cs-notes/database-system/query-processing#two-pass-algorithm","content":" One-pass algorithms read data from disk, process it, and write the result back to the disk. In contrast, two-pass algorithms read from the disk again after writing the result.  Two-pass algorithm can be based on sorting and hash. Sorting-based algorithms use sorting technique to organize data in the first and second pass. Hashing-based algorithms use hash functions to partition the data into buckets in the first pass and second pass.  Sorting-Based​  Example of sorting-based algorithms :  Two-Phase, Multiway Merge-Sort (TPMMS or 2PMMS) : Algorithm to sort very large relations in two passes, consisting two phases. This algorithm is a variant of merge sort that is designed to handle large data that cannot fit in the main memory. In phase 1, the algorithm sort the tuples from one relation in the main memory. The table is not entirely sorted, instead it repeatedly sorts the sublist of the relation. The sorted sublist is then written to the secondary storage. In phase 2, the sorted sublists generated in phase 1 are merged together. However, there is a limitation on the number of sorted sublists that can be merged in this phase. The maximum number of sorted sublists allowed is M−1M - 1M−1, where MMM is the number of available main-memory buffers. In the merging process, the smallest key among the remaining element in the sublist will be moved into the output block. This is done until the output block is full, if it is, then write it to the disk and create a new output block. Source : Book page 724 Eliminate Duplication : Based on sorting, we can remove duplicate tuples in a table easily. The first pass involve sorting the tuple into sublists, similar to the first phase of 2PMMS. In the second pass, we repeatedly select the tuples in the sorted sublists and copy the unconsidered tuple to the output block. Join : Given relations (tables) R(X, Y) and S(Y, Z) to join, where X and Y are attributes belong to R, same for Y and Z, which are attributes that belong to S. The process is as follows : Sort R and S using the 2PMMS algorithm, with Y as the sort key.Merge the sorted S and R using two buffers, the merge process is the next step : Find the smallest value based on the join attributes Y that appears at the front of both R and S.If that Y value doesn't appear at the front of the other relation, remove the tuple(s) with that Y value.If the Y value appears at the front of both R and S, identify all the tuples in R and S that have that Y value. If necessary, read more blocks from R and/or S until there are no more Y value in either relation.Output all the tuples that can be created by joining the identified tuples from R and S based on their common Y value.If either relation has no more unprocessed tuples in the memory, load more tuples from that relation into the buffer.  Hashing-Based​  In comparison to sorting-based algorithms :  Duplicate Elimination : The idea is, same record will hash to same hash code, and will fall into the same bucket. Each record is hashed, so it is partitioned into different bucket. Within a bucket, only one copy of the duplicate records is kept, and the rest are removed. This method works as long as the table is small, so that no non-duplicate record in the same bucket.Join (Hash Join Algorithm) : Similar to other operation, we will hash the record and partition them into buckets. Within a bucket, if the join attribute values match, the tuples are considered a match and can be combined.  ","version":"Next","tagName":"h3"},{"title":"Index-Based Algorithms​","type":1,"pageTitle":"Query Processing","url":"/cs-notes/database-system/query-processing#index-based-algorithms","content":" Index-based algorithms is a set of algorithms that leverage database indexes, such as selection and join. In some cases, clustered indexes may be faster than non-cluster index, because the physical ordering of data in disk is already known.  Selection : Index stores a pointer to record given an attribute that is used as the search key. An index-based selection would be straightforward retrieval.Index Join : Consider two relation R(X, Y) and S(Y, Z), and S has index on attribute Y. To join them together on attributes Y, we compare each tuple in relation R with the tuple(s) of S, which we retrieve using the index on attribute Y.  ","version":"Next","tagName":"h3"},{"title":"Buffer Management​","type":1,"pageTitle":"Query Processing","url":"/cs-notes/database-system/query-processing#buffer-management","content":" Buffer management is concerned with the management and allocation of buffers in main memory, which is an area to hold data recently read from or written to disk. The buffer manager stores the used/free buffer in a buffer pool. It has two options for managing buffers : it can either directly control the usage of main memory or use the virtual memory provided by the operating system.   Source : http://dbmsfortech.blogspot.com/2016/05/buffer-management.html  Buffer manager has several strategies to control the use of buffers, it decides which block/page should be provided when requested :  Least Recently Used (LRU) : This strategy evicts the buffer that has been accessed the least recently. In other word, the block that has not been used for long time is chosen to be replaced. It assumes that recently accessed buffers are more likely to be accessed again in the near future. The buffer manager will keep track the time in a table. First-In, First-Out (FIFO): This strategy removes the buffer that has been in the buffer pool for the longest duration. It operates on the principle that the buffer that has been present for the longest time is less likely to be needed again soon. Clock (Second Chance): The clock strategy uses a circular list to manage the buffers and a clock hand that will rotate clockwise to find the buffer. Buffers are marked with a reference bit, either 0 or 1 (initially 0). When the clock hand reaches a buffer, it checks the reference bit. If the reference bit is set (1), indicating recent use, the bit is set to 0, and the clock hand moves to the next buffer. If the reference bit is not set (0), the buffer is chosen for replacement. Source : Book page 749 ","version":"Next","tagName":"h3"},{"title":"Relational Data","type":0,"sectionRef":"#","url":"/cs-notes/database-system/relational-data","content":"","keywords":"","version":"Next"},{"title":"Tables, Column, Row​","type":1,"pageTitle":"Relational Data","url":"/cs-notes/database-system/relational-data#tables-column-row","content":" In a relational database, data is organized into tables. A table is a collection of related data, it contains column (or attributes) and row (or tuple, record).  A column has a particular data type (or domain) associated with it. The column represents what kind of data or the specific attribute of the data, such as a person's name, age, or address, in the case of a person table. The columns are given names or label to be identified.  A row entry represent individual instances of data within the table. Each row contains the actual data values for each attribute. For example, in a table representing employees, each row would represent a specific employee, and the columns would contain attributes such as employee ID, name, age, and so on.   A customer table containing column CustomerId, FirstName, LastName, and DateCreated. Source : https://database.guide/what-is-a-table/  info Table is also called relations, which consist of rows or formally a set of tuples.  ","version":"Next","tagName":"h3"},{"title":"Data Types​","type":1,"pageTitle":"Relational Data","url":"/cs-notes/database-system/relational-data#data-types","content":" There are many types of data that can be assigned to specific column, some common data types are :  Integer : Used to store whole numbers without decimal places, such as 1, 10, -5, etc.Floating-point : Used to store numbers with decimal places, such as 3.14, 10.5, -0.75, etc. The precision and scale of decimal numbers can often be specified.Character/String : Used to store alphanumeric text, such as names, addresses, or descriptions. The length of the string may be limited or unlimited, typically known as VARCHAR or TEXT data types, respectively.Date/Time : Used to store dates, times, or both. This includes data types like DATE, TIME, DATETIME, TIMESTAMP, etc., which can represent specific points in time or intervals.Boolean : Used to store true/false or binary values, representing logical conditions. Some RDBMS doesn't offer boolean data types, we may use Integer (1) to represent true value and Integer (2) to represent false value.Binary : Used to store binary data, such as images, files, or serialized objects.Enumerated Types : Some systems offer the ability to define custom data types with a limited set of predefined values.  ","version":"Next","tagName":"h3"},{"title":"Primary & Foreign Key​","type":1,"pageTitle":"Relational Data","url":"/cs-notes/database-system/relational-data#primary--foreign-key","content":" To be able to distinguish between record in a table, each record is associated with a unique identifier called primary key. A primary key can be a simple integer type, or other kind of data types. The point is, we must ensure its uniqueness. A name is probably not a good choice of primary key, because sometimes people can have the same name.  In a relational database, data is organized into table. A database may contain multiple table, each representing a specific entity within the database. Multiple table can be connected to together, this mean we are establishing a relationship between particular data across multiple tables.  tip A relational model organizes data into separate tables, with each table capable of having relationships. In contrast, a flat model, that represents data in a spreadsheet-like format, consist only tables without relationships.  The most common way to connect tables is by using primary key and foreign key relationships. A primary key in one table serves as a unique identifier for each record, and a foreign key in another table references that primary key to establish a relationship between the two tables.  Example​  For example, let's consider two tables : Customers and Orders.  The Customers table maintains a collection of customers, it has a primary key column called CustomerID and another column containing the customer's name.The Orders table contains order data, it has primary key column called OrderID and another column Item ordered, which specify particular item that was ordered.    An order is obviously ordered by a customer, it makes sense to also store which customer placed the order. Let's modify the table.    Now the order table has an information about which customer ordered the item. However, we need to include more information into the Orders table, and this information is from the Customers table. This lead to data redundancy in the Orders table.  Instead of storing customer information directly in the Orders table, we can use a foreign key column in the Orders table to refer to some customer in the Customers table. This way, we will know which customer is associated with specific order without needing to duplicate the customer data in the Orders table. So, let's modify the table again.  We removed the column Ordered by Customer (name) and only preserve the Ordered by Customer (id) column. This table is much more efficient than the previous table. When we process the table and we want to know who ordered an item, we will look at the foreign key column. The foreign key will guide us to the primary key of the Customers table.  In some case, when the primary key is a simple data such as integer ID, we will also save more space. This is because an integer data is smaller than storing the name of customer, which is a string data.    Furthermore, as the row and column of a table increase, the use of foreign key becomes even more useful to reduce duplicate data. Also, maintaining multiple table with the same data can increase the chance of data inconsistency, in which multiple instance of same data in a database or system do not match or are contradictory. If we modify data on the Customers table, we must also modify the Orders table as well. However, this introduces overhead when we modify data, imagine having to modify 10 tables just to modify a single entry of a table.   Another example of table with foreign key Source : https://www.thecrazyprogrammer.com/2019/04/difference-between-primary-key-and-foreign-key.html  Relationship​  Through the use of primary and foreign keys, there are different types of relationship that may be achieved :  One-to-One (1:1) : Each record in one table is associated with exactly one record in another table, and vice versa. A person can have only one driver’s license number, and a driver’s license number belongs to only person. One-to-Many (1:N) : Each record in one table can be associated with multiple records in another table, while each record in the other table is associated with only one record in the first table. An order can contain many items, but an item belongs to a single order. In this case, the orders table is the one side and the items table is the many sides. Many-to-Many (N:M) : Multiple records in one table can be associated with multiple records in another table. This type of relationship is typically implemented using an intermediary table that holds the associations between the two tables. A product can belong to many categories, and a category can contain many products.   Source : https://experienceleague.adobe.com/docs/commerce-business-intelligence/mbi/analyze/warehouse-manager/table-relationships.html  Other Keys​  There are other keys in relational model :  Composite Key : A composite key is a key that consists of two or more columns in a table. It is used when there are no single attributes that can uniquely identify a record, but the combination of multiple attributes can.Compound Key : A composite key, where one of the column is a foreign key.Candidate Key : A candidate key represent a column that can be used as primary key.  ","version":"Next","tagName":"h3"},{"title":"Schema​","type":1,"pageTitle":"Relational Data","url":"/cs-notes/database-system/relational-data#schema","content":" A database schema is a logical blueprint or structure that defines the organization, relationships of the database in an RDBMS. It specifies the tables, columns, data types, and relationships between tables.   Source : https://planetscale.com/blog/schema-design-101-relational-databases  For example, if we were to make a Customers table, it should consist of column customerID with Int data types, firstName with String data types, and so on. The customerID should be the foreign key of the Orders table, creating a one-to-one relationship.  ","version":"Next","tagName":"h3"},{"title":"Query​","type":1,"pageTitle":"Relational Data","url":"/cs-notes/database-system/relational-data#query","content":" Query is a request or command made to retrieve or manipulate data stored in a database. It is a structured statement or set of instructions written in a database query language, used to communicate with an RDBMS.  While querying, we can retrieve data based on specific criteria. For example, a query can be used to retrieve all customers who have made a purchase in the last month.   Source : https://manifold.net/doc/mfd9/queries.htm  In the image above, we are trying to retrieve data from the Employees table. The command used is SELECT * FROM [Employees]. In short, the SELECT statement indicates we are going to retrieve data, the * is a shorthand that represent all the available row in the table, and the FROM [Employees] means we are retrieving the data from the Employees table. Combining them all together, using that command is the equivalent of saying &quot;Retrieve all the row from Employees table&quot;. The database engine will then return the requested data, which includes all the rows and columns present in the Employees table.  View​  View is a virtual table that is derived from querying one or more existing tables or other views. It does not store any data itself but instead provides a way to present data from underlying tables in a customized or simplified manner. A view can be seen as a saved query that can be used like a table in subsequent queries, but it does not contain its own data. Database view can be thought as the summary of a series of table in a database, providing a way for us to see the overall data.   Source : https://aristeksystems.com/blog/database-views-what-you-need-to-know/  Operation​  We can perform various operation on our query, some examples are :  Projection : Specifies the columns to be included in the query result.Selection : Specifies the conditions or criteria to filter the rows to be included in the query result.Sorting : Specifies the order in which the query results should be arranged. It can be ascending (smallest to largest) or descending (largest to smallest) based on one or more columns.Aggregation : Performs calculations on a set of values, such as calculating the sum, average, count, minimum, or maximum values from a column or group of rows.Union : Combines the results of two or more queries into a single result set, removing duplicate rows.Intersection : Retrieves only the common rows between two or more queries, keeping only the rows that appear in all result sets.  info More about operation. ","version":"Next","tagName":"h3"},{"title":"Storage Management","type":0,"sectionRef":"#","url":"/cs-notes/database-system/storage-management","content":"","keywords":"","version":"Next"},{"title":"Disks​","type":1,"pageTitle":"Storage Management","url":"/cs-notes/database-system/storage-management#disks","content":" Database operates in main memory and secondary storage. The main memory is typically abstracted by file system and virtual memory.  When referring to disk, we typically refer to magnetic disk. Disk consist of platter stacked on top each other, the plate has special coating that hold magnetic field. The magnetic field become the place where data is stored. In the hardware level, disk access will be handled by disk controller. Disk controller controls the write/read head component on the platter to write or read data.  A concentric circular path on the platter is called a track. Within a section of track, the smallest addressable unit of storage and is fixed in size is a sector. When operating system or DBMS accesses disk, it will be abstracted by the file system, disk access will be in another fixed-size unit called block.  tip Find more about disk management.  ","version":"Next","tagName":"h3"},{"title":"Data Arrangement​","type":1,"pageTitle":"Storage Management","url":"/cs-notes/database-system/storage-management#data-arrangement","content":" One way to store database table and records on disk is the fixed-length record technique. A table is created by providing columns and its data types. We will accommodate the largest possible data attribute within a table. This ensures that all records have the same length, regardless of the actual data values stored in them.  We may also include header that stores pointer to the database schema, the length of the record, and timestamp that indicates the time record was last modified or read.   Source : Book page 591, 592  For example, the image above creates a table with column : name, address, gender, and birthdate. The name has CHAR data type, a single char typically worth 1 byte. For accommodation, we allocate 30 bytes for the name field and include an additional 2 bytes (12 + 30 = 42 + 2 = 44) because certain machines operate more efficiently when reading/writing addresses that are multiples of 4 or 8.  Records are stored consecutively within a block, the block serves as the unit of data transferred between the disk and main memory.  ","version":"Next","tagName":"h3"},{"title":"Block & Record Addresses​","type":1,"pageTitle":"Storage Management","url":"/cs-notes/database-system/storage-management#block--record-addresses","content":" Each block and record on the disk has a physical address, which consists of the device ID for the disk, the cylinder number, the track number within the cylinder, the block number within the track, and sometimes the offset of the beginning of the record within the block.  On the other hand, main memory uses logical addresses to identify blocks and records. The location of block of record in the main memory is different with the block of record that is stored in actual disk. This is because main memory is abstracted by virtual memory, making the way we access memory different.  Each block or record will need to store a logical address. When we need to locate a specific block or record on the disk, we will consult to map table. A map table is a table that exists somewhere in a known location on the disk and stores information about the mapping between logical addresses to physical addresses.   Source :http://www.cs.emory.edu/~cheung/Courses/554/Syllabus/2-disks/ident.html  The different addressing between main memory and secondary storage does not affect block and record alone, it also affects pointer inside them. A pointer stores the memory address of something, it is necessary to also translate its addresses. The process of translating pointers between the database address space and the virtual address space is called pointer swizzling, and the opposite pointer unswizzling.  One way to swizzle pointer is to store another table (called translation table) that maps between database addresses and memory addresses. The pointer will store a bit indicating whether the pointer is currently a database address or a memory address, and the actual database or memory pointer.  ","version":"Next","tagName":"h3"},{"title":"Variable-Length Record​","type":1,"pageTitle":"Storage Management","url":"/cs-notes/database-system/storage-management#variable-length-record","content":" In some cases, we want to store record in varying size. One approach is to keep the fixed-length portion of the record first, followed by the variable-length fields. To achieve this, we will include pointers (offsets) to the beginnings of the variable-length fields.   Source : Book page 604  In this example, the gender, birthdate, and name is fixed-length, while the address is variable-sized. We also included header, record length, and the pointer to the address field.  In the case of a record that contains repeating field, we can store the pointers to the place where each repeating fields begin and the amount of repetitions or where the repetitions end.   Source : Book page 605  Alternative approach of storing variable-length record is to store them separately from the record, in a separate block or blocks. This allows for flexibility in terms of storage allocation and retrieval. However, it also increases the number of disk I/O operations required to access all the components of the record.  ","version":"Next","tagName":"h3"},{"title":"BLOB​","type":1,"pageTitle":"Storage Management","url":"/cs-notes/database-system/storage-management#blob","content":" Binary Large Object (BLOB) refers to very large values, such as images and videos, that need to be stored across multiple blocks. BLOB must be stored consecutively to be retrieved efficiently. Also, we may not need to retrieve the entire BLOB. For example, it is not necessary to retrieve the entire 2-hour movie if the movie is played at a slow rate or only at the end. We may retrieve the several blocks gradually.  If fast real-time access is required, it may be necessary to distribute BLOB across several disks. This involves distributing the blocks of the BLOB among multiple disks, to allow multiple blocks to be retrieved simultaneously.  ","version":"Next","tagName":"h3"},{"title":"Record Modifications​","type":1,"pageTitle":"Storage Management","url":"/cs-notes/database-system/storage-management#record-modifications","content":" Insertion : In the case of unsorted table, we can find an empty space and insert the record there. To maintain sorted table on insertion, we will need to find where the record should be placed and potentially sliding the records around the block. If we can't find enough space to insert record in a particular block, we can either reorganize the record with the adjacent block or create another block (called overflow block) and associate that block with a pointer in the original block.Deletion : We can delete a record and slide the block. If we cannot do it, we may maintain an available-space list in the block header. We will store a marker that indicates whether a block is free or not. Another thing is, there could be pointer associated with the particular deleted block, so it is necessary to keep information within the block associated with the record. The information is called tombstone, it is a permanent bit which must exist until database is reconstructed.Update : If the resulting record size after update is same, then there is no issue. Otherwise, we will need to create more space on the block, then the problem becomes similar to insertion. ","version":"Next","tagName":"h3"},{"title":"Query Language","type":0,"sectionRef":"#","url":"/cs-notes/database-system/query-language","content":"","keywords":"","version":"Next"},{"title":"SQL​","type":1,"pageTitle":"Query Language","url":"/cs-notes/database-system/query-language#sql","content":" One of the commonly used query language is the Structured Query Language (SQL). SQL is a form of declarative language, meaning users specify what data they want to retrieve or manipulate, rather than the traditional programming that specify what should a program does.  tip SQL is just a query language, it is not an actual database. The actual database that is based on relational model (or called relational database) are database like MySQL, Oracle, Microsoft SQL Server, etc. These databases use SQL as the language in their relational database management systems (RDBMS).  Syntax​  In order to query from a database, we will need to enter SQL commands. SQL commands consist of a series of instructions that are human-readable. Each SQL command consists of one or more keywords, clauses, and expressions that together form a statement with a specific purpose. For example there is a keyword called SELECT which allow you to retrieve data. To retrieve data from a specific table in the database, the SELECT clause is followed by a list of columns and table name that we want to retrieve.   Source : https://en.wikipedia.org/wiki/SQL  Clauses​  SQL clauses are keywords or reserved words that perform specific functions in the statement. Some commonly used clauses include SELECT, FROM, WHERE, GROUP BY, HAVING, ORDER BY, INSERT INTO, UPDATE, and DELETE FROM. It's worth noting that some of these clauses are optional.  SELECT: As mentioned earlier, it is a keyword used to retrieve data. To retrieve data, we will need to specify the specific columns that we want to retrieve from one or more tables in the database that we also need to specify. FROM: This keyword specifies the table from which we want to retrieve data. It is followed by the name of the table. Using SELECT and FROM together, it will be like SELECT * FROM customers, which mean we are going to retrieve all the data from the customers table. The * is actually a symbol that represents all columns in a table. WHERE: It is a clause used to filter the rows based on specified conditions. For example SELECT * FROM customers WHERE age = 33, this mean we are going to retrieve all column from customers table, but only the data &quot;where&quot; column named age is equal to 33. The = is symbol that checks for equality, it is an example of operator, which is explained more below. GROUP BY: The GROUP BY groups rows based on one or more columns. It is typically used in a situation where there are multiple row, each with different value in a specific column. Grouping them allows us to see the data based on the different column in the perspective of those rows combined. Source : https://learnsql.com/blog/group-by-in-sql-explained/ HAVING: The HAVING clause is used to filter the groups created by the GROUP BY clause. It specifies extra conditions that must be met by the groups in order to be included in the result query. ORDER BY: The ORDER BY clause is used to sort the result set based on one or more columns. For example, we may sort the resulting query of customer table by their age, based on the age column. Without using this clause, by default it will sort the data in ascending (explicitly, ORDER BY ASC) order. We can make it to sort it in descending order by using the ORDER BY DESC.  SQL clauses perform specific functions, they have particular order of execution :   Source : https://en.wikipedia.org/wiki/SQL_syntax  info The SELECT statement is also known as the projection operator. Projection is simply selecting a set of columns, eliminating the rest of larger set of columns. Also, the WHERE statement is a selection operation. Selection is the operation of filtering rows that satisfies certain condition.  Operators​  Operators are symbols or keywords that are used to perform comparisons, calculations, or logical operations on values or expressions.  Comparisons : = (equal to)&lt;&gt; or != (not equal to)&lt; (less than)&gt; (greater than)&lt;= (less than or equal to)&gt;= (greater than or equal to) With the example earlier, we used the = to check if particular row has its age column equal to 33. We can also check if the age column is less than some number in order to filter the result to only include rows where the condition is met. Arithmetic Operators, they are used to perform mathematical calculations. + (addition)- (subtraction)* (multiplication)/ (division) When retrieving student data, maybe we want to check if the total of their score reaches the minimum threshold. We can do this by selecting all the student from the table and add each score's column and then combine it the comparison operator to compare it with the minimum threshold. This way we can easily retrieve the relevant student data. Logical Operators, these operators are used to combine or negate conditions and evaluate logical expressions. AND (logical AND)OR (logical OR)NOT (logical NOT) For example, we can select data that meets two conditions simultaneously by using the AND operator. Or maybe choose data if at least one of the conditions is satisfied by using the OR operator.  Literals​  Literals are fixed values that are directly specified in an SQL statement. They can be numeric literals (e.g., 123, 3.14), string literals (e.g., 'Hello', &quot;World&quot;), date literals (e.g., '2024-01-25'), or Boolean literals (e.g., TRUE, FALSE), etc.  Expressions​  Expressions are combinations of SQL syntax components that produce a single value. For example, in the statement SELECT score1 + score2 FROM student, the score1 + score2 is an expression. The score1 and score2 is a specific column in the student table. Because they are column, adding them means we are combining the value of both column into a single new column. The overall statement will retrieve the new column for each row in the student table.  Expressions can be used in different parts of an SQL statement, such as the SELECT list, WHERE clause, and ORDER BY clause. For example, column1 + column2 is an expression that adds the values of two columns.  Predicates​  Predicates are conditions or expressions that produces a condition or logical value, such as true, false, or unknown. It is used in the WHERE clause to filter rows based on specified criteria.  Predicates are produced by comparison operator such as = (equal to), &lt;&gt; (not equal to), &gt; (greater than), &lt; (less than). For example, age &gt; 18 is a predicate that filters rows where the age column is greater than 18.  Example​  Another example of fully working SQL statement that retrieves the column airport_name, iata_code, passengers from the table airport, but only includes rows where the number of passengers is greater than 10,000,000.    info SQL statement can be placed in multi or single line. The end of a statement is marked with the semicolon ;.  Data Types​  Numeric : INT or INTEGER : Integer values (whole numbers).FLOAT or REAL : Floating-point numbers (decimal numbers).DECIMAL(p, s) : Fixed-point numbers with specified precision p and scale s.NUMERIC(p, s) : Synonym for DECIMAL. Character : CHAR(n) : Fixed-length character string with a maximum length of n.VARCHAR(n) : Variable-length character string with a maximum length of n.TEXT : Variable-length character string with no maximum length. Date and Time : DATE : Date values in the format &quot;YYYY-MM-DD&quot;.TIME : Time values in the format &quot;HH:MI:SS&quot;.DATETIME or TIMESTAMP : Combination of date and time values.INTERVAL : Represents a duration or time span. Binary : BINARY(n) : Fixed-length binary data with a maximum length of n.VARBINARY(n) : Variable-length binary data with a maximum length of n.BLOB : Variable-length binary data with no maximum length. NULL : NULL is a special value that represents the absence of data or the unknown value for a particular column in a table.  info Some SQL database typically do not have boolean data types, they typically represent true as integer 1 and false as integer 0  ","version":"Next","tagName":"h3"},{"title":"DDL, DML, DQL, DCL​","type":1,"pageTitle":"Query Language","url":"/cs-notes/database-system/query-language#ddl-dml-dql-dcl","content":" SQL statements are categorized into 4 types based on their functionalities.  Data Definition Language (DDL) : Consist of statement that define and manage the structure of database objects. This includes creating, altering, and dropping database objects such as tables. Examples of DDL statements include CREATE, ALTER, and DROP.Data Manipulation Language (DML) : Statements that manipulates the data within the database objects, including insert, update, delete, and retrieve data from tables. Examples of DML statements include INSERT, UPDATE, DELETE, and SELECT.Data Query Language (DQL) : Statements that makes up for querying and retrieving data from the database. The primary DQL statement is SELECT, which allows to specify the columns, tables, and conditions to retrieve the desired data.Data Control Language (DCL) : Database may also be provided with specific permissions. DCL statements are used to manage the permissions and access rights within the database. They control the security aspects of the database by granting or revoking permissions to users and roles. Examples of DCL statements include GRANT and REVOKE.   Source : https://allthingssql.com/sql-create-table-examples/  The above is an example of creating a table in SQL (CREATE TABLE is a form of DDL). We will need to provide the schema of the database, or basically the structure of how the database. For example, there is a column called CompanyId with the type of INTEGER, and it is defined as the primary key.  Other Operations​  There are too many operations to cover on SQL. Also, each RDBMS such as MySQL or SQL server may provide their own custom function as extras.  Functions​  SQL provides various built-in functions to perform calculations, manipulate data, and retrieve information from the database. Functions can be used in SQL statements to transform data, aggregate values, extract substrings, perform mathematical operations, and more. Examples of SQL functions include COUNT, SUM, AVG, MAX, MIN, UPPER, LOWER, and SUBSTRING.   Source : https://www.programiz.com/sql/count  The image above is an example of the usage of COUNT function, which returns the number of rows that match a specified condition.  info The AS is a keyword that assign an alias or a temporary name to a column, table, or result set. The purpose is to provide more meaningful or concise names to the columns or tables in the queries, making the output easier to understand or work with.  The query originate from the table Customers, it selects column country along with the count of the customer, which is grouped by the country column. The * symbol which we have used for SELECT statement earlier, can also be used as the condition of the COUNT. Using * on COUNT would count all the rows in a table, regardless of the values in specific columns or any conditions. It essentially returns the total number of rows in the specified table or the result set.  INTERSECT &amp; UNION​  Another operations of SQL are INTERSECT and UNION.  The INTERSECT operation is used to retrieve the common rows between two SELECT statements. It returns only the distinct rows that exist in both result sets of the SELECT statements.  Example :  SELECT column1, column2 FROM table1 INTERSECT SELECT column1, column2 FROM table2;   The UNION operation is used to combine the result sets of two or more SELECT statements into a single result set. It returns all the distinct rows from all the SELECT statements, eliminating duplicates.  Example :  SELECT column1, column2 FROM table1 UNION SELECT column1, column2 FROM table2;    Source : https://www.c-sharpcorner.com/article/the-complete-reference-set-operations-in-ms-sql-union-all-intersect-excep/  JOINS​  SQL joins are used to combine rows from two or more tables based on a related column between them. There are many types of joins operations in SQL, they allow us to retrieve data by establishing relationships between tables.  INNER JOIN : Returns only the rows that have matching values in both tables being joined.LEFT JOIN : Returns all the rows from the left table and the matching rows from the right table.RIGHT JOIN : Returns all the rows from the right table and the matching rows from the left table.FULL OUTER JOIN : The FULL JOIN returns all the rows from both tables, regardless of whether they have a match or not. This is same as UNION, the difference is how it is processed by the database engine under the hood.  For left and right join, if there is no match for a row in either table, NULL values are returned for the table's columns.  Example of the statement :  SELECT column1, column2 FROM table1 INNER JOIN table2 ON table1.column = table2.column;   Regardless of what joins, the syntax is roughly the same, they only differ in how the data is processed under the hood.   Source: https://medium.com/@iammanolov98/mastering-sql-joins-coding-interview-preparation-innerjoin-e96bef58afc2  Below is an actual example of LEFT JOIN. Two tables are &quot;joined&quot; together on the column Student ID. Only the ID 1004 that exist in both table, therefore it will be the only one that has non-null value on Department column. Also, we are left joining them, thus all the row from first table is preserved.  It is worth noting that the behavior of JOIN differ in what order we specify the table. Left joining table1 with table2 will yield different result with left joining table2 and table1.   Source : https://www.shiksha.com/online-courses/articles/sql-left-join-examples-and-syntax/  ","version":"Next","tagName":"h3"},{"title":"How SQL works​","type":1,"pageTitle":"Query Language","url":"/cs-notes/database-system/query-language#how-sql-works","content":" Math​  Relational database, relational model, and SQL is originally based on several mathematical studies :  Set theory : Set theory is the foundation of the relational model in database systems. It encompasses the concepts of sets, relations, and operations on them. The relational model represents data as sets and uses set theory principles to define operations such as union, intersection, and difference.Relational algebra : Relational algebra is a mathematical system used to manipulate and query data in the relational model. It expresses operations on relations, such as selection, projection, join, and set operations.Relational calculus : Relational calculus is a formal language for expressing queries in the relational model. It defines the logical and declarative approach to query formulation, where queries are expressed as formulas or rules. There are two types of relational calculus: tuple calculus and domain calculus. Tuple calculus operates on individual tuples, while domain calculus operates on the attributes and values of tuples.Predicate logics : Predicate logic is a mathematical system used for formalizing logical statements and reasoning about them. It involves the use of predicates, which are statements that can be true or false depending on the values of their variables. Predicate logic are operators like logical (AND, OR, NOT) and comparison (=, &lt;, &gt;).  Because SQL is declarative, as a user, we will never be concerned in how the engine process the query. Under the hood, given a complex SQL statement, it is common for the database engine to process the operation based on these mathematical principles.  For example, the SQL INTERSECTION is an easy example of set theory application. In mathematics, a set is defined as a collection of distinct elements. In SQL, tables can be thought of as sets of rows.  Suppose we have two sets AAA and BBB : A={1,2,3,4}A = \\{1, 2, 3, 4\\}A={1,2,3,4}, B={3,4,5,6}B = \\{3, 4, 5, 6\\}B={3,4,5,6}  It can be thought as table AAA that has row {1,2,3,4}\\{1, 2, 3, 4\\}{1,2,3,4} and only a single column, similarly for table BBB.  Mathematical representation : A∩B={3,4}A \\cap B = \\{3, 4\\}A∩B={3,4}  In this case, the set intersection operation results in a new set that contains the common elements between sets AAA and BBB, which are 3 and 4.  Projection​  In the relational model, everything is represented as tuples. A tuple is a finite ordered sequence of elements, where each element corresponds to a specific attribute or column in a relation or table. Tuples in the relational model are analogous to rows in a database table.  info Same terminology : Table -&gt; relation Row -&gt; tuple Column -&gt; attribute  Consider a table called Employees with the following columns : EmployeeID, FirstName, and LastName. Each row in the Employees table can be represented in the following tuple :  Tuple 1: (1, &quot;John&quot;, &quot;Doe&quot;, 50000) Tuple 2: (2, &quot;Jane&quot;, &quot;Smith&quot;, 60000) Tuple 3: (3, &quot;David&quot;, &quot;Johnson&quot;, 55000)   As explained earlier, the SELECT statement is also known as projection. Using SELECT effectively selects certain column from a table. Mathematically, the projection operation refers to the extraction of specific attributes from a relation or set of tuples. It can be represented using mathematical notation as follows :  Let RRR be a relation or set of tuples, and let A1,A2,...,AnA_1, A_2, ..., A_nA1​,A2​,...,An​ be the attributes or columns to be selected from RRR. The projection operation, denoted as π\\piπ, selects the specified attributes from the relation RRR.  If we have nnn number of columns, mathematically it would be : πA1,A2,...,An\\pi A_1, A_2, ..., A_nπA1​,A2​,...,An​  If we choose to query the EmployeeID and FirstName column, the SQL statement would be : SELECT EmployeeID, FirstName FROM Employees. In math representation : π EmployeeID,FirstName\\pi \\text{ EmployeeID}, \\text{FirstName}π EmployeeID,FirstName.  Query Processing​   Source : Book page 6  A database query goes to several processes :  Parse &amp; Compilation : User made a query through DML, the query compiler will parse the query, breaking it down into a parse tree. The query preprocessor takes the parsed query and performs semantic check to ensure its correctness. The query will be transformed into the mathematical representation of relational algebra. The transformed representation will be optimized by finding the most efficient sequence of operations on the actual data. The optimizer will keep the metadata and statistics of the database. Source : https://www.researchgate.net/figure/A-Relational-Algebra-Tree_fig11_305333879 DDL Commands : The DDL commands provided by the database administrator, which include the schema of the database will be passed to the DDL compiler, serving information about the database for the query process. Transactions &amp; Concurrency Control : The query and other database operation will be grouped together into one unit of execution, called transactions. This is done to ensure the execution of all operations as a single unit, preventing the possibility of executing only some of them and being unable to complete the remaining operations due to system failures. A piece of transaction is delegated to concurrency control, which is a component that handles concurrency mechanism. Two or more database operations that are reading and writing the same data, needs to be prevented to access it at the same time to prevent data races. The information of concurrency is stored in the lock table. The concurrency control decides the execution order and send it to execution engine. Execution Engine : The query will be planned into a sequence of actions that the DBMS will perform. The execution engine is responsible to plan which operation is done first, in the case of multiple operation carried out within single query. For example, in a student database, let's say we wanted to get student name that has score greater than 80 and is a male. We would filter the table for the condition separately. In the first filter, we take data with score greater than 80. In the second filter, we will take data that satisfy male condition.We will find the intersection between the two filter, and then we will project or take the data with the specified column. We will also save some information about the query to the logging and recovery manager. Resource Manager : Execution engine issue commands to the resource manager, which is a component that knows the information about the table and the location of them in the storage. Buffer Manager : Buffer manager is the component that manages memory. The storage stores data in a form of block, which is a fixed-size unit of data for storage. The buffer manager will partition the main memory into buffers, which is a fixed-size unit of data for memory. The buffers serve as the memory region where disk block transfer take place. Storage &amp; Storage Manager : The storage is typically a secondary storage such as hard disk. Disk stores nothing but blocks of data, it will require a component that knows the placement of block in the disk. In other word, the component know the file structure of disk. The component that know it is storage manager, it will be responsible for controlling the data transfer between buffer manager and the underlying storage. In simple database system, the storage manager can be the file system of the operating system. ","version":"Next","tagName":"h3"},{"title":"Transactions","type":0,"sectionRef":"#","url":"/cs-notes/database-system/transactions","content":"","keywords":"","version":"Next"},{"title":"Two purpose of Transactions​","type":1,"pageTitle":"Transactions","url":"/cs-notes/database-system/transactions#two-purpose-of-transactions","content":" Database operation often times includes more than one related operation. For example, in a customer database, maybe we store the count of order a customer has and the total amount of money the customer has spent on the orders. The count of order and amount of money is related, if the customer order an item, the total money would also increase.  So, let's say the customer order something, we want to increase the count of order and amount of money spent. However, there is a possibility of system fails in one of the operation. In a real application that is integrated with a DBMS, it can be a very bad situation if somehow other component of the app causes the whole app to crash in the middle of database operation. We may have increased the count of order, but the app immediately crash before we get the chance to update the amount of money spent.  tip In some cases, a partial change can be worse than no change at all.  The purpose of transaction is to group these operations together. Grouping them allows the operation to be treated as a whole, if some operation failed, then we would reverse the chance and decide to not execute the subsequent operation.  tip An analogy would be sending several mails together in a box rather than sending each mail on its own. If some mail get lost, you would lose some piece of information, resulting in incomplete message. On the other hand, with a box of mail, you would either send all the information or lose all the information.  Another purpose of transactions is to control concurrency in the case of multiple database operation. Transaction allows us to isolate a group of operation. Whenever a group of operation is on the fly, other group of operation shouldn't do anything. This will prevent concurrency issues such as data races, that is when two or more database operation access the same data, and at least one of them is writing. This makes the other operation that reads the data, accidentally read the data before update or after the update, causing unexpected behavior.  ","version":"Next","tagName":"h3"},{"title":"Transactions State​","type":1,"pageTitle":"Transactions","url":"/cs-notes/database-system/transactions#transactions-state","content":"  Source : https://www.scaler.com/topics/dbms/transaction-in-dbms/  Active : The transaction is in progress and actively performing database operations.Partially Committed : In partially committed state, all database operation has been completed, but only on memory (volatile memory such as RAM). Changes made by the database operations are not permanently saved to the database, they are instead performed on memory. This mean other unit of transactions cannot see the changes made by this transaction until it is fully committed. In the case of failure, it will go to the failed state.Committed : If there is no failure, the database changes will be permanently stored on the real database (non-volatile memory such as hard disk). The changes made by the transaction are now visible to other transactions.Terminated : The terminated state indicates the end of transaction.Failed : In failed state, the transaction will be rolled back.Aborted : The transaction has been aborted or rolled back. This means that all the changes made by the transaction are discarded.  The ability to roll back is possible because when transaction begins, the DBMS keep tracks an undo log, which is a record of the original values of any data that the transaction modifies. When the system needs to roll back, the data in the log will be applied to the database again.  ","version":"Next","tagName":"h3"},{"title":"ACID​","type":1,"pageTitle":"Transactions","url":"/cs-notes/database-system/transactions#acid","content":" ACID (atomicity, consistency, isolation, durability) is a set of properties that guarantee reliable and predictable behavior of database transactions.  Atomicity : Atomicity ensures that a transaction is treated as a single, indivisible unit of work. It means that either all the operations within the transaction are successfully completed, or none of them are.Consistency : Consistency ensures that a transaction brings the database from one consistent state to another. It means that the data must satisfy certain constraints defined by the database schema before and after the transaction.Isolation : Isolation ensures that concurrent transactions do not interfere with each other. Each transaction operates as if it is the only transaction running on the database, and the intermediate states of a transaction are not visible to other transactions until it is committed.Durability : Durability ensures that once a transaction is committed, its effects are permanent and will survive any subsequent failures, such as power outages or system crashes. The changes made by the committed transaction are stored in non-volatile memory (such as a hard disk) to make it persistent. ","version":"Next","tagName":"h3"},{"title":"Trigger & Constraints","type":0,"sectionRef":"#","url":"/cs-notes/database-system/trigger-and-constraints","content":"","keywords":"","version":"Next"},{"title":"Constraints​","type":1,"pageTitle":"Trigger & Constraints","url":"/cs-notes/database-system/trigger-and-constraints#constraints","content":" Constraints are rules or conditions that are enforced on the data in a database table. In SQL, there are several types of constraints that can be applied to database tables :  PRIMARY KEY : Used to uniquely identify each row in a table. It enforces specified column(s) have unique values and cannot contain null values. Typically, a primary key is created on columns that uniquely identify the rows, such as an ID column.FOREIGN KEY : A FOREIGN KEY constraint enforces a relationship between two tables. It ensures that the values in a column(s) of one table match the values in the primary key column(s) of another table. This constraint maintains referential integrity and enforces the relationship between the tables.DEFAULT : Specifies a default value for a column when no value is explicitly provided during an INSERT statement.UNIQUE : Ensures that the non-null values in a column(s) are unique across all rows in a table.NOT NULL : Ensures that a column cannot have null values. It requires that the column must have a value for every row and disallows the insertion of null values.CHECK : Specifies a condition that must be satisfied for the values in a column(s). For example, we can use the CHECK constraint to specify that a numeric column must be positive or that a character column must have a certain length.CREATE INDEX : Creates index in table for performance optimization.  ","version":"Next","tagName":"h3"},{"title":"Trigger​","type":1,"pageTitle":"Trigger & Constraints","url":"/cs-notes/database-system/trigger-and-constraints#trigger","content":" Database Trigger is a stored program or set of instructions that is automatically executed or fired in response to certain events or actions occurring in a database. Triggers are defined and associated with specific tables, and they are triggered by specific data manipulation language (DML) statements or data definition language (DDL) statements.  info A CHECK constraints checks each row based on data modification, while database trigger perform actions based on events. DBMS will reject the data modification operation and raise an error in CHECK constraint, while trigger is more flexible and customizable, allowing us to integrate our own logic.  Triggers can be classified into two levels :  Row-Level Triggers : Row-level triggers are executed once for each row affected by the triggering event.Statement-Level Triggers : Statement-level triggers are executed once for each triggering statement, regardless of the number of rows affected by the statement (even if no rows are affected).  Based on the execution time, there are three :  Before (Statement-Level Trigger) : Executed before the triggering event takes place and before any modifications are made.After (Statement-Level Trigger) : Executed after the triggering event and the modification of data have occurred.Before (Row-Level Trigger) : Executed before each affected row is modified by the triggering event.  Example of after statement-level trigger (syntax may vary depending on the DBMS) :  CREATE TRIGGER insert_employee_audit AFTER INSERT ON Employees FOR EACH STATEMENT BEGIN INSERT INTO EmployeeAudit (employee_id, employee_name, hire_date) SELECT employee_id, employee_name, hire_date FROM inserted; END;   One of the use of trigger is to log some information. We have a table called Employees that contains information about employees in a company. Whenever a new employee is inserted into the table, we will log the employee's information into EmployeeAudit table. ","version":"Next","tagName":"h3"},{"title":"Deep Learning","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning","content":"","keywords":"","version":"Next"},{"title":"All pages​","type":1,"pageTitle":"Deep Learning","url":"/cs-notes/deep-learning#all-pages","content":" Deep Learning FoundationDeep Learning TasksNeural NetworkCNNResNetU-NetSiamese NetworkRNNLSTMGRUAutoencoderVariational AutoencoderGANTransformers Attention MechanismTransformers ArchitectureTransformers AudioBERTGPTBARTVision Transformers Diffusion ModelReinforcement Learning Reinforcement Learning FundamentalModel-Based Markov ModelsMarkov Decision Process Model-Free Monte Carlo MethodTemporal DifferenceSARSAQ-LearningPolicy GradientImitation Learning Multi-Agent ","version":"Next","tagName":"h3"},{"title":"Autoencoder","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/autoencoder","content":"","keywords":"","version":"Next"},{"title":"Encoder & Decoder​","type":1,"pageTitle":"Autoencoder","url":"/cs-notes/deep-learning/autoencoder#encoder--decoder","content":"  Source : https://medium.com/@birla.deepak26/autoencoders-76bb49ae6a8f  Autoencoder network consist of two components :  Encoder​  Encoder is responsible for taking the input data and transforming it into a lower-dimensional representation, this process is also called encoding or latent space representation.  The encoder consists of several layers of neural network such as convolutional neural network that gradually reduce the dimensionality of the input data and capture the essential charateristics. The networks has model's parameters such as weight and bias which will be adjusted later in the learning process. Activation function will also be included inside to introduce non-linearity.  The result of the layers will be flattened and fed into a fully-connected layer, similar to traditional CNN. The output of fully-connected layer is connected to another layer called bottleneck, this is where data is represented in latent space.   Source : https://www.gabormelli.com/RKB/Convolutional_Autoencoder  Decoder​  The bottleneck layer which has the lower-dimensional representation of the data serve as the input for the decoder. Decoder is the reverse process of encoder, it has direct connection with it, meaning they will have the identical layers.  From the bottleneck layer, the data is fed into a fully-connected layer and will be reshaped into the same size before it was flattened. Continuing the process, the decoder will have the same convolution layers and if pooling layers were used in the encoder, the reverse process will be done.  The final layer of decoder is responsible for outputting or reconstructing the input data. Keep in mind that because autoencoder is an unsupervised learning, loss calculation will be different with the traditional CNN. The loss will be calculated by comparing the original input data with the reconstructed data. The model will measure how different is the reconstructed data and will adjust its parameters to be able to reconstruct a more similar data.   Source : https://analyticsindiamag.com/how-to-implement-convolutional-autoencoder-in-pytorch-with-cuda/ ","version":"Next","tagName":"h3"},{"title":"Deep Learning Tasks","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/deep-learning-tasks","content":"","keywords":"","version":"Next"},{"title":"Computer Vision (CV)​","type":1,"pageTitle":"Deep Learning Tasks","url":"/cs-notes/deep-learning/deep-learning-tasks#computer-vision-cv","content":" Computer Vision (CV) is the study of enabling computer to understand and analyze visual data such as images and videos. Computer vision uses various algorithm from traditional technique like image filters to a more advanced technique that uses deep learning.  note Common deep learning architecture for CV: CNN, ResNet, and GAN.  Image Detection​  The idea of computer recognizing image is achieved through identifying via color of the image. For example, image below shows a simple image detection that detects whether an object is apple or banana. This is very simple because these two object have distinct color. Apple is red colored, while banana is yellow colored. Computer will scan the image color pixel by pixel, if it finds red-ish color, we can assume it's a apple.   Source : https://www.tensorflow.org/lite/examples/object_detection/overview  Edge Detection​  Another technique of computer vision is to detect edge in an image. Edge detection can be useful to segment object in an image. For example, in self-driving cars, we need to distinguish between cars, pedestrians, road boundaries, and other obstacles.  Edge detection works by analyzing the change of color intensity between neighbouring pixel. A significant change might be an indication of edges or boundaries. This is typically done by converting the image into grayscale color first, this is useful to simplify intensity detection.   Source : https://www.semanticscholar.org/paper/Object-Detection-using-the-Canny-Edge-Detector-George-Lakshmi/fe51b808b33783bbde5dbf19f00c6f56c5dcd446  ","version":"Next","tagName":"h3"},{"title":"Natural Language Processing (NLP)​","type":1,"pageTitle":"Deep Learning Tasks","url":"/cs-notes/deep-learning/deep-learning-tasks#natural-language-processing-nlp","content":" Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and human language. Deep learning technique can be applied to NLP, the idea is to turn human language into numbers that can be processed in the neural network.  note Common deep learning architecture for NLP: RNN, LSTM, GRU, and Transformers.  Here are the high-level overview of NLP :  Tokenization : Text, sound, or any form of human language data will be broken down into smaller units called tokens. Token are obtained from dividing a sentence into individual word. For example, the sentence &quot;I love natural language processing&quot; would be tokenized into the following tokens: [&quot;I&quot;, &quot;love&quot;, &quot;natural&quot;, &quot;language&quot;, &quot;processing&quot;]. Other component such as punctuation will be ignored. Numericalization : This is the process of encoding each token to some number, to be able to be processed by the machine learning model. For example, if we have these token : [&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;], we might assign indices like {&quot;apple&quot;: 0, &quot;banana&quot;: 1, &quot;orange&quot;: 2}. Sequencing : Sequencing is the process of turning sentences into data using the encoded token before. This image below illustrate how turning sentence into data looks like, it uses multi-dimensional array to separate each sentences. Source : https://youtu.be/r9QjkdSJZ2g?si=LZXkR6HOyXbVGNbO&amp;t=90 During the actual usage of the model, we may encounter words that are not present in the vocabulary. These words are referred to as OOV words. OOV words can arise due to several reasons, such as encountering new or rare words that were not seen during training, misspellings, slang, or domain-specific terminology. We may also pad out the sequence data with general value so it can be easily processed by the model. Source : https://youtu.be/r9QjkdSJZ2g?si=pGM_9okSjJR4Wxmp&amp;t=295 Embedding : We now has produced a sequence of word in form of number, but how will the model actually understand what a word means? This is where embedding come, embedding is a technique that involves grouping together words that are similar or commonly used in similar contexts. The model won't understand a word, but at least it knows how to use these words. The embedding process is typically learnable, meaning the model will adjust how it groups the number together. Word embedding are usually done by representing each word in a vector inside a vector space, where words with similar meanings or usage patterns are located close to each other. This way we can capture what is the meaning of a word while also having a numerical representation by representing it in vector. The image below visualize how it looks when they are represented in vector. Source : https://towardsdatascience.com/a-guide-to-word-embeddings-8a23817ab60f?gi=fc122b70f34a Source : https://devopedia.org/word-embedding Deep Learning Model : After all the preprocessing technique, we can now use the sequence of word in number as input for the deep learning model. The exact process of how will it works depends on the model architecture. ","version":"Next","tagName":"h3"},{"title":"CNN","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/cnn","content":"","keywords":"","version":"Next"},{"title":"Convolution​","type":1,"pageTitle":"CNN","url":"/cs-notes/deep-learning/cnn#convolution","content":" Convolution is the process of combining two function to produce a third function. In the context of image processing, a mathematical function will be applied to an image and it will produce another image.  The function applied to image is called kernel or filters. The function is a small square matrix with some numerical values. Convolution operation is when the kernel is applied to image by sliding it and performing a multiplication (dot product) between the number in matrix and the pixel value in the image. The resulting product wil be summed up and it will result in another image.   Source : https://towardsdatascience.com/computer-vision-convolution-basics-2d0ae3b79346  Following the nature of dot product, the choice of values in matrix will determine the result of convolution process. For example, the sharpen filters has high value in the central pixel and negative values in the neighbouring pixels. This enhances the difference between central pixel and its neighbours, making it receive higher emphasis leading to a sharpened appearance.   Source : https://en.wikipedia.org/wiki/Kernel_(image_processing)  Stride​  While doing convolution, we can adjust how the process is done. Stride is the hyperparameter used to determine the movement of the kernel while doing the convolution operation, it can be thought as the step size.  Like the image below, stride is used to skip some pixel value, this will make the resulting image size different.   Source : https://www.codingninjas.com/studio/library/convolution-layer-padding-stride-and-pooling-in-cnn  Padding​  Another hyperparamter of convolution is padding. Padding is additional pixel added around the image. The pixels at border have less neighbour than other pixels in the center, this can make the resulting image biased towards the central pixels. By adding padding, pixels in the border can be more balance.  Another purpose is to reduce information loss, while applying kernel to the image, the border pixels can't be on the center of the kernel, this can result in information loss at the borders.   Source : https://www.numpyninja.com/post/how-padding-helps-in-cnn (with modification)  Pooling​  Pooling is the process of downsampling or reducing the dimension of features. Color of image is considered as features, as it provide information about image. In the case of CNN, pooling is done to reduce the image size, this is done by taking the representation of image. Example of pooling technique is the max-pooling, which takes the largest values in a region.  Reducing the features is useful in CNN to help it focuses on some patterns and relationship between features.   Source : https://www.geeksforgeeks.org/cnn-introduction-to-pooling-layer/  ","version":"Next","tagName":"h3"},{"title":"CNN​","type":1,"pageTitle":"CNN","url":"/cs-notes/deep-learning/cnn#cnn","content":" CNN is specialized type of neural network used for image analysis, CNN typically consists of 4 different layer. Same as traditional neural network, these layer are connected together and will pass its result to next layer.  Input Layer : Input of CNN is an image in the form of number which is the representation of the image color. Input layer typically expect : [batch_size, channels, width, height], where width and height is the size of image, channels represent how many color channel in the image (e.g. RGB image means 3 channel), and batch_size is the number of image processed together in a single forward and backward pass. Convolutional Layer : Which is the layer that performs convolution operation explained above. So instead of multiplying input data by weights as we did in traditional neural network, CNN does convolution process to the input data which is the image in numerical representation then pass it onto the next layer that does convolution again. The convolution layer may also include an activation function. Pooling Layer : Pooling layer that does the pooling process to reduce the image dimension. The pooling layer reduce image size but it also take the representation of the overall image. Each neuron in pooling layer will capture different representation of image depending on the input. Fully Connected Layer : The fully connected layers are the same layers as the traditional neural network. After extracting the information from image from convolution and pooling layer, this layer is responsible for making prediction.   Source : https://developersbreach.com/convolution-neural-network-deep-learning/  Convolution​  The input data will be an image in the form of its pixel colors. Convolution process will be done into the input data, however, the filter used here is a matrix that consist of random numbers. The convolution process is the same as above explanation, this will produce another image.  These filter allows us to detect low-level patterns. The first convolution process may won't be able to capture important patterns or features. However, these numbers will be adjusted later to improve the detection.   Source : https://youtu.be/YRhxdVk_sIs?si=ZLAexy3x4VYMjQ0E&amp;t=347  Some of the filters in the convolution maybe are supposed to detect edges, other may supposed to detect curve, and etc.   Source : https://www.analyticsvidhya.com/blog/2022/03/basic-introduction-to-convolutional-neural-network-in-deep-learning/  The number of output image depends on the number of filter use. When we say 32 filter, this mean we have 32 distinct matrix with its own numbers. Each filter will be applied to the image, therefore producing 32 different image, the result is called feature maps as it is actually not a separate image, just a different form of image. (note: each conv layer can have different number of filter)  When we connect the layer together, we may have than 1 input image or feature maps. In this case, the convolution process works by applying a single filter in the next layer to all the feature maps from the previous layer, and the results are summed to generate a single feature map. This process is repeated for each filter in the second layer.  The notation for convolution layer is (numberOfFilter x imageWidth x imageHeight). The input image image is typically separated by its color channel, so red, green, and blue color of the image is processed separately.   Source : https://youtu.be/pj9-rr1wDhM?si=-7Fnrb2DpTa71Jo8&amp;t=450  Pooling​  Pooling is done to reduce the image dimension, which also reduces the features of the image. It can be thought as summarizing the whole image in a smaller dimension that consist of pixel that represent the overall image or called feature extraction.  This layer doesn't have adjustable parameters, it is a fixed process.   Source : https://youtu.be/pj9-rr1wDhM?si=-7Fnrb2DpTa71Jo8&amp;t=450  Prediction​  After receiving result from the last layer convolution or pooling layer, the fully connected layer will make prediction. The result from previous layer will first be flattened into one-dimensional vector. This will combine the result from all the filter that does different detection.  From now on, how will the network works is same as traditional neural network, it consist of weight, bias, and activation function. After reaching the last layers, it can now make prediction. In this step, loss function will be calculated, the specific function depends on the task.  After calculating the loss, it's time for the model to learn with backpropagation technique. Gradient of the loss function will be calculated with respect to each parameters, including fully connected layer weights and the coefficient in the filter matrix. Gradients with respect to the filters are calculated and used to update the filter weights.  The input in the fully connected layer is based on the previous layer, the previous layer is also based on the previous layer, backward pass will be done adjusting all adjustable parameter.  Dropout​  Dropout is a data augmentation techinique to reduce overfitting in neural network by deactivating some neuron in the layer. Dropout can be thought as discarding some random color in an image. The idea of dropout is, we human, still able to recognize corrupted image. Computer should also know how to recognize these.   Source : https://www.researchgate.net/figure/9-An-illustration-of-the-dropout-mechanism-within-the-proposed-CNN-a-Shows-a_fig23_317277576 ","version":"Next","tagName":"h3"},{"title":"Deep Learning Foundation","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/deep-learning-foundation","content":"","keywords":"","version":"Next"},{"title":"Loss Function​","type":1,"pageTitle":"Deep Learning Foundation","url":"/cs-notes/deep-learning/deep-learning-foundation#loss-function","content":" Loss function is a function that measure how well does a machine learning model performs. Loss function is typically calculated from mathematical function that takes actual or true output and the predicted output from the model. For example, a simple loss function in machine learning is the Mean Squared Error (MSE) function, commonly used in linear regression.   Source : https://suboptimal.wiki/explanation/mse/  The model predict what may be the y value for specific x value. It sums all the error or the difference between actual and predicted value and then square it and get the average. In the case of MSE, the larger means the worse performance and the lower means the better.  The point of machine learning is we keep measuring the performance of our model and adjust our model to make it performs better. The less result we get from the loss function (or more depending on the loss function itself) reflect of how our model performs. We need to optimize the loss function, there are many way to optimize it, such as the gradient descent algorithm    ","version":"Next","tagName":"h3"},{"title":"Entropy​","type":1,"pageTitle":"Deep Learning Foundation","url":"/cs-notes/deep-learning/deep-learning-foundation#entropy","content":" In machine learning, entropy is a measure of uncertainty or randomness in a set of data. It is often used as a criterion to quantify the impurity or disorder within a group of samples.  The formula for entropy is (base of the log can vary) :  E=−Σ(pilog⁡2(pi))E = - \\Sigma (p_{i} \\log_{2} (p_{i}))E=−Σ(pi​log2​(pi​))  Entropy is calculated from a set of data or event, each of it has a probability of occuring which is the pip_ipi​ or the probability of event iii.  A higher entropy means the data is uncertain, it's the opposite when entropy is low.  For example, an equally likely probabilities of coin flip has high entropy. If probabilities of getting head and tail is same, it's hard to predict what happen next. If the head has an odds of 0.9 and the tail has an odds of 0.1, then entropy will be lower.  Entropy can be thought as calculating the disorder of probability distribution of the event. Probability distribution describes the probability of all different outcomes in an event.   Source : https://twitter.com/page_eco/status/1631267143890407426  Cross Entropy​  Cross entropy has a similar concept with entropy, the difference is cross entropy calculate the disorder of 2 probability distribution of an event instead.  The formula for cross entropy is :  H(P,Q)=−Σ(P(x)log⁡(Q(x)))H(P, Q) = - \\Sigma (P(x) \\log (Q(x)))H(P,Q)=−Σ(P(x)log(Q(x)))  Machine learning is typically used for prediction, the prediction output can be probabilities. Cross entropy is used in the context of machine learning, the probability distribution included is the actual probability P(x)P(x)P(x) and the predicted probabilities by the machine learning model Q(x)Q(x)Q(x).  Cross Entropy Loss​  Loss function in machine learning, measure how well a model performs in a training. Knowing how well it performs make us able to train the model to improve it.  The cross entropy function explained before can be used to calculate a loss function, typically for classification tasks that outputs probabilities. It still use the same formula, however, the notation for actual probability is typically denoted as yyy and the prediction is denoted as y^\\hat{y}y^​.  Binary Cross Entropy Loss​  Binary cross entropy loss is another form of cross entropy loss which is used for binary classifcation, or a classification that only has 2 output. The formula is below :  L(y,y^)=−(ylog⁡(y^)+(1−y)log⁡(1−y^))L(y, \\hat{y}) = - (y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}))L(y,y^​)=−(ylog(y^​)+(1−y)log(1−y^​))  ","version":"Next","tagName":"h3"},{"title":"Stochastic Gradient Descent​","type":1,"pageTitle":"Deep Learning Foundation","url":"/cs-notes/deep-learning/deep-learning-foundation#stochastic-gradient-descent","content":" In traditional gradient descent, model's parameter (e.g. the slope and y-intercept in linear regression) are updated every iteration, this can be slow for large datasets. Traditional gradient descent &quot;walks&quot; slowly, it may reach a bad local minima or even stuck at saddle point.  Stochastic Gradient Descent (SGD) is a variant of gradient descent which is suited for larger datasets. The idea of SGD is, instead of considering all dataset to calculate the gradient and update the parameters, SGD randomly selects a single data point (or a small batch of data points) at each iteration and calculate that particular gradient and use it to update the model's parameters.  By not considering all the data, SGD may not be stable. However, with the faster computation, we can update more and eventually catch up with traditional gradient descent.   Source : https://youtu.be/UmathvAKj80?si=jHExCTVk7diEA6_6&amp;t=92  With randomness, SGD able to escape saddle point or bad local minima, this is because it allows the algorithm to explore different directions and not get stuck in a single negative curvature.   Source : https://youtu.be/UmathvAKj80?si=Tpo1K5_hXo94UGJh&amp;t=107  Adam​  Adaptive Moment Estimation (Adam) is an upgrade to SGD. In high-level, Adam is able to adapt the model learning to different case we are facing. Adam have several parameters, these parameters will be adjusted based on past gradients.  Adam uses several adaptation technique including :  Adaptive Learning Rate : SGD has a fixed learning rate which is set before model training begins. Adam is able to adjust the learning rate for each parameter based on their past gradients.Momentum : Momentum is a configuration that helps us goes into minimum region like saddle point in the loss function faster by making the model adjust the parameter in the successful direction.Bias Correction : During the early training, we typically set the parameters of our model (weight and bias) to some specific value. This may make our model biased toward that specific value, in simple term, Adam adjust our estimation based on the number of iterations or epochs.  ","version":"Next","tagName":"h3"},{"title":"Activation Function​","type":1,"pageTitle":"Deep Learning Foundation","url":"/cs-notes/deep-learning/deep-learning-foundation#activation-function","content":" While predicting in machine learning, we often find that the relationship between dependent and independent variable is non-linear. Sometimes, it can't be easily approximated using simple line like we did in linear regression. Remember that the goal is to predict next data by fitting a line, so we must construct a function that fits the data. The more complex relationship the variables have, the more complex the function would be.  We can construct any complex function that captures non-linear relationship by summing up several function or lines. However, we may not achieve a desired function just by summing up all the line.  This is where an activation function comes, an activation function is used to filter or &quot;decide&quot; which line should we sum, this way we control the sum and make any function we want. This is when we use activation function to introduce non-linearity and constructing complex function.  There are many activation function, for example Rectified Linear Unit (ReLU) is a simple linear activation function that defined as : ReLU(x)=max(0, x)\\text{ReLU(x)} = \\text{max(0, x)}ReLU(x)=max(0, x), in other word, it filters any negative value.  In image below, we summed 2 relu function, it now looks like an elbow like graph which obviously can't be constructed by linear function. However, if we sum all the linear function and only takes what we need, we may be able to construct it.  The relu line is defined with the mx+bmx + bmx+b, we can sum as many relu as we want, and in a more complex problem, the equation may also involve more variable.   Source : https://youtu.be/hBBOjCiFcuo?si=B8PDlJRj4QTa99k-&amp;t=2721   Source : https://medium.com/@shrutijadon/survey-on-activation-functions-for-deep-learning-9689331ba092  ","version":"Next","tagName":"h3"},{"title":"Other Terminology​","type":1,"pageTitle":"Deep Learning Foundation","url":"/cs-notes/deep-learning/deep-learning-foundation#other-terminology","content":" Hyperparameter : Settings or configuration choices that are set before the learning process begins. They are not learned from the data but are determined by the practitioner or researcher. For example, learning rate is a hyperparameter that controls rate at which a model adjusts its parameters during training (e.g. used in gradient descent). After we trained the model and evaluate it, we may want to adjust the hyperparameter to explore if there is a more optimal result, this is called hyperparameter tuning. Epoch : Sometimes, training data is used more than one times while training model. The number of how many iteration pass through the entire dataset is called epoch. For example, when we say 5 epoch, it means that we are training the model with the same dataset for 5 times. Epoch is considered as hyperparameter as it is set before learning process. Batch : Batch is the number of samples or data points that is processed together in a single learning process. If we have 1000 data and we used batch of 32, it means during each iteration, we will process 32 at each time. Iteration : Iteration is the number of time we want to repeat each batch. It is different with epoch, epoch is the iteration of whole dataset, while this is just an iteration of a batch. Metric : Metric is a measure of the quality of model's predictions. Metric is different with loss function, loss function is used to train while metric is used to evaluate the prediction. By evaluating the models, we can compare with different model or configuration. For example, a simple metric is accuracy, it is defined as the number of correct predictions divided by the total number of predictions. Data Split : Data is typically split into 3 : Training Data : Training data is the part of data used to train the machine learning model, the model will learn from this data by adjusting its parameters (weight and bias) based on the actual data.Test Data : Test data is the data used to evaluate performance of the model after the training is done.Validation Data : Validation data is a subset of training data used to validate or assess model's performance, this way we can adjusts the hyperparamater if required. Fine Tuning : Fine tuning is the process of training a pre-trained model, typically with a smaller dataset or new task. The purpose of fine-tuning is to leverage the knowledge and learned representations from the pre-trained model and adapt it to the new task or dataset. Overfitting : Overfitting, also known as high variance, is a situation where a model learns too much from the training data, making it becomes too specialized and performs poorly on new, unseen data. This is the same when you practice alot on a math problem without studying the concept and only memorizing it, turns out the actual exam asked different questions. For a machine learning model, the concept is the pattern of the data, it is important to capture the broader patterns rather than a specific example. Underfitting : Underfitting, also known as high bias, is where the model is too simple or didn't capture the underlying patterns in the data. It fails to learn the relevant relationships or make a very general assumptions for the training data. Source : https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76 Optimizer : An optimizer is an algorithm or method used to optimize the loss function. Example of optimizer are gradient descent, stochastic gradient descent, Adam, and etc. Gradient Accumulation : While training model using optimizer like gradient descent, model's parameters are updated after computing gradient for each batch. Gradient accumulation is a technique that defer parameters update, the gradient for each batch is accumulated and updated once with the accumulated gradient. Using this technique can helps reduce memory as it reduce the training process. Regularization : In simple term, regularization is a technique to make model prediction simpler (prevent overfitting) by forcing some feature coefficients to be 0. In other word, we exclude some variable contribution to our prediction. The features we are excluding are the one that has low influence toward the overall prediction. This way we can focus on the more important and influential features. Data Augmentation : Data augmentation is the process of increasing the diversity and variability of the training data, to make our model more generalized to unseen data. This technique is typically used when our data is limited or in an image classification task. For example, while training a dog or cat classifier, with the same image data, we can do some transformation such as rotating the image, skew it, flip, scale, or adjust its color in order to create more variety of data to reduce overfitting. Multi-target Model : A multi-target model is a model trained to predict multiple target variables simultaneously. For example, a multi-target model would also predict the number of bedrooms a house has rather than just predicting the house price. The training process will minimize overall loss of each target variable. The loss function itself is calculated individually and will be combined. The metric (e.g. error) for evaluating the prediction is also measured individually. Transfer Learning : A technique which a pre-trained model, initially trained on a specific task, is tweaked on a related task. Instead of training a model from scratch, an existing model trained on a similar task is tweaked to suit it to the new task. For example, if there is a model for classifying cars, we can suit it to classify trucks. Early Stopping : A technique to prevent overfitting by stopping the training of a model in an optimal time. One way to determine the optimal time to stop, is to plot the model's loss curve and find out when the loss is slowly decreasing. Source : https://miro.medium.com/v2/resize:fit:708/1*LSjaVNMa-ku85I35of-mAw.png Learning Rate Decay : Learning rate is a hyperparameter that is set before the training of a model. In some scenario, the model successfully learn and reach the minima of a loss function, this mean it has reached an optimal loss. However, if we set a large learning rate, the model will still try to optimize the loss function, making the optimization become unstable. Learning rate decay, also called learning rate scheduling, helps us reduce the learning rate over time during the optimization process. The idea is to start with a high learning rate for rapid progress in the early stages of training. Then, as the we get closer to the optimal loss, the learning rate is decreased to reach more precise convergence. State-of-the-Art (SOTA) : A model is called SOTA if that particular model or technique achieve the highest known performance or accuracy in a specific benchmark or task. It represents the current best-known solution or method for a particular problem. Cutting-Edge : A cutting-edge model means a model or technique that represents the latest advancements in a field. These model are characterized by most recent innovation, methodologies, or architectural designs. Normalization : Normalization is the process of transforming data into a common scale without distorting its shape, meaning the relative relationships and patterns within the data are preserved even after the normalization process. The purpose of normalization is to generalize the data to prevent a feature with large number dominate the learning process. This will be useful for case when absolute values of the features are not as important as their relative positions within the range. There are several technique to normalize data, an example is the min-max technique that scale all the data value in a specific range (e.g. 0 to 1). It uses the minimum and maximum value of the data as the baseline of the scaled value, the formula is scaled_value = (value - min_value) / (max_value - min_value). Source : https://zaffnet.github.io/batch-normalization Batch Normalization : Batch normalization is the process of normalizing data in a batch, they are done by adding extra layer that does normalization process in the networks. Standarization : If normalization scale the data to a specific range, standarization, also known as z-score normalization, transforms the data to have a mean of 0 and standard deviation of 1. By centering the data around the mean and scales it based on the standard deviation, standardization make the data less affected by outliers while also preserving the shape of distribution. ","version":"Next","tagName":"h3"},{"title":"GAN","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/gan","content":"","keywords":"","version":"Next"},{"title":"Adversarial Training​","type":1,"pageTitle":"GAN","url":"/cs-notes/deep-learning/gan#adversarial-training","content":" Both generator and discriminator are trained in adversarial way, the generator's objective is to produce new samples that can fool the discriminator, while the discriminator's objective is to accurately distinguish between real and fake samples.  The discriminator takes input from the data generated by the generator and the data from training samples. The data are combined together, every data from generator is labeled as fake and every data from the training samples is labeled as real.  The loss will be calculated from the classification result, the loss represent how good is the data generated by the generator and how correct is the classification of the discriminator.  GAN is considered as a minimax game or a zero-sum game, which is a mathematical game concept that represents a competitive interaction between two players. The goal of one player is to minimize their maximum possible loss (minimizer), while the goal of the other player is to maximize their minimum possible gain (maximizer).  In this case, the generator aims to minimize the discriminator's ability to differentiate between real and fake samples, while the discriminator aims to maximize its ability to correctly classify real and fake samples.   Source : https://sthalles.github.io/intro-to-gans/  ","version":"Next","tagName":"h3"},{"title":"Generator​","type":1,"pageTitle":"GAN","url":"/cs-notes/deep-learning/gan#generator","content":" The generator first takes a random input usually sampled from a normal or uniform distribution. The random input is often referred as latent vector or noise vector, which is a term for features that captures the charateristics of data. However, since it's just a random input they don't have necesarry meaning, they simply serve as a random seed for the starting point.   Source : https://www.researchgate.net/figure/The-semi-supervised-GAN-architecture-Random-noise-is-used-by-the-Generator-to-generate_fig4_335359919 (with modification)  The random input will then be transformed into a higher-dimensional representation. They are passed into some neural network architecture such as convolutional neural network for image related tasks that also include activation function to enable the network to capture complex patterns.  The final layer of generator produces the generated data, which could be images, text, or any other type of data depending on the application. The generated sample are then fed into discriminator to be classified.   Source : https://towardsdatascience.com/dcgans-deep-convolutional-generative-adversarial-networks-c7f392c2c8f8  ","version":"Next","tagName":"h3"},{"title":"Discriminator​","type":1,"pageTitle":"GAN","url":"/cs-notes/deep-learning/gan#discriminator","content":" The discriminator can be any network architecture capable of classifying, if it's an image related task, convolutional neural network can be used. The input data will be combined from the training samples and the generated samples. They will be labeled as fake or real and the discriminator will assign probability or how likely is the data to be real or fake.  Training Process​  In GAN, the training process of both generator and discriminator are closely related. When the discriminator fails, meaning it fail to accurately distinguish between real and generated samples, this will also provide weak or incorrect feedback to the generator, which hold back the learning process. The generator may receive misleading signals that its generated samples are realistic, even if they are not, making it might not learn to improve and generate a better samples.  This is often called as convergence failure.  Both generator and discriminator are trained in alternating turns. This is to stabilize the learning process and to establish a feedback loop by improving iteratively from each other feedback.  ","version":"Next","tagName":"h3"},{"title":"Loss Function​","type":1,"pageTitle":"GAN","url":"/cs-notes/deep-learning/gan#loss-function","content":" The loss function used for GAN will be two, one for generator and another for discriminator. We used two loss function to be able to measure both of their performance. As explained before, the generator tries to minimize its loss by generating more realistic data, while the discriminator tries to minimize its loss by becoming better at classifying the data.  Minimax​  The minimax loss is the loss function used in the original paper that introduced GAN. The formula is :   Source : https://developers.google.com/machine-learning/gan/loss  The D and G is commonly used to refer the discriminator and generator, respectively. It is related to the cross entropy loss function formula. The expected value measure of what we can expect to observe on average when dealing with uncertainty, in this case the uncertainty of the real data ExE_xEx​ and the fake instance EzE_zEz​.  The first term Ex[log⁡(D(x))]E_x[\\log(D(x))]Ex​[log(D(x))] represents the expected log-likelihood (the logarithm of a probability) of the discriminator correctly classifying real data as real. The generator aims to minimize this term to generate data that can &quot;fool&quot; the discriminator.  The second term Ez[log⁡(1−D(G(z)))]E_z[\\log(1 - D(G(z)))]Ez​[log(1−D(G(z)))] represents the expected log-likelihood of the discriminator correctly classifying generated data as generated. The discriminator aims to maximize this term by correctly distinguishing between real and generated data.  The minimization or maximization of specific term is done by taking the partial derivative with respect to the specific parameters. The loss will be optimized and backpropagation process will be done to adjust all the parameters involved. The generator is connected to the discriminator to receive its gradient for optimization step.  ","version":"Next","tagName":"h3"},{"title":"Wasserstein (WGAN)​","type":1,"pageTitle":"GAN","url":"/cs-notes/deep-learning/gan#wasserstein-wgan","content":" The Wasserstein loss is the alternative loss function for minimax, it modified the concept of standard GAN, becoming Wasserstein Generative Adversarial Networks (WGANs). This was introduced to address GAN problem including the lack of meaningful loss metric.  The minimax loss does not provide a direct and meaningful measure of the quality of generated samples, it only provides a real or fake indicator. The objective function does not correlate well with the visual quality or desired characteristics of the generated samples, making it difficult to assess the progress of training.  In WGAN, the discriminator is no longer trained to classify samples as real or fake. Also, it is no longer called a discriminator, instead it is called a critic.  The real data from training sample and the fake data from generator is represented in a probability distribution. The discriminator or the critic still have its own network, however, the output of the network is a measure of how far is the input it received from the real data probability distribution. It is calculated using the earth mover distance or the wasserstein distance.  After that, the result of critic will be used in the wasserstein loss function, which is the below formula :   Source : https://developers.google.com/machine-learning/gan/loss  The critic is trained to maximize the critic loss which is the difference between the output on real and fake instance. This mean we are training the critic to be able to accurately estimate the distance between input distribution and the real distribution. And the generator is trained to maximize the critic output for fake instance (or minimizing the negative output of critic), where the higher means the more close it is to real data.  In summary, the benefit of having the critic produce real-valued outputs instead of binary real or fake labels is that it allows the critic to provide a measure of the degree of realism in the generated samples.  After that, the backpropagation process that optimize all the parameters involved, including the generator's parameters are done. The gradient from the discriminator as the output will be passed to the generator.  ","version":"Next","tagName":"h3"},{"title":"Variations of GAN​","type":1,"pageTitle":"GAN","url":"/cs-notes/deep-learning/gan#variations-of-gan","content":" GAN also have several variations, the two example are :  Conditional GAN (cGAN)​  In traditional GAN, the generator takes random noise as input and produces new samples. In cGAN, both the generator and discriminator are conditioned on additional information, in the form of labels or class information. It uses the concept of conditional probability with the given condition instead of the concept of joint probability of the noise and real data.  This conditioning enables the generation of samples that are conditioned on specific attributes, making it useful for tasks like generating samples based on specific class labels or generating another image based on some input image (called image-to-image translation).   Source : https://learnopencv.com/conditional-gan-cgan-in-pytorch-and-tensorflow/, https://developers.google.com/machine-learning/gan/applications  Variational Autoencoder GAN (VAE-GAN)​  VAE-GAN combines the variational autoencoder with the traditional GAN. The generator in GAN is replaced by the decoder of VAE. The step are :  Input data goes through encoder, the data will be transformed into latent space representation in the form of probability distribution.The distribution will be sampled and goes into the decoder, transforming it back to higher-dimensional representation.The generated data from decoder will be passed together with training sample to the discriminatorDiscriminator classify real or fake data.  There are 3 loss function used which is combined from loss in VAE and GAN :  Regularization / KL Divergence Loss : The loss for encoder which is responsible to map input data into latent space representation, the loss is supposed to encourage the encoder to make the latent space representation close to the prior distribution, which is set to normal distribution.Reconstruction Loss : The loss of VAE decoder, the difference between reconstructed image with the original image.Adversarial / GAN Loss : The loss resulted from discriminator that classify whether data is real or fake. The generator or the decoder of VAE need to generate data that can fool the discriminator while the discriminator need to classify real images as real and generated images as fake.   Referenced from : https://wandb.ai/shambhavicodes/vae-gan/reports/An-Introduction-to-VAE-GANs--VmlldzoxMTcxMjM5, https://medium.com/dataseries/variational-autoencoder-with-pytorch-2d359cbf027b  CycleGAN​  Cycle-Consistent GAN is a type of GAN designed for image-to-image translation tasks. It is an unsupervised learning model that can learn to convert/transform one image to another image smoothly without paired training data. It is achieved using two paired of generator and discriminator.  The generator consists of two, the GAG_AGA​ that is responsible for transforming image from domain A to domain B, another one GBG_BGB​ is responsible for transforming image from domain B to domain A.  The discriminator DAD_ADA​ classify between real and fake image from domain A which is from GAG_AGA​, while DBD_BDB​ does the same from GBG_BGB​.  note The domain here means a specific style of an image that is visually distinguishable, a common example is a horse image (domain A) transitioning to zebra image (domain B).  The CycleGAN process operates in a cycle :  Generator A takes an image from Domain A as input and translates it to Domain B. The goal is to generate a realistic image in Domain B as it is really coming from domain B.Generator B takes that translated image and translates it back to Domain A. The goal is to reconstruct an image that is similar to the original input image from Domain A.The discriminator compare the generated images with the same real images which we used as input for generator A in step 1.  The process is same for the other discriminator.   Source : https://www.oreilly.com/library/view/hands-on-artificial-intelligence/9781788836067/c2e7d914-4e45-4528-8627-c590d19107ef.xhtml  There are two loss function used, the same adversarial/GAN loss which is obtained from the discriminator classification (classify whether image is real or fake). Another loss is the cycle consistency loss, which is loss introduced to ensure the translation from A to B and then from B to A or vice versa reconstructs the original image. ","version":"Next","tagName":"h3"},{"title":"Diffusion Model","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/diffusion-model","content":"","keywords":"","version":"Next"},{"title":"Denoising Diffusion Model​","type":1,"pageTitle":"Diffusion Model","url":"/cs-notes/deep-learning/diffusion-model#denoising-diffusion-model","content":" There are many variation of diffusion model, each with their own concept, Denoising Diffusion Model is the type of diffusion model that uses the diffusion concept.  ","version":"Next","tagName":"h2"},{"title":"Denoising Diffusion Probabilistic Model (DDPM)​","type":1,"pageTitle":"Diffusion Model","url":"/cs-notes/deep-learning/diffusion-model#denoising-diffusion-probabilistic-model-ddpm","content":" The type of denoising diffusion model that learns the underlying probability distribution of a dataset and generate new samples from that distribution.  The overall process of denoising diffusion model consist of two steps, the forward process that gradually add noise to the image, and the reverse process that tries to reverse the process or remove the noise to generate clean samples.  Forward Process​  In DDPM, the noising process is modeled using a Markov chain. Markov chain is a mathematical model that assumes the future state of a system only depends on current step. In other word, the current state of a system depends only on the previous state. The key idea behind using a Markov chain in diffusion models is to describe the evolution of the system's state as an iterative stochastic process.  The forward process begins with adding noise to the image in gradual manner, the process will be divided into discrete time steps. The noise, we are adding is modeled using a Gaussian (normal) distribution. A Gaussian distribution is characterized by its mean (μ\\muμ) and variance (σ2\\sigma^2σ2), which determine the central tendency and spread of the distribution, respectively.   Source : https://analyticsindiamag.com/a-guide-to-different-types-of-noises-and-image-denoising-methods/  Utilizing the Markov chain with diffusion model, the distribution of noise at some time step ttt only depends on previous time step t−1t-1t−1. Following the calculation in Markov chain, current step distribution will be calculated by the product of each previously conditional step.   Source : https://youtu.be/fbLgFrlTnGU?si=tR6le4piBvVpeR_9&amp;t=109  note The forward process is fixed, meaning it doesn't have learnable parameters.  In the context of diffusion model, the type of Gaussian distribution used is the diagonal Gaussian distribution. The variance, denoted as β\\betaβ, varies at each time step and is constrained to be within the range of 0 and 1. The lower variance implies that the diffusion or transformation of the distribution occurs more gradually and with smaller perturbations, which may help us on the reverse process.   Source : https://youtu.be/fbLgFrlTnGU?si=mN1d8DKDP9vYJjQ0&amp;t=129  note The beta parameters is used to control or adjust the noise added in forward process or removed in the reverse process, this technique is also called noise scheduling.  As we iteratively perform the forward diffusion process, the noise gradually converges towards a Gaussian distribution. Mathematically speaking, the noise can be approximated as a multivariate Gaussian distribution with a mean vector of zero and an identity covariance matrix.   Source : https://youtu.be/fbLgFrlTnGU?si=C8gzMh3VVKSUXqBg&amp;t=163  Reverse Process​  The reverse process will also be modeled using Markov chain, the noise will be assumed as a unimodal diagonal Gaussian distribution (the formula above in the image below), which takes current state (xtx_txt​) and current time step (ttt) as input.  In the reverse process of a diffusion model, it involves inferring the previous step given the current step. The calculation is done by multiplying the product of conditional distributions at each time step in reverse order (xt−1∣xtx_{t - 1} | x_txt−1​∣xt​) with the Gaussian noise distribution, denoted as p(xT)p(x_T)p(xT​), which was generated during the forward process (We assume the forward diffusion approaches Gaussian distribution).   Source : https://youtu.be/fbLgFrlTnGU?si=v6ixlxk-gmWiHtDW&amp;t=279  The inference process is where the process is made learnable or adjustable. In the implementation of reverse process, there are two parameters, the mean (μ\\muμ) and the variance (Σ\\SigmaΣ) of the Gaussian distribution. The variance is made fixed and only the mean is made learnable, for training stabilization purposes. In essence, the model will dynamically learn and adapt the optimal parameters to effectively reverse the diffusion process.   Source : https://youtu.be/fbLgFrlTnGU?si=o0xlAFVkGm6B4nHr&amp;t=651  tip The implementation of the reverse or denoising process typically uses the U-Net architecture for image data or a transformers for non-image data.  Training Objective​  The forward and reverse process can be understood as process that transform data or distribution (the input image) in two different directions. The forward process involves adding noise that will make the data distribution approach Gaussian distribution. The reverse process involves transforming the data back to its original distribution. This is done by approximating the unnoised data, by doing this, we effectively generate new data points during this process.   Source : https://youtu.be/fbLgFrlTnGU?si=lOk5eb9Au4EJumjW&amp;t=376 (with modification)  The process and objective of diffusion model is similar to variational autoencoder (VAE). In VAE, the encoder takes the input data and maps it to a lower-dimensional representation called the latent variables. This latent variables serves as a compressed representation that captures the essential information and underlying structure present in the input data. Latent variables will then be modeled in a probability distribution with some mean and variance, this is now called a latent space. The decoder sample from the latent space distribution, to generate new data samples. The objective is to approximate the true data distribution from the sampled distribution.  The similar objective can be applied to diffusion model, &quot;Given transformed data, how to untransform it?&quot;. The primary aim of a diffusion model is to enhance the inference process, particularly by focusing on the reverse process that involves computing the preceding state of the Markov chain.  The training objective can be summarized with the following formula ([similar to loss in VAE]) :   Source : https://youtu.be/fbLgFrlTnGU?si=D6pH6EyDnxSz0j1P&amp;t=458 (with modification)  The log⁡pθ(x)\\log p_{\\theta}(x)logpθ​(x) represent the likelihood of the original data, it must be greater or equal to the first term subtracted by second term. We represent the original data as a likelihood because our goal is to reconstruct it. By maximizing the likelihood, we aim to ensure that the reconstructed data closely resembles the original data.  The first term is the reconstruction term, which is the comparison of generated data and the original data. The second term is the KL divergence measures the difference of probability distribution between the target distribution (input data) and the learned distribution (generated distribuion).  Summary​  In summary, diffusion model starts with the forward process where we add Gaussian distribution noise to the image. After a bunch of step, we obtained a distribution approximately close to Gaussian distribution. In the reverse process, the model aims to remove the noise by inferring the previous distribution based on the current distribution, which is where the model learns. This process is often described as sampling because it entails generating samples from an approximated distribution obtained at the current step.   Source : https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/foundation-diffusion-generative-models  After training the model, to generate new data using a trained diffusion model, we start with a random noise sample and then perform the reverse diffusion steps.   Source : https://tree.rocks/make-diffusion-model-from-scratch-easy-way-to-implement-quick-diffusion-model-e60d18fd0f2e  ","version":"Next","tagName":"h3"},{"title":"Denoising Diffusion Implicit Models (DDIM)​","type":1,"pageTitle":"Diffusion Model","url":"/cs-notes/deep-learning/diffusion-model#denoising-diffusion-implicit-models-ddim","content":" The type of denoising diffusion model previously we talked about uses the probabilistic Markov chain as the framework, which can be slow during the sampling process in the reverse diffusion (there is some technique to skip the diffusion in forward process). Furthermore, DDPM require alot of forward diffusion step, which also increase the step in reverse process.  DDIM is a non-Markovian diffusion model, it removed the use of Markov chain. In Markov chain, during the reverse process, we assume that previous step only depends on current step (e.g. x1x_1x1​ depends on x2x_2x2​ = q(x2∣x1)q(x_2|x_1)q(x2​∣x1​)).   Source : https://betterprogramming.pub/diffusion-models-ddpms-ddims-and-classifier-free-guidance-e07b297b2869 (with modification)  DDIM does not rely on a strict sequential dependence on previous steps. The elimination of the Markov chain allows for the consideration of multiple states beyond just the previous one in the reverse diffusion process. This transformation turns the process into an optimization problem, as the inclusion of more states introduces complex dependencies and variations in the data.  The goal is to find an optimal latent code, latent code is nothing but a set of parameters that captures the information required to generate a denoised or clean sample from a corrupted input. We adjust the parameters by minimizing the reconstruction loss between the generated sample and the corrupted input.  tip DDPM can be seen as a special case of DDIM, where we only consider the previous state for current state. The term &quot;implicit&quot; in DDIM means that we do not explicitly try to denoise the image, instead we just find the best parameters that minimize the lost.  ","version":"Next","tagName":"h3"},{"title":"Conditional & Unconditional Generation​","type":1,"pageTitle":"Diffusion Model","url":"/cs-notes/deep-learning/diffusion-model#conditional--unconditional-generation","content":" Diffusion model can also be integrated with a conditional prompt. The conditional prompt serves as a guide or constraint during the generation process, allowing for more controlled and targeted generation of data. The conditional prompt can be in the form of text, images, or any other type of input.  Unconditional​  In the unconditional generation, we do not include any additional constraint. The model will start with a random noise sample and applies the reverse process to progressively reduce the noise and generate new data samples. The resulting data will be similar to the training data.   Source : https://betterprogramming.pub/beginners-guide-to-unconditional-image-generation-using-diffusers-c703e675bda8  Conditional​  In the conditional generation, the model is trained to learn the distribution of the training data given certain input conditions. During the generation process, the model takes the input conditions into account and adjusts the reverse process accordingly to generate samples that satisfy the given conditions.  The target data in reverse process which is pθ(x0)p_{\\theta}(x_{0})pθ​(x0​) becomes pθ(x0∣y)p_{\\theta}(x_{0}|y)pθ​(x0​∣y). The reverse step also takes the additional condition. The conditional generation can actually simplify the generation process by constraining the range of possible outputs.   Source : https://youtu.be/fbLgFrlTnGU?si=luVZ1O9GCWV8_2PH&amp;t=714  ","version":"Next","tagName":"h3"},{"title":"Score-Based Diffusion Model (SBDM)​","type":1,"pageTitle":"Diffusion Model","url":"/cs-notes/deep-learning/diffusion-model#score-based-diffusion-model-sbdm","content":" Score-Based Diffusion Model (SBDM) is also known as Noise Conditional Score Network (NCSN) or Score-Matching with Langevin Dynamics (SMLD). SBDM introduces the concept of score, mathematically, it is the gradient of the log-likelihood of the data distribution.  ","version":"Next","tagName":"h2"},{"title":"Score Function​","type":1,"pageTitle":"Diffusion Model","url":"/cs-notes/deep-learning/diffusion-model#score-function","content":"  Source : https://youtu.be/fbLgFrlTnGU?si=lEL8BBYdL0f1cetc&amp;t=907  A gradient with respect to some variables tells us the direction and magnitude of the steepest ascent or descent of a function at a particular point. Specifically in the context of SBDM, the gradient measures the direction in which the noise should be adjusted to move towards denoising or generating the desired output. It act as a guide for diffusion process by providing information on how to update the noise, but it does not provide a direct measure of similarity or dissimilarity.  The integration of score-based guidance with diffusion model fall between each diffusion step, score or gradient of the log-likelihood of the data distribution will be estimated with respect to the noise process. After that, the noise is updated in the direction of the estimated score.  ","version":"Next","tagName":"h3"},{"title":"Continuous Diffusion Model​","type":1,"pageTitle":"Diffusion Model","url":"/cs-notes/deep-learning/diffusion-model#continuous-diffusion-model","content":" Continuous diffusion model is a type of diffusion model that model the noise distribution of the data in a continuous manner. Instead of modeling how the noise distribution changes over discrete time step, SBDM instead treat the distribution as a continous process that evolves over a continuous time.  The diffusion process is modeled using stochastic differential equations (SDEs). SDEs are mathematical equation that describe how a system transitions from one state to another, which is affected by deterministic and random factor.  The score-based diffusion model can be combined together with SDE-based modeling. The SDE-based deterministic and random factor terms can be incorporated into the score-based model to guide the denoising and transformation steps of the diffusion process.   Source : https://theaisummer.com/diffusion-models/#score-based-generative-modeling-through-stochastic-differential-equations-sde  ","version":"Next","tagName":"h2"},{"title":"Sampler​","type":1,"pageTitle":"Diffusion Model","url":"/cs-notes/deep-learning/diffusion-model#sampler","content":" Continous diffusion model uses SDE to model the system, in order to know the system state over time and sample from it to actually generates data, the SDE needs to be solved.  While solving an SDE, we do not find the exact analytical solution, we instead approximate the solution (also called numerical integration). Some of the methods are Euler–Maruyama method, Heun's method, and linear multistep methods, these are also called sampler.   Source : https://stable-diffusion-art.com/samplers/ (sampler that gradually reduce the noise)  Ancestral Sampler​  There is another type of sampler called ancestral sampler, these sampler introduce randomness to the sampling process. At each sampling step, they add noise to the image to introduce variations in the generated images. This makes the image produced may not reach a consistent and reproducible state, the image can turn differently at each step.  Ancestral sampler\tNormal sampler\t  https://stable-diffusion-art.com/samplers/  ","version":"Next","tagName":"h3"},{"title":"Latent Diffusion Model (LDM)​","type":1,"pageTitle":"Diffusion Model","url":"/cs-notes/deep-learning/diffusion-model#latent-diffusion-model-ldm","content":" In diffusion model, we are approximating what an unnoised image will look like from a random noised image. In other word, we are approximating the probability distribution of a target image from a simple base distribution. This makes diffusion model is often called a general method to model a probability distribution.  An encoder-decoder pair is a type of network that consist of two component. The encoder, serve as the one that takes input and transform it into latent variables or lower-dimensional representation of the input data. The other component, decoder, will take the output of encoder and do the reverse process. An example of encoder-decoder pair are autoencoder and Variational autoencoder (VAE), which model the latent variables in probability distribution, called latent space.  Latent diffusion model or LDM is a type of diffusion model combined with an encoder-decoder pair (typically a VAE). The idea of using diffusion model with an encoder-decoder pair is because the encoder outputs a probability distribution which we can use as the input for diffusion model.  Instead of forward diffusing a raw image and then do the reverse process, LDM instead takes the input from encoder and model the probability distribution. To actually generate image, we can sample from the output of the diffusion model and use the decoder to decode it back to image. By using latent space as the input, we can reduce the dimensionality of the input for diffusion process, which will save alot of computation resources.  LDM can also be integrated with additional condition such as text, image, or any other meaningful representation. This integration can leverage technique like cross-attention, which is also used in the transformers architecture. First, the encoder encodes the data into the latent space, followed by the forward diffusion process. The conditional input, typically the encoded representation of the original input, is concatenated together with the output of forward diffusion process. Subsequently, the cross-attention mechanism is incorporated to guide the reverse diffusion or denoising process. Once the reverse diffusion is completed, we can sample the output and fed it into the decoder.  ","version":"Next","tagName":"h2"},{"title":"Stable Diffusion​","type":1,"pageTitle":"Diffusion Model","url":"/cs-notes/deep-learning/diffusion-model#stable-diffusion","content":" Stable Diffusion is a deep learning model based on diffusion model, specifically, it's an open source implementation of latent diffusion model (LDM). It is typically used to generate image conditioned on text or image, inpainting, outpainting, super-resolution, and etc.  Architecture​  Stable diffusion is based on Latent Diffusion Model (LDM), which uses variational autoencoder (VAE) as both the encoder and decoder. The diffusion process takes place within the latent space generated by the encoder.   Source : https://towardsdatascience.com/stable-diffusion-best-open-source-version-of-dall-e-2-ebcdf1cb64bc (stable diffusion or LDM architecture)  Image Encoding : An image is provided as the input, which is passed through the encoder to transform it into a representation in the latent space. Transforming it into latent space allows for smaller dimension which can significantly reduce the computational resources. Diffusion Process : Diffusion process which consist of forward diffusion that adds noise gradually and the reverse diffusion process which removes noise gradually. The reverse diffusion process is implemented using a neural network, specifically a denoising U-Net. The U-Net predicts what is the denoised image in the previous time step, given a noised image in the current time step. However, rather than directly predicting the denoised image, the U-Net predicts the noise present in the input image. Subsequently, this predicted noise is subtracted from the noisy input image to obtain the actual denoised image. To ensure a gradual reduction of noise, the predicted noise is multiplied by a fraction (E\\mathcal{E}E) before subtracting it from the input image. This gradual process enhances stability and reliability while also accommodating the injection of conditional information, such as a text prompt. Source : https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166, https://youtu.be/J87hffSMB60?si=_iCGv-rnh_sXt_dL&amp;t=352 (with modification) Conditioning : The model also takes a conditional input such as text, it will be encoded or converted into lower-dimensional representation, this can be done using transformers encoders or CLIP text encoder. The integration with conditional input is incorporated in two ways : The encoded conditioning will be concatenated with the output of forward diffusion which is used for the input of reverse diffusion process.We will also utilize the cross-attention mechanism during the reverse diffusion process in U-Net. Inside the attention mechanism, the encoded conditioning act as the query vector. Source : https://towardsdatascience.com/stable-diffusion-best-open-source-version-of-dall-e-2-ebcdf1cb64bc (with modification) Output &amp; Decoder Generation : Once the reverse process is completed and a refined latent space representation is obtained, it is fed into a decoder that performs an upscaling operation on the image. However, the resulting upscaled image may not have the highest resolution since the input is typically a low-resolution image to reduce computational costs. To address this, another diffusion model specialized for super-resolution tasks can be used. Source : https://youtu.be/J87hffSMB60?si=Y01BDAdH9hNa-dqQ&amp;t=486  After completing the training process, in the actual image generation, we have the option to generate new images by inputting only text, or we can choose to input both text and an image simultaneously, which will modify the input image based on the text we provide.  ","version":"Next","tagName":"h3"},{"title":"Contrastive Language-Image Pre-Training (CLIP)​","type":1,"pageTitle":"Diffusion Model","url":"/cs-notes/deep-learning/diffusion-model#contrastive-language-image-pre-training-clip","content":" CLIP is a model that combines vision and language understanding, it is a model that learns the similarity between image and text. CLIP takes an input image with its corresponding text description, they will be encoded. In other words, the higher-dimensional data will be converted into a lower-dimensional representation. The place where all the encoded input is combined is called the embedding space.  During training, CLIP will learn how to map each image and text into the shared embedding space. The objective is to group the pairs of encoded representation (image and text) together, while pushing the dissimilar pairs apart.  The loss function in CLIP consists of two main components: the image-text similarity loss and the contrastive loss. Both of the loss is calculated in the embedding space, the similarity metrics such as cosine similarity can be used. Similarity loss is maximized to encourage the maximum similarity score between correct pair of image and text. On the other hand, the contrastive loss is minimized to encourage a minimum similarity score between mismatched pairs of image and text.   Source : https://blog.dataiku.com/leveraging-joint-text-image-models-to-search-and-classify-images  The encoding process, involves the use of image and text encoder. We can choose variety of model for text and image encoding, for example, we can use transformers encoder for text encoding and CNN (without its classifier) for image encoding.   Source : https://towardsdatascience.com/simple-implementation-of-openai-clip-model-a-tutorial-ace6ff01d9f2?gi=de7f9822c57a  After training, CLIP can be used for variety of tasks including image classification, image retrieval, text-to-image generation.  For example in an image classification tasks, an image is fed into the encoder, the encoder encodes the image and generates the encoded representation. In the embedding space, the model will find which label is the most similar with the encoded representation of the image. The highest similarity is considered the predicted class label for the image.  note CLIP objective is used in text-to-image models model like DALL-E. ","version":"Next","tagName":"h2"},{"title":"GRU","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/gru","content":"","keywords":"","version":"Next"},{"title":"LSTM vs GRU​","type":1,"pageTitle":"GRU","url":"/cs-notes/deep-learning/gru#lstm-vs-gru","content":" GRU was designed to simplify LSTM architecture to reduce the number of parameters and computations involved. GRU doesn't have cell state, instead the information from previous time step is passed in the hidden state. It also use 2 gate instead of 3 :  Reset Gate : The reset gate is like the forget gate in LSTM, it determines how much information from previous time step, passed from the hidden state is forgotten.Update Gate : Update gate can be thought as the forget gate and input gate combined. It determine how much information from previous time step is passed to next time step which is in the form of hidden state.   Source : https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21  ","version":"Next","tagName":"h3"},{"title":"GRU Process​","type":1,"pageTitle":"GRU","url":"/cs-notes/deep-learning/gru#gru-process","content":" Reset Gate : The current time step input (xtx_txt​) with the previous hidden state (ht−1h_{t - 1}ht−1​) are concatenated, it will be multipled by the reset gate weight (WrW_rWr​), added with reset gate bias term (brb_rbr​), and transformed into the sigmoid activation function. Source : https://youtu.be/mQ5CbaCK_Tg?si=wl4wDGuYneso475E&amp;t=61 Candidate Activation : The result of reset gate will be multiplied with the previous hidden state (ht−1h_{t - 1}ht−1​), the output will be concatenated with the current time step input (xtx_txt​). Similarly, it will be multiplied by the candidate activation weight (WcW_cWc​), added with candidate activation bias term (bcb_cbc​), but transformed into the tanh activation function. Source : https://youtu.be/mQ5CbaCK_Tg?si=Lt5xHS8Y8w6Tjt1H&amp;t=70 Update Gate : Again, the previous hidden state (ht−1h_{t - 1}ht−1​) and the current input time step (xtx_txt​), it will be multiplied with the update gate weight (WzW_zWz​), added with update gate bias term (bzb_zbz​), transformed into the sigmoid activation function. Source : https://youtu.be/mQ5CbaCK_Tg?si=eNrIPhBA171dZl1k&amp;t=85 Hidden State Output : The current time step hidden state is calculated by the following formula. (1−zt)∗ht−1(1 - z_t) * h_{t - 1}(1−zt​)∗ht−1​ : This term uses the complement of update gate to calculate how much information from the previous hidden state ht−1h_{t - 1}ht−1​ should be forgotten.zt∗ht~z_t * \\tilde{h_t}zt​∗ht​~​ : This term represent how much information should be used for the next time step. The candidate activation is similar to input candidate in LSTM. Both of them will be added and the result is the current time step hidden state. Source : https://youtu.be/mQ5CbaCK_Tg?si=3e1OgCWKpCDeyVgW&amp;t=125 (with modification)  Below are the gif animation of GRU process Source : Stacked_HG_CoordConvGRU - ayushgaud ","version":"Next","tagName":"h3"},{"title":"LSTM","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/lstm","content":"","keywords":"","version":"Next"},{"title":"LSTM Architecture & Process​","type":1,"pageTitle":"LSTM","url":"/cs-notes/deep-learning/lstm#lstm-architecture--process","content":"  Source : https://towardsdatascience.com/lstm-recurrent-neural-networks-how-to-teach-a-network-to-remember-the-past-55e54c2ff22e  The 3 gates consist of :  Forget Gate : Determines which information from the previous time step should be discarded or forgotten.Input Gate : Decides which new information should be stored in the memory cell.Output Gate : Controls which information from the memory cell should be exposed to the subsequent layers or the final output.  LSTM also consist of 2 kind of activation function, sigmoid and tanh.  Sigmoid : The sigmoid activation function is used in the gating mechanism, this function squeezes values from 0 to 1.Tanh : Squeezes the value from -1 to 1.   Source : https://medium.com/@toprak.mhmt/activation-functions-for-deep-learning-13d8b9b20e, https://paperswithcode.com/method/tanh-activation  The Process​  Forget Gate : LSTM takes the first input combined with the previous hidden state, if it's the first input, the previous hidden state can be zero. They are combined and multiplied by the forget gate weight (WfW_fWf​), added with the forget gate bias term (bib_ibi​), and then goes into the sigmoid activation function. The sigmoid that squeezes the value helps the forget gate to determine which information should be discarded. The closer it to 0, means we should forget it, the closer it to 1, meaning we should remember it. Source : https://youtu.be/orG90H9E-nI?si=BmMpVi4UI8RPa2gD&amp;t=171 Input Gate : LSTM calculates the input gate value, which decides how much of the new information should be stored in the memory cell. It takes the same input as forget gate which is the current input xtx_txt​ and previous hidden state ht−1h_{t - 1}ht−1​. The same operation will be done, which multiplying them by weight, but now its the input gate weights (WiW_iWi​). It will be added with the input gate bias term (bib_ibi​) and transformed into the sigmoid activation function again. Source : https://youtu.be/orG90H9E-nI?si=6fcGWak25y9tnh4m&amp;t=219 Input Candidate : Again, with the same input which is the current input xtx_txt​ and previous hidden state ht−1h_{t - 1}ht−1​, multiplied by the candidate weight (WcW_cWc​), added with the candidate bias term (bcb_cbc​), but now it goes into the tanh activation function instead. The tanh is used instead of sigmoid to also capture the positive and negative information. Source : https://youtu.be/orG90H9E-nI?si=FcDF1MEPfOE0PLxR&amp;t=231 Cell State : The memory cell or the cell state for current time step is now calculated. It is calculated by multiplying the result of forget gate with the previous cell state value and added with input gate result which is multiplied by the input candidate value. The first term (ft∗Ct−1f_{t} * C_{t - 1}ft​∗Ct−1​) represent how much information from the previous cell state should we forget. The second term (it∗Ct~i_{t} * \\tilde{C_{t}}it​∗Ct​~​) represent how much information should be stored in the current cell state. Source : https://youtu.be/orG90H9E-nI?si=p3hDGT7jSJIhssxC&amp;t=246 Output Gate : The LSTM calculates the output gate value, it will be used to calculate the hidden state. It takes the same as before which is current input xtx_txt​ and previous hidden state ht−1h_{t - 1}ht−1​, multiplied by output gate weight (WoW_oWo​), added by output gate bias term (bob_obo​), and squeezed in the sigmoid activation function. Source : https://youtu.be/orG90H9E-nI?si=wZkEiANJPbV7PAHw&amp;t=261 Output &amp; Hidden State : The current time step hidden state is calculated by multiplying the result of output gate with the current cell state fed into the tanh activation function. The result of hidden state will then be used for the next time step processing. The hidden state is also the output of the current time step. Source : https://youtu.be/orG90H9E-nI?si=SJx5evYA1N3L41B4&amp;t=284  Below are the process of LSTM in gif Source : Introduction to Long Short-Term Memory (LSTM) by Archit Saxena - Medium    After all the process and the model produced the final output, the similar learning process will be done to adjust all the parameter including the weights and bias on each the gate. The model will adjust the weights to improve the gating mechanism, to forget the unrelevant information, store, or sent the relevant information to the next time step. ","version":"Next","tagName":"h3"},{"title":"Neural Network","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/neural-network","content":"","keywords":"","version":"Next"},{"title":"Neural Network Idea​","type":1,"pageTitle":"Neural Network","url":"/cs-notes/deep-learning/neural-network#neural-network-idea","content":" In traditional machine learning, the features of the data need to be extracted manually by the researcher. The process involves extracting relevant features and identifying relationship or patterns. After that, the data with the corresponding feature can be feed to the machine learning model.  Neural Network is a method of machine learning inspired by the human brain to teach computer without needing much human assistance. Neural network is able to learn automatically and capture the relationship and underlying patterns in the data.  A neural network is built upon a mathematical function that takes input data. This function has variables or parameters called weights and biases, which are used with input data to somehow suit with the actual data to make prediction.  In high-level, the network processes the input data by calculating the mathematical function with the given weights and biases. The resulting output is then compared to the actual data. The network measures how wrong the predictions are and aims to minimize this error by adjusting the weights and biases. The adjustment can use algorithm like stochastic gradient descent.  To capture complex relationships and patterns in the data, the network utilizes activation functions. The process of calculating the mathematical function, utilizing activation functions, and adjusting the weights and biases is repeated multiple times during training. With each iteration, the network learns from its mistakes and updates the parameters to improve its predictions.    ","version":"Next","tagName":"h3"},{"title":"Neural Network Architecture​","type":1,"pageTitle":"Neural Network","url":"/cs-notes/deep-learning/neural-network#neural-network-architecture","content":" Perceptron​  The simplest model of neural network is called perceptron, it consists of several input data and weights. The input will be calculated with their corresponding weight, summed up, and applied to an activation function to produce an output.   Source : https://www.javatpoint.com/perceptron-in-machine-learning  Multilayer Perceptron​  Perceptron consists only a single layer, it only processes the input once. Multilayer perceptron (MLP) also known as fully connected feedforward network is similar to perceptron, however, it consists of multiple layers of processing. MLPs passes it through multiple layers with small units called neuron. Each neuron will be connected with each other.  The overall architecture consists of several layers of neuron, including input layer, hidden layer, and output layer. In MLP, information only flows in one direction, from the input layer to the hidden layer and finally to the output layer, without any loops.  Input Layer : The input layer receives the raw input data and passes it to next layer for further processing. The number of neuron in input layer represent the dimensionality of the input data. Hidden Layers : Hidden layers are intermediate layer between input and output layer. The hidden layer is the actual process in neural network, it involves recognizing patterns of the data and using mathematical function as explained before. It also applies activation function to decide if a specific result of a particular neuron matters to the prediction or not. Number of hidden layers and neuron in each layer can vary depending on the complexity of the problem. For example, Output Layer : The output layer produces the predictions based on the computations performed in the preceding layers. The number of neurons in the output layer depends on the nature of the task. For example, a binary classification task may have a single neuron in the output layer, representing yes or no. Source : https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a  tip Neural network is a concept in both machine learning and deep learning, however, in deep learning the network is more advanced. There are multiple hidden layer in deep learning while machine learning uses simpler network architecture.  ","version":"Next","tagName":"h3"},{"title":"Neural Network Learning Process​","type":1,"pageTitle":"Neural Network","url":"/cs-notes/deep-learning/neural-network#neural-network-learning-process","content":" Forward Propagation​  Forward Propagation is where the input data is processed and forwarded into hidden layer and finally to output layer. This involves calculating mathematical function with the input data. The mathematical function used is Σ wixi+b\\Sigma \\space w_ix_i + bΣ wi​xi​+b, where wiw_iwi​ is the weight, the weight for each neuron can be different with each other, xix_ixi​ is input data except for the further layer, the xix_ixi​ is the result from previous layer, and bbb is bias.  Basically the neuron in the front will receive input from all the neuron in the previous layer, it will use the above formula, multiply each input with weight and sum up all the result with additional value from bias.  The result is represented as zzz : z=(w1⋅x1)+(w2⋅x2)+...+(wn⋅xn)+bz = (w_1 \\cdot x_1) + (w_2 \\cdot x_2) + ... + (w_n \\cdot x_n) + bz=(w1​⋅x1​)+(w2​⋅x2​)+...+(wn​⋅xn​)+b  After that, the zzz (called weighted sum) will be applied to activation function resulting in another variable called aaa : a=f(z)a = f(z)a=f(z). The activation function can vary depending on the use case, for example, if we use sigmoid function : a=σ(z)a = \\sigma(z)a=σ(z).   Source : https://www.researchgate.net/figure/The-forward-propagation-of-a-neural-network-a-the-operating-process-of-neural-networks_fig1_355876971  The weight is the coefficient of corresponding input data, it can be interpreted as how important is the neuron. The bias term is used to shift the activation function (similar to how we shift function in algebra), it controls how the function behave filtering a specific neuron.  Backpropagation​  Backpropagation or backward pass is where all the learning process occurs. After getting result from all the preceding layers, the output layers is used to predict. The prediction will then be compared with actual data, the difference will be calculated in some loss function.  Same like traditional machine learning, we want to minimize the loss function, we can use the same principle as using gradient descent to minimize the loss in linear regression, where we calculate the gradient of the loss function with respect to slope and y-intercept. Remember that gradient shows us which direction to go to the minima of the loss function.  Backpropagation process is similar with some difference, it relies on principles of calculus, specifically the chain rule.  The gradient of loss function will be calculated with respect to weight and bias. The weight affects the zzz variable before wx+bwx + bwx+b and the zzz itself affects the aaa variable which is zzz applied to activation function. The aaa variable is the output from the previous layer and used to predict, which means it affect the loss function and it is the actual things we take the gradient of.  This is where the chain rule comes, it allows us to decompose the contribution of weight to the loss function. The same also applies for calculating gradient of loss function with respect bias.  The output layer get its result from the preceding layer, it is affected by the previous layer. The previous layer itself is also based on the previous layer again, this is why its called backward pass, as it will adjust each weight and biases on each neuron of the preceding layers.  After calculating the gradient, now it's time to adjust of weight and bias. The adjustment is similar to the traditional linear regression, but now we adjust for weight and bias instead of slope and y-intercept. We can also use algorithm like stochastic gradient descent.  For example, after predicting, the loss function calculated results in a large value. The loss function is affected by specific neuron on the previous layer, basically that particular neuron &quot;messed up&quot; our prediction. By calculating the gradient of weight and bias and adjusting it based on the optimization algorithm, we can controls the behavior of that particular neuron. We can make the weight smaller to indicate that neuron shouldn't contribute much to our prediction or we can adjust the bias so that the activation function can decide whether to contribute the output of that neuron to the next layers or not.  This learning process will be repeated for many times based on the hyperparameter (e.g. batch, epoch, iteration).   Source : https://www.analyticsvidhya.com/blog/2023/01/gradient-descent-vs-backpropagation-whats-the-difference/  Vanishing Gradient Problem​  Vanishing gradient problem is a phenomenon occurs in backpropagation due to it's nature of calculation.  In backpropagation, gradient of loss function is calculated starting from the output layer, it will then be propagated backward. With the chain rule, the gradient will keep being multiplied with the previous layer and their value might become small and smaller as we keep multiplying them together.  A smaller gradient can cause a slower learning because the updates to the weight and bias are going to be small. One of the way to help mitigate vanishing gradient problem is to use a correct activation function such as ReLU and its variant. ReLU helps preventing the gradient from becoming too small because it doesn't saturate the positive region compared to other activation function like sigmoid that saturate at large and negative values.  ","version":"Next","tagName":"h3"},{"title":"Softmax Activation Function​","type":1,"pageTitle":"Neural Network","url":"/cs-notes/deep-learning/neural-network#softmax-activation-function","content":" Softmax is an activation function typically used in multi-class classification problem. It is a mathematical function used to assign probability to each class, the highest probability is the one our model will predict.  tip A class is a distinct categories we are classifying the input to. For example, we might want to classify if an image is dog or cat. This mean, the image is either dog or cat, therefore dog and cat are the class.  The formula for softmax is :   Source : https://towardsdatascience.com/softmax-activation-function-explained-a7e1bc3ad60  Where :  ziz_izi​ : output for i-th class KKK : the number of class zjz_jzj​ : output from j-th class, will be summed up with all the output  Softmax is typically used as the activation function for the output layer of neural network. Output will be produced after processing the input from all the preceding layer. The result will be calculated using the above formula.  The formula will produce probability for a particular class by dividing the result of that class from preceding layer with sum of all result from all output neuron.  For example, consider a scenario where we have 4 class classification task.   Source : https://youtu.be/8ah-qhvaQqU?si=llIp3EluG-zh3gjX&amp;t=80  After processing through all the layers, each output layer produces an output. The denominator is the sum of constants eee raised to each output value. Therefore, it will be e−1+e0+e3+e5e^{-1} + e^{0} + e^{3} + e^{5}e−1+e0+e3+e5 = 169.87169.87169.87.  To assign probability to each class, we will use that particular class output as the numerator.   Source : https://youtu.be/8ah-qhvaQqU?si=3ogI3IgJnZZXLkZC&amp;t=103  And the highest probability is the one that the model believes to be the most likely prediction for the given input. ","version":"Next","tagName":"h3"},{"title":"Imitation Learning","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/reinforcement-learning/imitation-learning","content":"","keywords":"","version":"Next"},{"title":"Behavioral Cloning​","type":1,"pageTitle":"Imitation Learning","url":"/cs-notes/deep-learning/reinforcement-learning/imitation-learning#behavioral-cloning","content":" Behavioral cloning is a direct imitation learning technique, it is a simple technique where the agent directly imitating the behavior of an expert. Behavioral cloning is a supervised learning technique where the data is the expert behavior in state-action pair.  The agent can use standard supervised learning architecture such as classifier or regressor, the model will aim to minimize the difference between predicted actions and the expert actions using a loss function.  Behavioral cloning can be beneficial if a well-defined expert demonstration is available, however, this assume the demonstration is optimal or near-optimal so that the agent doesn't replicate the expert's mistakes.  ","version":"Next","tagName":"h3"},{"title":"Inverse Reinforcement Learning (IRL)​","type":1,"pageTitle":"Imitation Learning","url":"/cs-notes/deep-learning/reinforcement-learning/imitation-learning#inverse-reinforcement-learning-irl","content":" In behavioral cloning, the agent imitate the expert behavior. In other word, the agent try to learn its state-action map. On the other hand, Inverse Reinforcement Learning (IRL) instead learns the reward function and find the optimal policy that maximizes the reward function.  Same as behavioral cloning, IRL takes state-action pair from the expert, it also assume they are optimal or near optimal behavior. The goal is to infer the reward function that best explains the observed expert behavior. Once the reward function is inferred, the next step is to find the optimal policy. We will then compare the learned policy with the expert's policy.  This process is repeated until the learned policy is close enough to the expert's policy. Overall, IRL is valuable in scenarios where the reward function is hard to define.  An IRL problem can be divided into two, the first case is when we know the environment (model-based), and the other case is when we do not have explicit model of environment (model-free). The model-free case relies on trial and error approach, it can potentially have large state-action spaces. ","version":"Next","tagName":"h3"},{"title":"Monte Carlo Method","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/reinforcement-learning/monte-carlo-method","content":"","keywords":"","version":"Next"},{"title":"Algorithm​","type":1,"pageTitle":"Monte Carlo Method","url":"/cs-notes/deep-learning/reinforcement-learning/monte-carlo-method#algorithm","content":"  Source : https://youtu.be/o8XGKkIA1gE?si=NpJ_6VxZwT06ZHta  Example​  For example, consider a maze problem. In this case, we don't know the information about the environment including its state, action, and rewards. We don't know where we at, we don't know if we should move in definite direction (e.g. left, right, up, or down). The goal of the problem may also be unknown, it is purely driven by rewards.  The agent starts exploring the maze randomly in any direction. During the visits, we also record the states it observed to improve the simulation and the rewards it received. For each state it visited, update the value estimate by averaging the returns obtained after visiting that states. Based on the value estimation, we will also update the policy. This process is done in a single episode and will be repeated for multiple times. ","version":"Next","tagName":"h3"},{"title":"Markov Decision Process","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/reinforcement-learning/markov-decision-process","content":"","keywords":"","version":"Next"},{"title":"Component of MDP​","type":1,"pageTitle":"Markov Decision Process","url":"/cs-notes/deep-learning/reinforcement-learning/markov-decision-process#component-of-mdp","content":" An MDP contains four key component, they are represented in 4-tuple (S,A,Pa,RaS, A, P_a, R_aS,A,Pa​,Ra​):  State space (SSS) : Represent all the possible state in the MDPAction space (AAA) : Represent all the possible action the agent can take. Alternatively, AsA_sAs​ represent all the possible action from state sss.Transition Probabilities (PaP_aPa​) : A function that defines the probability of transitioning from one state to another when a particular action a is taken. The function is defined as: Pa(s,s′)=Pr⁡(st+1=s′∣st=s,at=a){\\displaystyle P_{a}(s,s')=\\Pr(s_{t+1}=s'\\mid s_{t}=s,a_{t}=a)}Pa​(s,s′)=Pr(st+1​=s′∣st​=s,at​=a), probability transitioning from state sss to state s′s's′ is equal to the probability of being in state s′s's′ at the next time step t+1t + 1t+1 given that at current time step ttt, the state is sss and action taken is aaa.Reward Function : Which is a function defined as Ra(s,s′){\\displaystyle R_{a}(s,s')}Ra​(s,s′), it tells the reward or penalty received for transitioning from state sss to s′s's′ when action aaa is chosen.  Last but not least, the policy function (π\\piπ) (potentially probabilistic) which is a rule that tells the agent what action to take at some specific state.  ","version":"Next","tagName":"h3"},{"title":"Objective​","type":1,"pageTitle":"Markov Decision Process","url":"/cs-notes/deep-learning/reinforcement-learning/markov-decision-process#objective","content":" Similar to the main objective of RL, the optimization objective of MDP is to find an optimal policy that maximizes the expected cumulative rewards (return) over time. The return can be represented in value function which is a function that tells us the expected return an agent can obtain from a state under a given policy.  The formula are formulated in Bellman equation :  V(s):=∑s′Pπ(s)(s,s′)(Rπ(s)(s,s′)+γV(s′))V(s) := \\sum\\limits_{s'} P_{\\pi(s)} (s,s') \\left( R_{\\pi(s)} (s,s') + \\gamma V(s') \\right)V(s):=s′∑​Pπ(s)​(s,s′)(Rπ(s)​(s,s′)+γV(s′)) The first formula represent the value function update. According to the formula, when calculating the expected return from state sss, we consider the immediate reward Rπ(s)(s,s′)R_{\\pi(s)}(s, s')Rπ(s)​(s,s′) received while transitioning to state s′s's′ from state sss, as well as the discounted future reward from γV(s′)\\gamma V(s')γV(s′). π(s):=argmax⁡a{∑s′Pa(s,s′)(Ra(s,s′)+γV(s′))}\\pi ( s ) := \\operatorname{argmax}_a \\left\\{ \\sum\\limits_{s'} P_a (s , s') \\left( R_a (s , s') + \\gamma V(s') \\right) \\right\\}π(s):=argmaxa​{s′∑​Pa​(s,s′)(Ra​(s,s′)+γV(s′))} The second formula represent the policy function update. It will select the action aaa that yields the highest return from the value function.  The goal is to find the best value and policy function. In order to achieve this goal, we employ these two formulas to iteratively estimates the value function and policy. The technique to approximate value function is also called value function approximation.  note The two formula above with the four in the Bellman equation demonstrate the same usage of Bellman equation to update the value function and policy iteratively.  Value &amp; Policy Iteration​  Both value and policy iteration are the actual algorithm that uses the formula explained above to solve MDP by estimating the optimal function. It demonstrates dynamic programming or the technique to solve a problem by breaking it down into smaller subproblem.  The algorithm starts with an initial function and proceeds to iteratively compute it until reaching a point of convergence. The value of a state depends on another state, in other word, a problem depends on another problem, this can be referred as subproblem. This is where dynamic programming comes, we can start solving the subproblem first and then build up to the main problem. When we encounter a subproblem that has already been solved subproblem, we can efficiently use the information we have previously acquired.  The iteration involves updating the estimated value and policy based on the current estimate itself, this is known as bootstrapping  note By converge, it means the result stabilizes and does not change abruptly or significantly between iterations. ","version":"Next","tagName":"h3"},{"title":"Markov Models","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/reinforcement-learning/markov-models","content":"","keywords":"","version":"Next"},{"title":"Markov Chain​","type":1,"pageTitle":"Markov Models","url":"/cs-notes/deep-learning/reinforcement-learning/markov-models#markov-chain","content":" Markov chain is the simplest type of Markov model, consisting of a set of states or the condition that describe the system at a specific point of time and the probabilities of transitioning between those states.   Source : https://math.stackexchange.com/questions/2730955/markov-model-to-compute-the-probability-on-the-nth-day  In this example, we are trying to model weather condition in Markov model.  We have 3 states: &quot;nice&quot;, &quot;rain&quot;, &quot;snow&quot;, each of them have their own probabilities of transitioning between the states which may vary. Following the unique charateristics of Markov model, future weather solely depends on current weather condition. This means that the occurrence of &quot;rain&quot; yesterday, for example, does not impact the weather condition for tomorrow.  The probability associated with each arrow represents the chance of transitioning to another state or remaining in the current state. For example, if current condition is &quot;rain&quot;, then it has 0.5 probability for the future state to arrive at &quot;rain&quot; again. It has 0.25 probability and 0.5 probability to transition to &quot;snow&quot; and &quot;nice&quot; respectively.  The formula for markov chain is :   Source : https://youtu.be/i3AkTO9HLXo?si=C9O8_H6l9o3D2ts9&amp;t=129  Conditional probability of the next state, Xn+1X_{n+1}Xn+1​, being a specific value, xxx, given that the current state, XnX_nXn​, is xnx_nxn​. This formula represent the system will be in state xxx at next time step n+1n+1n+1, given that we are currently at state xnx_nxn​ at time step nnn.  Equilibrium State​  Markov chain is a stochastic process, after many transition step, it may reach a state where the probability of each state converge to some value. This is known as equilibrium state, also known as stationary distribution.   Source : https://youtu.be/i3AkTO9HLXo?si=aeY6kx691nIP6x8W&amp;t=292  Adjacency Matrix​  However, that was done using a direct experiment, another way to achieve equilibrium state is by changing the markov chain from a directed graph into an adjacency matrix.   Source : https://youtu.be/i3AkTO9HLXo?si=wFt8ueAFpZ-2aKhf&amp;t=347  A row must add up to 1, reflecting the fact that the system must move to one of the possible states. The left represent current state, the upper represent future state. For example, if we are currently at &quot;pizza&quot;, the probability of the next state to be &quot;burger&quot; is 0.3.  Using adjacency matrix, the probability distribution of the state is denoted using a π\\piπ vector. The image below shows the initial π\\piπ vector, the 1 in the middle denotes that the first state is on &quot;pizza&quot;.   Source : https://youtu.be/i3AkTO9HLXo?si=nej1XVJsMfkMel94&amp;t=405  Multiplying the π\\piπ vector with the adjacency matrix will give us the future state, given that the current state is the current π\\piπ vector.   Source : https://youtu.be/i3AkTO9HLXo?si=k_uGplu0Ayxoc-vL&amp;t=395  To transition further, the result of the multiplication is then multiplied again with the adjancency matrix. We know that if we reach equilibrium state, then the multiplication result will be same or close to the current result. Similar to the convergence of transitioning using random walk.  The multiplication between the adjacency matrix with the π\\piπ vector will result in the π\\piπ vector itself, it can be mathematically written as :   Source : https://youtu.be/i3AkTO9HLXo?si=_wHrzK50S0IrXmiR&amp;t=466  Which is an eigenvector equation, where the eigenvector is the π\\piπ vector and its eigenvalue is 1. We also have another constraint, which is the sum of the π\\piπ vector must add up to 1. So, we have 2 equation, after solving it, we can obtain the equilibrium state.   Source : https://youtu.be/i3AkTO9HLXo?si=vgPy44kBYp1kYLIt&amp;t=482  ","version":"Next","tagName":"h3"},{"title":"Hidden Markov Models​","type":1,"pageTitle":"Markov Models","url":"/cs-notes/deep-learning/reinforcement-learning/markov-models#hidden-markov-models","content":" In a Markov chain, the states are directly observable and known at each time step. The transition from one state to another is determined solely by the current state and follows a probabilistic rule. The transition probabilities between states are explicitly defined in the graph.  Hidden Markov Models (HMMs) also consist of a Markov chain, however, the states are not directly observable. This mean we don't know what is the current state in our Markov chain. HMM also have another component called observable variables, these are variables that we can observe.   Source : https://youtu.be/RWkHJnFj5rY?si=HRrIudfCGLgH3hr9&amp;t=160  An HMM look like the image above, consisting a Markov chain and the extra observable variables, the variables are dependent on the states. Because of the dependencies, these variables help us infer or estimate the hidden states. The current observable variable solely depends on today weather. For example, the state of raining today given that the observable variable is the &quot;sad&quot; mood is 0.9.  The markov chain and the observable variable can be represented as a matrix, it is called transition and emission matrix, respectively.   Source : https://youtu.be/RWkHJnFj5rY?si=uoBYGBw5aKzTSB5m&amp;t=187  Scenario Example​  Suppose a scenario where we know the current hidden state, the first state is &quot;sunny&quot; with the &quot;happy&quot; observable variable, second state is &quot;cloud&quot; with &quot;happy&quot; again, and the third state is &quot;sunny&quot; with &quot;sad&quot;. The probability of each event can be calculated by looking at the previous matrix. The single event probability, such as the P(X1=sunny)P(X_1 = \\text{sunny})P(X1​=sunny) can be calculated by solving the eigenvector equation just like the original markov chain.   Source : https://youtu.be/RWkHJnFj5rY?si=u08WzVyxzEOiciK_&amp;t=308  If we multiply all of the probability, it will result in 0.00391.   Source : https://youtu.be/RWkHJnFj5rY?si=wssnrc_7-UPWsEj0&amp;t=314  Real Scenario​  In the real scenario, we don't know the current hidden state, we only know the observable variable. We could ask a question: &quot;what is the most likely weather sequence for the observed mood sequence&quot;, the answer should be the maximum probability for given mood. With the corresponding transition and emission matrix, we should achieve : &quot;sunny&quot;, &quot;sunny&quot;, and &quot;cloud&quot;.   Source : https://youtu.be/RWkHJnFj5rY?si=hTDdHvP--RoDw6bE&amp;t=391  Mathematically​  Mathematically, the maximum probability or the joint probability, which is the probability of two or more events occurring together or simultaneously can be written as :   Source : https://youtu.be/RWkHJnFj5rY?si=jYuMuZ1vtTy2Ygp4&amp;t=413  Where X is the weather condition and Y is the mood condition, the &quot;arg max&quot; means we are maximazing them.  This equation can be solved using Bayes Theorem :   Source : https://youtu.be/RWkHJnFj5rY?si=Z_It2HuG7ClolvxV&amp;t=452  The P(Y∣X)P(Y|X)P(Y∣X) denotes what is the probability of current mood given some weather state. We know that current mood only depends on current weather state, we can calculate it with product as before, by filling them with the probability inside our transition and emission matrix. The product will then be simplified using the big product symbol. The P(X)P(X)P(X), which is the probability of current weather state, depends on previous weather state, just like the standard Markov chain property. And the P(Y)P(Y)P(Y) can be neglected.  Thus the final equation becomes :   Source : https://youtu.be/RWkHJnFj5rY?si=TgSCI_wcvnsbghl6&amp;t=530 ","version":"Next","tagName":"h3"},{"title":"Policy-Gradient","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/reinforcement-learning/policy-gradient","content":"","keywords":"","version":"Next"},{"title":"Vanilla Policy Gradient (VPG)​","type":1,"pageTitle":"Policy-Gradient","url":"/cs-notes/deep-learning/reinforcement-learning/policy-gradient#vanilla-policy-gradient-vpg","content":" VPG, also known as REINFORCE, is a very simple policy gradient method.   Source : https://spinningup.openai.com/en/latest/algorithms/vpg.html  Collect Trajectory : Trajectory, or the sequence states, actions, and rewards the agent made during the interaction with environment using the current policy. Compute Reward &amp; Advantage Function : Compute the return for each state and action encountered in the trajectory, also compute the advantage function. Compute Policy Gradient : The policy function outputs a probability distribution over actions in a given state. By updating the policy parameters, it means we are updating how will it produce the distribution. The agent take action by sampling from that distribution, we have option to sample it in a stochastic or greedy (select highest reward) manner. The computation of policy gradient involve taking the gradient of the logarithm of the policy's probability distribution with respect to the policy parameters, which is scaled by the advantage function to encourage actions that are better than the expected return and discourage actions that are worse. The result will be summed up for each time step. Update Policy Parameters : The policy parameters will be updated with the gradient descent algorithm. Value Function Update : The value function, which estimates the expected return, can be fitted using regression techniques such as linear regression. This involves minimizing the difference between the predicted values and the observed returns by using the mean squared error loss function and the gradient descent algorithm. ","version":"Next","tagName":"h3"},{"title":"Multi-Agent","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/reinforcement-learning/multi-agent","content":"","keywords":"","version":"Next"},{"title":"Example​","type":1,"pageTitle":"Multi-Agent","url":"/cs-notes/deep-learning/reinforcement-learning/multi-agent#example","content":" Let's consider an example, a tasks where several robot works together to move object scattered around to a desired place. The objective is to move all the object efficiently. The goal is to teach the robot to not collide with each other while efficiently moving objects to their desired locations in the presence of obstacles.  The action is the movement of the agent (e.g. left, right, up, down) and also manipulating object (e.g. dropping or taking an object). The positive reward is awarded for successful object delivery, collision avoidance, and transportation efficiency, while the negative is the opposite of these.  The agent should have proper coordination in order to efficiently move scattered object. A mechanism like sharing task to each agent can be useful, this will enable them to communicate their intentions to handle specific objects. The agents should prioritize handling objects that are closer to their own positions and allow other agents to handle objects that are further away.   ","version":"Next","tagName":"h3"},{"title":"Q-Learning","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/reinforcement-learning/q-learning","content":"","keywords":"","version":"Next"},{"title":"Algorithm​","type":1,"pageTitle":"Q-Learning","url":"/cs-notes/deep-learning/reinforcement-learning/q-learning#algorithm","content":" Q-Learning is another type of TD, the algorithm is similar to SARSA, however, it differs in the update rule.  Initialization : First, the Q-values is initialized to some arbitary or some initial values. Choose action : The agent selects an action to take in the current state based on the Q-values. It can use the epsilon-greedy strategy to balance the exploration and exploitation. Update Rule : Based on the observed transition (state, action, reward, next state), the agent update the Q-value using the following update rule : new Q(s,a)←Q(s,a)+α (r+γ max[Q(s′,a′)]−Q(s,a))\\text{new } Q(s, a) \\leftarrow Q(s, a) + \\alpha \\space (r + \\gamma \\space \\text{max}[Q(s', a')] - Q(s, a))new Q(s,a)←Q(s,a)+α (r+γ max[Q(s′,a′)]−Q(s,a)) The difference between SARSA is the max[Q(s′,a′)]\\text{max}[Q(s', a')]max[Q(s′,a′)] term. It represent the maximum Q-value among all possible actions in the next state s′s's′. In SARSA we instead used the Q(s′,a′)Q(s', a')Q(s′,a′), which is the Q-value of the action the agent takes in the next state by following a policy. So in Q-learning, we do not follow policy to select action nor use it to update its own policy. The choice of next action depends on the estimated next maximum Q-value, in a greedy manner. Repeat : Repeat the step 2 and 3. At the end of the learning process, we then find the optimal policy based on the Q-value we gathered.  ","version":"Next","tagName":"h3"},{"title":"Q-Table​","type":1,"pageTitle":"Q-Learning","url":"/cs-notes/deep-learning/reinforcement-learning/q-learning#q-table","content":" Q-table (also known as the action-value table) is a data structure that stores the estimated Q-values for each state-action pair in Q-learning. Initially, the Q-table is usually initialized with arbitrary values or set to zeros. Then, as the agent interacts with the environment and learns from the observed rewards, we will use Bellman equation to update the Q-values in the Q-table.   Source : https://www.researchgate.net/figure/The-model-of-Q-learning-and-the-structure-of-Q-table_fig1_339665871  ","version":"Next","tagName":"h3"},{"title":"Q-Network​","type":1,"pageTitle":"Q-Learning","url":"/cs-notes/deep-learning/reinforcement-learning/q-learning#q-network","content":" A Q-network, also known as a Q-function approximator or Q-value function approximator, is a type of neural network used in reinforcement learning to approximate the Q-values for state-action pairs.  Neural network is pretty good at approximating complex and non-linear function. The Q-values in reinforcement learning problems can be highly complex and depend on intricate relationships between states and actions. Using neural network will also help us generalize on unseen states.  The network can be implemented using a simple fully connected network or a convolutional neural network (CNN) if related to image problem.  The network will take a state, it will be passed through the network and will produce a set of Q-values of all possible action over a given state. We will then choose the maximum Q-values and continue the agent exploration.  The parameter of the network will be updated to minimize the loss. The loss is calculated by comparing the predicted Q-values and the target Q-values. The target Q-values are computed using the Bellman equation, which takes current Q-values and reward the agent received.   Source : https://wikidocs.net/174548   Source : https://www.baeldung.com/cs/q-learning-vs-deep-q-learning-vs-deep-q-network  Target Network​  In the previous approach, we updated the Q-network parameter based on the target value or the current estimate of Q-values. This can be unstable, because they are constantly changing. Q-values is estimated using Bellman equation, which takes current Q-values, however, the Q-values itself depend on the estimated Q-values.  The approach is to use two distinct network, the other network is called target network. The target network is a separate copy of the Q-network that is periodically updated to match the current Q-network's weights.  The tight coupling can be reduced by introducing separate target network. The use of target network is to improve the stability and convergence of the learning process. We will then update the Q-network based on the target network, not directly based on the computed target values.   Source : https://arshren.medium.com/deep-q-learning-a-deep-reinforcement-learning-algorithm-f1366cf1b53d ","version":"Next","tagName":"h3"},{"title":"Temporal Difference","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/reinforcement-learning/temporal-difference","content":"","keywords":"","version":"Next"},{"title":"TD Error​","type":1,"pageTitle":"Temporal Difference","url":"/cs-notes/deep-learning/reinforcement-learning/temporal-difference#td-error","content":" The key idea of TD is by predicting the value and see how wrong is the prediction and use that knowledge to make better prediction. In essence, it is similar to the traditional machine learning algorithm: linear regression.  The how wrong our prediction is called TD error, it is the difference between our current prediction or estimate for the value with the value we are getting in the current state. The value we are getting in the current state also depends on the value of the next state (similar to MDP).  Here is the formula for TD error :   Source : https://www.slideserve.com/menefer/reinforcement-learning-part-2  rtr_trt​ : The immediate reward received after taking an action in the current state. γ\\gammaγ : Discount factor. α\\alphaα : Learning rate.  After calculating the error, we will then update the value with the formula on the right. The formula says that the new value will be the current value plus the error multiplied by some constant called learning rate, basically it controls how big do we want to update the value. Value can increase or decrease depending on the TD error, which can be positive or negative. The update process is very similar to gradient descent. ","version":"Next","tagName":"h3"},{"title":"SARSA","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/reinforcement-learning/sarsa","content":"","keywords":"","version":"Next"},{"title":"Algorithm​","type":1,"pageTitle":"SARSA","url":"/cs-notes/deep-learning/reinforcement-learning/sarsa#algorithm","content":" TD is a general framework to estimate value based on the observed rewards and the estimated values of future states. SARSA is the specific algorithm used to update Q-value, therefore the algorithm is very similar to TD.  Initialization : Initialize the Q-values for all state-action pairs arbitrarily or to some default values. Choose an action : Based on the current state, we have option to explore new things to potentially find better policy that could lead to higher reward, or we can prioritize the maximum cumulative reward by being &quot;greedy&quot; and doing action that yields the highest immediate rewards according to our current policy. Take action : Perform selected action, transition to new state, and agent receives a reward based on the action taken. Q-value Update : Update the Q-value for the current state-action pair using the SARSA update rule: Qnew(st,at)←Q(st,at)+α [rt+γ Q(st+1,at+1′)−Q(st,at)]Q^{new}(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\space [r_t + \\gamma \\space Q(s_{t+1}, a_{t+1}') - Q(s_t, a_t)]Qnew(st​,at​)←Q(st​,at​)+α [rt​+γ Q(st+1​,at+1′​)−Q(st​,at​)] Where s′s's′ and a′a'a′ means the next state and the next action, respectively. The new Q-value is updated based on the current Q-value estimate, the observed reward, the estimated future Q-value, and the learning rate as the weight to control the update influence. Repeat : We start choosing and taking action again, transition to new state, and then update the Q-value again. ","version":"Next","tagName":"h3"},{"title":"ResNet","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/resnet","content":"","keywords":"","version":"Next"},{"title":"The Problem​","type":1,"pageTitle":"ResNet","url":"/cs-notes/deep-learning/resnet#the-problem","content":" The how wrong our model's predictions is calculated using a loss function. As usual, we want to minimize the loss function. Minimizing the loss function involves calculating the gradient of the loss function with respect to the parameters (such as weights) used in the prediction.  Once the gradient is calculated, the network adjusts all of its parameters, including the preceding layers. The gradient of the preceding layer is calculated based on the subsequent layer using the chain rule aspect of backpropagation. In very deep networks, the gradient becomes smaller and smaller as we go into the first layer. When the gradient becomes small, it can slow down the learning process by only updating the parameters by a small amount.   Source : https://botpenguin.com/glossary/vanishing-gradient-problem  ","version":"Next","tagName":"h3"},{"title":"Residual Connection​","type":1,"pageTitle":"ResNet","url":"/cs-notes/deep-learning/resnet#residual-connection","content":" The idea of ResNet is to skip some layer in the network, this will prevent the gradient from vanishing. ResNet introduces residual connection, also known as skip connections. These connections bypass one or more layers and directly connect the output of one layer to the input of a later layer.  ","version":"Next","tagName":"h3"},{"title":"Identity Function​","type":1,"pageTitle":"ResNet","url":"/cs-notes/deep-learning/resnet#identity-function","content":" When we skipped a layer, for example, the output from layer 1 is sent directly to the input of layer 3. This makes us skipped some of the information that should be brought by the layer 2 before going to the layer 3. Skip connection uses comething called identity function.  Identity function is used to capture and bring that lost information from layer 2 to the output of layer 1. It can be thought as a bridge that connect the previous layer to the next layer by combining the information lost from the skipped connections.   Source : https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec  In the image above, the output of previous layer is considered as xxx, it then bypass the next layer and goes into the + symbol directly. If we consider the skipped layer as a function f(x)f(x)f(x), which means it transform the previous layer's output xxx to a new output. The input of the next layer (+ symbol) is produced from adding the first layer with the skipped layer. Basically it combine the output from the bypassing route (identity path) with the route without bypassing (residual path).  When we skip some of the layer in ResNet, the output layer may receive raw output or less transformed input from the early layers that have skipped many intermediate layers. Typically, when calculating gradients, we consider all subsequent layers in the network, making it result in small gradients. This is when ResNet prevent vanishing gradient, by skipping certain layers, the gradient can become larger because we don't calculate them with respect to all subsequent layers.  For example, with the same illustration as the above image, normally the first layer's gradient is calculated with respect to function fff which is the next layer, and then it is calculated with respect to xxx itself (chain rule). This what makes vanishing gradient occurs in deeper networks, the gradient will become smaller and small as we multiply them in chain rule.  However, after skipping the second layer and sending its input directly to the input of third layer, we can calculate the gradient with respect to xxx directly, making the resulting gradient larger. We also add the result as if it passes the skipped layer, so that the skipped layer can still adjust its parameters.  Residual Block​  Residual block is the building block of ResNet, it combines each individual layer that includes convolution layer, pooling layer, activation function, and any other layer with the skip connection.  This image below compare the normal network architecture with the ResNet-34 architecture, which is the variant of ResNet that consist of 34 layers.   Source : https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8 ","version":"Next","tagName":"h3"},{"title":"RNN","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/rnn","content":"","keywords":"","version":"Next"},{"title":"Limitation of Neural Network​","type":1,"pageTitle":"RNN","url":"/cs-notes/deep-learning/rnn#limitation-of-neural-network","content":" Traditional neural network is not capable of processing sequential data :  Lack of Memory : Neural networks takes an input data, process it, and output a result, they do not have memory or the ability to remember information from previous steps. We can't use it to remember words that we have processed before, each input is treated independently. Fixed Input Size : They expect a fixed-size inputs. Processing sequences of words with various lengths becomes challenging. Inability to Capture Long-Term Dependencies : This is related to the vanishing gradient problem, when processing a sequence, each input is treated independently without considering the relationship with previous inputs. As the sequence length increases, the influence of earlier inputs on the current prediction diminishes as the network keep learning new information and forget the older one. Parameters : Indepenent input causes another issue, because they are independent, it means each word will have different parameters. As the network receive many input, the network will also need many parameter, making it computationally expensive.  In natural langauge, there are many word that may have different meaning depending on the context. For example, the word &quot;bank&quot; have three different meaning, place where people deposit and withdraw money, side of a river, or an action of bouncing a ball off the backboard in basketball.  This makes the word &quot;bank&quot; has different parameter depending on another input.  ","version":"Next","tagName":"h3"},{"title":"RNN Architecture​","type":1,"pageTitle":"RNN","url":"/cs-notes/deep-learning/rnn#rnn-architecture","content":" RNN introduces the concept of recurrent connection, which enable the network to maintain memory or context of previous inputs. This is done by combining the output from previous steps with the new input from current steps. This process will be repeated for each input in the sequence. The output from previous steps can be thought as the context for current steps, this way we can process the input dependently.  RNN Process​  The concept of memory or output from previous step is stored in something called hidden state. It represents the network's memory or internal representation of the information it has encountered in previous time steps.  Input to hidden state : First, RNN takes an input data. The input will be multiplied by an Input-to-Hidden weight (WxhW_{xh}Wxh​). Source : https://youtu.be/2GgGu6kMSqE?si=qTrSLRydaD2AMiTS&amp;t=58 Combined with previous hidden state : As explained before, we will combine the result of current state with the previous state. The previous hidden state will be multiplied by another weight called Hidden-to-Hidden weight (WhW_hWh​). If we are at the starting point, the previous hidden state which is h0h_0h0​ can be 0. Source : https://youtu.be/2GgGu6kMSqE?si=qTrSLRydaD2AMiTS&amp;t=58 Bias term &amp; activation function : The result from step 1 and 2 along with a bias term (hidden bias [bhb_hbh​]) will be added together and goes into some activation function (e.g. tanh). The result of it define the current hidden state (h1h_1h1​). Source : https://youtu.be/2GgGu6kMSqE?si=qTrSLRydaD2AMiTS&amp;t=58 Result of a time step : A single time step represent the single processing of an input with its hidden state. The result of a time step is produced by multiplying the current hidden state (h1h_1h1​) with the Hidden-to-Output weight (WhyW_{hy}Why​), added with another bias term (output bias [byb_yby​]), and transformed into another activation function. Source : https://youtu.be/2GgGu6kMSqE?si=qTrSLRydaD2AMiTS&amp;t=58  This will be repeated together for each input in the sequence, the architecture can also be simplified with a loop. As each time step produces output, which output to use depend on the task. For example, in a text interpretation task, the output used may be the last output considering it contains the most information from all the sequence.  It doesn't have to be the last output, another mechanism called attention captures the only important or relevant information, which isn't always the last.   Source : https://youtu.be/2GgGu6kMSqE?si=XuJlH_-vVmjZQeup&amp;t=164  The learning process of RNN is similar to traditional neural network. It differs in the backpropagation process. First, it calculates the loss at the last output after the prediction. The gradient of loss function will be calculated with respect to RNN parameters that takes account each input and their hidden state. This process is called Backpropagation Through Time (BPTT) since it involves propagating the gradients backward through the entire sequence of time steps.   Source : https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/  ","version":"Next","tagName":"h3"},{"title":"Vanishing Gradient​","type":1,"pageTitle":"RNN","url":"/cs-notes/deep-learning/rnn#vanishing-gradient","content":" Same as the traditional neural network, RNN still run into the vanishing gradient problem. The gradients used to update the RNN's parameters during backpropagation diminish exponentially as they propagate from the output layer to the earlier layers. This makes learning slower and also making it difficult to learn long-term dependencies.   Source : https://youtu.be/LHXXI4-IEns?si=BnLAKYkQ54MRrH_l&amp;t=505  ","version":"Next","tagName":"h3"},{"title":"Type of RNN​","type":1,"pageTitle":"RNN","url":"/cs-notes/deep-learning/rnn#type-of-rnn","content":" There are type of RNN architecture based on how many input and output are involved.  One to One : Takes one input and produce one output at a single time stepOne to Many : Only takes a single input but still produce many outputMany to One : Takes many input but only produce single output at the last time stepMany to Many : The number of input is same as the number of output   Source : https://iq.opengenus.org/types-of-rnn/  ","version":"Next","tagName":"h3"},{"title":"RNN Encoder-Decoder​","type":1,"pageTitle":"RNN","url":"/cs-notes/deep-learning/rnn#rnn-encoder-decoder","content":" A standard RNN (the one explained above) is suited for tasks that involve taking an input and outputting a specific element. The example of the tasks are sequence classification or sentiment analysis where we predict or classify the input into some label, such as classifying whether a review is positive or negative or predicting a single word that will come up next.  RNN Encoder-Decoder is an architecture designed for sequence-to-sequence tasks, where the input and output sequences have different lengths or meanings. Examples include machine translation, text summarization, or chatbot systems. Using standard RNN for task like translation wouldn't always work, as human language can't be translated word by word.  The architecture consist of an encoder and a decoder. The encoder is the one that takes input and responsible for summarizing the input information. The decoder takes the summarized input and generates the output sequence, this is done step by step just like standard RNN, where current step depends on previous output.  In high level, this is the process of RNN encoder-decoder architecture :  Encoder takes input : As explained before, encoder is the one that summarizes or captures the input. The encoder can be a standard RNN model, the output of it or the final hidden state can be thought as the summarized information from the input sequence. The output of encoder is often called context vector. The formula for hidden state inside is : Source : https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346 Hidden state is calculated by considering the previous hidden state, which is multiplied by the hidden-to-hidden weight (W(hh)W^{(hh)}W(hh)), added with the current input multiplied by hidden-to-input weight (W(hx)W^{(hx)}W(hx)), also fed into activation function. Decoder initialization : The decoder can also be a standard RNN model, the context vector serve as the initial hidden state. The decoder however, doesn't take additional input, it simply calculates current hidden state from the previous hidden state. In the formula below, the second term from the encoder's formula is removed. Source : https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346 Each decoder connection produces output at time step ttt, it will stop producing an output or final output will be generated when a token called end-of-sequence (EOS) is produced that marks as the completion of the output sequence. Source : https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346 The output multiplies hidden state at time ttt with some weight and it goes into the softmax activation function.  Overall, RNN encoder-decoder is basically just two RNN working together, with one component serve as the one that captures information and the other component serve as the one that keep producing output until the desired sequence is produced.   Source : https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346  note By standard RNN model, it can also be other type of RNN like LSTM and GRU. ","version":"Next","tagName":"h3"},{"title":"Siamese Network","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/siamese-network","content":"","keywords":"","version":"Next"},{"title":"Basic Idea​","type":1,"pageTitle":"Siamese Network","url":"/cs-notes/deep-learning/siamese-network#basic-idea","content":" The basic idea of how siamese network works is there will be two identical network, called twins or branches which has the shared weights and architecture. The input data will go into both twins, the network will process it and give the input some score based on its features. If the score are close, it means they are similar, and vice versa.  The pair of input will be provided along with the label (whether they are similar or not), the network will adjust its parameters to make sure similar input will also have similar score.   Source : https://pyimagesearch.com/2020/11/30/siamese-networks-with-keras-tensorflow-and-deep-learning/  ","version":"Next","tagName":"h3"},{"title":"Architecture​","type":1,"pageTitle":"Siamese Network","url":"/cs-notes/deep-learning/siamese-network#architecture","content":" Siamese networks share the same architecture, if the task is to recognize similar image, then they will have the identical convolutional networks. The convolution network will flatten its data at the last layer before it goes into the fully-connected layer, same as the original CNN. However, the classifier in the fully-connected layer will not be included. It will go into the next layer called embedding layer.  Embedding Layer​  This is where we give each input its corresponding score, it works by mapping the input data with many features into a lower-dimensional space so that the similarity can be easily measured.  The input which comes from the previous layer will be represented in vectors. They will be transformed into vector by applying a normalization operation, which is an operation to transform data into some scale. The normalization is called L2 normalization or Euclidean normalization.  This process is also called image encoding, where we transform input into a compact representation or embedding that captures its charateristics. The embedding process is similar to the embedding in NLP.   Source : https://towardsdatascience.com/illustrated-guide-to-siamese-network-3939da1b0c9d  Calculating Distance​  After they are represented in a lower-dimensional space, the inputs are compared in the space using technique like Euclidean distance or cosine similarity. The result distance is the measure of how similar are the input, the model will predict by categorizing the input as similar or not similar based on some threshold.  After the prediction is done, the learning process will be the same as traditional network which includes loss calculation, backpropagation, and parameters update. The goal is to make our embedding better, we should map similar samples to have small distances between their embeddings and dissimilar samples to have large distances.   Source : https://www.researchgate.net/figure/Improving-the-similarity-distance-in-an-embedding-space_fig1_346902941  ","version":"Next","tagName":"h3"},{"title":"Triplet-based Siamese Network​","type":1,"pageTitle":"Siamese Network","url":"/cs-notes/deep-learning/siamese-network#triplet-based-siamese-network","content":" There is a variant of siamese network called triplet-based Siamese network, which uses 3 types of sample in the training process.  Anchor : Specific sample that serves as the reference point for comparison.Positive : Positive sample is the input which is a similar to the anchor.Negative : Negative sample is the input which is not similar to the anchor.  Compared to standard siamese network, the triplet-based approach provides a more fine-grained learning signal. It enables the network to learn to rank or order examples based on their similarity to the anchor, rather than just predicting similar or not. By using the anchor as a reference, the network can learn to focus on the specific characteristics that distinguish positive examples from negative examples.  In this context, a good embedding mean that we successfully map the positive sample and the anchor sample close while the negative is distinguished from these two.   Source : https://www.v7labs.com/blog/triplet-loss  Triplet Loss​  The loss function used for triplet-based siamese network is called the triplet loss. It involve calculating the distance of positive and negative sample with the anchor point. Triplet loss measure how correct our embedding is, we will aim to minimize the loss. We will minimize the distance between positive and anchor while maximizing the distance between negative and the anchor.  The formula for triplet loss is :  L=max(d(A,P)−d(A,N)+margin,0)L = \\text{max}(d(A, P) - d(A, N) + \\text{margin}, 0)L=max(d(A,P)−d(A,N)+margin,0)  Where :  d(A,P)d(A, P)d(A,P) : Distance between anchor and positive example in the embedding space. d(A,N)d(A, N)d(A,N) : Distance between anchor and negative example in the embedding space. margin : Hyperparameter that defines the minimum desired difference between the distances of the anchor-positive pair and the anchor-negative pair.   Source : https://www.researchgate.net/figure/Distances-among-anchor-positive-and-negative-samples-during-training-using-Siamese_fig2_358019630  Triplet Mining​  Triplet mining is the strategy used in triplet-based networks to select which dataset should we choose as the anchor, positive, or negative sample. The goal is to find the dataset that contribute the most to the learning process.  It can be categorized into two based on when the triplet is generated :  Online Mining : Dynamically generates triplets sample during training process, we can adapt the samples to suit the model performance.Offline Mining : Pre-selected before the training process, it will be more efficient.  Three category based on how hard is the train triplet :  Hard Triplet Mining : Negative sample is closer to anchor than the positive.Semi-Hard Triplet Mining : Negative is close to the anchor, but not a zero distance.Easy Triplet Mining : Positive sample is close to anchor while negative is far.Random Triplet Mining : Randomly selected without considering their distance. ","version":"Next","tagName":"h3"},{"title":"BART","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/transformers/bart","content":"","keywords":"","version":"Next"},{"title":"Denoising Auto-Encoding​","type":1,"pageTitle":"BART","url":"/cs-notes/deep-learning/transformers/bart#denoising-auto-encoding","content":" During the BART pre-training, an objective called denoising auto-encoding is used. BART is trained to reconstruct the original text from the corrupted versions of the same text. The corrupted version involve randomly masking or replacing the world in the input text, similar to the mask language modeling in BART. Then it uses the decoder to generate the original text based on surrounding words, just like GPT.  ","version":"Next","tagName":"h3"},{"title":"Architecture​","type":1,"pageTitle":"BART","url":"/cs-notes/deep-learning/transformers/bart#architecture","content":" BART modifies architecture the architecture of BERT and GPT and incorporates some additional techniques. BART uses 12 stack for each encoder and decoder, each of the decoder takes the encoded representations from the encoder. BERT originally uses feed-forward network for its classification output, it is instead passed into the decoder.   Source : https://paperswithcode.com/method/bart  Input : Input processing includes tokenization, embedding, position encoding, and also some data augmentation : Token Masking : The same technique used in BERTToken Deletion : Deletes a random tokenText Infilling : Randomly removing tokens from the input sequence and replacing them with a special &quot;mask&quot; token. This will teach the model to handle missing or incomplete inputSentence Permutation : Input sentences is shuffled randomlyDocument Rotation : Rotating the order of documents to help the model learn to identify the start of the document Source : https://arxiv.org/abs/1910.13461 Encoder Layer : Embedded tokens are passed into the stack of transformers encoder layers. Each with its own multi-head self-attention and other component. Decoder Layer : Decoder takes input from the output of encoder in the cross-attention mechanism and also its own previously generated output. Output : The output is the most likely next word at each step, based on its learned knowledge and the context.  After the pre-training process, BART will be fine tuned and have different objective depending on its tasks. The labeled dataset is used to train BART on the task-specific objective. In tasks like text classification or summarization, the objective may involve minimizing the cross-entropy loss between the model's predicted outputs and the ground truth labels. ","version":"Next","tagName":"h3"},{"title":"Attention Mechanism","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/transformers/attention-mechanism","content":"","keywords":"","version":"Next"},{"title":"Attention Mechanism​","type":1,"pageTitle":"Attention Mechanism","url":"/cs-notes/deep-learning/transformers/attention-mechanism#attention-mechanism","content":" Attention mechanism is a mechanism introduced to help address sequence model limitation. This mechanism works by capturing only the relevant parts of the input sequence. It assign weights or scores to different element of input to indicate their importance or relevance to the current step of processing.  LSTM and GRU maintain memory across time step by carrying the information through hidden states, attention mechanism instead selectively focus on different parts of the input sequence. It also assign importance regardless of the input position in the sequence. So, element that are far away from the current step can still be considered important as they may have strong relationship.  Another technique is the positional encoding, which capture the input position in the sequence. This is useful for a task where it require precise positional information.  Because attention mechanism focuses on specific part of the input, meaning it doesn't depend on the previous step. This allows for parallel computation while calcaluting the attention scores for all elements in the sequence, leading to a faster training.   Source : https://blog.floydhub.com/attention-mechanism/  ","version":"Next","tagName":"h3"},{"title":"RNN with Attention​","type":1,"pageTitle":"Attention Mechanism","url":"/cs-notes/deep-learning/transformers/attention-mechanism#rnn-with-attention","content":" The standard RNN can be equipped with attention mechanism. RNN with attention mechanism typically extend the RNN encoder-decoder architecture, which is the more advanced architecture of RNN used for sequence-to-sequence tasks.  Standard RNN encoder-decoder​   Source : https://medium.datadriveninvestor.com/attention-in-rnns-321fbcd64f05  Image above shows the standard RNN encoder-decoder architecture, the ctc_tct​: (c1c_1c1​, c2c_2c2​, c3c_3c3​) represent the encoder's hidden states, the ccc is the summarized information from all encoder. The sts_tst​: (s1s_1s1​, s2s_2s2​, s3s_3s3​) is the state of each decoder. Each of it also produces output of yty_tyt​: (y1y_1y1​, y2y_2y2​, y3y_3y3​, y4y_4y4​).  The limitation of this architecture falls in the summarized information ccc, the entire input is compressed into a single fixed-length context vector. It limits the model capacity, especially when there is a degrees of relevance to different parts of the output sequence.  Attention​  Each encoder's hidden state will be connected to an attention layer, which is the layer that stores all the information from all encoder. The result of attention layer which will distribute the information to the decoder later is called attention vector.   Source : https://youtu.be/y7YWo6XaVHc?si=RM1mtj9tFWSU3vGb&amp;t=122 (The 4 box represent the desired attention vector length, this depends on tasks or input)  Each encoder process an input and have its own hidden state. All the hidden state value from encoder will be passed to the attention layer together. Now it's time to determine which information is relevant enough to be passed into the decoder.  The attention vector is determined using probability, hidden state value from encoder is passed to attention layer and it goes into several fully connected layer (where they have shared weights) with tanh activation function on it and all the output will be transformed into probability with the softmax activation function. The probability output with the one being the highest represent the most important information.   Source : https://youtu.be/y7YWo6XaVHc?si=-lrYn-yFFkqoiCj7&amp;t=258  Actually, hidden state passed into the fully connected layer is not only from the encoder, the ht−1h_{t - 1}ht−1​ represent the previous hidden state of decoder. Decoder, like the standard RNN encoder-decoder architecture, will have their own hidden state. We will use their hidden state in the attention layer combined together with all encoder's hidden state in a distributed manner. This is done to ensure proper processing if current output depends on the previous output.  Because we used ttt variable in the formula, it means the attention layer will be recalculated in each time step. This makes attention mechanism dynamically or adaptively focuses on different parts of the input sequence at each decoding step. This allows the model to utilize the current most relevant information or context at each output step.  Each probability output will be multiplied with their corresponding hidden state and summed up, the result is the final vector for attention layer, which will be passed into the decoder.   Source : https://youtu.be/y7YWo6XaVHc?si=bFftYfpJpQ9nS2uZ&amp;t=274  The attention layer will distribute the information to each decoder, each of it will produce an output. The process will be repeated until the end of the processing.   Source : https://youtu.be/y7YWo6XaVHc?si=ThCduvO7eaqGYc2L&amp;t=159 (with modification)  In conclusion, what will be produced in the current output step depends on the previous output and all the input we've gathered, all of it is summarized into some vector of information. We use the most relevant information for current step. For example, in text translation, when we encounter a subject word in the first step, it will make sense to put predicate in the next step rather than putting another subject.  In fact, the model doesn't recognize a word is subject or predicate, as every input sequence will be transformed into number. The model will adjust its parameters to remember it based on number.   Source : https://medium.datadriveninvestor.com/attention-in-rnns-321fbcd64f05 (with modification) ","version":"Next","tagName":"h3"},{"title":"BERT","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/transformers/bert","content":"","keywords":"","version":"Next"},{"title":"Masked Language Modeling (MLM)​","type":1,"pageTitle":"BERT","url":"/cs-notes/deep-learning/transformers/bert#masked-language-modeling-mlm","content":" MLM is the key aspect of BERT, it is a pretraining objective used in BERT that randomly mask or hide some of the input and then train the model to predict what actually is hidden based on the context provided by the other input.  The image below show an illustration of masking, the input is &quot;how are &lt;mask&gt; doing today&quot;. The &lt;mask&gt; indicate the portion the model should predict. The output of the model is a token that has the highest probability to fit into the input sentence.  note [CLS] stands for classification, indicates the beginning of a sentence used for classification task [SEP] stands for separator, to help model understand which token belong to which sentence   Source : https://www.sbert.net/examples/unsupervised_learning/MLM/README.html  ","version":"Next","tagName":"h3"},{"title":"Architecture​","type":1,"pageTitle":"BERT","url":"/cs-notes/deep-learning/transformers/bert#architecture","content":"  Source : https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270  Input : The input is the masked sentence, they are tokenized or converted into numerical representation (tokenization) and will be converted into vector (token embedding). Positional embedding that captures the position of each token is also done. Transformers Encoder : Next, they goes into transformers encoder to capture the dependencies and relationship as well as the importance of each token to other token (attention mechanism). The BERT architecture consist of multiple transformers encoder that works together. Output : The output goes into classification layer that consist of fully connected layer, GELU activation function, and a normalization layer. The result of that layer will be embedded back to actual vocabulary. The actual output is the vocab that has highest probability produced by the softmax activation function.  The output processing may vary depending on the task, we may need other kind of output processing for other task beyond MLM. For example, in sentiment analysis, we might want the output to be a label. The model may also be fine tuned to be adapted into a more specific task. ","version":"Next","tagName":"h3"},{"title":"Reinforcement Learning Fundamental","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/reinforcement-learning/reinforcement-learning-fundamental","content":"","keywords":"","version":"Next"},{"title":"Terminology​","type":1,"pageTitle":"Reinforcement Learning Fundamental","url":"/cs-notes/deep-learning/reinforcement-learning/reinforcement-learning-fundamental#terminology","content":" Agent​  Agent is the machine learning model or the entity that make decision and interacts with environment. The agent's actions are influenced by its current state, which represents the information it has about the environment at a given time.  Environment​  Environment is the world or system in which RL agent operates. It can be a real-world or a simulated environment. The environment should have well-defined state, action, and reward to ensure the learning efficiency of the agent.  An environment can also be stochastic, which means it doesn't entirely depend on the current state and action, it involves element of randomness or uncertainty.  State​  State, denoted as sss is the condition of the environment at a particular time. It contains all the information about the environment such as the agent's location. While a state represent complete condition, an observation is a partial representation of that state, it can be a sensor measurement in a robotics problem.  A state can be discrete or continuous. A discrete or categorical state can be the coordinates of the agent's position in the grid of maze. A continuous state can be found in a robotic problem such as joint angles, end-effector position, and velocities.  Action​  Action, denoted as aaa is the choice or decision made by agent to interact with the environment. The action done by agent will influence the state of the environment.  A set of all valid action in a given environment is called action space. Similar to state, action can also be discrete or continuous. For example, we can move discretely in a grid-based task like maze or move continuously in a self-driving car task, where the action space could be the steering angle of the car.  Policy​  A policy is a strategy or instructions that guide an RL agent to make decisions. It determines the agent's behavior by specifying which actions it should take in different situations or states. Policy is defined as a function that takes a state and returns an action.  A policy can be deterministic or stochastic :  Deterministic : A deterministic policy maps a state directly to specific actions, it is represented as a=μ(s)a = \\mu(s)a=μ(s). This mean, according to the policy if we are at state sss, we need to take action aaa.Stochastic : A stochastic policy gives different action in a probabilistic manner based on the given state, it is represented as a∼π(⋅∣s)a \\sim \\pi(\\cdot | s)a∼π(⋅∣s). The a∼a \\sima∼ means we are randomly selection an action aaa from a probability distribution given by the policy function π(⋅∣s)\\pi(\\cdot | s)π(⋅∣s), conditioned on state sss. The dot here is a placeholder for an action variable.  Policy act as a brain for the agent, it will keep being updated to adjust with the environment so that it can guide the agent toward best result.  Here is an example of a stochastic policy for a specific state :  π(up∣s)=0.25\\pi(\\text{up} | s) = 0.25π(up∣s)=0.25π(down∣s)=0.25\\pi(\\text{down} | s) = 0.25π(down∣s)=0.25π(left∣s)=0.25\\pi(\\text{left} | s) = 0.25π(left∣s)=0.25π(right∣s)=0.25\\pi(\\text{right} | s) = 0.25π(right∣s)=0.25  The policy act as the rules for agent to select an action. The action of the agent will be randomly sampled from these policy. So, when the agent is at state sss, it may sample or select one of these four.  In the context of deep RL algorithm, where we utilize parameters of neural network (e.g. weight and bias), we can set the policy as a learnable parameter. The parameters are often denoted as θ\\thetaθ or ϕ\\phiϕ and then it will be written on the policy's subscript (e.g. a=μθ(s)a = \\mu_{\\theta}(s)a=μθ​(s)).  Trajectory​  A trajectory is a sequence of state and action that an agent experiences while interacting with an environment. It is denoted as τ=(s0,a0,s1,a1,...)\\tau = (s_0, a_0, s_1, a_1, ...)τ=(s0​,a0​,s1​,a1​,...), where sss and aaa are state and action, respectively. The very first state s0s_0s0​ is randomly sampled from p0p_0p0​, which is the initial state distribution  The movement from one state to another is called a state transition. A state transition is described by a transition function, together they are denoted as st+1=f(st,at)s_{t+1} = f(s_t, a_t)st+1​=f(st​,at​), this means the next state is given by the transition function (depends on the problem) which takes current state and action.  The state transition can also be stochastic : st+1∼P(⋅∣st,at)s_{t+1} \\sim P(\\cdot | s_t, a_t)st+1​∼P(⋅∣st​,at​), when the agent takes a particular action in a given state, there is a probability distribution over the possible resulting states.  Reward, Return &amp; Horizon​  Reward rrr serves as a value that provides feedback indicating the quality of its action. It is determined by the reward function : rt=R(st)r_t = R(s_t)rt​=R(st​) (state-based reward) or rt=R(st,at)r_t = R(s_t, a_t)rt​=R(st​,at​) (state-action-based reward). This mean the reward at time ttt is determined by the reward function that takes either current state only or takes both current state and action.  The return, also known as cumulative reward or the discounted sum of future rewards, is the total amount of rewards the agent accumulated over a time horizon. A horizon is a predetermined length or number of time steps in an episode or a task. An episode is a complete sequence of interactions between an agent and its environment.  A horizon represents the limit or duration of the agent's interaction with the environment. It can be finite, meaning there is a fixed number of time steps until the end of the episode, or it can be infinite, allowing the interaction to continue indefinitely.  Finite-Horizon Undiscounted Return : In the case of a finite horizon, the agent's objective is to maximize the undiscounted return. The undiscounted return is the sum of rewards obtained from the current time step until the end of the episode, without any discounting factor applied. Source : https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#reward-and-return Infinite-Horizon Discounted Return : When the horizon is infinite, the agent's objective is to maximize the discounted return. The discounted return is the sum of rewards obtained from the current time step until infinity, but each reward is discounted by a factor (\\gamma) (gamma) between 0 and 1. The gamma will be raised to the power of ttt, which causes it to decrease as ttt increases. Source : https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#reward-and-return  Both have the same purpose, which is to maximize return, they differ in the use of discount factor. The discount factor introduces a trade-off between immediate and future rewards, serving as a balancing mechanism. The agent can choose to prioritize obtaining high reward to minimize the impact of the discount factor or delay the rewards in the risk of large discount amount.  RL Main Objective​  The main goal of RL is to maximize the expected cumulative return or total reward obtained by an agent over time. Combining all together, we can construct the following formula :   Source : https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#the-rl-problem  The P(τ∣π)P(\\tau|\\pi)P(τ∣π) represent the trajectory or event that will occurs (in probability distribution) given that a policy π\\piπ applies. It is obtained by the product of the very first state s0s_0s0​ with initial the state p0p_0p0​ multiplied with the product of each state transition (P(st+1∣st,at)P(s_{t+1}|s_t, a_t)P(st+1​∣st​,at​)) with the policy (π(at∣st)\\pi(a_t|s_t)π(at​∣st​)) from time step 0 to T−1T - 1T−1.  The J(π)J(\\pi)J(π) is the expected return, it represents the expected cumulative reward that an agent will receive when following a specific policy π\\piπ over an extended period of time.  And the last expression basically means that we are looking for optimal policy π\\piπ that yields the highest expected return among all possible policies J(π)J(\\pi)J(π).  Value Function​  Value function is a function that assigns a value to each state or state-action pair, the value represent the expected return an agent can obtain from that state or state-action pair under a given policy.  There are four main value function :  On-Policy Value Function (V-function) : It describe the value that agent can obtain when starting in state sss and following a particular policy.On-Policy Action-Value Function (Q-function) : It describe the value that agent can obtain when starting in state sss, taking action aaa, and following a particular policy.Optimal Value Function (V∗\\text{V}^*V∗-function) : The optimal or maximum of V-function.Optimal Action-Value Function (Q∗\\text{Q}^*Q∗-function) : The optimal or maximum of Q-function.   Source : https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions  In practice, the value function is usually not known to us. There are several reasons for this :  Complex or Unknown Environment : It is often impossible to have complete knowledge of all possible states, actions, rewards, and transition dynamics.Sparse Rewards : Sometimes, the reward is not directly given to the agent after doing an action, if the agent doesn't receive any feedback after a long time, it may lead to difficulties in finding the optimal policy. A reward that is too high or too low can also make the learning unstable.Stochasticity : Many environments are stochastic, meaning that the outcomes of actions are subject to randomness which makes it difficult to predict the exact reward or value for a state because it can vary from one interaction to another.  Bellman Equation​  Bellman equation is an equation, typically used in dynamic programming, it is an equation that decompose a problem into smaller subproblems and finding the optimal solution by iteratively updating the value.  In the context of RL, Bellman equation is applied to describe how the value of a state (or state-action pair) is related to the values of its successor states. It says that the value of a state is the reward for current state plus the discounted value for next state. The value for the next state also depends on the next and next state, making it a recursive equation.   Source : https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#bellman-equations  Bellman equation provides a mathematical framework to model the values of states or state-action pairs in reinforcement learning. We can compute the value iteratively and the agents can adjust their estimates of the value of states by considering the rewards they have observed so far and the values of the states that follow.  Advantage Function​  Advantage function is a function that estimate the advantage or benefit of taking a particular action in a given state compared to other actions corresponding to a policy. It measures the relative value of an action with respect to the value function.   Source : https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions  The Aπ(s,a)A^{\\pi}(s,a)Aπ(s,a) represent the advantage of taking action aaa in state sss. A positive advantage indicates that the action is better than average, while a negative advantage suggests that the action is worse than average. A value close to zero means the action is roughly equivalent to the average.  ","version":"Next","tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"Reinforcement Learning Fundamental","url":"/cs-notes/deep-learning/reinforcement-learning/reinforcement-learning-fundamental#conclusion","content":" The stochastic aspect along with reward and penalty terms on each action in reinforcement learning makes it different with supervised or unsupervised learning. In the latter two machine learning paradigms, the model always choose the highest probability based on the data they learned (e.g. while predicting or classifying). However, in reinforcement learning, even if we have high probability we will always try to explore different choice and reward. This enables reinforcement learning model to adapt itself with the environment, without needing an explicit instruction or example on a specific tasks.  ","version":"Next","tagName":"h3"},{"title":"Model-Based & Model-Free​","type":1,"pageTitle":"Reinforcement Learning Fundamental","url":"/cs-notes/deep-learning/reinforcement-learning/reinforcement-learning-fundamental#model-based--model-free","content":" Model-Based and Model-Free are the two approach of reinforcement learning.  Model-Based : In model-based RL, the agent has access to a model of the environment. Model of an environment is a simulation of the environment in which an agent operates. It provides information about the dynamics of the environment, including transition probabilities and rewards. The agent uses this model to simulate the environment and plan its actions. The use of model is to enable the agent to simulate and plan ahead before actually taking actions. Model-Free : In model-free RL, the agent does not have access to the model of the environment. Instead, it learns directly from interacting with the environment without prior knowledge of the transition probabilities and rewards. Model-free RL algorithms learn by trial and error, exploring the environment and updating their policy or value estimates based on observed rewards. They aim to find an optimal policy or value function that guides them toward the best return.  ","version":"Next","tagName":"h3"},{"title":"Exploration & Exploitation​","type":1,"pageTitle":"Reinforcement Learning Fundamental","url":"/cs-notes/deep-learning/reinforcement-learning/reinforcement-learning-fundamental#exploration--exploitation","content":" Exploration and Exploitation is a common dilemma in reinforcement learning.  Exploration : Refers to the process of seeking out and gathering new information about the environment. By exploring, it means the agent takes action that isn't considered the best according to the current policy. The agent explores to potentially finds better actions or states that may lead to higher rewards. Exploitation : Exploitation is often referred as the &quot;greedy&quot; technique that prioritize immediate rewards or maximize the cumulative rewards based on the agent's current policy. In other word, the agent will choose actions that yield the highest reward according to current policy.  Both strategy is very important in machine learning, we need to balance the exploration and exploitation. Too much exploration may lead to excessive randomness and inefficient use of learned knowledge, while too much exploitation may result in the agent getting stuck in a suboptimal policy and missing out on better opportunities.  Epsilon-Greedy​  Epsilon-greedy is a common strategy used in reinforcement learning to balance exploration and exploitation. The balancing is determined by probabilities, which is set by the epsilon (E\\mathcal{E}E) parameter.  First, we choose the value of epsilon (E\\mathcal{E}E), the epsilon represent the probability of exploring, the exploration include selecting random action. The probability to exploit is determined by 1 - E\\mathcal{E}E, the agent will select the highest estimated value based on its current policy.  A higher value of E\\mathcal{E}E encourages more exploration, while a lower value of E\\mathcal{E}E favors exploitation. The E\\mathcal{E}E can be decreased gradually, over time, the agent can transition from initially exploring extensively to eventually exploiting the learned knowledge more frequently. ","version":"Next","tagName":"h3"},{"title":"Transformers Audio","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/transformers/transformers-audio","content":"","keywords":"","version":"Next"},{"title":"Spectogram​","type":1,"pageTitle":"Transformers Audio","url":"/cs-notes/deep-learning/transformers/transformers-audio#spectogram","content":" Spectogram is a visual representation of the frequency of a signal over time. The audio signal is divided into short segment, each segment will be transformed into frequency domain using technique like fourier transform. Representing it in frequency domain helps us know which different frequency contribute to the overall signal, therefore it's a good way to capture information lies on the audio data for processing it in the transformers architecture.  Another variation of spectogram is the Mel-spectogram, it is a form of spectogram specifically made to focus on human perception of sound. Basically, we scale the audio signal so that lower frequency is more emphasized than higher frequency, this is because human perception is more sensitive to lower frequency range.  Another way to represent audio is using waveform, the representation shows how the amplitude of the signal varies over time. However, waveform tend to have longer length than spectogram, leading to more computation and memory usage.   Source : https://www.researchgate.net/figure/Waveform-spectrogram-and-mel-spectrogram-of-a-10-s-speech-segment-obtained-from-Google_fig1_333834541  tip Find more about signal processing or about audio.  Input &amp; Output​  To input waveform or spectogram into transformers encoder, they are typically divided into small segment first. The smaller segment will be fed into a convolutional neural network (CNN), which act as the feature extractor. CNN can be a good feature extractor as it capable of capturing local patterns (e.g. frequency shape variation) from input data through the convolution operation.  If the output is a text, it will be similar to the original transformers architecture, we uses linear layer and softmax function on top of the decoder's output. To produce audio output, we will need some layer that produces audio sequence. The layer can be additional neural network such as linear layer or CNN. The layer can also refine the audio output for better quality.  ","version":"Next","tagName":"h3"},{"title":"Seq2Seq​","type":1,"pageTitle":"Transformers Audio","url":"/cs-notes/deep-learning/transformers/transformers-audio#seq2seq","content":" There are many variation of transformers architecture for audio processing. Some use encoder only to classify the audio or use decoder only to generate audio, other like Seq2Seq which is used in Whisper AI, uses both encoder and decoder to takes sequence and output another sequence.   Source : https://huggingface.co/learn/audio-course/chapter3/seq2seq  Input : It takes log-mel spectogram which is the log scaled of mel spectogram as the input. Spectogram Pre-processing (Conv1D + GELU) : The spectrogram is fed into a convolutional layer to extract features and patterns. It will use the GELU activation function, which is the variation of ReLU that has more smoother shape. The output of the CNN layers is then tokenized into smaller segments, similar to the tokenization step in the standard transformer models. Positional Encoding : Positional encoding is added to the tokens to provide information about their relative positions in the sequence. Encoder Block : This includes the similar process to the standard transformers architecture, it includes attention layer that provide high-level summary and captures the &quot;meaning&quot; of the audio data. Decoder Input : The decoder takes input from its own previously generated token. It will be positional encoded and will be passed into the next decoder's layer. Cross-Attention : Cross-attention is when the encoder output is combined with the decoder previous result. The encoder, provides key and value vector while the decoder provides the query vector, similar to the original transformers architecture. Decoder Output : The decoder outputs a token, the token can be converted back into text or converted into spectogram with further processing depending on the use case. The decoder will then use it for the next output step.  Overall, the transformers architecture for audio is similar to the standard transformers, the difference falls in the input processing. ","version":"Next","tagName":"h3"},{"title":"GPT","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/transformers/gpt","content":"","keywords":"","version":"Next"},{"title":"Architecture​","type":1,"pageTitle":"GPT","url":"/cs-notes/deep-learning/transformers/gpt#architecture","content":" GPT consist of 12 transformers decoder stacked on top of each other. Each decoder layer consider the previously generated words and uses self-attention mechanisms to capture the relationships between the words.   Source : https://paperswithcode.com/method/gpt  Input : The input of GPT which is a decoder is its own previous output or the initial prompt. The input is tokenized or turned into numerical representation and then transformed into vector (embedding). Alongside the input embedding, GPT also incorporates positional encoding to capture the position of the tokens in the sequence. Decoder Layer : A single decoder layer includes masked multi self attention, add &amp; layer normalization, feed forward network, and another add &amp; layer normalization. Output : The output from previous layer is passed into linear layer and a softmax activation function, the probability produced represents the likelihood of each word in the vocabulary being the next word in the generated text. The output processing can be customized into the task requirement.  ","version":"Next","tagName":"h3"},{"title":"GPT Version​","type":1,"pageTitle":"GPT","url":"/cs-notes/deep-learning/transformers/gpt#gpt-version","content":" GPT has introduced improvements and advancements over its predecessors :  GPT-1 : The original version of GPT released in June 2018, consist of 12 transformer decoder layers and approximately 117 million parameters. GPT-2 : GPT-2 was released in February 2019, ranging from 117 million to 1.5 billion parameters and have up to 48 decoder layers. GPT-3 : Released in June 2020, it introduced significant leap in scale and performance. Has 125 million to 175 billion parameters and 96 decoder layers. GPT-3.5 : Released in March 2022 and used for ChatGPT which is the fine tuned model from GPT-3.5 series released in November 2022. GPT-4 : The latest model released in March 2023, it was pre-trained on a combination of public data and fine tuned with reinforcement learning for a better feedback for human. ","version":"Next","tagName":"h3"},{"title":"Transformers Architecture","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/transformers/transformers-architecture","content":"","keywords":"","version":"Next"},{"title":"Transformers Architecture​","type":1,"pageTitle":"Transformers Architecture","url":"/cs-notes/deep-learning/transformers/transformers-architecture#transformers-architecture","content":"  Source : https://machinelearningmastery.com/the-transformer-model/ (with modification)  Transformers follow the encoder and decoder architecture, meaning one component should capture and summarize information from input and another is the one that produces output.  ","version":"Next","tagName":"h2"},{"title":"Encoder​","type":1,"pageTitle":"Transformers Architecture","url":"/cs-notes/deep-learning/transformers/transformers-architecture#encoder","content":" The encoder is responsible for taking and processing the input sequence, it consist of multiple layer that works together. Here is the walkthrough of input processing in encoder :  Input Embedding : The input sequence is first transformed into a dense vector (often called as token) that of course contain numbers. The input embedding is a learnable process, meaning it can be adjusted during the backpropagation process. Positional Encoding : Since transformers doesn't include RNN that process input sequentially, positional encoding is a technique to capture the relative position of tokens within a sequence. Positional encoding is done by adding a sinusoidal function (sin and cos) of different frequency to the token vector. Basically the sinusoidal function will model the position information of each element in the sequence using different variation of frequency. There are few reason why sinusoidal function is used, one of the important is the periodic nature offers an unlimited encoding regardless of our input length. Other reason are its smooth transition and non-linear function. Source : https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model  The first and second is the input pre-processing, the third and so on is the actual processing inside an encoder layer.  Self-Attention : Transformers uses a mechanism called self-attention. The standard attention mechanism produces a context vector (vector that contains important information about the sequence we are processing) by considering the input sequence and the current output we are generating. In other word, it needs two sequence which is the input and current output to generate another output. On the other hand, self-attention only need a single sequence. Self-attention weigh the relevance or importance of each element by comparing it with other element in the sequence itself. This will make different element have different relevance in the sequence. What makes it superior than the standard attention mechanism, it offers us parallelization. Remember that in RNN with attention on each output step, the attention vector is different, they are computed based on current output step. Self-attention that uses single sequence allows us to compute attention scores for all elements simultaneously, leading to better performance. Self-attention process : The self-attention is implemented by matrix multiplication. The token embedding are fed into the first layer in encoder. The self-attention mechanism is applied, it is done by calculating three types of vector called query, key, and value vectors. The tokens embedding are combined in a matrix, it will be multiplied by three types of matrices that correspond to each vector mentioned. The matrix are Wq, Wk, and Wv, they are basically weights in form of matrix, which mean they are learnable. These three vector carries the information of token, they will be used to calculate an attention weight, which is the value of importance of a token in sequence. The query vector represent specific position of token in the input sequence that we want to compute the attention weights for.Key vector represent the other token in the sequence that is being compared to the query vector. As explained in self-attention mechanism, we will compare all the element with each other to consider which one is more important than other.Value vector contain the actual information or the features of each token in the sequence. Visualization of the relation between each word or token in self-attention. Source : https://www.researchgate.net/figure/A-visualization-of-a-learned-self-attention-head-on-a-sentence-The-visualization-shows_fig5_346522738, https://babich.biz/transformer-architecture/ The query and key vector (transposed) will be multiplied together, producing something called attention scores, it can be interpreted as the similarity between two token. The higher the result is the stronger the relevance. They will be divided by the square root of the dimensionality of the key vectors, to prevent large number. Softmax activation function will be applied to the previous result, resulting normalized value (they sum up to 1). The normalized value will be multiplied with the value vector. The result of it is what we call attention weights. The normalized value which represent the similarity of information is multiplied by the actual information of the token in input sequence. This mean we are assigning the similarity of information to each token. The output (called attention layer output) will be the sum of all attention weights and this will be done for all element in sequence. The properties of encoder that consider each token in sequence including the previous and subsequent token to capture the information that lies on the input is called bi-directional. In other word, it can pay attention to every token in the sequence. Source : https://theaisummer.com/transformer/ Source : https://youtu.be/z1xs9jdZnuY?si=czJyixA7IV3DxG7k&amp;t=475 Multi-Head Attention : Multi-head attention is an extension of the self-attention mechanism that improves its efficiency through parallel computation. The set of query, key, and value vector we have obtained is grouped in something called attention head. They first goes into a linear layer to be projected into different vector spaces, basically projecting them mean we are looking through these vector from different perspective. This will allows the model to capture different representation of the input. So, to calculate attention weights in parallel, we will compute all the attention head simultaneously. The same calculation that includes multiplication between query and key vector, softmax normalization, and multiplication with the value vector, is also performed. By calculating them simultaneously, the model can have longer dependencies when processing the input sequence. The model attends to all other tokens and can access different parts of the sequence during the matrix multiplication process. Each result of attention head will be concatenated together and will be passed into a linear layer again, producing the final output of multi-head attention layer. Source : https://paperswithcode.com/method/multi-head-attention Residual Connection and Layer Normalization (Add &amp; Norm) : We did alot of calculation, during the backpropagation process, we may lose some information including the positional encoding we did in the earlier step. Transformers uses the residual connection concept to help prevent the vanishing gradient issue. This is implemented by adding the input that bypass the attention layer with the same input that goes to the attention layer. This layer also include a normalization process to normalize the output of attention layer, to prevent large number and stabilize the training process. Feed-Forward Networks (FFN) : The attention output is passed through a feed-forward network within the encoder layer. The FFN that includes activation function like ReLU introduces non-linearity. The output of the FFN is then passed through another residual connection and layer normalization. Similar to residual connection in multi-head attention layer, we will add the input that bypass the feed-forward network with the one that goes through it. Source : https://deepgram.com/learn/visualizing-and-explaining-transformer-models-from-the-ground-up  The step from 3 to 6 represent the process of a single encoder layer. In conclusion, a single layer of encoder processes and transform the input sequence to capture relevant information in the sequence. The output of encoder is a key and value pair that represent the information about the input sequence.  Transformers architecture may includes multiple encoder layer, they have identical architecture but they don't share weights.   Source : https://machinelearningmastery.com/the-transformer-model/ (with modification)  ","version":"Next","tagName":"h3"},{"title":"Decoder​","type":1,"pageTitle":"Transformers Architecture","url":"/cs-notes/deep-learning/transformers/transformers-architecture#decoder","content":" The decoder is responsible for generating the output sequence, it consist of similar layer with encoder like multi-head attention, add &amp; norm, and feed-forward network. The decoder takes input from encoder layer (the relevant information) and from the previous output as well.  The decoder first process the previous output and then it will be combined with the output from encoder.  Output Embedding : The previously generated output embedded or will be turned into a vector representation (token), similar to input embedding in encoder. Positional Encoding : The similar encoding process to capture the relative position of each token in the sequence. Masked Multi-Head Attention : In the normal multi-head attention, we calculated the attention weights using matrix multiplication which include multiplying every element with each other. Multiplying with every element includes accessing its query, key, vector, meaning we have information about them. However, we don't want this to happen in decoder, we don't want to generate output with the information from future. This is implemented by changing some of the value in the matrix during the matrix multiplication to a very large negative number. The properties of decoder that only access previously generated token is called uni-directional. Source : https://tigris-data-science.tistory.com/entry/%EC%B0%A8%EA%B7%BC%EC%B0%A8%EA%B7%BC-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-Transformer4-Masked-Multi-Head-Attention%EA%B3%BC-Decoder Add &amp; Norm : It then goes to the add &amp; norm layer again which consist of residual connection and normalization layer. Multi-head Attention : Also known as encoder-decoder attention or cross-attention, in this step, the multi-head attention will be done again. Multi-head attention will need query, key, and value vector, the input for them will be the combination of the output from encoder and previous result from decoder. Encoder provides the key and value pair, the decoder's previous output is transformed into a query. So basically, the encoder provides the contextual information from key and value pair and its combined with the query from decoder that represent what we need to generate the output now. It then goes to add &amp; norm layer again. Feed-Forward Network + Add &amp; Norm : The next component in the decoder layer is the feed-forward network. Same as the encoder, it consist of fully connected layer with non-linear activation function. Next, it will be normalized again in the add &amp; norm layer. Output : Finally, the output from previous layer will go into a linear layer, followed with softmax activation function to produces the probability of each token. The model select the token with highest probability and use it as the input for next decoder step. Decoder is considered as auto-regressive or causal-attention, meaning it output sequence one step at a time, in an iterative and sequential manner. Source : https://www.linkedin.com/pulse/intro-transformer-architecture-jithin-s-l  ","version":"Next","tagName":"h3"},{"title":"Learning Process​","type":1,"pageTitle":"Transformers Architecture","url":"/cs-notes/deep-learning/transformers/transformers-architecture#learning-process","content":" After output is generated, the prediction or whatever the output is will be compared with the actual label of the input. For example :  In machine translation, the labels would be the target sequences or translations corresponding to the source sequences.In text classification, the labels represent different categories of the input text.  After loss is calculated, the similar learning process will be done, including the backpropagation process through all the layer of transformers model from the decoder output until the encoder input.  Extras​  Transformers consists of encoder and decoder, however, they are not necesarry used together. In tasks like text classification or sentiment analysis, where understanding of the input is the primary objective, the encoder captures contextual information about the data, the information can be fed into classifier directly. A decoder-only model is designed for generating output. It is applicable in tasks such as dialogue generation, where it generates the subsequent word based on previously generated words.  They are often used together for sequence-to-sequence tasks like text translation that require understanding of input data and the output generation.  Transformers model is considered as semi-supervised learning. The semi-supervised learning involve techniques like pre-training and fine tuning. The pre-training technique mean the model is trained on unlabeled data. During the pre-training process, the model learns the general language representation and capture how each word relate with each other. It will then be fine tuned, a smaller labeled dataset will be fed to the model to adapt it on specific tasks or specific topic (the method is also called transfer learning). ","version":"Next","tagName":"h3"},{"title":"Vision Transformers","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/transformers/vision-transformers","content":"","keywords":"","version":"Next"},{"title":"Architecture​","type":1,"pageTitle":"Vision Transformers","url":"/cs-notes/deep-learning/transformers/vision-transformers#architecture","content":" The original ViT architecture is an encoder only model. At the end of encoder's output, a feed-forward network is used to classify the image.   Source : https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b  Input : The input of ViT is indeed an image, the image will be divided into smaller non-overlapping region called patch. The number of patches depend on the image size and the chosen patch size. For example, if the image size is 224x224 pixels and the patch size is 16x16 pixels, then it will result in 196 patches. The patch will be turned into vector representation or linearly projected into an embedding space (embedding process). In addition, positional embedding that encode the relative position of the patch in the input image is also added. Source : https://gowrishankar.info/blog/transformers-everywhere-patch-encoding-technique-for-vision-transformersvit-explained/ Transformers or Encoder Block : The embedded patches are fed into the transformers encoder block, consisting multi-head self-attention, fully connected layer, and add &amp; norm layer. The ViT model stack several encoder block together. Transformers architecture make it possible to capture the global information about the image, compared to CNN that captures only the local patterns. Output : The output is a sequence of patch embedding that captured the global representation of the image. Depending on the task, the output processing may be different. In a classification tasks, the output goes to a fully connected layer to produce probability distribution over different classes or labels. ","version":"Next","tagName":"h3"},{"title":"Variational Autoencoder","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/variational-autoencoder","content":"","keywords":"","version":"Next"},{"title":"Encoder​","type":1,"pageTitle":"Variational Autoencoder","url":"/cs-notes/deep-learning/variational-autoencoder#encoder","content":" Input data such as image is fed to the encoder. Similar to traditional autoencoder, it consist of several layers such as convolution layers and pooling layers.  After going through encoder layers and arrived at the final fully-connected layer, the output kinda different. As explained before, the data samples will be represented in a distribution, and we will sample from the distribution. The output is still gonna be the lower-dimensional representation of the data, often called as latent variables. However, these latent variables will be modeled into a probability distribution instead of being flattened, they will be transformed into a probability distribution called latent space distribution.  The distribution follows the multivariate Gaussian distribution, it will be constructed based on two vectors parameters : mean(μ\\muμ) vector and the standard deviations (σ\\sigmaσ) or variance (σ2\\sigma^2σ2) vector. These vectors are produced in the two branch of the final fully-connected layer of the encoder.  In a more advanced VAE, the lower-dimensional representation of data may include multiple dimension of feature data, to be able to capture more aspect of the data.  note The mean vector provides the central tendency of the distribution, while the standard deviation (or variance) vector determines the spread or uncertainty of the distribution, similar to statistics in math.   Source : https://www.compthree.com/blog/autoencoder/  ","version":"Next","tagName":"h3"},{"title":"Sampling​","type":1,"pageTitle":"Variational Autoencoder","url":"/cs-notes/deep-learning/variational-autoencoder#sampling","content":" After constructing the distribution, we can start the sampling process. We will sample from the distribution defined by the mean and the variance. We will take sample as much as required to suit with the shape of the latent variables. The sampled latent is often denoted as zzz.   Source : https://notebook.community/diegocavalca/Studies/books/deep-learning-with-python/8.4-generating-images-with-vaes  Interpolation​  Another technique in sampling is the interpolation, basically it takes multiple sample in the latent space and take the interpolation of it. By taking the interpolation, we will generate a similar data with the previous sample. The purpose of this is to make a smooth transition between different input data.  For example, we can use this to transform a face that originally doesn't wear sunglasses to wear it. We will first encode the original face to obtain the latent variables. We will then encode a face with sunglasses to obtain the latent variables aswell. Linear interpolation will be done between these two latent variables, we can choose the degree of interpolation, whether we want the result to be closer to original face or closer to the face with sunglasses. The interpolated vector will be passed to the decoder network and a reconstructed image between the interpolated path will be generated.   Source : https://www.compthree.com/blog/autoencoder/  ","version":"Next","tagName":"h3"},{"title":"Decoder​","type":1,"pageTitle":"Variational Autoencoder","url":"/cs-notes/deep-learning/variational-autoencoder#decoder","content":" The decoder layer of VAE is similar to the decoder layer in autoencoder, it is the reverse process of encoder that gradually increase the dimensionality of the sampled latent vectors, transforming them back into the original input space dimensions. Deconvolutional and reverse pooling process will be done.  Loss Calculation​  After getting back the original input dimension, a new data is successfully generated. The loss calculation involve two terms, reconstruction loss, which measures the discrepancy between the reconstructed data and the original input data, same as the loss in traditional autoencoder. The second term is the Kullback-Leibler (KL) divergence, which measures the dissimilarity or information loss between two probability distributions. We compared the probability distribution we used to sample the new data with a normal distribution.   Source : https://youtu.be/9zKuYvjFFS8?si=wLkw3K_EXqw_1jpP&amp;t=391  ","version":"Next","tagName":"h3"},{"title":"Reparameterization Trick​","type":1,"pageTitle":"Variational Autoencoder","url":"/cs-notes/deep-learning/variational-autoencoder#reparameterization-trick","content":" The sampling process uses the mean and the standard deviation/variances, however, the sampling process is not differentiable, which means we can't use the backpropagation process to optimize the loss function and update the model's parameters.  This is where an epsilon (ϵ\\epsilonϵ) vector is introduced. Epsilon vector is randomly drawn from the normal distribution. However, during the backpropagation process, we aren't actually optimizing it, it is simply a trick to make the function differentiable.   Source : https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important ","version":"Next","tagName":"h3"},{"title":"Digital Media Processing","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing","content":"","keywords":"","version":"Next"},{"title":"All pages​","type":1,"pageTitle":"Digital Media Processing","url":"/cs-notes/digital-media-processing#all-pages","content":" Image Processing Image PropertiesImage EnhancementImage Acquisition &amp; SensingImage RestorationImage Editing Audio Processing Sound &amp; Audio PropertiesAudio Input &amp; OutputAudio EqualizationAudio EffectsAudio EditingSpeech Processing Video Processing Video RepresentationVideo RecordingVideo Effects &amp; EnhancementFlash Player SWF Digital Media Formats MIME TypeImage Bitmap (BMP)JPG / JPEGPNGWebPSVGGIF Audio WAVOGG VorbisMP3 Video AVIMP4 Document XMLMarkdown (MD)Text File (txt)PDF ","version":"Next","tagName":"h3"},{"title":"U-Net","type":0,"sectionRef":"#","url":"/cs-notes/deep-learning/u-net","content":"","keywords":"","version":"Next"},{"title":"Architecture​","type":1,"pageTitle":"U-Net","url":"/cs-notes/deep-learning/u-net#architecture","content":" As the name suggests, the architecture has a &quot;U&quot; shape and is symmetric. It begins by extracting the features of input image, downsampling, and then the reverse process.   Source : https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5  Input : U-Net typically used for image segmentation tasks, the input of U-Net is an image that requires segmentation.Encoder : The encoder, which is the left part of the architecture, extracts the features of the image, it consists of a series of convolutional layers with ReLU activation function followed by pooling or down-sampling operations.Skip Connection : Each encoder's layers level is included with skip connection that connects directly to decoder, allowing the features to flow without being sampled in the encoder's layer.Decoder : The last level of encoder's layer produced the highly abstract and global information about the input image. The decoder takes it and perform the reverse process of encoder including upsampling followed by convolutional layers.  The skip connection combined with decoder makes U-Net suitable for tasks requiring image segmentation, particularly in domains like biomedicine. The decoder is able to combine high-level features from the output of encoder with the information from multiple level of encoder's layer accessed using skip connection.  ","version":"Next","tagName":"h3"},{"title":"Output & Learning​","type":1,"pageTitle":"U-Net","url":"/cs-notes/deep-learning/u-net#output--learning","content":" The output of U-Net is a segmentation mask or a probability map that indicates the presence or absence of different classes or regions in the input image. The initial output will obviously be messed up.   Source : https://catchzeng.medium.com/the-easiest-way-to-train-a-u-net-image-segmentation-model-using-tensorflow-and-labelme-fe130de45a19  The loss is calculated by comparing the output of U-Net with the ground truth segmentation masks. This mean we need labeled segmentation masks for each input we have. The loss will then backpropagated through the network and the gradients with respect to network's parameters will be adjusted. ","version":"Next","tagName":"h3"},{"title":"Audio Editing","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/audio-editing","content":"","keywords":"","version":"Next"},{"title":"Audio Restoration​","type":1,"pageTitle":"Audio Editing","url":"/cs-notes/digital-media-processing/audio-editing#audio-restoration","content":" Audio recordings suffer from a range of issues, such as background noise, clicks, pops, hisses, crackles, distortion, hum, or other artifacts. Audio Restoration is the process of enhancing or repairing audio recordings that have been degraded or damaged over time.  The Process​  The first process is to identify the unwanted elements. Unwanted elements have some charateristics, for example, a clicks are sudden and short-duration audio, pops are similar to clicks but are usually slightly longer in duration, hisses are continuous, high-frequency noise that can be present in audio recordings.  Overall the charateristics of unwanted elements are a sudden change in the constant audio or an unrelated patterns or rhythm to the main audio and not synchronized with specific events or sounds.  After identifying, we can apply some algorithm and filter, for example, we can smoothen the surrounding audio near unwanted elements, apply frequencies filter, noise reduction in frequency domain using FFT.   Source : https://youtu.be/fWQ86r14Ei0 ","version":"Next","tagName":"h3"},{"title":"Audio Effects","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/audio-effects","content":"","keywords":"","version":"Next"},{"title":"Spatial Audio​","type":1,"pageTitle":"Audio Effects","url":"/cs-notes/digital-media-processing/audio-effects#spatial-audio","content":" Spatial Audio create a three-dimensional sound experience that simulates the perception of sound in physical space. It aims to reproduce sound in a way that resembles how we perceive sound in the real world, including the sense of direction, distance, and location of sound sources.  Spatial audio uses multiple channels to produce sound. Multiple channel can be thought as multiple sources.  There are some technique to achieve spatial audio :  Head Tracking : Spatial audio uses the device's built-in gyroscope and accelerometer to track the movement of the listener's head. This allows the soundscape to be adjusted in real time to reflect the listener's current position. For example, if the listener turns their head to the left, the soundscape will be adjusted so that the sound sources appear to be coming from the left. One way to do this is to delay the sound in the right ears, this will create illusion in human brain as if it coming from the left. Source : https://android-developers.googleblog.com/2023/04/delivering-immersive-sound-experience-with-spatial-audio.html Binaural Audio : Binaural audio is a technique that uses two audio channels to create a sense of depth and space. The two channels are played back through headphones, and the listener's brain interprets the difference in timing and volume between the two channels to create a 3D soundscape. Binaural Audio can also use sound delay, for example a sound coming from further will perceived later than a closer sound in human ear. Ambisonics : Ambisonics is a more advanced form of binaural audio that can create a more realistic 3D soundscape. Ambisonics uses spherical soundfield, in simple term, the person is simulated in a sphere and a position around listener will have information : horizontal (azimuth), vertical (elevation), and distance (radius) components. Source : https://en.wikipedia.org/wiki/3D_audio_effect ","version":"Next","tagName":"h3"},{"title":"Audio Equalization","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/audio-equalization","content":"","keywords":"","version":"Next"},{"title":"Frequency Range​","type":1,"pageTitle":"Audio Equalization","url":"/cs-notes/digital-media-processing/audio-equalization#frequency-range","content":" Human audible range is divided into specific frequency range, categorizing these make us easier to target specific sound. Overall, they are divided into 3 general range, bass, mids, and highs.  Sub-bass : Below 60 Hz, these frequencies are felt more than they are heard. They provide the foundation for bass-heavy sounds like kick drums and rumbling effects. Bass : Ranging from 60 Hz to 250 Hz, these frequencies are responsible for the low-end body and warmth in music. Low-mid : Spanning from around 250 Hz to 2,000 Hz, these frequencies contribute to the fullness and presence of instruments and vocals. Mid : Extending from roughly 2,000 Hz to 5,000 Hz, these frequencies often define the intelligibility and clarity of vocals and the bite of certain instruments. High-mid : Occupying the range of 5,000 Hz to 8,000 Hz, these frequencies influence the presence and brilliance of sounds. High : Above 8,000 Hz, these frequencies add sparkle and airiness to the audio. Source : https://www.audioreputation.com/audio-frequency-spectrum-explained/  ","version":"Next","tagName":"h3"},{"title":"Equalizer​","type":1,"pageTitle":"Audio Equalization","url":"/cs-notes/digital-media-processing/audio-equalization#equalizer","content":" EQ works in between human audible frequency which is from 20 Hz to 20 kHz. An EQ also consists of a number of filters, they are used to filter off specific frequency.   Source : https://www.musicguymixing.com/eq-filters/  EQ is typically used in audio processing software, the range is divided by 3. The straight line represent the default or when all the frequency range are set to the same level. This mean the intensity of either bass, mids, or highs should be the same. The level or the loudness scale of the sound is measured in dB (decibels).  By adjusting the line, we can make specific frequency may sounds louder than the others. For example dragging the line between the 7 kHz frequency to up would make it sounds louder, and the surrounding frequency will also be adjusted.  The range of adjusted surrounding frequency is called Q-factor. A lower Q or smaller range will make the sound sharper or louder in small range of frequency. A higher Q or bigger range will make the sound sounds smoother.   Source : https://youtu.be/4FkKiWJfd00?t=179   ","version":"Next","tagName":"h3"},{"title":"AVI","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/avi","content":"","keywords":"","version":"Next"},{"title":"AVI Structure​","type":1,"pageTitle":"AVI","url":"/cs-notes/digital-media-processing/avi#avi-structure","content":" AVI is the subset of Resource Interchange File Format (RIFF), the data are divided by chunks and each chunks is identified by FourCC (four-character code).  AVI Header : This is the first section of the file and contains information about the overall structure and properties of the AVI file, such as the file size, duration, and the number of streams (audio and video). Main AVI List : This section contains various lists and chunks that define the structure of the AVI file. It includes: hdrl (Header List) : This list contains information about the streams, such as their format, codec, and properties. It typically includes a Stream Header (strh) chunk for each stream. movi (Movie Data) : This list stores the actual audio and video data. It contains chunks called &quot;chunks&quot; that store the media samples. Each chunk is identified by a FourCC (four-character code) that indicates the type of data it contains. Index : The index section of the AVI file contains an index table that allows for efficient seeking and playback of the media content. It consists of index entries that map the file offsets of the data chunks to their corresponding timestamps. Optional Lists : AVI files may include additional optional lists that contains additional information such as the maximum file size, frame rate, and segment information. An example is odml (OpenDML) list, which provides support for files larger than 2GB or non-standard frame rates.   Source : https://www.filefix.org/format/avi.html  ","version":"Next","tagName":"h3"},{"title":"AVI Compression​","type":1,"pageTitle":"AVI","url":"/cs-notes/digital-media-processing/avi#avi-compression","content":" AVI itself is just container for audio and video, it needs specific codecs to compress both the audio and video. There are many codecs that can be used, MP3 and vorbis can be used for audio.  Here is the example for video is :  DivX Codecs​  DivX is the codecs for video, it uses lossy compression.  Interframe Compression : In a video, often times an object doesn't always move. Interframe compression is when we encode only the difference between the frames. The same region can reference to the previous or future frames. This technique exploits temporal redundancy in video sequences, resulting in more efficient compression. Quantization : DivX codecs use quantization to reduce the precision of video data. The original video data are approximated to a smaller set of values, it approximate to the nearest representation. Variable Bit Rate (VBR) Encoding : Variable bit rate encoding allocates a higher bit rate to complex or high-motion scenes and a lower bit rate to less complex or static scenes. Huffman Encoding: DivX codecs use Huffman encoding, a lossless compression technique that assigns shorter codes to frequently occurring data patterns and longer codes to less frequent patterns. ","version":"Next","tagName":"h3"},{"title":"Bitmap (BMP)","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/bitmap-bmp","content":"","keywords":"","version":"Next"},{"title":"Indexed Color​","type":1,"pageTitle":"Bitmap (BMP)","url":"/cs-notes/digital-media-processing/bitmap-bmp#indexed-color","content":" Indexed color image is a type of digital image where the colors used in the image are selected from a predefined color palette. Instead of directly specifying the color of each individual pixel such as RGB, indexed color images represent each pixel with an index value that corresponds to a specific color in the palette.  For example, in rgb we may represent color in pixel x and y with (255, 0, 0) which is a red color, this way we have to store all of them in 24 bit. With indexed color, we can have a predefined color, we may define (255, 0, 0) or red color as 0, this way we can save alot of data. However, if there are various color, this may result in a larger file size.   Source : https://en.wikipedia.org/wiki/Indexed_color  ","version":"Next","tagName":"h3"},{"title":"BMP Structure​","type":1,"pageTitle":"Bitmap (BMP)","url":"/cs-notes/digital-media-processing/bitmap-bmp#bmp-structure","content":" BMP file contains several component :  File Header : File begins with a fixed-size header that provides general information about the file, such as the file type, size, and offset to the pixel data. DIB Header : The Device Independent Bitmap or DIB contains the more specific information such as width and height in pixels, color depth, compression method (if any), and color palette. Color Palette (optional) : For indexed color images (color depths of 8 bits or less), an optional color palette may be present. The color palette is an array of color entries that maps pixel values to specific colors. Pixel Data : After the headers, the file contains the actual pixel data. The pixel data represents the image itself, with each pixel's color or intensity information stored in a specific format based on the color depth. For example, in a 24-bit color depth BMP file, each pixel is represented by three bytes, typically in the order of blue, green, and red (BGR).  Overall, BMP are relatively straightforward to make or to read by software.   Source : https://en.wikipedia.org/wiki/BMP_file_format ","version":"Next","tagName":"h3"},{"title":"Flash Player SWF","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/flash-player-swf","content":"","keywords":"","version":"Next"},{"title":"How flash player works (high-level)​","type":1,"pageTitle":"Flash Player SWF","url":"/cs-notes/digital-media-processing/flash-player-swf#how-flash-player-works-high-level","content":" Flash player content are stored in SWF file. After developer creates flash player content, all the assets and script bytecode will be compiled to binary data with some format and tags in SWF format.  A browser must has flash player plugin in order to run flash content or SWF file. Flash player plugin is an extension for browser, it act as a flash player engine for the browser.  The browser with flash player plugin will then parse the SWF file. It reads the binary data, interprets the format and tags, and extracts the necessary information.  The next step is to render the content, the plugin will handle all of this. This will include converting graphics to pixels data in screen, playing audio or video, doing geometry transformation such as scaling and rotation.  Flash player plugin also contains virtual machine for running ActionScript code. The bytecode will be interpreted and executed line by line by the plugin. ","version":"Next","tagName":"h3"},{"title":"Audio Input & Output","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/audio-input-output","content":"","keywords":"","version":"Next"},{"title":"Audio Input Device​","type":1,"pageTitle":"Audio Input & Output","url":"/cs-notes/digital-media-processing/audio-input-output#audio-input-device","content":" Common type of audio input device is micrphone, there are many types of microphone, the common are condensed and dynamic. The difference is condenser microphone is more suitable for recording a higher frequency while dynamic microphone is better for capturing loud and strong sounds.  How Microphone Works​  Microphone utilize the principle of electromagnetic induction. They consist of a diaphragm, a coil of wire, and a magnet.  Inside a mic, there is a diaphragm which is a thin, flexible membrane that vibrates in response to sound waves. Behind the diaphragm, there is a coil which is wrapped around a permanent magnet.  When sound wave hit the diaphragm, it vibrates, causing the attached coil to move within the magnetic field. This motion generates an electrical current in the coil through electromagnetic induction. The electrical current represents the variations in air pressure caused by the sound waves.  The electrical signal generated is in analog signal form, it is then sent through the microphone's output connector for further processing.   Source : https://en.wikipedia.org/wiki/Microphone  Signal Processing​  One of the technique to further process the audio is amplification. Amplification to the process of increasing the amplitude or level of an electrical signal. It involves boosting the strength or the loudness of sound to a desired level. The analog electrical signal is amplified using amplifier tools.  The idea of amplifier is amplifying the signal using transistors. Transistor increase the voltage and power of an electrical audio signal by adding up voltage or current from another source, thereby amplifying it.   Source : https://en.wikipedia.org/wiki/Amplifier   Source : https://en.wikipedia.org/wiki/Integrated_amplifier  Digital Signal Processing​  After electrical signal is amplified, it will be converted to a digital format using an analog-to-digital converter (ADC). Digital form of sound data or audio can be further processed using various digital signal processing techniques. This can include equalization, compression, filtering, effects, and other audio processing algorithms.  After processing, audio data can be encoded into specific digital audio formats, such as MP3, AAC, WAV, or FLAC, depending on the intended use or distribution requirements.   Source : https://www.gearrice.com/update/4-tricks-with-aimp-to-better-listen-to-rock-and-metal-music/  ","version":"Next","tagName":"h3"},{"title":"Audio Output Device​","type":1,"pageTitle":"Audio Input & Output","url":"/cs-notes/digital-media-processing/audio-input-output#audio-output-device","content":" Digital audio file contains sequence of binary data, the first step in outputting an audio data is to decode or read the audio file. It will be decode by an audio player software. The binary data will converted into an analog electrical signal using a digital-to-analog converter (DAC).  The analog audio signal is sent to the computer's audio output device, such as the sound card or built-in audio interface. The sound card is connected to audio output ports, which is speakers, headphones, or other audio playback devices.  How Speaker Works​  The concept of speaker is the reverse process of microphone and based on the principle of electromagnetism. The speaker also consists of a permanent magnet and a coil. The voice coil is a wire coil attached to a diaphragm.  Electrical audio signal passes through the coil, it creates a changing magnetic field around the coil. Changing magnetic field interacts with the fixed magnet, resulting in a force acting on the coil.  Force generated by coil causes the attached diaphragm to move back and forth. Movement of diaphragm will create pressure variations or variation of sound wave, it depends on the force which is also dependent on the electrical signal.  Sound wave will then propagate through the air and reach our ears.   Source : https://www.how2shout.com/what-is/what-is-speaker-and-how-it-functions.html   Source : https://soundcertified.com/how-do-speakers-work/ ","version":"Next","tagName":"h3"},{"title":"GIF","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/gif","content":"","keywords":"","version":"Next"},{"title":"GIF Representation​","type":1,"pageTitle":"GIF","url":"/cs-notes/digital-media-processing/gif#gif-representation","content":" GIF Color Palette​  GIF colors palette are represented in color table, instead of specifying color in each pixel, we use index instead that refer to the color table.  Each entry in the color table represents a specific color and is typically 24 bits in size, with 8 bits for the red component, 8 bits for the green component, and 8 bits for the blue component.  The number of colors in the color table is limited to a maximum of 256 (2^8) due to the 8-bit index used to reference the colors.  For example, we define red (255, 0, 0) as the 0 index. This mean we can specify pixel in specific coordinate as red color by using index 0.   Source : http://www.flounder.com/colortable.htm  GIF Animation​  Animated images is made possible through the sequence of image that is played in short amount of time, creating the illusion of motion. Each image in sequence is called frame, the rate of frame played is not defined with FPS like animation or video. Instead, each frame has its own duration.  For example, a frame duration of 10 centiseconds would mean that the frame is displayed for 0.1 seconds, resulting in a perceived frame rate of 10 frames per second. The minimum of GIF frame duration is 1 centisecond (0.01 seconds), and the maximum duration is 65535 centiseconds (655.35 seconds or approximately 10 minutes and 55 seconds).  Basically, GIF displays an image, after it is run for specific duration, it changes to next image or frame. The continous process creates the illusion of motion. However, GIF may not be suitable for long animation, this is because GIF is not very smooth, uses lossless compression that makes video file larger, limited color palettes.   Source : https://www.litmus.com/blog/a-guide-to-animated-gifs-in-email  ","version":"Next","tagName":"h3"},{"title":"GIF Structure​","type":1,"pageTitle":"GIF","url":"/cs-notes/digital-media-processing/gif#gif-structure","content":" GIF file consist of several component :  File Header : GIF starts with fixed-size header that identifies it as a GIF file. It contains signature such as &quot;GIF89a&quot; or &quot;GIF87a&quot; to indicate the GIF version. Also includes width and height of the image. Logical Screen Descriptor : This contains the global information of the GIF file. This includes width and height of canvas, color index, color resolution, background color, and pixel aspect ratio. Global Color Table (optional) : If the logical screen descriptor indicates the presence of a global color table, it is stored immediately after the logical screen descriptor. Image &amp; Animation Data : Image and animation follows the color table (if present). Each frame or image consist of : Graphics Control Extension : Frame Delay : Time delay before displaying the next frameTransparency Flag &amp; Index : Indicates if transparency is used with the index of the transparent color in the color table Image Descriptor : Width and Height : Dimensions of the image frame in pixelsLocal Color Table Flag : Indicates the presence of a local color tableInterlaced Flag : Indicates if the image is interlacedSort Flag : Specifies if the local color table is sorted Image Data : The color of the image using the color table index Trailer : End of the GIF file marker, consist of single a single-byte value (0x3B).   Source : https://openpreservation.org/blogs/good-gif-hunting/  ","version":"Next","tagName":"h3"},{"title":"GIF Compression​","type":1,"pageTitle":"GIF","url":"/cs-notes/digital-media-processing/gif#gif-compression","content":" GIF uses several lossless compression :  LZW Compression : LZW (Lempel-Ziv-Welch) compression is the primary compression algorithm used in GIF. It reduces the file size by using a shorter codes for each symbol LZW compression operates by creating a dictionary of symbol encountered. This process is dynamic, it keeps expanding the dictionary if new pattern or sequences encountered in the data. We then replace each symbol with any element exist in the dictionary. Source : https://www.semanticscholar.org/paper/Optimization-of-LZW-Compression-Algorithm-With-of-Maulunida-Solichin/54ecf3209d9e52f6795b55a8e4760f8e3b10009c/figure/4 Palette-Based Color : GIF utilizes a palette-based color model, allowing only a limited number of colors to be used in an image. By using a color table or color palette, GIF can represent each pixel as an index into this table, rather than storing the full RGB color information for each pixel. This indexed color approach significantly reduces the file size, especially for images with a limited color range. Color Substitution : GIF employs color substitution to optimize the compression further. In cases where a color occurs only a few times in the image, the GIF format can replace that color with a neighboring color from the color table. This substitution reduces the number of unique colors needed to represent the image accurately, resulting in additional compression. While this sounds like a lossy compression, the loss of color accuracy is often considered acceptable as it doesn't affect much to the overall image.  ","version":"Next","tagName":"h3"},{"title":"GIF Interlacing​","type":1,"pageTitle":"GIF","url":"/cs-notes/digital-media-processing/gif#gif-interlacing","content":" GIF interlacing is a technique used to progressively display images as they are being downloaded or loaded. When a GIF image is interlaced, it is divided into a series of scan lines that are displayed in a specific order, allowing a low-resolution version of the image to be seen quickly, followed by a gradual improvement in quality as more scan lines are loaded.  The interlacing process rearranges the pixels in the image so that the first pass displays every eighth scan line, the second pass displays every fourth line, the third pass displays every second line, and the fourth pass displays every line. This process continues until all the lines have been displayed, resulting in a fully rendered image.  Overall, it provides an initial rendering that displays a lower resolution version of the image. It gradually improve the quality and keeps rendering more scanline slowly.   Source : https://commons.wikimedia.org/wiki/File:Interlacing_demo.gif ","version":"Next","tagName":"h3"},{"title":"Image Acquisition & Sensing","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/image-acquisition-sensing","content":"","keywords":"","version":"Next"},{"title":"How sensor works​","type":1,"pageTitle":"Image Acquisition & Sensing","url":"/cs-notes/digital-media-processing/image-acquisition-sensing#how-sensor-works","content":" Common sensor includes :  Temperature Sensor : Measures the temperature of the environment or an object.Pressure Sensor : Measures the pressure or force exerted on an object or in a system.Proximity Sensor : Detects the presence or absence of an object within a certain range.Gyroscope : Measures the orientation or angular velocity of an object.Magnetic Sensor : Measures the presence or strength of a magnetic field.Gas Sensor : Detects the presence and concentration of specific gases in the environment.  The exact working principle of a sensor depends on its type and the physical phenomenon it is designed to detect.  The general idea of a sensor is to detect and measure physical phenomena or environmental conditions and convert them into a usable form, typically an electrical or digital signal.  For example, a gas sensor designed to detect a specific target gas can utilize a sensing element that interacts with the target gas through a chemical reaction. This reaction can result in a change in the electrical conductivity, resistance, or voltage of the sensing element. Another example is an infrared temperature sensor that measures the infrared radiation emitted by an object. The principle is that all objects above absolute zero temperature emit infrared radiation. The sensor absorbs the infrared radiation that reaches it and converts it into a heat signal. The change in heat is then converted into an electrical signal proportional to the intensity of the heat.   Source : https://www.electronicshub.org/different-types-sensors/  ","version":"Next","tagName":"h3"},{"title":"Face Detection​","type":1,"pageTitle":"Image Acquisition & Sensing","url":"/cs-notes/digital-media-processing/image-acquisition-sensing#face-detection","content":" Face detection is a computer vision technology that involves identifying and locating human faces in images or video frames. The idea of face detection is too look for specific features in an image that are characteristic of faces.  Eyes : Eyes are one of the most distinctive features of a face. They are typically dark and round, and they are located in the upper part of the face.Nose : The nose is another distinctive feature of a face. It is typically located in the middle of the face, and it is slightly larger than the eyes.Mouth : The mouth is also a distinctive feature of a face. It is typically located below the nose, and it is usually open or closed.Facial Landmarks : Facial landmarks are specific points on the face, such as the corners of the eyes, the tip of the nose, and the corners of the mouth. Facial landmarks are often used to help with face detection, as they can provide more accurate information about the location of the face in an image. Source : https://en.wikipedia.org/wiki/Face_detection  ","version":"Next","tagName":"h3"},{"title":"Self-driving Cars​","type":1,"pageTitle":"Image Acquisition & Sensing","url":"/cs-notes/digital-media-processing/image-acquisition-sensing#self-driving-cars","content":" Self-driving cars are vehicles that can navigate and operate without human input. They use a variety of sensors, including cameras, radar, lidar, and artificial intelligence (AI) to perceive their surroundings and make decisions about how to move.  The use of AI in self-driving cars allows them to learn and adapt to their surroundings. The AI is trained on a large dataset of images, videos, and sensor data. This allows the AI to learn to recognize objects and to make predictions about how the environment will change.  Sensor and AI use characteristics such as shape, size, speed, motion to detect if it's a human, a bike, a car. For example, cars are typically boxy, while humans are more rounded.  Cars and humans can be a variety of colors, but some colors are more common for cars, such as white, black, and silver. Humans are more likely to be wearing clothes of various colors.   Source : https://centralcoastdatascience.org/projects/all/2021/how-pixel-differences-can-affect-sensors-self-driving-cars  ","version":"Next","tagName":"h3"},{"title":"Image Segmentation​","type":1,"pageTitle":"Image Acquisition & Sensing","url":"/cs-notes/digital-media-processing/image-acquisition-sensing#image-segmentation","content":" Image segmentation is the process of partitioning an image into multiple segments, where each segment corresponds to a different object or region of interest. This has many application including medical imaging, robotics, and self-driving cars.  Image segmentation is classified into :  Semantic Segmentation : Assign a class label to each pixel in an image, thereby dividing the image into meaningful semantic regions. For example, we might label each pixel in an image as &quot;road&quot;, &quot;car&quot;, &quot;person&quot;, or &quot;building.&quot; Instance Segmentation : This is more further approach than semantic segmentation. It doesn't only assign class labels to pixels but also differentiating between individual instances of objects within the same class. For example, we may differentiate two different car as &quot;car1&quot; and &quot;car2&quot; or &quot;red car&quot; and &quot;blue car&quot;. Panoptic Segmentation : Combines the prediction from both instance and semantic segmentation into a general unified output.  There are two main approaches in image segmentation :  Boundary-based : This focuses on identifying the boundaries or edges between different objects or regions in an image. This approach typically involves detecting discontinuities in pixel intensities, gradients, or other image features to locate the edges. Region-based : Region-based aims to group pixels together based on similarity criteria, such as color, texture, intensity, or other image properties. This approach seeks to create homogeneous regions within the image that share common characteristics.   Source : https://data-flair.training/blogs/image-segmentation-machine-learning/ ","version":"Next","tagName":"h3"},{"title":"Image Editing","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/image-editing","content":"","keywords":"","version":"Next"},{"title":"Adobe Photoshop​","type":1,"pageTitle":"Image Editing","url":"/cs-notes/digital-media-processing/image-editing#adobe-photoshop","content":" Adobe Photoshop is a commonly used and popular editing software.  Photoshop Layer​  Photoshop is a layer-based editing system. Layer can be thought as transparent sheet that are stacked on top of each other. These layer stack are responsible for which layer should be visible first. Each layer in Photoshop has its own properties, such as opacity, blending mode, and effects. In real case, a part of image can be stacked so that the part can be edited individually without affecting others.  Layer can also be used as non-destructive editing, this mean we don't modify the original image at all, instead we just add layer on top of it. This way we can undo a changes that has been made   Source : https://www.desainku.info/2020/01/mengenal-layer-pada-adobe-photoshop.html  Example of Changing Background​  For example, consider a person photo with sky background behind. We can modify the sky to be a space background. We don't want to modify the person, so we will first identify which section belong to the person. Photoshop has tool called &quot;selection tool&quot;, it is used to isolate specific areas or objects within an image.  After isolating it, we can move it to another layer. By separating it to the background layer, we can change the background with a blue color and put the person layer again.   Source : https://www.bwillcreative.com/how-to-change-background-color-in-photoshop/ ","version":"Next","tagName":"h3"},{"title":"Image Enhancement","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/image-enhancement","content":"Image Enhancement Main Source : Various source from Google and Youtube Image Enhancement is the process of improving the visual quality of an image by adjusting its properties. There are many properties to be adjusted and many techniques to use. Image enhancement is different with image editing. Image editing is a bigger term that includes changing the content of an image such as removing objects or changing the background. On the other hand image enhancement doesn't change the content, it adjusts the properties of image. Source : https://www.behance.net/gallery/115047653/PORTRAIT-EDIT-BEFORE-AFTER Brightness Adjustment : Modify the overall brightness level of an image. It involves increasing or decreasing the intensity of pixel values to make the image appear brighter or darker. The simple method is to add a constant value to all the pixel. Other method such as gamma correction or histogram equalization also works. Source : https://www.gifgit.com/image/adjust-image-brightness Histogram Equalization : This is a method to transform pixel intensity of an image by stretching it. The color of image is represented in histogram, then the central distribution will be spread out. Source : https://commons.wikimedia.org/wiki/File:Histogram_equalization.png Contrast Enhancement : It aims to improve the visual contrast and distinguishability of details in an image. The method are : linear contrast transformation, histogram equalization like the previous. Linear Contrast Stretching : maps original pixel values to a new range of values. For example a color ranging between 0 to 255 will be mapped to 50 to 255. This effect will be an increase in brightness or lightness of the image. Source : https://theailearner.com/2019/01/30/contrast-stretching/ Color Correction : Correct the colors of an image by altering the color values of an image. Color correction includes : balancing white color to effect overall warmth or coolness of the colors, removing unwanted color in the scene, adjusting hue and saturation, adjusting color using curves to better smoothen the color. Color correction differs with color grading. The goal of color correction is to fix color related issues, while color grading enhance the color to achieve desired mood. Source : https://www.studiobinder.com/blog/color-grading-vs-color-correction-process/ Image Filtering : Applies a set of filters which is a mathematical function to the pixel values of the image. Find out more in Computer Graphics Image Filters. Source : https://www.cosmo.ph/entertainment/how-to-choose-the-best-instagram-filter Sharpening : Enhance the perceived sharpness and clarity of edges and fine details in an image. Sharpening is typically achieved by applying a sharpening filter. Source : https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html Noise Reduction : Process of reducing or removing unwanted noise from an image. Noise is an unwanted signal or data, in the case of image, the random pixel color that degrades image quality and affect visual clarity. Find out more in DSP Denoising Source : https://www.dvdfab.cn/resource/video/denoise","keywords":"","version":"Next"},{"title":"Image Properties","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/image-properties","content":"Image Properties Main Source : Various source from Google and Youtube Computers represent images as a collection pixels which is the smallest unit of an image. A pixel represent color or intensity in a specific location. Image with pixel are called raster images. tip Find out more about images representation in Computer Images (Part 1) and Computer Images (Part 2) A image has more than just a color, it includes various properties that describe more of the image characteristics and attributes. Some of the information such as date taken, image format and compression methods, size, and etc; about image is stored in metadata. Metadata is typically stored in the image file header itself. Here are some image properties : Size / Resolution : The dimensions of the image, related to amount of pixels present in image, measured by width x height. The higher resolution the more detailed the image is. Source : https://vimeo.com/blog/post/the-basics-of-image-resolution/ Color Space : Represent how the color are represented, such as RGB, RGBA, CMYK, HSV or grayscale. Source : https://en.wikipedia.org/wiki/HSL_and_HSV Bit Depth : Measure of how many number of bits used to represent each pixel in the image. A common bit depth is 8 bit, it is able to represent 256 different color from 0-255. With red, green, blue channel altogether represent 16.777.216 different color. Source : https://youtu.be/6yXYxp0UiVg Image Format : The image format used, such as JPEG, PNG, TIFF, BMP, GIF, or RAW. They represent different feature and compression methods. Spatial Domain : Properties that describe the spatial characteristics of the image, such as brightness, contrast, sharpness, texture, edges, and patterns. Brightness : Overall lightness or darkness of an image. Contrast : The difference of brightness between the lightest and darkest parts of an image. Higher contrast make lighter part and darker part easier to differentiate. Alpha : Transparency or opacity of an image or a specific pixel within an image. Typically in RGBA color space where the value range between 0 (invisible) to 1 (visible). Gamma : Measure of the contrast between the darker and lighter parts of an image. A higher gamma value means that the darker parts of the image will appear darker, while a lower gamma value means that the darker parts of the image will appear lighter. Gamma correction is an adjustment applied to image pixel values to compensate for differences in the way human eyes perceive brightness. Saturation : Saturation controls the intensity or purity of colors in an image. A high saturation value produces vibrant and vivid colors, while a low saturation value results in more muted or grayscale-like colors. Hue : Represents the color itself, such as red, blue, green, etc. It denotes the position of a color on the color wheel. Dynamic Range : Range of brightness levels that can be represented in an image, from the darkest shadows to the brightest highlights. Sharpness : Represents how abrupt is the changes of a pixel color to another. Color Temperature : Color temperature is a measure of the warmth or coolness of a light source. It is measured in degrees Kelvin. Warm light sources, such as candles and incandescent bulbs, have a low color temperature, while cool light sources, such as fluorescent bulbs and sunlight, have a high color temperature. Source : Brightness,Contrast,Alpha,Gamma,Saturation, Hue,Dynamic Range,Sharpness,Color Temperature tip Find out more about computer display here","keywords":"","version":"Next"},{"title":"Image Restoration","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/image-restoration","content":"","keywords":"","version":"Next"},{"title":"How does it works​","type":1,"pageTitle":"Image Restoration","url":"/cs-notes/digital-media-processing/image-restoration#how-does-it-works","content":" Image restoration works by reversing the process, this will including analyzing the image.  For example if we have a blurred image, the image must lack of sharpness between the pixel color. We will have to figure out what was the method and the parameters such as the blur size, orientation, or intensity used to blur the image.  We will try to estimate the parameter and apply the reverse blur to the image. One of the technique to estimate is the Point Spread Function (PSF).  PSF works by treating the blurring process as an optical system. It describe how a point object is spread out by an optical system, such as a camera or microscope. It describes the intensity of light at each point in the image plane. PSF can be thought of as a mathematical representation of the blurring process.  After getting the blur approximation, we can apply reverse blur which involves predicting the original pixel values for each pixel in the blurred image.  A reverse blurred image may introduce another artifacts, so a further enhancement may be done.   Source : https://en.wikipedia.org/wiki/Point_spread_function  ","version":"Next","tagName":"h3"},{"title":"Image Restoration by AI​","type":1,"pageTitle":"Image Restoration","url":"/cs-notes/digital-media-processing/image-restoration#image-restoration-by-ai","content":" Image restoration can also use AI. The AI will be trained with large dataset of images that have been labeled with their ground truth (original) state. This allows the algorithms to learn the patterns that are present in natural images and to use this knowledge to restore degraded images.  We can also use AI for the enhancement technique such as increasing the resolution, denoising, recoloring, and etc.   Source : https://developer.nvidia.com/blog/ai-can-now-fix-your-grainy-photos-by-only-looking-at-grainy-photos/ ","version":"Next","tagName":"h3"},{"title":"JPG / JPEG","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/jpg-jpeg","content":"","keywords":"","version":"Next"},{"title":"JPG Structure​","type":1,"pageTitle":"JPG / JPEG","url":"/cs-notes/digital-media-processing/jpg-jpeg#jpg-structure","content":" JPG file are divided to several segment, each begins with marker as the indicator.  File Header : The JPG file begins with a file header that contains markers indicating the start of the JPG image. The most common marker is the Start of Image (SOI) marker (FF D8). Application and Comment Markers : JPG files can include markers that provide additional information or comments about the image. For example, the APP0 marker (FF E0) is often used to indicate the JFIF (JPG File Interchange Format) segment, which contains metadata about the image, such as resolution and pixel aspect ratio. Comment markers (COM) can be used to store textual comments. Quantization Tables : The Quantization Tables (DQT) segment (FF DB) contains the quantization tables used during compression. These tables define the quantization values applied to the DCT coefficients during the compression process. Huffman Tables : The Huffman Tables (DHT) segment (FF C4) contains the Huffman coding tables used for entropy encoding. Huffman coding is a lossless compression technique applied to the quantized DCT coefficients. Image Information : The image data is represented by one or more compressed image segments. These segments include the Start of Frame (SOF) segment (FF C0 or FF C2), which contains information about the image size, color space, and other parameters. The Scan (SOS) segment (FF DA) specifies the order and location of the image components, including the Huffman tables used for each component. End of Image : The JPG file ends with an End of Image (EOI) marker (FF D9), indicating the completion of the JPG image.   Source : https://stackoverflow.com/questions/48669812/how-do-i-read-and-compare-single-bytes-from-jpeg-file-in-c  ","version":"Next","tagName":"h3"},{"title":"JPG Compression​","type":1,"pageTitle":"JPG / JPEG","url":"/cs-notes/digital-media-processing/jpg-jpeg#jpg-compression","content":" Many technique are used to compress JPG, including lossy and lossless compression. Here are some techniques :  Color Space Conversion : Color space is the specific color format to represent image. Typically RGB is used, however in JPG, it is converted to YCbCr color space. YCbCr separates luminance (Y), blue chrominance channel (Cb) and red chrominance channel (Cr). Chrominance is the measure how vary is the color from the neutral gray level of the image. The reason why it uses YCbCr color space is because human visual system is more sensitive to changes in brightness than color, by using this color space, we can separate the color into these component allowing us to be able to compress more unrelevant data. Source : https://developingdaily.com/article/technology/rgb-vs-ycbcr/398 Chrominance Subsampling : This is the compression that take advantage over human visual system after color space is converted to YCbCr. Basically the chrominance are reduced or downsampled. This won't affect image quality significantly. Source : https://www.videomaker.com/article/f6/15788-the-anatomy-of-chroma-subsampling/ DCT &amp; Quantization : In simple term, DCT and quantization replace color that changes often with the neighbour color. In other words, it groups similar colors together to reduce data. Entropy Coding : Entropy coding is a lossless compression, it is a combination between run-length encoding optimized with zigzag scanning and Huffman encoding. Zigzag scanning is used to optimize run-length encoding because, by scanning in a zigzag pattern, we avoid directly jumping to the left every time we reach the right boundary of the image. In the zigzag pattern, we can scan a closer group of colors, which makes run-length encoding more efficient as it can include duplicates within the color group. Source : https://en.wikipedia.org/wiki/JPEG ","version":"Next","tagName":"h3"},{"title":"Markdown (MD)","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/markdown-md","content":"","keywords":"","version":"Next"},{"title":"Markdown Parsing​","type":1,"pageTitle":"Markdown (MD)","url":"/cs-notes/digital-media-processing/markdown-md#markdown-parsing","content":" Markdown works by analyzing plain text and converting it into HTML while also applying format accroding to the symbol used. Here is the simplified process :  Tokenization : The first thing to do is to break down the input Markdown text into individual tokens. Tokens are the elements of the Markdown syntax, such as headers, lists, paragraphs. This step is often done using regular expressions or other pattern matching techniques. Parsing : Once the tokens are identified, the Markdown processor analyzes their structure and relationships to build a hierarchical representation of the document. It identifies the nesting of elements, such as nested lists, and creates a data structure (usually tree) to represent the document's structure. Conversion : After the document structure is determined, the Markdown processor applies transformation rules to convert the Markdown tokens and structure into the desired output format. For example, it may convert headers to HTML heading tags, lists to HTML lists, or inline formatting to appropriate HTML tags or styles. Rendering : The converted output is rendered or displayed according to the target medium. For web-based applications, the rendered output may be displayed directly in the browser.   Source : https://accu.org/journals/overload/26/146/balaam_2532/ ","version":"Next","tagName":"h3"},{"title":"MIME Type","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/mime-type","content":"","keywords":"","version":"Next"},{"title":"MIME type syntax​","type":1,"pageTitle":"MIME Type","url":"/cs-notes/digital-media-processing/mime-type#mime-type-syntax","content":" MIME type syntax looks like the following :mime-type = type &quot;/&quot; [tree &quot;.&quot;] subtype [&quot;+&quot; suffix]* [&quot;;&quot; parameter];  type is the general category of the data, such as text, image, audio, or video.tree is an optional prefix that can be used to further categorize the data. For example, the type application can have the tree x- to indicate that it is an experimental format.subtype is the specific format of the data, such as plain text, HTML, PDF, MS word documentsuffix is an optional suffix that can be used to represent specific structure of the format. For example, the suffix xml can be used to classify XML data, and the suffix json can be used to classify JSON data.parameter is an optional parameter that can be used to provide additional information or metadata about the data. For example, the parameter charset can be used to specify the character encoding of the text data.  For example, a MIME type for JPEG file is image/jpeg; charset=utf-8  Other example :  text/plain : plain text.image/jpeg : JPEG images.audio/mp3 : MP3 audio.video/mp4 : MP4 videos.application/pdf : PDF documents.application/msword : Microsoft Word documents.application/zip : ZIP archives. ","version":"Next","tagName":"h3"},{"title":"MP3","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/mp3","content":"","keywords":"","version":"Next"},{"title":"MP3 Structure​","type":1,"pageTitle":"MP3","url":"/cs-notes/digital-media-processing/mp3#mp3-structure","content":" MP3 file is made up of a sequence of frames, each of which consists of a header and a data block.  Header : The file begins with an MP3 header that contains important information about the file, such as the audio format, bitrate, sampling rate, stereo/mono mode, and more. The header provides essential details for decoding and playing the file correctly. Audio Frames : Each frame represents a small segment of audio and contains compressed audio data. The size of each frame can vary depending on the bitrate and other settings used during encoding. The frames are usually consistent in size throughout the file. Side Information : For each audio frame, there is corresponding side information that provides details about the frame's structure and compression parameters. This information includes data such as the scale factors, Huffman coding tables, and stereo/mono mode information. Main Data : The main data section contains the compressed audio data. It includes the actual audio samples that have been encoded and compressed using various techniques. Ancillary Data : MP3 files can also include ancillary data, such as ID3 tags, which store additional metadata about the audio file, such as the song title, artist, album, and other details. These tags provide information that can be displayed by media players.   Source : https://en.wikipedia.org/wiki/MP3  ","version":"Next","tagName":"h3"},{"title":"MP3 Compression​","type":1,"pageTitle":"MP3","url":"/cs-notes/digital-media-processing/mp3#mp3-compression","content":" MP3 is known for its compression, it can commonly achieve a 75 to 95% reduction in size.  Transform Coding : MP3 uses a technique called the Modified Discrete Cosine Transform (MDCT) to convert the audio signal from the time domain to the frequency domain. The MDCT breaks down the audio signal into frequency components, allowing for more efficient compression of the spectral information. Psychoacoustics Modeling : Psychoacoustics Modeling is a technique used to remove less relevant data by analyzing human hearing. This can include removing frequency and components that are masked by other louder or more important sounds. Quantization : Quantization is the process of assigning a limited number of bits to represent the amplitude of the audio samples. In MP3, quantization process also takes account the psychoacoustic modeling. Huffman Encoding : Huffman encoding is a technique to compress data by assigning shorter codes to frequently occurring audio values and longer codes to less frequent values, reducing the overall number of bits required to represent the data.   Source : https://ledgernote.com/blog/q-and-a/how-does-mp3-compression-work/ ","version":"Next","tagName":"h3"},{"title":"MP4","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/mp4","content":"","keywords":"","version":"Next"},{"title":"MP4 Structure​","type":1,"pageTitle":"MP4","url":"/cs-notes/digital-media-processing/mp4#mp4-structure","content":" MP4 consists of a file header and multimedia data that are organized into containers called atoms or boxes. These atoms represent the actual multimedia data and can also contain other nested atoms.  Atoms have variable sizes and specific types. The minimum size of an atom is 8 bytes. The first 4 bytes of an atom indicate its size, while the following 4 bytes specify the atom's type.  File Header : This contains the file type, version, and other information about the file.Atoms : The main components of an MP4 file comes after the header, it could contain many atoms such as : ftyp (file type) : Specify the file type and the common data structures used.moov (movie) : Contains the movie metadata, such as the video and audio tracks, the timecode, and the tracks properties.mdat (media data) : Contains the media data, such as the video and audio samples.udta (user data) : Contains the user-defined metadata.trak (track) : Contains information about a single track, such as the track's type, the track's timecode, and the track's samples.   Source : https://www.trekview.org/blog/2022/injecting-camm-gpmd-telemetry-videos-part-2-mp4-overview/  ","version":"Next","tagName":"h3"},{"title":"MP4 Compression (H.264)​","type":1,"pageTitle":"MP4","url":"/cs-notes/digital-media-processing/mp4#mp4-compression-h264","content":" H.264, also known as AVC (Advanced Video Coding), is a the most commonly used lossy video compression. Like the general compression technique, H.264 removes redundancies and irrevelant information.  H.264 process each frame as &quot;block&quot; which is a fixed-size rectangular region within a frame of video. The block size can vary, example are 4x4, 16x16, or 32x32 pixels.  In H.264, two main technique of compression are used. Spatial compression compress each frame of video individually just like compressing an image. Temporal compression compress multiple of frames together, it takes advantage of similarities between consecutive video frames. The idea is that not all pixels within each frames changes. We can reference the same block of pixels to the previous or future frames.  Other techniques used in H.264 :  Transform Coding : H.264 uses transform coding, specifically the discrete cosine transform (DCT), to convert image data from the spatial domain to the frequency domain. This conversion allows for additional compression by representing the video content using fewer frequency components. Rate Control : H.264 can adjust bit rate for specific scenes. For example, we can use higher bit rate to complex or high-motion scenes and a lower bit rate to less complex or static scenes.   Source : https://www.eetimes.com/emerging-h-264-standard-supports-broadcast-video-encoding-2/ ","version":"Next","tagName":"h3"},{"title":"OGG Vorbis","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/ogg-vorbis","content":"","keywords":"","version":"Next"},{"title":"OGG​","type":1,"pageTitle":"OGG Vorbis","url":"/cs-notes/digital-media-processing/ogg-vorbis#ogg","content":" OGG is multimedia container format including audio, video, text, and metadata. As a container, OGG can hold multiple type of data, this is achieved by multiplexing. By supporting multiple type of data, OGG also supports multiple codecs for each data. Codecs is a software, device or program that is responsible for encoding and decoding a data stream or signal.  For example, an OGG file can contain audio, video, text, and metadata. The audio codec used is Vorbis, video codec is Theora, and the metadata provides additional information about the content such as title of the media, artist/author information, album/movie information, genres, and more.    ","version":"Next","tagName":"h3"},{"title":"OGG Structure​","type":1,"pageTitle":"OGG Vorbis","url":"/cs-notes/digital-media-processing/ogg-vorbis#ogg-structure","content":" OGG file consists of a sequence of pages. Each page begins with a 27-byte header, followed by a variable-length payload. Each page size is generally between 4 to 64 kilobytes.  OGG Header : The OGG file begins with an OGG header, which provides essential information about the file and its streams. The header includes details such as the OGG format version, the number of streams within the file, and the serial numbers assigned to each stream. Page Header : Each page starts with a page header that contains metadata about the page itself. The header includes information such as the granule position, which represents the position of the data within the stream, and the page sequence number. Packet Data : Following the page header, the page contains one or more packets of data. Each packet represents a chunk of encoded audio, video, text, or other multimedia data. The packets may belong to different streams within the file. Page Segments : The page segments section follows the packet data and specifies the sizes of the individual packets within the page. It allows for variable-sized packets within a fixed-sized page. Page CRC Checksum : Each page concludes with a cyclic redundancy checksum (CRC) value, which is used for error detection. In simple term, a data is calculated using mathematical function called hash function to produce a value. The checksum is when we check if the produced value is the same as the value of the data that should be. Metadata : Metadata can include details such as track titles, artist/author information, album/movie information, genres, and more. The metadata is typically stored in dedicated packets within the stream.   Source : https://en.wikipedia.org/wiki/Ogg * Image of OGG page header  ","version":"Next","tagName":"h3"},{"title":"OGG Vorbis​","type":1,"pageTitle":"OGG Vorbis","url":"/cs-notes/digital-media-processing/ogg-vorbis#ogg-vorbis","content":" Vorbis is the specific codecs for audio data in OGG file, it uses a lossy compression. The process of vorbis codecs begins with the audio data is divided into small sections called &quot;blocks&quot; or &quot;windows.&quot; Each block typically contains a few milliseconds of audio.  Encoding Process​  Psychoacoustic Modeling : The Vorbis codec applies a psychoacoustic model to analyze the audio within each block. This model takes into account the characteristics of human hearing and discard the less important audio. Transform : The audio data within each block is transformed from the time domain to the frequency domain using the Modified Discrete Cosine Transform (MDCT). The modified DCT is used for processing overlapping blocks of audio or video data. This transformation will map the audio to frequency domain. Quantization : The transformed audio data is quantized, meaning the amplitudes of the frequency components are approximated and represented with fewer bits. Encoding : The quantized audio data is further processed and encoded using variable bitrate encoding. The codec allocates more bits to preserve important audio details and fewer bits for less important parts, based on the psychoacoustic analysis. Bitstream Generation : The encoded audio data, along with metadata such as track information and tags, is packaged into a bitstream or sequence of bit to the OGG container format.  Decoding Process​  Bitstream Parsing : The Vorbis decoder reads and parses the encoded bitstream, extracting the compressed audio data and associated metadata. Decoding : The quantized and compressed audio data is decoded, reversing the encoding process. The inverse operations of quantization, inverse variable bitrate decoding, and inverse MDCT are applied to reconstruct the frequency representation of the audio. Time-domain Reconstruction : The frequency data is transformed back into the time domain through additional processing. Audio Playback : The decoded audio is played back through speakers or headphones, allowing the listener to hear the reconstructed audio. ","version":"Next","tagName":"h3"},{"title":"PNG","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/png","content":"","keywords":"","version":"Next"},{"title":"PNG Structure​","type":1,"pageTitle":"PNG","url":"/cs-notes/digital-media-processing/png#png-structure","content":" PNG consist of ordered component called chunks, each chunks represent information that makes up PNG file.  PNG Signature : The file starts with an 8-byte signature that identifies it as a PNG file. The signature helps software recognize and validate the file format. IHDR Chunk : The IHDR (Image Header) chunk stores essential information about the image, such as its dimensions (width and height), bit depth, color type, compression method, and interlace method. Ancillary Chunks : After the IHDR chunk, there can be an optional ancillary chunks that provide additional information and features. There are many types of ancillary chunks, common examples include: bKGD (Background) : The default background color.PLTE (Palette) : This chunk defines a color palette for indexed color images.tRNS (Transparency) : It specifies transparency information for the image, such as alpha values for individual palette entries or a single transparent color.pHYs (Physical Dimensions) : This chunk indicates the intended physical size of the image in terms of pixels per unit.tIME (Last Modification Time) : It stores the timestamp of the last image modification. Image Data : The actual image data comes after the header and optional ancillary chunks. The image data is compressed using a lossless compression, this chunk is called IDAT. IEND Chunk : The IEND (Image End) chunk marks the end of the PNG file. It serves as a marker to indicate that there are no more chunks following it. Source : https://github.com/HugoJH/HideIntoPNG  ","version":"Next","tagName":"h3"},{"title":"PNG Compression​","type":1,"pageTitle":"PNG","url":"/cs-notes/digital-media-processing/png#png-compression","content":" The PNG compression consist of two steps :  Filtering : The first step is to &quot;predict&quot;, it's pre-processing before the actual compression. The purpose of this step is to identify patterns or correlations between pixels. It works by applying prediction algorithm to each pixel data in scanline based. The error or the difference between the predicted and actual pixel value is also stored.  There are five types of filter method in PNG, the purpose of these are to capture pattern so that it can be compressed more efficiently :  None : When there is little or no spatial correlation in the image data, no filtering is applied. Each pixel value is stored as is. Sub : The pixel value is predicted based on the value of the pixel immediately before it in the same scanline. Up : The pixel value is predicted based on the value of the pixel above it in the previous scanline. Average : The pixel value is predicted based on the average of the pixel above it and the pixel immediately before it. Paeth : The pixel value is predicted based on a linear combination of the pixel above it, the pixel immediately before it, and the pixel diagonally above it.  DEFLATE : This is a combination of LZ77 and Huffman Encoding LZ77 : LZ77 scans through the input data and identifies repeated sequences of data called &quot;matches&quot; or &quot;phrases.&quot; Instead of storing each repeated sequence as is, LZ77 replaces them with references, consisting of an offset (indicating the distance to the start of the repeated sequence) and a length (representing the number of characters in the sequence). Source : https://www.researchgate.net/figure/An-example-of-LZ77-encoding_fig4_322296027 Huffman Encoding : After LZ77, huffman encoding is also applied to further reduce data by making frequent symbols in smaller size and less frequent symbols in larger size. ","version":"Next","tagName":"h3"},{"title":"PDF","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/pdf","content":"","keywords":"","version":"Next"},{"title":"PDF Structure​","type":1,"pageTitle":"PDF","url":"/cs-notes/digital-media-processing/pdf#pdf-structure","content":" PDF has 4 main component, the body is the actual content of the document.  Header : The header is the starting point of a PDF file and contains information about the version of the PDF specification. The header format looks like %PDF-1.x where x is the version. Body : The content of PDF file is self-contained in a container called object. The object can be text, images, fonts, or other types of data, it also supports other nested object. Cross-Reference Table (xref) : The cross-reference table contains a list of all objects in the file, their byte offsets, and their status (whether they are still in use or not). This table allows for efficient random access and updating of the file. Trailer : The trailer section provides essential information about the PDF file, including the location of the cross-reference table, the total number of objects in the file, the root object of the document, and the end of file marker.   Source : https://www.researchgate.net/figure/An-example-of-a-simple-PDF-file-structure-that-consists-of-one-page-that-contains-a_fig1_326102942 ","version":"Next","tagName":"h3"},{"title":"Speech Processing","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/speech-processing","content":"Speech Processing Main Source : Various source from Google and Youtube Speech Processing is the analysis, synthesis, and understanding of spoken language. It involves various techniques and algorithms including speech recognition, speech synthesis, speaker recognition, and speech enhancement. Speech processing involves a lot of machine learning and deep learning technique. Speech processing involves some steps, here is a high-level overview : Speech Capturing : The first thing is to capture the speech sound signal, we can use a microphone or other audio recording devices. Sound signal will be converted into digital using an analog-to-digital converter (ADC). Pre-processing : Digital signal is then pre-processed, this includes enhancing quality such as filtering, noise reduciton, and removing unwanted elements like background noise. Feature Extraction : Feature extraction means we identify and capture sound characteristics such as loudness, pitch, rhythm. This will be give us more information and it will be useful later on. Speech Recognition : This is the main process of speech processing, it includes : Acoustics Modeling : Predict the likelihood of a specific word or phrase being spoken given the current state of the audio signal based on probability. For example, after a subject is spoken, a verb will have a higher chance than an adjective to be spoken next. Language Modeling : Language modeling uses statistical properties and patterns of natural language to predict the next word or phrase that is likely to be spoken. The machine learning language model is trained alot of text data. Decoding : In the decoding stage, the acoustic and language models are combined to find the most likely sequence of words that matches the speech input. Source : https://www.kardome.com/blog-posts/difference-speech-and-voice-recognition tip Find more about speech processing related to Natural Language Processing (NLP)","keywords":"","version":"Next"},{"title":"Sound & Audio Properties","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/sound-audio-properties","content":"","keywords":"","version":"Next"},{"title":"Sound​","type":1,"pageTitle":"Sound & Audio Properties","url":"/cs-notes/digital-media-processing/sound-audio-properties#sound","content":" Sound is a form of energy that is produced when an object vibrates, creating waves of pressure in a medium such as air, water, or solids. The vibrating object will vibrate particle surrounding and the particle will vibrate it again to the next particle, this is why sound need medium to travel.   Source : https://blog.docsity.com/en/living/science-2/physics-sound-visual-representation-gifs/  When sound waves reach our ears, they cause our eardrums to vibrate, and these vibrations are then transmitted to the inner ear. The inner ear converts these mechanical vibrations into electrical signals that are sent to the brain, which processes them as sound, allowing us to perceive and interpret what we hear.  Sound vibration is measured in frequency (Hz) which refers to the number of cycles or vibrations per second. Higher frequencies are perceived as higher pitches, while lower frequencies are perceived as lower pitches. Human is able to hear sound in range of 20 Hz to 20 kHz (called audiosonic range).  ","version":"Next","tagName":"h3"},{"title":"Sound Properties​","type":1,"pageTitle":"Sound & Audio Properties","url":"/cs-notes/digital-media-processing/sound-audio-properties#sound-properties","content":" The behavior of sound wave that vibrates can be modeled in a sine or cosine wave. This brings sound to have some properties similar to wave :  Frequency : Number of cycles or vibrations per second and is measured in Hertz (Hz)Period : The time it takes for one cycle vibrationAmplitude : The maximum length of displacement, it measures strength or intensity of a sound wave.Wavelength : Physical distance between two corresponding points of a sound wavePhase : The position of a sound wave at a particular point in time within its cycle   Source : https://www.jagranjosh.com/general-knowledge/what-are-the-characteristics-of-sound-waves-1525678871-1 (cropped)  The loudness of sound are measured in dB (desibels), it express the relative level or intensity of a sound signal, electrical signal, or other quantity. It is a logarithmic scale that compares the magnitude of a quantity to a reference level.   Source : https://www.vecteezy.com/vector-art/7207013-decibel-scale-sound-levels  ","version":"Next","tagName":"h3"},{"title":"Audio​","type":1,"pageTitle":"Sound & Audio Properties","url":"/cs-notes/digital-media-processing/sound-audio-properties#audio","content":" Audio is the electronically reproduced or recorded sound. It involves the capture, processing, storage, and reproduction of sound using electronic devices and systems. Audio in digital is represented using binary data  Sound in analog form is first sampled. The sound will then be quantized or approximated to nearest representation and will be encoded in binary. After that, it will be transmit and be converted back to analog signal for human to hear.   Source : https://id.yamaha.com/id/products/contents/proaudio/docs/better_sound/part1_06.html  tip More about sampling, quantization in the signal transmission section of Digital Signal Processing.  ","version":"Next","tagName":"h3"},{"title":"Audio Properties​","type":1,"pageTitle":"Sound & Audio Properties","url":"/cs-notes/digital-media-processing/sound-audio-properties#audio-properties","content":" Digital audio introduces additional properties and effects that are specific to the digital domain.  Sampling Rate : Sample rate is the number of samples taken per second to represent an analog audio signal in the digital domain. Commonly used sample rates are 44.1 kHz and 48 kHz. A higher sample rate allows for more accurate representation of high-frequency content in the audio. Bit Depth : The bit depth determines the number of levels of amplitude that can be represented in the digital audio recording. A higher bit depth will capture more levels of amplitude, resulting in a smoother and more accurate representation of the original sound. Common bit depth include 16-bit, 24-bit, and 32-bit. Sound Enhancement &amp; Effect : We can modify sound signal to improve the quality or for specific desire. There are many tools and technique used, for example an Equalizer (EQ) used to adjust frequencies in an audio signal. Audio effect can also be applied to apply effect such as simulating echo, sound reflection, making the sound slower. Find out more in Audio Effect and Audio Equalization. Spatial Audio : Digital audio make it possible to create the illusion of sound coming from different direction, distance, and location of sound sources in a virtual or augmented environment. Audio Channels : Audio channel is a representation of sound coming from or going to a single point. A single microphone can be used to produce one channel of audio, while a single audio speaker can also accept one channel of audio. Mono : Uses a single audio channel to reproduce the entire audio signal, without any sense of stereo separation or directionality.Stereo : Stereo uses two audio channels, typically labeled as left (L) and right (R), to create a sense of spatial separation and directionality.Surround Sound : Contains more than two channels of audio. This allows for a more immersive audio experience, with sound coming from all around the listener. Dynamic Range Compression : Dynamic range of an audio signal is the range between the quietest and loudest parts. A wide range of sound means that it can either have a very loud sound or a very quiet sound. This operation can reduces the volume of loud sounds or amplifies quiet sounds, it helps to create more consistent level throughout the audio. Noise Reduction : Audio processed in digital can also be denoised, this can help improve the quality of sound compared to a live sound. ","version":"Next","tagName":"h3"},{"title":"Text File (txt)","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/text-file-txt","content":"","keywords":"","version":"Next"},{"title":"txt File Representation​","type":1,"pageTitle":"Text File (txt)","url":"/cs-notes/digital-media-processing/text-file-txt#txt-file-representation","content":" txt file are stored in binary data encoded with specific character encoding scheme. txt file may also contains metadata for additional information such as file name, file size, creation date, last modified date, and file permissions. The specific metadata and where is it located depends on the operating system used.  For example, consider a text file that uses ASCII encoding and contains &quot;Hello, World!&quot;. In ASCII, the letter &quot;H&quot; is represented by the decimal value 72, &quot;e&quot; by 101, &quot;l&quot; by 108, &quot;o&quot; by 111, comma (&quot;,&quot;) by 44, space by 32, and so on. We can then transform each ASCII value of the text to binary.  While decoding it, we will reverse the process, for example if we encounter binary data of &quot;01000001&quot;, this means it is &quot;A&quot;. We will keep going until the last piece of binary.   Source : https://www.thecrazyprogrammer.com/2018/05/difference-between-text-file-and-binary-file.html ","version":"Next","tagName":"h3"},{"title":"SVG","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/svg","content":"","keywords":"","version":"Next"},{"title":"SVG Vector​","type":1,"pageTitle":"SVG","url":"/cs-notes/digital-media-processing/svg#svg-vector","content":" SVG defines a coordinate system where the origin (0,0) is typically at the top-left corner of the SVG viewport. The x-axis extends horizontally to the right, and the y-axis extends vertically downwards.  SVG uses various shape elements, such as &lt;rect&gt;, &lt;circle&gt;, &lt;ellipse&gt;, &lt;line&gt;, and &lt;path&gt;. Each shape element is represented by an XML tag and associated attributes that define its size, position, and other properties. SVG draws these shape using coodinate system, this whats make SVG scalable.   Source : https://developer.mozilla.org/en-US/docs/Web/SVG/Tutorial/Positions  ","version":"Next","tagName":"h3"},{"title":"SVG Structure​","type":1,"pageTitle":"SVG","url":"/cs-notes/digital-media-processing/svg#svg-structure","content":" SVG follows the tree hierarchical structure of XML :  &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;...&quot; height=&quot;...&quot; viewBox=&quot;...&quot;&gt; &lt;!-- SVG content goes here --&gt; &lt;/svg&gt;   &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; : Begins with the version and encoding declaration of XML.svg : Followed by svg tag as the root element, that includes namespace, width and height of the SVG file, viewBox that used to specify the portion of the SVG canvas that should be visible and scaled to fit the available space.  For example, this is an SVG file that defines a red filled heart shape :  &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;100&quot; height=&quot;100&quot; viewBox=&quot;0 0 100 100&quot;&gt; &lt;path d=&quot;M50 90c-15-15-30-30-30-50c0-20 20-30 30-40c10 10 30 20 30 40c0 20-15 35-30 50z&quot; fill=&quot;red&quot; /&gt; &lt;/svg&gt;   &lt;path&gt; : We specify the path we want to draw, the d attribute indicate the command. The path uses letter as the command to draw which kind of path or curves. This includes moveto (M), lineto (L), curveto (C), and closepath (Z). The number is used to specify the coordinate with - as negative number. The uppercase indicate that we move to absolute coordinate while the lowercase is relative to last coordinate. ","version":"Next","tagName":"h3"},{"title":"Video Effects & Enhancement","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/video-effects-enhancement","content":"Video Effects &amp; Enhancement Main Source : Wikipedia Visual effects Video Effects is a visual alterations applied to video to create artistic or stylistic changes, simulate specific environments or conditions, or convey a particular mood or atmosphere. Video effects are related to image enhancement, it uses the similar principle to achieve desired behavior. Video can also be enhanced, this includes denoising, upscaling, and restoration. The concept used are similar to image processing. Think of it as applying those image enhancement technique to each frame of the video. Example of video effects : Color Grading : Adjusting the color and tone of a video to create a specific look or mood. Source : https://www.studiobinder.com/blog/color-grading-vs-color-correction-process/ Transitions : Transitions are effects used to smoothly transition between two video clips or scenes. For example, a fade effect between two clips of video gradually lower down the transparency or opacity of the first clip, removing it from the scene and slowly increase the opacity of the second clip. Source : https://www.premiumbeat.com/blog/the-hidden-meaning-behind-popular-video-transitions/ Filters : Filter has various effect such as adjusting the color balance, brightness, contrast, saturation, and overall color tone of the video, sharpening or blur effect, and other stylized filter. Source : https://support.microsoft.com/en-au/topic/how-to-add-filters-to-a-video-7105a494-0d10-4bb1-ac41-1770606498d1 Motion Animation : Motion animation creates illusion of movement of object. There are many ways to achieve this, a common one is through the use of keyframes. Keyframes are settings that are applied at specific points, these settings are responsible for modifying position, scale, rotation, opacity, etc. Slow Motion : Slow motion reduces the speed of the video, some ways to achieve are stretching the duration of the video clip by specific amount, other ways is to capture the video at higher frame rate and then play it back at lower frame rate. Source : https://www.pinterest.com/pin/16-slowmo-gifs-that-are-pretty-cool--815433076268236747/ Green Screen : Also known as chroma key, is a popular technique used in video production to replace a specific color (usually green) in a video footage with another image or video. The idea is the editing software and the user agree on a color, the color will then be made transparent by the software. After that, we can use the transparent background to replace it with another footage. Source : https://makeavideohub.com/green-screen/what-is-a-green-screen-and-how-does-it-work.html Special Effects : Special effect includes Visual Effects (VFX) and Computer-Generated Imagery (CGI). These are made possible through computer graphics and simulation. tip Find more about animation in Computer Animation and Simulation","keywords":"","version":"Next"},{"title":"WAV","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/wav","content":"","keywords":"","version":"Next"},{"title":"WAV Structure​","type":1,"pageTitle":"WAV","url":"/cs-notes/digital-media-processing/wav#wav-structure","content":" WAV is a subset of Resource Interchange File Format (RIFF), this means WAV follows the structure of RIFF format that consists of a file header, a sequence of data chunks, and a trailer. Each chunk has a unique identifier (FourCC - Four Character Code) and a size field that indicates the length of the chunk's data.  RIFF Header : The RIFF header serves as the starting point of the WAV file. It identifies the file as a RIFF file and specifies the overall file size. It consists of the following fields : Chunk ID : A four-byte identifier indicating the type of chunk (usually &quot;RIFF&quot;).Chunk Size : A four-byte value specifying the size of the entire file in bytes.Format : A four-byte identifier indicating the file format (usually &quot;WAVE&quot;). Format Chunk : The format chunk provides information about the audio format used in the WAV file. The format chunk consists of: Chunk ID : A four-byte identifier indicating the type of chunk (usually &quot;fmt &quot;).Chunk Size : A four-byte value specifying the size of the format chunk in bytes.Audio Format : A two-byte value indicating the audio format (e.g., PCM = 1).Channels : A two-byte value specifying the number of audio channels or the output source of the audio.Sample Rate : A four-byte value indicating the audio sample rate.Byte Rate : A four-byte value representing the average number of bytes per second.Block Align : A two-byte value specifying the number of bytes for each audio block (sequence of bytes).Bits Per Sample : A two-byte value indicating the number of bits used to represent each audio sample. * PCM : Pulse Code Modulation, this means audio is sampled at uniform intervals. Data Chunk : The data chunk contains the actual audio samples. It includes the audio waveform data in binary format. The data chunk consists of: Chunk ID : A four-byte identifier indicating the type of chunk (usually &quot;data&quot;).Chunk Size : A four-byte value specifying the size of the audio data chunk in bytes.Audio Data : The actual audio samples, stored as binary data. Optional Metadata Chunks : WAV files may include optional metadata chunks that provide additional information about the audio file. These chunks can contain details like track titles, artist names, album information, and more. Common metadata chunk types include &quot;LIST&quot; and &quot;INFO&quot;.   Source : http://soundfile.sapp.org/doc/WaveFormat/ ","version":"Next","tagName":"h3"},{"title":"Video Recording","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/video-recording","content":"","keywords":"","version":"Next"},{"title":"Screenshot​","type":1,"pageTitle":"Video Recording","url":"/cs-notes/digital-media-processing/video-recording#screenshot","content":" A screenshot is a digital image or snapshot of what is currently displayed on a computer or mobile device's screen. It captures a static image of the screen contents, including any open windows, applications, or displayed content such as images, text, or videos.  Screenshot works by capturing the current state of screen's pixels data. At low level, the operating system request an access to framebuffer which is a portion of the computer's memory that stores the pixel data representing the screen contents. The framebuffer contains information about each pixel's color and position on the screen.  The operating system reads the pixel data from the framebuffer and copies it to a temporary storage area in memory. The pixel data is then encoded to an image format such as PNG.    ","version":"Next","tagName":"h3"},{"title":"Screencast​","type":1,"pageTitle":"Video Recording","url":"/cs-notes/digital-media-processing/video-recording#screencast","content":" While a screenshot takes a single image, to be able to record a video which is a sequence of image, we need to capture the screen a bunch of time to create a continous sequence of image.  The idea of screencast to screenshot is similar as image to video. Screencast is made by a bunch of screenshot captured multiple times per second. The number of how many screenshot is taken is called frame rate, same as the terminology of sequence of image in video.  The computer takes screenshot at regular intervals, such as 24 or 60 times per second. This will create a video with 24 or 60 FPS. Along the recording, recording software may also capture external audio from microphone. The video and audio is then combined together and encoded in format such as MP4.    ","version":"Next","tagName":"h3"},{"title":"Livestreaming​","type":1,"pageTitle":"Video Recording","url":"/cs-notes/digital-media-processing/video-recording#livestreaming","content":" Livestreaming is a real-time broadcasting of video and audio content over the internet. Livestreaming involves capturing user's screen just like screencasting, the result of video is then sent over the network to some livestreaming service such as Youtube. The livestreaming service will then broadcast it to user anywere.  The encoded audio and video data is divided into smaller packets for transmission over the network. The packets are then sent over the network using protocols such as RTC. The packets are transmitted in real-time or near real-time to maintain the live aspect of the stream.  Bit Rate​  In livestreaming, bit rate is the measurement of how many bits are processed or transmitted in a given period of time. The higher the bitrate, the better the quality of the stream will be, but the more bandwidth it will require.  A low bitrate means we need to represent the video in a limited amount of data (bits). This can make some of the detail are skipped to adjust with the available data.   Source : https://restream.io/blog/what-is-video-bitrate/  tip More about real-time communication here ","version":"Next","tagName":"h3"},{"title":"Video Representation","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/video-representation","content":"Video Representation Main Source : Computer Graphics : Computer Display Video is basically a sequence of image. An image is stationary, however, if we keep changing the image with a small change in the detail of image, we will be able to create illusion of animation. The image is called frame and the measurement of how many frame is usually measured per second as FPS (frame per second). The more FPS will result in smoother and better animation. Source : https://gfycat.com/discover/fps-comparison-gifs tip More about FPS and some standard for display here Resolution​ Because video is an image, it also has resolution which is the number of how many pixel exist in an image. Resolution is measured in width x height of the image, the higher resolution would result in a higher quality image but also takes more space in memory. Common video resolution include : Standard Definition (SD) : 720x480 pixels (NTSC) or 720x576 pixels (PAL)High Definition (HD) : 1280x720 pixels (720p) or 1920x1080 pixels (1080p)Ultra High Definition (UHD) : 3840x2160 pixels (4K UHD) or 7680x4320 pixels (8K UHD) The resolution also affected by the display size. For example, a small monitor that displays 720p image may looks fine, but a bigger monitor that displays the same image may appear blurry or less sharp and much loss of detail. This is because in a bigger monitor, individual pixels are spread out over a larger physical area. The measurement of how many pixel are spread out is PPI (pixels per inch), a bigger monitor will have more PPI. Source : https://www.displayninja.com/what-is-pixel-density/ tip Find more about video related to computer display here and about computer animation here","keywords":"","version":"Next"},{"title":"WebP","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/webp","content":"","keywords":"","version":"Next"},{"title":"WebP Structure​","type":1,"pageTitle":"WebP","url":"/cs-notes/digital-media-processing/webp#webp-structure","content":" RIFF Header : A WebP file begins with a RIFF (Resource Interchange File Format) header, which identifies the file as a WebP file. The RIFF header is four bytes long and contains the ASCII characters &quot;RIFF&quot;. File Size : Following the RIFF header is a four-byte value that specifies the total size of the WebP file, including the RIFF header itself and the file content. WebP FourCC : The next four bytes represent the FourCC (Four-Character Code) identifier for WebP files, which is &quot;WEBP&quot;. WebP Chunk : After the FourCC identifier, the WebP file contains a chunk structure that consists of multiple chunks. Each chunk contains a chunk header followed by chunk data. Chunk Header : Each chunk has a four-byte FourCC identifier and a four-byte value indicating the size of the chunk data. Chunk Data : The chunk data stores the actual content of the chunk, such as image data metadata, or other relevant information. The size of the chunk data is determined by the chunk header. These chunks are : VP8/VP8L/VP8X Chunk : This chunk stores the compressed image data using VP8, VP8L (lossless), or VP8X (extended) compression formats. ICCP Chunk : This chunk contains an embedded ICC profile, which provides color management information. ANIM Chunk : The ANIM chunk is used to store animation-related information in WebP Animation format (WebPANIM). ALPHA Chunk : This chunk stores the transparency data for an image with an alpha channel. Metadata Chunk : WebP files can include metadata chunks for storing additional information, such as EXIF data, XMP data, or custom metadata. File Footer : The WebP file may end with an optional file footer that contains additional information or markers related to the WebP file.  ","version":"Next","tagName":"h3"},{"title":"WebP Compression​","type":1,"pageTitle":"WebP","url":"/cs-notes/digital-media-processing/webp#webp-compression","content":" Overall, WebP uses similar technique as JPG compression, however, there are small difference.  Predictive Coding : WebP uses predictive coding to reduce redundant information within an image. It predicts pixel values based on neighboring pixels and encodes the difference between the predicted and actual pixel values. By knowning the differences between pixel values, we can capture the pattern or correlation to achieve better compression. YUV Color Space Conversion : YUV is similar to YCbCr in JPG, the difference is YUV valus are centered around 0, ranging from (-127 to 128) while YCbCr values are centered around 128, ranging from (0 to 256).   Source : https://developers.google.com/speed/webp/docs/compression ","version":"Next","tagName":"h3"},{"title":"Digital Signal Processing","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing","content":"","keywords":"","version":"Next"},{"title":"All pages​","type":1,"pageTitle":"Digital Signal Processing","url":"/cs-notes/digital-signal-processing#all-pages","content":" SignalFourier Analysis Fourier SeriesFourier TransformConvolutionDiscrete Fourier TransformFast Fourier TransformDiscrete Cosine TransformWaveletsLaplace TransformZ Transform Signal Transmission SamplingQuantizationCodingMultiplexingSignal Transmission Medium Signal Processing FilteringCompressionDenoising ","version":"Next","tagName":"h3"},{"title":"Coding","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/coding","content":"","keywords":"","version":"Next"},{"title":"Digital Coding​","type":1,"pageTitle":"Coding","url":"/cs-notes/digital-signal-processing/coding#digital-coding","content":" In digital world such as media processing, encoding involves the conversion of media data, such as audio, video, or images. After encoding, the data can be further processed to remove redundancies and to represent the data more efficiently by applying some compression techniques.  Decoding, also known as playback or rendering, is the process of converting an encoded media file back into its original format for playback. For example, in the case of a music player, before playing the music, the player needs to decompress the original music files. After that, the music will output the sound through the speakers.   Source : https://youtu.be/qSEmEpv5ct8?t=269  ","version":"Next","tagName":"h3"},{"title":"Unit of Data​","type":1,"pageTitle":"Coding","url":"/cs-notes/digital-signal-processing/coding#unit-of-data","content":" In the digital world, a bit is the smallest unit of data that can be stored or transmitted. It can be either a 0 or a 1, which represents two possible states.  Bits are grouped together to form bytes. A byte is a group of 8 bits, and it can represent 256 different values. Bytes are used to store characters, numbers, and other data.  The amount of data transferred is called Bandwidth   Source : https://semiconductor.samsung.com/support/tools-resources/dictionary/bits-and-bytes-units-of-data/  ","version":"Next","tagName":"h3"},{"title":"Data Representation​","type":1,"pageTitle":"Coding","url":"/cs-notes/digital-signal-processing/coding#data-representation","content":" Information Interchange​  To represent character, text, or number, there is a standard used. ASCII, or American Standard Code for Information Interchange, is a character encoding standard for electronic communication ASCII was first published in 1963 by the American Standards Association (now known as ANSI).  ASCII works by assigning a unique 7-bit binary number to each character. This allows for a total of 128 different characters, including letters, numbers, punctuation marks, and control characters.  There is ASCII table, which is a reference table that shows how the 128 characters that are defined based on the standard. For example using the ASCII table, character A is 65 and the binary equivalent is 01000001   Source : https://youtu.be/H4l42nbYmrU  ASCII has limited uses due to only being able to represent 128 different characters. In the modern world nowadays, ASCII is no longer used, instead we uses other standard called Unicode. Unicode is able to represent 149.186 different characters in the 15.0 version.  Image &amp; Color Representation​  Image is represented by a grid pixels. Pixel is the smallest unit of information in an image. A pixel typically owns 3 color value which is red, green and blue. Each color is represented as 1 byte or 8 bit (0-255), which means there are 256 different combination for each color. These three colors are combined together to construct an image, making it able to represent 16.777.216 different color in each pixels.  tip Find out more about image and color or about data representation in computer ","version":"Next","tagName":"h3"},{"title":"XML","type":0,"sectionRef":"#","url":"/cs-notes/digital-media-processing/xml","content":"","keywords":"","version":"Next"},{"title":"XML Component​","type":1,"pageTitle":"XML","url":"/cs-notes/digital-media-processing/xml#xml-component","content":" For example, suppose we want to transfer an employee data. We can define XML and the element tags as the follow :  &lt;employee&gt; &lt;name gender=&quot;male&quot;&gt;John Doe&lt;/name&gt; &lt;age&gt;30&lt;/age&gt; &lt;department&gt;Engineering&lt;/department&gt; &lt;/employee&gt;   employee is the root element. name, age, department are the child element. We then fill out the data surrounded by the element name. XML can also have attributes, which is an additional information or extra metadata about the element. The gender=&quot;male&quot; is the attribute of name element.  ","version":"Next","tagName":"h3"},{"title":"XML Schema​","type":1,"pageTitle":"XML","url":"/cs-notes/digital-media-processing/xml#xml-schema","content":" Document Type Definition (DTD)​  Before filling XML file with data or information, we need define the structure and the valid elements of an XML document. This is called DTD and it is declared within the XML document using the &lt;!DOCTYPE&gt; declaration.  For example, the previous XML example above may contains this DTD :  &lt;!DOCTYPE employee [ &lt;!ELEMENT employee (name, age, department)&gt; &lt;!ELEMENT name (#PCDATA)&gt; &lt;!ATTLIST name gender (male|female) #REQUIRED&gt; &lt;!ELEMENT age (#PCDATA)&gt; &lt;!ELEMENT department (#PCDATA)&gt; ]&gt;   &lt;!DOCTYPE&gt; : Define the root element &quot;employee&quot;.&lt;!ELEMENT&gt; : Declaration that define the structure of elements. It states that &quot;employee&quot; must contain &quot;name&quot;, &quot;age&quot;, and &quot;department&quot; elements. The &lt;!ELEMENT&gt; declarations for name, age, and department specify that they contain parsed character data (#PCDATA), which means they can contain text content.&lt;!ATTLIST&gt; : Specifies that the element has an attribute, the name has attribute called gender and must be either &quot;male&quot; or &quot;female&quot; using the (|). The attribute is required (#REQUIRED).  XML Schema (XSD)​  XML Schema, also known as XML Schema Definition (XSD), is the successor of DTDs, it is far more powerful than DTDs. XSD provide more way to define XML structure including data types, strong typing, complex data structures (such as nested elements and sequences), ability to define custom data types and complex constraints.  XSD also provide a way to distinguish between same XML schema definitions. It ensures that elements, attributes, and types defined within a specific schema are uniquely identified. The unique identifier is called XML namespace.  &lt;xs:schema xmlns:xs=&quot;http://www.w3.org/2001/XMLSchema&quot;&gt; &lt;!-- Schema definition goes here --&gt; &lt;/xs:schema&gt;   In the above example, a schema is defined with &lt;xs:schema&gt; &lt;/xs:schema&gt;, the attribute xmlns:xs= is the namespace. The namespace value http://www.w3.org/2001/XMLSchema means that the XML file is associated with that mentioned URI.  Combining with the same employee example from above, this is the schema :  &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;xs:schema xmlns:xs=&quot;http://www.w3.org/2001/XMLSchema&quot;&gt; &lt;xs:element name=&quot;employee&quot;&gt; &lt;xs:complexType&gt; &lt;xs:sequence&gt; &lt;xs:element name=&quot;name&quot; type=&quot;xs:string&quot;&gt; &lt;xs:complexType&gt; &lt;xs:attribute name=&quot;gender&quot; type=&quot;xs:string&quot; use=&quot;required&quot;/&gt; &lt;/xs:complexType&gt; &lt;/xs:element&gt; &lt;xs:element name=&quot;age&quot; type=&quot;xs:positiveInteger&quot;/&gt; &lt;xs:element name=&quot;department&quot; type=&quot;xs:string&quot;/&gt; &lt;/xs:sequence&gt; &lt;/xs:complexType&gt; &lt;/xs:element&gt; &lt;/xs:schema&gt;   &lt;xml&gt; : An XML file begins with xml tags specifying version and the encoding used to encode the text.&lt;xs:schema&gt; : Act as the root of the schema with the namespace attribute.&lt;xs:element&gt; : Define an element with the attribute employee as the name. We can also specify attribute type for the data types.&lt;xs:complexType&gt; : Defines the complex type for the employee element. It allows for the specification of the child elements attributes.&lt;xs:sequence&gt; : Define the sequence or order in which child elements should appear within a complex type.&lt;xs:attribute&gt; : Define the attribute of element name with attribute use to mark it as required.  ","version":"Next","tagName":"h3"},{"title":"XML Parsing​","type":1,"pageTitle":"XML","url":"/cs-notes/digital-media-processing/xml#xml-parsing","content":" XML file follows a hierarchical tree-like structure. It consists of elements, attributes, and text content. The parsing process includes :  Tokenization/Lexical Analysis : XML parser reads the raw XML data character by character and breaks it down into small piece called tokens. Syntax Analysis : Parser analyzes the tokens to ensure they conform to the XML syntax rules. It checks for correct opening and closing tags, balanced nesting of elements, proper attribute syntax, and other rules defined by the XML specification. Document Object Model (DOM) Construction : The parser construct a tree with the token as the node. Constructing a tree will make parsing easier as it corresponds to the XML hierarchy itself. Source : https://www.w3schools.com/xml/xml_tree.asp There is another approach to parse XML, it is called Event-Based Parsing. In event-based parsing, the parser generates events as it encounters XML tokens. Events may include the start of an element, end of an element, attribute values, and text content. The purpose of this is to approach a more memory-efficient manner since it doesn't need to load the entire XML document into memory. Validation : If a Document Type Definition (DTD) or XML Schema is specified, the parser can perform validation to ensure the XML document adheres to the specified structure and constraints. This step involves checking element and attribute types, data formats, required fields, and other rules defined in the DTD or XML Schema. Application Processing : Once the XML data is parsed and validated, the application can access and process the structured XML representation stored in memory (such as the DOM tree) such as extracting information, performing computations, storing data, or generating output based on the XML content.   Source : https://www.edureka.co/blog/java-xml-parser/ ","version":"Next","tagName":"h3"},{"title":"Convolution","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/convolution","content":"","keywords":"","version":"Next"},{"title":"Convolution Theorem​","type":1,"pageTitle":"Convolution","url":"/cs-notes/digital-signal-processing/convolution#convolution-theorem","content":" Convolution and Fourier transform is related, they relates in the following properties :    The theorem tells us that Fourier transform of a convolution between two function is the same as the product of their individual Fourier transforms. This theorem provides a way of relating the time domain and frequency domain representations of signals.  The Fourier transform of a signal is a mathematical representation of the signal in the frequency domain. The frequency domain is a way of looking at a signal that breaks it down into its constituent frequencies.  Multiplying both Fourier transform of the signals together, we are essentially multiplying together the frequencies of those two signals. Multiplying frequencies together means we add them up.  And this is the same as convolution of two signals in the time domain which is a way of multiplying the signals together at different time delays (defined based on convolution formula). When you multiply two signals together at different time delays, you are essentially adding together the waves of the two signals at different frequencies.   Source : https://www.sharetechnote.com/html/RF_Handbook_TimeDomain_FrequencyDomain.html ","version":"Next","tagName":"h3"},{"title":"Compression","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/compression","content":"","keywords":"","version":"Next"},{"title":"Lossless Compression​","type":1,"pageTitle":"Compression","url":"/cs-notes/digital-signal-processing/compression#lossless-compression","content":" Lossless compression algorithms aim to represent the signal in a compressed format without any loss of information. The original signal can be perfectly reconstructed from the compressed representation. Common example includes :  Run-length Encoding (RLE)​  RLE is a simple compression algorithm to reduce repeating data without any loss of information. RLE works by replacing consecutive repeated symbols with a count of the repetition. Its effectiveness depends on the characteristics of the data whether it contains a lot of repetition or not.   Source : https://iq.opengenus.org/run-length-encoding/, https://api.video/what-is/run-length-encoding/  Huffman Encoding​  Huffman encoding is a technique that assigns some codes to input based on their occurrence frequency. The most frequent occurring data will have shorter code, this way we can represent the data in shorter and smaller size.  Huffman Process​  Huffman Frequency Table : First, each occurrence of data is counted and a frequency table will be created. For example, consider this data abbaacbbdefffddfff, the frequency table will be : Huffman Tree Construction : With the frequency table, we will construct a binary tree where the child-most node is the less frequent data and the top most is the most frequent data. From the bottom child node, we will merge the two of the lowest frequency to create the parent node representing their frequency sum. We will keep doing this up to the top most node. Constructing the tree includes the use of min-heap by comparing all the parent node frequency with the smallest frequency we are left off in the frequency table. Source : https://commons.wikimedia.org/wiki/File:Huffman_huff_demo.gif (with speed modification) We will then assign 0 for left child and 1 for right child. Each of the leaf node represent the data. Source : https://commons.wikimedia.org/wiki/File:Huffman_huff_demo.gif (with modification) Now the top most node will be the entry point of encoding and decoding process. All the possible bit from reading the binary tree is called Huffman codes. During transmission, we can include frequency table or the Huffman codes. So we can either reconstruct the Huffman tree or use to decode the data using Huffman codes directly. For example, if we have &quot;0001&quot; that means it is &quot;u&quot; character. This way we can represent high frequent data in shorter term but we have to sacrifice the less frequent data.  ","version":"Next","tagName":"h3"},{"title":"Lossy Compression​","type":1,"pageTitle":"Compression","url":"/cs-notes/digital-signal-processing/compression#lossy-compression","content":" Lossy compression algorithms aim to achieve higher compression ratios by allowing some degree of loss of information. The compressed representation does not perfectly reconstruct the original signal, but it retains the perceptually significant information. Lossy compression is often used where minor losses in quality or accuracy are acceptable.  For example in an image, a color that is similar to the neighbor color can be discarded and changed with the similar one as it won't affect the overall image.  Lossy compression techniques are typically designed and optimized for specific types of data, such as images, audio, or video. The general algorithm for lossy compression is Transform Coding. The idea is we will discard data that doesn't have significant contribution to the overall data.  Transform Coding​  Transform Coding is a lossy compression method that works by transforming data or signal using a function and it will output some coefficient. The coefficient contains information about the original data that can later be analyzed to achieve desired result.  The most commonly used transform function in transform coding is the Discrete Cosine Transform (DCT), Discrete Fourier Transform (DFT) and Wavelet Transform.   Source : https://www.dspguide.com/ch27/6.htm ","version":"Next","tagName":"h3"},{"title":"Denoising","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/denoising","content":"","keywords":"","version":"Next"},{"title":"Machine Learning Denoising​","type":1,"pageTitle":"Denoising","url":"/cs-notes/digital-signal-processing/denoising#machine-learning-denoising","content":" Involves the use of machine learning algorithms to remove noise from signals. Machine learning algorithms can learn the characteristics of noise and signal, and use this knowledge to remove the noise without distorting the signal.  The idea of this is the same as general machine learning technique. We will train the models to be able to identify pattern and make prediction. Machine learning technique has advantages due to the adaptability to different noise types and able to recognize complex noise patterns.  Some techniques are :  Autoencoders : Autoencoders are neural networks that are trained to reconstruct a signal from its noisy version. The autoencoder learns to identify the noise in the signal and remove it, leaving the signal intact.Generative Adversarial Networks (GANs) : GANs are a type of neural network that can be used to generate realistic images. GANs can also be used to remove noise from images by training them to generate images that are similar to the noisy image, but without the noise.   Source : https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798  tip Find out more about machine learning and deep learning ","version":"Next","tagName":"h3"},{"title":"Discrete Cosine Transform","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/discrete-cosine-transform","content":"","keywords":"","version":"Next"},{"title":"The idea​","type":1,"pageTitle":"Discrete Cosine Transform","url":"/cs-notes/digital-signal-processing/discrete-cosine-transform#the-idea","content":" The basic idea of DCT is we try to represent signal from image or audio as the sum of cosine wave with different frequency. With the similar concept as FT, we can discard some of the high-frequency components that are less important to the overall signal.  ","version":"Next","tagName":"h3"},{"title":"Type-II DCT​","type":1,"pageTitle":"Discrete Cosine Transform","url":"/cs-notes/digital-signal-processing/discrete-cosine-transform#type-ii-dct","content":" DCT has many version each with its own set of properties and applications. The Type-II is the JPEG image compression standard. It is well-suited for image compression applications and is also used in other applications, such as video compression and audio compression.  The cosine wave has output varying from -1 to 1, we can represent the output as gray-scale color.   Source : https://www.researchgate.net/figure/A-sine-wave-left-with-the-corresponding-2D-representation-a-sine-grating-shown-right_fig5_37986721  If we represent cosine wave output with grayscale color where -1 is black and 1 is white, with many combination of cosine wave frequency and addition, we can make this set of color. This set of color is called the DCT basis function.   Source : https://en.wikipedia.org/wiki/Discrete_cosine_transform  The more right and bottom, the frequency of cosine wave is increasing. High frequency will corresponds to frequently color changing. If color changes frequently, we can discard this because it doesn't contribute much to the overall shape or structure of the image.  ","version":"Next","tagName":"h3"},{"title":"DCT Process​","type":1,"pageTitle":"Discrete Cosine Transform","url":"/cs-notes/digital-signal-processing/discrete-cosine-transform#dct-process","content":" With the basis function we can represent any image, to represent the image, we will need to know which cosine wave and which frequency is needed.  In type-II DCT we will process image by breaking it down into small pieces of 8x8 pixels. Each pixel will represent color from 0 to 255, where 255 is white and 0 is black.  Because the color can vary from 0 to 255, we will try to make it vary between 0 to suit with the cosine function. To do this, we will subtract each color by 128, so the color will vary between -128 to 127.  After that, we will apply DCT to the 8x8 pixel. Applying DCT involves summing each of the cosine wave that makes up the basis function and multiply it with the pixel color to know how much it contribute. This is similar idea as Fourier transform when we multiply signal with exponential term.  ","version":"Next","tagName":"h3"},{"title":"DCT Compression​","type":1,"pageTitle":"Discrete Cosine Transform","url":"/cs-notes/digital-signal-processing/discrete-cosine-transform#dct-compression","content":" The DCT process will output 64 DCT coefficients that represent the contribution of each cosine wave to the pixel's value. The higher coefficient represents higher frequency and the lower coefficient represents lower frequency.  We can use the coefficient to filter out the data we don't need. For example, we can quantize each coefficient, this mean that we are rounding the coefficients to a smaller set of values.  The DCT coefficients represent the frequency content of an 8x8 block of image pixels, with the lower-frequency coefficients corresponding to the overall shape or structure of the block, and the higher-frequency coefficients corresponding to the fine details. Removing the higher frequency can helps reducing the amount of data that needs to be stored or transmitted.  The last step of DCT is Huffman Encoding. Huffman encoding is a lossless data compression technique that is used to further reduce the number of bits needed to represent the quantized DCT coefficients.   Source : https://www.researchgate.net/figure/Algorithm-and-procedure-of-the-JPEG-image-compression-The-original-image-is-compressed_fig1_338524056  tip Find out more about JPG compression here ","version":"Next","tagName":"h3"},{"title":"Discrete Fourier Transform","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/discrete-fourier-transform","content":"","keywords":"","version":"Next"},{"title":"How does it works​","type":1,"pageTitle":"Discrete Fourier Transform","url":"/cs-notes/digital-signal-processing/discrete-fourier-transform#how-does-it-works","content":" Here is the formula for DFT :    nnn : Sample indexNNN : Number of sampleskkk : Frequency bin indexX(k)X(k)X(k) : The k-th frequency component of the signalx(n)x(n)x(n) : The n-th sample of the input sequencejjj : The imaginary unite−j2πkn/Ne^{-j2 \\pi k n / N}e−j2πkn/N : The exponential term involving 2πkn/N2\\pi k n /N2πkn/N to convert frequency from cycles per second to radians per sample  A discrete signal is made from sampling a continous signal. Sampling process involves taking a sequence of equally spaced samples of the continuous signal. The samples is typically called N and the number of samples must be an integer power of 2.  FT and DFT is kinda similar, they differs in how we multiply with the complex exponential term. In FT, we multiply by f(t)f(t)f(t) which is the function of signal. While in DFT, we multiply by each of the sample.  After multiplying by each sample, the resulting product is a complex number that represents the contribution of that sample to the k-th frequency component of the signal in the frequency domain. It has a magnitude and a phase angle that depend on the value of the sample x(n)x(n)x(n) and the frequency bin index k.  ","version":"Next","tagName":"h3"},{"title":"Computation​","type":1,"pageTitle":"Discrete Fourier Transform","url":"/cs-notes/digital-signal-processing/discrete-fourier-transform#computation","content":" FT is a continuous-time transform that involves integration over an infinite time interval. Its computation is typically performed using numerical integration techniques. FT can be computationally expensive for large bandwidth or a complex frequency structure.  In computation, DFT multiplication between input signal and exponential term is typically represented in matrix multiplication. With a straight forward algorithm, DFT can be computed in O(N2)O(N^2)O(N2) time, where N is the number of samples in the signal.  There is also an improvement for DFT algorithm which is the Fast Fourier Transform (FFT) algorithm. ","version":"Next","tagName":"h3"},{"title":"Fast Fourier Transform","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/fast-fourier-transform","content":"","keywords":"","version":"Next"},{"title":"How does it works​","type":1,"pageTitle":"Fast Fourier Transform","url":"/cs-notes/digital-signal-processing/fast-fourier-transform#how-does-it-works","content":" There are some version of FFT developed over time, the most common version is the radix-2 which was originally described by Cooley-Tukey in 1965.  DFT Properties​  DFT has some properties due to the nature of wave.  The symmetry properties states that if the input signal is real and even (symmetric), then the output will also be real and even (symmetric). Similarly, if the input signal is real and odd (antisymmetric), then the output will be imaginary and odd (antisymmetric).  On the other hand, the periodicity properties states that the output of DFT will repeat after a certain number of samples.  Using these properties, we can use it to perform a more efficient computation.  ","version":"Next","tagName":"h3"},{"title":"The idea​","type":1,"pageTitle":"Fast Fourier Transform","url":"/cs-notes/digital-signal-processing/fast-fourier-transform#the-idea","content":" The basic idea of FFT algorithm is to use divide-and-conquer algorithm strategy. FFT divides problem into smaller subproblems and solve it recursively. This can make computation more efficient because DFT has those properties.  The periodicity means that the output is periodic, this makes us able to decompose it to smaller calculation. If we have period of N, then we can divide it by N. And with the symmetry properties, we only need to calculate the first half of DFT output and the second half can be obtained by symmetry.  For example, if signal has the same output 8 times, we can divide it by 8. For each period, if signal is real-valued, then only the first half of the DFT need to be computed, because the second half of the coefficients can be obtained by symmetry.   Source : https://towardsdatascience.com/fast-fourier-transform-937926e591cb  After dividing into sub problems, each computation of the sub problem is the original matrix multiplication. So in summary, FFT provides a significant improvement over original DFT by using its properties and the nature of wave.  ","version":"Next","tagName":"h3"},{"title":"FFT Output​","type":1,"pageTitle":"Fast Fourier Transform","url":"/cs-notes/digital-signal-processing/fast-fourier-transform#fft-output","content":" A signal can have a various frequency in different time, using FFT we can split all the wave based on their frequency. The x-axis of FFT output shows the frequency bin or the frequency group. The y-axis represent the magnitude of corresponding frequency. Magnitude means how strong or how significant is the contribution of that wave.  For example, if in the x-axis = 100 there is a peak or high magnitude, this means that the wave with frequency 100Hz contribute the most to the original signal.  With the information from the FFT output, we can filter out the signal. For example, we can remove noise or unwanted signals by identifying high magnitudes in the output. If the majority of the output magnitudes range from 0 to 20, but there is one frequency with a magnitude of 100, a simple filter can be used to eliminate the infrequent magnitude.  A high magnitude or peak in the output can be interpreted as a wave that contribute the most or a noise which is unwanted signal. Because of this, a further analysis such as spectral shape, examining the relative magnitudes of different peaks are used to distinguish.   Source : https://learn.adafruit.com/fft-fun-with-fourier-transforms/background ","version":"Next","tagName":"h3"},{"title":"Filtering","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/filtering","content":"Filtering Main Source : Various source from Google and Youtube In digital signal processing, Filtering refers to the process of modifying or manipulating a digital signal to remove unwanted components or extract specific information from the signal. For example, we can remove unwanted component by applying techniques like the Fourier transform to separate the signal based on each frequency and we can analyze it further. Source : https://towardsdatascience.com/the-fourier-transform-4-putting-the-fft-to-work-38dd84dc814 Applying a filter works by selectively reduce or amplify certain frequency components based on the desired filtering characteristics. Some common filtering includes : Lowpass Filter : Allows low-frequency components of a signal to pass through while reducing higher-frequency components.Highpass Filter : Allows high-frequency components of a signal to pass through while or reducing lower-frequency components.Bandpass Filter : Filters out both low and high-frequency content, allowing only the frequencies within the desired range to be present in the output signal.Bandstop Filter / Notch Filter : Reduces a specific range or band of frequencies while allowing frequencies outside that range to pass through. Source : https://en.wikipedia.org/wiki/Filter_(signal_processing) tip Find out more about filter related to image in here.","keywords":"","version":"Next"},{"title":"Laplace Transform","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/laplace-transform","content":"","keywords":"","version":"Next"},{"title":"Laplace Transform Visualization​","type":1,"pageTitle":"Laplace Transform","url":"/cs-notes/digital-signal-processing/laplace-transform#laplace-transform-visualization","content":" Laplace transform is typically represented in 3D graph, where x-axis represent the real part, y-axis represent the imaginary part, and the z-axis represent the magnitude or phase.  In Fourier transform, because the real part is set to 0, then it will be flat 2D graph instead.   Source : https://www.sharetechnote.com/html/EngMath_LaplaceTransform.html  The part of visualization where it goes to infinity generally suggests that the corresponding function exhibits exponential growth or decay. The part where it creates a hole is when the function go into discontinuity or oscillatory behavior. ","version":"Next","tagName":"h3"},{"title":"Fourier Transform","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/fourier-transform","content":"","keywords":"","version":"Next"},{"title":"How does it works​","type":1,"pageTitle":"Fourier Transform","url":"/cs-notes/digital-signal-processing/fourier-transform#how-does-it-works","content":" Fourier transform formula for continous signal is given by the following formula    ω\\omegaω is the frequency with unit of radians per unit timeF(ω)F(\\omega)F(ω) is the output of fourier transform with input frequency \\omegaf(t)f(t)f(t) is the original signal in the time domain  Using complex exponential term, we represent the wave as a vector that has magnitude that represents the amplitude of the sinusoidal wave, while the phase angle represents the phase shift of the wave relative to a reference point.  Multiplying the f(t)f(t)f(t) with the exponential term would make the vector now rotate based on the function.   Source : https://tutorial.math.lamar.edu/Extras/ComplexPrimer/Forms.aspx  To be able to separate based on each waves frequency, we can separate it based on the amplitude and the phase. The magic comes from taking the integral of the product of exponential term and input signal f(t)f(t)f(t).  The products represents the contribution of the frequency component at frequency ω\\omegaω to the original signal f(t)f(t)f(t). The idea is we need to find out how much of the signal f(t)f(t)f(t) is contributed by the frequency component at frequency ω\\omegaω. This mean we need to know the product for all time domain.  This is where integral comes to play, essentially taking the integral for all values of t measures the amount of overlap between the original signal and the oscillating function at frequency ω.  After taking the integral, you will get the complex-valued number which represents the amplitude and phase of the frequency component at frequency ω\\omegaω in the original signal f(t)f(t)f(t). ","version":"Next","tagName":"h3"},{"title":"Fourier Series","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/fourier-series","content":"","keywords":"","version":"Next"},{"title":"Approximating Square Wave​","type":1,"pageTitle":"Fourier Series","url":"/cs-notes/digital-signal-processing/fourier-series#approximating-square-wave","content":" A function is called periodic when it satisfies the following : f(t+T)=f(t)f(t + T) = f(t)f(t+T)=f(t). Meaning function will repeats its values at some intervals or periods.   Source : https://www.analyzemath.com/function/periodic.html  A square wave like this is also a periodic function Source : https://youtu.be/wmCIrpLBFds?t=89  We can approximate a square wave with the sum of sine wave. With a single sine wave, we will get a pretty bad approximation.   Source : https://youtu.be/wmCIrpLBFds?t=131  Here, a closer look at the approximation using 4πsin⁡(x)\\frac{4}{\\pi} \\sin(x)π4​sin(x) function.   Source : https://youtu.be/wmCIrpLBFds?t=195  We can add another sine wave so that the amplitude will lower down. Adding wave will either add up or cancel each other.   Source : https://youtu.be/wmCIrpLBFds?t=221  It will result in much better approximation.   Source : https://youtu.be/wmCIrpLBFds?t=257  We can keep adding sine wave until we are satisfied with the result.   Source : https://youtu.be/wmCIrpLBFds?t=312  Gibbs Phenomena​  We may encounter a problem in our series of sine wave. While approximating a discontinued function, the Fourier series attempts to &quot;fill in&quot; the gaps with the sum of the sine and cosine functions. However, the sum of the infinite number of sine and cosine functions is not able to perfectly match the discontinuous function, resulting in the overshoot or ringing effect.   Source : https://youtu.be/wmCIrpLBFds?t=384  ","version":"Next","tagName":"h3"},{"title":"Fourier Series Formula​","type":1,"pageTitle":"Fourier Series","url":"/cs-notes/digital-signal-processing/fourier-series#fourier-series-formula","content":" Below are the formula for Fourier series for a periodic function f(x)f(x)f(x) with period of 2π2\\pi2π, note that we can also add up with cosine wave.   Source : https://www.cuemath.com/fourier-series-formula/  These coefficient can be thought of as how we will represent the amplitude and phase of each frequency component in the Fourier series expansion of a periodic function.  ","version":"Next","tagName":"h3"},{"title":"Complex Fourier Series​","type":1,"pageTitle":"Fourier Series","url":"/cs-notes/digital-signal-processing/fourier-series#complex-fourier-series","content":" Fourier series is represented in sine and cosine wave, we can represent this as a sum of complex exponential functions using Euler's formula.   The formula for complex Fourier series with period of 2π2\\pi2π : Where nnn is frequency and iii is the imaginary terms.   Source : https://pidlaboratory.com/4-complex-fourier-series/ (With modification) ","version":"Next","tagName":"h3"},{"title":"Multiplexing","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/multiplexing","content":"","keywords":"","version":"Next"},{"title":"Frequency Division Multiplexing (FDM)​","type":1,"pageTitle":"Multiplexing","url":"/cs-notes/digital-signal-processing/multiplexing#frequency-division-multiplexing-fdm","content":" FDM is a multiplexing technique used to combine multiple signals onto a single transmission medium by separating each signal based on their frequency. These frequency won't be overlapped with each other to prevent interference between the signals.  FDM are typically used by analog signals such as :  AM, FM radioAnalog TelevisionAnalog Telephone   Source : https://electronicspost.com/what-is-multiplexing-frequency-division-multiplexing-fdm-and-time-division-multiplexing-tdm/  ","version":"Next","tagName":"h3"},{"title":"Time Division Multiplexing (TDM)​","type":1,"pageTitle":"Multiplexing","url":"/cs-notes/digital-signal-processing/multiplexing#time-division-multiplexing-tdm","content":" Multiplexing technique which combine multiple signals onto a single transmission medium by allocating specific time slots to each signal.  There are also Time-Division Multiple Access (TDMA) which allow multiple source or transmitter.  TDM/TDMA is used in :  Traditional telephone (T1 and E1 carrier)Digital Subscriber Line (DSL)GSM telephone system (2G networks)   Source : https://www.spiceworks.com/tech/networking/articles/what-is-tdm/  ","version":"Next","tagName":"h3"},{"title":"Wavelength Division Multiplexing (WDM)​","type":1,"pageTitle":"Multiplexing","url":"/cs-notes/digital-signal-processing/multiplexing#wavelength-division-multiplexing-wdm","content":" Multiplexing technique which combine multiple signals onto a single transmission medium by separating each signals by their wavelength. WDM is typically transmitted together through the optical fiber.  WDM is used in :  High-speed internet infrastructureCable Television (CATV)Submarine Communication Systems   Source : https://www.wwt.com/article/how-does-wdm-technology-work  ","version":"Next","tagName":"h3"},{"title":"Code Division Multiplexing (CDM)​","type":1,"pageTitle":"Multiplexing","url":"/cs-notes/digital-signal-processing/multiplexing#code-division-multiplexing-cdm","content":" Multiplexing technique which combine multiple signals onto a single transmission medium by separating each signals using unique codes identifier to distinguish the signals.  Each signal needs to be encoded using unique codes, the encoded signals are then combined. At the end, the signals are separated and extracted using the corresponding codes. The separated signals are then decoded to recover the original data or information.  Code Division Multiple Access (CDMA) is also a version of CDM which allows multiple sources.  CDM is used in :  3G and 4G NetworksWireless communicationSatellite Communication   Source : https://www.semanticscholar.org/paper/Wide-Bandwidth%2C-High-Frame-Rate-Electrical-a-Code-McEwan-Holder/551f0f673fa93e8cfa3a9deee0ea2189ff057b12/figure/0 ","version":"Next","tagName":"h3"},{"title":"Signal Transmission Medium","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/signal-transmission-medium","content":"","keywords":"","version":"Next"},{"title":"Type of Transmission Based on Guidance​","type":1,"pageTitle":"Signal Transmission Medium","url":"/cs-notes/digital-signal-processing/signal-transmission-medium#type-of-transmission-based-on-guidance","content":" Guided Transmission​  Guided transmission refers to the transmission of signals through a physical medium that provides a guided path for the signals to propagate. Guided media includes :  Twisted Pair : Consists of pairs of insulated copper wires twisted together. It is commonly used in Ethernet networks and telephone systems. Source : https://www.ayokonfig.com/2016/11/pengertian-kabel-utp-stp-coaxial-dan.html Coaxial Cable : Consists of a central conductor, an insulating layer, a metallic shield, and an outer insulating layer. It is used in cable television (CATV) networks and broadband connections. Source : https://www.techtarget.com/searchnetworking/definition/coaxial-cable-illustrated Optical Fiber : Uses thin strands of glass or plastic fibers to transmit signals as pulses of light by bouncing it between the fibers. It is widely used for high-speed internet connections, long-distance communication, and data center connectivity. Source : https://www.nai-group.com/optical-fiber-technology-how-it-works/ , https://computer.howstuffworks.com/fiber-optic2.htm  Unguided Transmission​  Unguided tranmission refers to the transmission of signals through wireless or free space channels where there is no physical medium to guide or confine the signals. Examples of unguided transmission media include electromagnetic waves such as radio, microwaves, and infrared.  For example, radio communication require the transmitter and receiver to be in the same frequency. There are various frequency spectrum in radio waves.   Source : https://www.quora.com/What-is-the-bandwidth-of-radio-waves  Each region is divided into cells, which are typically represented as hexagons on network planning diagrams. Each cell is served by a base station or cell tower. The purpose of using hexagon, is because they provide flexible shape over the earth.   Source : https://www.semanticscholar.org/paper/Joint-Transmission-and-Detection-in-Hexagonal-Grid-Ibing-Jungnickel/b5a6a4265438242906d2f913660cb966b23d1b17  Light is then bounced from source tower through Ionosphere or satelite to the receiver tower.   Source : https://www.javatpoint.com/unguided-transmission-media ","version":"Next","tagName":"h3"},{"title":"Quantization","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/quantization","content":"","keywords":"","version":"Next"},{"title":"Uniform & Non-uniform Quantization​","type":1,"pageTitle":"Quantization","url":"/cs-notes/digital-signal-processing/quantization#uniform--non-uniform-quantization","content":" Uniform quantization is a quantization technique where the intervals between the quantization levels are evenly spaced. In other words, the range of all possible values is divided into equal intervals.  Non-uniform quantization is a quantization technique where the intervals between quantization levels are not evenly spaced. Instead, the intervals are non-uniformly spaced to allocate more quantization levels in regions of higher signal importance or sensitivity, and fewer quantization levels in regions of lower importance.   Source : https://analogquantized.wordpress.com/tag/non-uniform-quantization/ ","version":"Next","tagName":"h3"},{"title":"Signal","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/signal","content":"","keywords":"","version":"Next"},{"title":"Signal Types Based On Periodicity​","type":1,"pageTitle":"Signal","url":"/cs-notes/digital-signal-processing/signal#signal-types-based-on-periodicity","content":" Periodic Signal : A periodic signal repeats its pattern over time at regular intervals. This means that the signal waveform or pattern recurs identically after a specific time duration called the period. Source : https://www.open.edu/openlearn/science-maths-technology/exploring-communications-technology/content-section-1.1 Examples of periodic signals include sine waves, square waves, sawtooth waves, triangle waves, semicircle waves, and etc. Source : https://byjus.com/maths/fourier-series/ Non Periodic Signal : Also known as aperiodic signals or transient signals, do not exhibit repetitive patterns over time. Examples of non-periodic signals include a single pulse, a burst of noise, or a speech signal. Source : https://www.k-space.org/Class_Info/EE470/SigSys_chapter1_lec.pdf Quasi-Periodic Signal : Quasi-periodic signals exhibit a pattern that is similar to periodic signals, but not exactly identical. They have a repetitive nature, but with slight variations or irregularities. Source : https://www.researchgate.net/figure/Predictability-Periodic-quasi-periodic-and-chaotic-regimes-Prediction-Zt-x-of_fig2_341395962  ","version":"Next","tagName":"h3"},{"title":"Signal Types Based On Continuity​","type":1,"pageTitle":"Signal","url":"/cs-notes/digital-signal-processing/signal#signal-types-based-on-continuity","content":" Continuous Time Signal : Continuous-time signal are defined and exist for all values of time within a specified interval. They are represented by a continuous function of time. Examples of continuous-time signals are typically encountered in analog systems or natural phenomena including audio signals, analog electrical signals, sine waves, etc. Discrete-Time Signal : Discrete-time signal are defined only at specific instances or discrete points in time. They are represented by a sequence of values called sample taken at specific time intervals or time instances. Examples of discrete-time signal are commonly encountered in digital systems such as computer systems including digital audio signals, sampled analog signals, and sequences of numbers representing measurements or data such as image colors. Source : https://electronicsprojects.in/signals_and_systems/continuous-time-signal-and-discrete-time-signal-difference-diagram-and-information/ (with modification)  ","version":"Next","tagName":"h3"},{"title":"Signal Types Based On Representation​","type":1,"pageTitle":"Signal","url":"/cs-notes/digital-signal-processing/signal#signal-types-based-on-representation","content":" Analog Signal : Analog signal are continuous-time signals that represent physical quantities with a continuous range of values. They can take on any value within a specified range. Analog signal typically represent varying voltage or current levels. Examples of analog signals commonly encountered in natural phenomena including audio signals, analog electrical signals, and continuous variations of physical quantities such as temperature, pressure, or light intensity. Digital Signal : Digital signal are discrete-time signals that represent information using a finite set of discrete values or levels. These discrete values are typically represented by binary digits, or bits, such as 0s (low voltage) and 1s (high voltage). Digital signal are commonly encountered in digital world example of it includes digital audio signals, binary data streams, and sequences of discrete values representing measurements or data. Source : https://instrumentationtools.com/what-are-analog-and-digital-signals-differences-examples/ ","version":"Next","tagName":"h3"},{"title":"Sampling","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/sampling","content":"","keywords":"","version":"Next"},{"title":"Sampling Problem​","type":1,"pageTitle":"Sampling","url":"/cs-notes/digital-signal-processing/sampling#sampling-problem","content":" Sampling signal could introduce several challenges :  Noise : Noise is an unwanted signal that is added to the desired signal during the sampling process. This noise can come from a variety of sources, including the sampling device, the environment, or the signal itself. For example, while you record your sound, you may indirectly recorded traffic, people talking, and appliances sounds. Source : https://www.predig.com/whitepaper/reducing-signal-noise-practice Aliasing : After sampling to a discrete signal, we may want to reconstruct it back to continous signal. However, this could introduce inaccuracy. Aliasing occurs when the sampling rate is insufficient making loss of detail and accuracy in the reconstructed signal. In other words, we don't have enough information to reconstruct it back. Source : https://www.physik.uzh.ch/local/teaching/SPI301/LV-2015-Help/lvanlsconcepts.chm/Aliasing.html Quantization : Quantization is a way of simplifying a set of values by putting them into groups. It works by dividing the range of possible values into smaller intervals and assigning each value to the closest interval. This process helps to reduce the complexity of the data, but it also introduces some inaccuracy because the original values are approximated. For example in the image below, we divided this continous signal into 8 groups of binary bits. Source : https://www.differencebetween.com/difference-between-uniform-and-nonuniform-quantization/  ","version":"Next","tagName":"h3"},{"title":"Nyquist Theorem​","type":1,"pageTitle":"Sampling","url":"/cs-notes/digital-signal-processing/sampling#nyquist-theorem","content":" Sampling Rate​  Sampling Rate also referred to as the sampling frequency is the number of samples taken per unit of time during the process of analog-to-digital conversion. It is typically measured in samples per second and is denoted in hertz (Hz). A higher sampling rate means more samples are taken per unit of time, resulting in a more detailed and accurate representation of the original analog signal.  For example in audio applications, the commonly used sampling rates are 44.1 kHz, this means every second, we will take 44.100 representation of the sound wave.  Nyquist-Shannon Sampling Theorem​  Also known as Nyquist Rate, is a fundamental principle in digital signal processing to accurately reconstruct a continuous analog signal from its discrete samples. The theorem states that sampling rate should be greater than or equal to twice the bandwidth of the signal.  Mathematically written as : Sampling rate≥2B\\text{Sampling rate} \\ge 2BSampling rate≥2B, where BBB is the highest frequency in the signal. Nyquist rate is used to avoid aliasing and accurately represent signal.   Source : http://195.134.76.37/applets/AppletNyquist/Appl_Nyquist2.html  Nyquist Frequency​  Nyquist frequency, also known as folding ferquency, represents the maximum frequency that can be accurately represented in a discrete digital signal without introducing aliasing. It is written as : fNyquist=Sampling rate2f_{\\text{Nyquist}} = \\frac{\\text{Sampling rate}}{2}fNyquist​=2Sampling rate​  We can think Nyquist rate as the minimum and Nyquist frequency as the maximum sampling rate to avoid aliasing and to make sure we don't take too much sample.  For example, Audio CDs: Audio CDs have a sampling rate of 44.1 kHz. This means that the Nyquist frequency is 22.05 kHz. This is enough to represent the human hearing range, which is up to 20 kHz.  ","version":"Next","tagName":"h3"},{"title":"Upsampling & Downsampling​","type":1,"pageTitle":"Sampling","url":"/cs-notes/digital-signal-processing/sampling#upsampling--downsampling","content":" Upsampling and downsampling are signal processing operations that involve changing the sampling rate of a discrete signal.  Upsampling increases the sampling rate of a signal by inserting additional samples between existing samples. A common method is to take an average of neighbour or technique like linear interpolation.  Downsampling reduces the sampling rate of a signal by removing samples from the original signal. It applies a low-pass filter, known as an anti-aliasing filter, to remove high-frequency components above the new Nyquist frequency.   Source : https://www.divilabs.com/2014/07/upsampling-interpolation-of-discrete.html  tip Find out more about anti-aliasing for image in here ","version":"Next","tagName":"h3"},{"title":"Z-Transform","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/z-transform","content":"","keywords":"","version":"Next"},{"title":"How does it works​","type":1,"pageTitle":"Z-Transform","url":"/cs-notes/digital-signal-processing/z-transform#how-does-it-works","content":" Z-transform is just the discrete version of Laplace transform, the way of how it works is similar as the Discrete Fourier Transform to Fourier Transform. Multiplying each discrete sample by the complex variable would capture the phase and magnitude.  Z-transform is just the discrete version of Laplace transform, the way of how it works is similar as the Discrete Fourier Transform toFourier Transform. Multiplying each discrete sample by the complex variable z−nz^{-n}z−n, we are essentially converting the sample from the time domain to the frequency domain by shifting the sample to a different frequency. It will be shifted by power of n that vary between samples.  After getting the complex function output, it can be used for a variety of purposes, such as frequency analysis, filter design, system analysis.  Visualization​   Source : https://wirelesspi.com/a-visualization-of-causality-and-stability-in-z-transform/  Same as Laplace transform, x-axis represents real part, y-axis represents imaginary part, and z-axis represents magnitude. ","version":"Next","tagName":"h3"},{"title":"Wavelets","type":0,"sectionRef":"#","url":"/cs-notes/digital-signal-processing/wavelets","content":"","keywords":"","version":"Next"},{"title":"Wavelets Operations​","type":1,"pageTitle":"Wavelets","url":"/cs-notes/digital-signal-processing/wavelets#wavelets-operations","content":" Using basis wavelets function, we can construct any signal shape. We can do this by doing transformation into these basis function such as scaling or shifting.  Scaling​   Source : https://www.mathworks.com/help/wavelet/gs/continuous-wavelet-transform-and-scale-based-analysis.html  Shifting​   Source : https://inst.eecs.berkeley.edu/~ee225b/sp14/lectures/wavelets-g&amp;w.pdf  The wavelets basis function are multiplied by the original signal to obtain wavelet coefficients, the similar concept as Tourier transform which is the contribution of each wavelets to the signal.  And using the wavelets coefficient we can do further analysis such as filtering noise, compression, etc by removing or discarding specific portion of signal based on the coefficient. ","version":"Next","tagName":"h3"},{"title":"Web Development","type":0,"sectionRef":"#","url":"/cs-notes/frontend-web-development","content":"","keywords":"","version":"Next"},{"title":"All pages​","type":1,"pageTitle":"Web Development","url":"/cs-notes/frontend-web-development#all-pages","content":" HTMLHTML DOMCSSJavascriptJSONHTML + CSS + JavascriptBrowserStatic &amp; Dynamic SiteWeb HostingNPM (Node Package Manager)Node JSReact JSDocker &amp; Kubernetes ","version":"Next","tagName":"h3"},{"title":"Docker & Kubernetes","type":0,"sectionRef":"#","url":"/cs-notes/frontend-web-development/docker-and-kubernetes","content":"Docker &amp; Kubernetes See Cloud Computing &gt; Docker &amp; Kubernetes","keywords":"","version":"Next"},{"title":"HTML","type":0,"sectionRef":"#","url":"/cs-notes/frontend-web-development/html","content":"","keywords":"","version":"Next"},{"title":"Semantic HTML​","type":1,"pageTitle":"HTML","url":"/cs-notes/frontend-web-development/html#semantic-html","content":" Semantic HTML is the practice of using HTML markup to convey the meaning and structure of content on a web page. Semantic HTML elements, such as &lt;header&gt;, &lt;nav&gt;, &lt;main&gt;, &lt;article&gt;, &lt;section&gt;, &lt;aside&gt;, and &lt;footer&gt;, provide a clear and meaningful structure to web pages.  Using just traditional div container would works fine, but using semantic HTML provide a clear and meaningful structure to web pages, which can improve accessibility, search engine optimization (SEO), and user experience.   Source : https://www.semrush.com/blog/semantic-html5-guide/  ","version":"Next","tagName":"h3"},{"title":"XHTML​","type":1,"pageTitle":"HTML","url":"/cs-notes/frontend-web-development/html#xhtml","content":" XHTML stands for Extensible HyperText Markup Language, and it is a markup language that is designed to be a stricter, more standardized version of HTML. XHTML is based on XML, which means that it is syntactically stricter than HTML and conforms to the rules of XML syntax such as element nesting, attribute quoting, and other syntax rules.  XHTML was developed as a response to the proliferation of non-standard, poorly formed HTML documents on the web, which made it difficult for web browsers to render pages consistently. By adopting XML syntax and enforcing strict rules for document structure and syntax, XHTML aims to create a more consistent and predictable web browsing experience.  For example in XHTML, any syntax errors will cause the document to fail to parse, and the browser will display an error message. In HTML, the browser will attempt to correct any errors and continue parsing the document, which can lead to unexpected and inconsistent behavior. ","version":"Next","tagName":"h3"},{"title":"CSS","type":0,"sectionRef":"#","url":"/cs-notes/frontend-web-development/css","content":"","keywords":"","version":"Next"},{"title":"HTML Selectors​","type":1,"pageTitle":"CSS","url":"/cs-notes/frontend-web-development/css#html-selectors","content":" HTML Selectors are used to identify each HTML elements, this includes :  Tag Selectors : These selectors target all elements with a specific tag name, such as &lt;p&gt; or &lt;h1&gt;.Class Selectors : These selectors target elements with a specific class attribute, which can be applied to multiple elements on a page. Class selector can be specified like &lt;div class=&quot;my-class&quot;&gt;.ID Selectors : These selectors target elements with a specific ID attribute, which should be unique on a page. ID selector can be specified like &lt;div id=&quot;my-div&quot;&gt;.Attribute Selectors : These selectors target elements with a specific attribute or attribute value, such as &lt;a href=&quot;#&quot;&gt;.  ","version":"Next","tagName":"h3"},{"title":"CSS Selectors​","type":1,"pageTitle":"CSS","url":"/cs-notes/frontend-web-development/css#css-selectors","content":" CSS selectors are used to apply styles to specific elements or groups of elements on a webpage and it often uses HTML Selectors to identify the elements. There are many types of CSS selectors, including :  Type Selectors : These selectors target elements of a specific type, such as &lt;p&gt; or &lt;h1&gt;.Class Selectors : These selectors target elements with a specific class attribute.ID Selectors : These selectors target elements with a specific ID attribute.Descendant Selectors : These selectors target elements that are descendants of other elements, such as &lt;ul&gt; elements inside &lt;li&gt; elements.Child Selectors : These selectors target elements that are direct children of other elements, such as &lt;li&gt; elements that are direct children of &lt;ul&gt; elements.Attribute Selectors : These selectors target elements with a specific attribute or attribute value. For example using input[type=&quot;text&quot;] means that it will apply style to all input elements which has type of text.Pseudo-Selectors : These selectors target elements based on their state or position, such as :hover or :first-child. For example defining a style on :hover means that style will be applied if such elements is hovered.  ","version":"Next","tagName":"h3"},{"title":"CSSOM​","type":1,"pageTitle":"CSS","url":"/cs-notes/frontend-web-development/css#cssom","content":" CSSOM (CSS Object Model) is a representation of the styles and layout of a web page that is generated by a web browser during the rendering process.  The CSSOM is created by parsing the CSS code of a web page and generating a hierarchical tree-like structure that represents the styles and layout of the web page. Each node in the tree corresponds to a different CSS selector, property, or value, and the hierarchy of the tree reflects the cascading nature of CSS, where styles defined in one place can be overridden by styles defined in another place.   Source : https://www.hongkiat.com/blog/css-object-model-cssom/  ","version":"Next","tagName":"h3"},{"title":"Applying Styles​","type":1,"pageTitle":"CSS","url":"/cs-notes/frontend-web-development/css#applying-styles","content":" When the browser encounters a CSS file linked to an HTML document, it reads the CSS code and matches the selectors in the CSS rules to the elements in the DOM tree.  The syntax is :  Selectors is specified which can be class, ID, attribute, type, or any other selector.Define the style property and value wrapped in curly brackets, CSS has various style property such as color, font-size, font-weight, and etc.  For example, if a CSS rule specifies that all &lt;p&gt; elements should have a font size of 16 pixels, the browser will apply that style to all &lt;p&gt; elements in the DOM tree.  p { font-size: 16px; }   This image below shows how style is applied using class selectors. We specify the class name and put a dot behind it .&lt;class name&gt;   Source : https://web.dev/learn/css/selectors/  If we are using id selector, then we will use # instead of . in front of the HTML selector name.  ","version":"Next","tagName":"h3"},{"title":"CSS Features​","type":1,"pageTitle":"CSS","url":"/cs-notes/frontend-web-development/css#css-features","content":" CSS supports many feature such as :  Responsive Design : We can apply different styles to adapt to different webpage based on the screen size, orientation, resolution, or other features of the device used to view the webpage. /* If height of the screen is less than 768px, then apply the following styles */ @media screen and (min-height: 768px) { body { background-color: #f0f0f0; } /* Sets the height of the header element to 50% of the vh (viewport) height which is the part of the webpage that is visible to the user */ header { height: 50vh; } } Flexbox and Grid : These are two layout models that allow you to create complex and flexible layouts for webpages. Flexbox is designed for one-dimensional layouts, such as rows or columns, while Grid is designed for two-dimensional layouts, such as grids. Source : https://codepolitan.com/blog/css-grid-vs-flexbox-5b4336849183d Transition and Animation : CSS transitions and animations allow you to create dynamic and interactive effects, such as hover effects, fade-ins, slide-outs, and more. Transitions and animations can be applied to various style properties, such as color, position, size, and opacity. Transition can be defined by specifying transition effect and the duration while animation can be defined using keyframes. /* Will make opacity transition in exactly 1 second and uses easing function which starts the animation slowly, accelerates through the middle of the animation, and then slows down again at the end */ .button { transition: opacity 1s ease-in-out; } /* Will animate the element opacity from 0 to 1 which creates fade out effect */ @keyframes my-animation { 0% { opacity: 1; } 50% { opacity: 0.5; } 100% { opacity: 0; } }  ","version":"Next","tagName":"h3"},{"title":"Javascript","type":0,"sectionRef":"#","url":"/cs-notes/frontend-web-development/javascript","content":"","keywords":"","version":"Next"},{"title":"V8 Javascript Engine​","type":1,"pageTitle":"Javascript","url":"/cs-notes/frontend-web-development/javascript#v8-javascript-engine","content":" Javascript is an interpreted language meaning that it is executed line-by-line at runtime rather than being compiled before execution. JavaScript code is executed by a JavaScript engine which is typically included in web browsers.  One of a Javascript engine is V8 which was developed by Google and is used in Google Chrome. The V8 engine is a just-in-time (JIT) compiler that compiles JavaScript code into machine code at runtime. The engine consists of several components that work together to execute JavaScript code efficiently :  Parsing and Bytecode Generation : The first step is V8 engine parse Javascript and turns it into an abstract syntax tree (AST) which is a tree-like data structure that represents the abstract syntactic structure of a program in a programming language. The parser checks the syntax of the code for correctness and creates a tree structure that represents the structure of the code. After it's parsed, bytecode is generated and will be interpreted in the next step. Source : https://en.wikipedia.org/wiki/Abstract_syntax_tree Interpreter : Once AST has been created, V8 engine executes the code using an interpreter called Ignition reading the codes line by line following the control flow. Profiler &amp; Optimizer : V8 engine includes a profiler that collects information about how the code is being executed. The information collected by the profiler, the V8 engine uses an optimizing compiler to generate more efficient machine code. Hidden Class : Hidden Class is one of the optimization technique to improves the performance of object creation. V8 generate a class based on the structure of the objects we create in our JavaScript code. Javascript is a dynamic language which means variables, data types, and function calls are evaluated and executed at runtime. This can slows down the process of accessing properties, because the engine need to do type checks. Hidden classes provide certainty for the engine about the types of object properties which can improve efficiency of property accessing and object creation. Garbage Collector : V8 engine manages memory allocation and deallocation using a garbage collector. The garbage collector identifies objects that are no longer needed by the code and deallocates the memory they occupy. Code Generation : After everything is parsed and optimized, the code is compiled into machine code by the V8 engine's compiler. The machine code is then executed directly by the CPU, which results in faster execution times compared to interpreting the bytecode.  ","version":"Next","tagName":"h3"},{"title":"Javascript Run-Time Environment​","type":1,"pageTitle":"Javascript","url":"/cs-notes/frontend-web-development/javascript#javascript-run-time-environment","content":" Javascript relies on run-time environment for managing the execution of JavaScript code. Javascript runs on a single-threaded environment which means it can only execute one task at a time. Javascript uses call stack which is a data structure that keeps track of the currently executing functions in the JavaScript call hierarchy.  The environment also includes callback queue which is a data structure that holds callback functions that are waiting to be executed such as asynchronous events like waiting for user input or making a network call.  Another mechanism to manages the execution is the event loop. When the call stack is empty, the event loop checks the callback queue for new events. If there is a callback function waiting in the queue, the event loop retrieves the function and adds it to the call stack for execution. This allows JavaScript to handle asynchronous events without blocking the execution of other code.   ","version":"Next","tagName":"h3"},{"title":"HTML DOM","type":0,"sectionRef":"#","url":"/cs-notes/frontend-web-development/html-dom","content":"","keywords":"","version":"Next"},{"title":"DOM Tree​","type":1,"pageTitle":"HTML DOM","url":"/cs-notes/frontend-web-development/html-dom#dom-tree","content":" DOM is represented as tree-like structure, with each HTML element represented as a node in the tree and is represented as an object, with properties. The root of the tree is the document object, which represents the entire HTML document.  Objects and properties make each HTML element in the document possible to be accessed and manipulated using JavaScript. For example, the document object has properties such as title and URL, while HTML elements have properties such as innerHTML, className, and style.  Developers can use JavaScript to access and modify individual elements in the document, change their attributes and styles, add or remove elements from the document, and respond to user interactions.  See how Javascript access the DOM : DOM Manipulation   Source : https://www.tutorialstonight.com/js/js-dom-introduction  ","version":"Next","tagName":"h3"},{"title":"Nodes Type​","type":1,"pageTitle":"HTML DOM","url":"/cs-notes/frontend-web-development/html-dom#nodes-type","content":" There are several different types of nodes in the Document Object Model (DOM), with each type of node representing a different kind of object in an HTML or XML document. Here are the most common types of nodes in the DOM :  Element Nodes : Element nodes represent the actual elements in an HTML or XML document, such as &lt;div&gt;, &lt;p&gt;, and &lt;img&gt;.Text Nodes : Text nodes represent the text content of an element, such as the text between two HTML tags.Attribute Nodes : Attribute nodes represent the attributes of an element, such as the src attribute of an &lt;img&gt; element.Comment Nodes : Comment nodes represent comments in an HTML or XML document, such as &lt;!-- This is a comment --&gt;.Document Nodes : Document nodes represent the entire HTML or XML document and are the root node of the DOM tree.Document Type Nodes : Document type nodes represent the document type declaration in an HTML or XML document, such as &lt;!DOCTYPE html&gt;.Processing Instruction Nodes : Processing instruction nodes represent processing instructions in an XML document, such as &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;.  ","version":"Next","tagName":"h3"},{"title":"DOM Level​","type":1,"pageTitle":"HTML DOM","url":"/cs-notes/frontend-web-development/html-dom#dom-level","content":" The Document Object Model (DOM) has evolved over time, with different versions or levels of the DOM being developed to support new features and capabilities. Here are some of the key levels of the DOM :  DOM Level 1 : The first level of the DOM was released in 1998 introducing the basic concepts of the DOM, such as nodes, elements, and attributes, and provided a set of core methods and properties for working with these objects.DOM Level 2 : The second level of the DOM was released in 2000 and added support for new features such as CSS, event handling, and user interface controls. It introduced new interfaces and methods for working with stylesheets, events, and user interface elements, and improved support for XML namespaces and namespaces.DOM Level 3 : The third level of the DOM was released in 2004 and included support for additional features such as XPath, keyboard navigation, and validation. It introduced new interfaces and methods for working with XPath expressions, keyboard events, and XML Schema validation, among other things.DOM Level 4 : The fourth level of the DOM was released in 2015 and has since become the standard. DOM Level 4 offers notable enhancements in areas such as performance, security, internationalization, and accessibility. Additionally, it introduces new elements and attributes to the model. ","version":"Next","tagName":"h3"},{"title":"HTML + CSS + Javascript","type":0,"sectionRef":"#","url":"/cs-notes/frontend-web-development/html-css-javascript","content":"","keywords":"","version":"Next"},{"title":"DOM Manipulation​","type":1,"pageTitle":"HTML + CSS + Javascript","url":"/cs-notes/frontend-web-development/html-css-javascript#dom-manipulation","content":" When a web page is loaded in a browser, the browser creates a DOM tree based on the HTML code in the page. The browser parses the HTML code and creates a tree of nodes that represent the elements in the page. This DOM tree can be accessed and manipulated using JavaScript.  This allows developers to manipulate HTML and CSS in a web page. Javascript will access specific element by searching the DOM tree based on ID, class, selector, and tags.  For example consider the following code that shows how Javascript access an element based on id selector :  &lt;div id=&quot;myDiv&quot;&gt;Hello, world!&lt;/div&gt;   // Get the element with the ID &quot;myDiv&quot; var myDiv = document.getElementById(&quot;myDiv&quot;); // Change the text content of the element myDiv.textContent = &quot;Hello, JavaScript!&quot;;   DOM manipulation with JavaScript can do a lot of things to a web page, including :  Changing text content of an element using .textContent. Change attribute of an element using .setAttribute, for example changing an img src. Adding or removing classes from an element by .classList.add(&quot;anotherClass&quot;) or .classList.remove(&quot;existingClass&quot;). Modify style with .style followed by the style element such as backgroundColor. Creating new elements and adding them to the DOM like below : var newParagraph = document.createElement(&quot;p&quot;); newParagraph.textContent = &quot;This is a new paragraph.&quot;; document.body.appendChild(newParagraph);   And many other methods and techniques... ","version":"Next","tagName":"h3"},{"title":"Node JS","type":0,"sectionRef":"#","url":"/cs-notes/frontend-web-development/node-js","content":"","keywords":"","version":"Next"},{"title":"Node JS Features​","type":1,"pageTitle":"Node JS","url":"/cs-notes/frontend-web-development/node-js#node-js-features","content":" Node.js also provides a collection of modules related to server such as file system I/O, networking (DNS, HTTP, TCP, TLS/SSL, or UDP), binary data (buffers), cryptography functions, data streams, and other core functions.  Node.js uses an event-driven, non-blocking I/O model, single event loop to handle incoming requests. This means that instead of waiting for a request to complete before moving on to the next one, Node.js processes incoming requests in an asynchronous manner. Node.js registers a callback function, meaning another task will be executed once the currently executing task is completed.   Source : https://www.freecodecamp.org/news/what-exactly-is-node-guide-for-beginners/ ","version":"Next","tagName":"h3"},{"title":"JSON","type":0,"sectionRef":"#","url":"/cs-notes/frontend-web-development/json","content":"JSON Main Source : Wikipedia JSON JSON stands for JavaScript Object Notation. It is a lightweight data interchange format that is easy for humans to read and write, and easy for machines to parse and generate. JSON is often used to transmit data between a server and a web application, as an alternative to XML. It is also used as a data storage and communication format in many programming languages and web services. tip JSON was derived from Javascript but it is not limited to Javascript only, in fact JSON is used as data exchange between different systems and programming languages. JSON is defined as text format that consists of key-value pairs, where keys are strings and values can be strings, numbers, objects, arrays, or Boolean values. The data is structured in a hierarchical manner, with nested objects and arrays. { &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Smith&quot;, &quot;isAlive&quot;: true, &quot;age&quot;: 27, &quot;address&quot;: { &quot;streetAddress&quot;: &quot;21 2nd Street&quot;, &quot;city&quot;: &quot;New York&quot;, &quot;state&quot;: &quot;NY&quot;, &quot;postalCode&quot;: &quot;10021-3100&quot; }, &quot;phoneNumbers&quot;: [ { &quot;type&quot;: &quot;home&quot;, &quot;number&quot;: &quot;212 555-1234&quot; }, { &quot;type&quot;: &quot;office&quot;, &quot;number&quot;: &quot;646 555-4567&quot; } ], &quot;children&quot;: [&quot;Catherine&quot;, &quot;Thomas&quot;, &quot;Trevor&quot;], &quot;spouse&quot;: null } Source : Wikipedia JSON example","keywords":"","version":"Next"},{"title":"NPM","type":0,"sectionRef":"#","url":"/cs-notes/frontend-web-development/npm","content":"","keywords":"","version":"Next"},{"title":"Dependencies​","type":1,"pageTitle":"NPM","url":"/cs-notes/frontend-web-development/npm#dependencies","content":" A dependency is a requirement that one piece of code has on another piece of code to function properly. If a web app uses specific library, the library will be included in the packages.  When a package is installed, NPM automatically resolves and installs any dependencies that are required by that package, which can include other packages, libraries, or tools. This helps to simplify the process of managing dependencies, as developers do not need to manually download and install each individual dependency.  package.json​  package.json is the file used in Node.js projects to define the metadata, dependencies, and scripts for the project. NPM will reads the dependencies property in package.json and downloads and installs the listed modules and all their dependencies.  Here is an example package.json file :  { &quot;name&quot;: &quot;my-project&quot;, &quot;version&quot;: &quot;1.0.0&quot;, &quot;description&quot;: &quot;My awesome project&quot;, &quot;main&quot;: &quot;index.js&quot;, &quot;scripts&quot;: { &quot;start&quot;: &quot;node index.js&quot;, &quot;test&quot;: &quot;echo \\&quot;Error: no test specified\\&quot; &amp;&amp; exit 1&quot; }, &quot;dependencies&quot;: { &quot;express&quot;: &quot;^4.17.1&quot;, &quot;lodash&quot;: &quot;^4.17.21&quot; } }   package-lock.json​  While package.json is file that contains all the required dependencies and other data, package-lock.json specifically used for exact versions of dependencies and their transitive dependencies that should be installed for a project.   Source : https://medium.com/helpshift-engineering/package-lock-json-the-complete-guide-2ae40175ebdd ","version":"Next","tagName":"h3"},{"title":"Search Engine","type":0,"sectionRef":"#","url":"/cs-notes/frontend-web-development/search-engine","content":"","keywords":"","version":"Next"},{"title":"How search engine works​","type":1,"pageTitle":"Search Engine","url":"/cs-notes/frontend-web-development/search-engine#how-search-engine-works","content":" Search engine involves several process :  Crawling : The first step in the search engine process is crawling. Crawling is the process by which search engines look into web pages. Search engines use automated software known as &quot;spiders&quot; or &quot;bots&quot; to crawl the web and follow links from one page to another. The bots collect information about each page they crawl, such as the page title, meta tags, and content. Some website also have robots.txt which is a file containing directives for search spiders, telling it which pages to crawl and which pages not to crawl. Indexing : Once the search engine bots have crawled a web page, they then store the information they've collected in a massive database known as an index. The index is a vast collection of all the pages that have been crawled by the search engine. Ranking : When a user enters a search query into the search engine, the search engine algorithm analyzes the index to determine which pages are most relevant to the user's search query. The algorithm takes into account hundreds of factors, including keyword usage, page content, links pointing to the page, and many other factors. Results : Once the algorithm has analyzed the index and determined which pages are most relevant to the user's search query, the search engine displays a list of results on the search engine results page (SERP). The results are usually displayed in order of relevance, with the most relevant pages at the top of the results page. Continuous Improvement : Search engines are constantly refining their algorithms to deliver more relevant results to users. They use machine learning and artificial intelligence techniques to analyze user behavior and improve the relevance of their search results. Search engines also use feedback from users to improve their algorithms and provide better search results.    ","version":"Next","tagName":"h3"},{"title":"PageRank Algorithm​","type":1,"pageTitle":"Search Engine","url":"/cs-notes/frontend-web-development/search-engine#pagerank-algorithm","content":" PageRank is an algorithm used by the Google search engine to determine the relevance and importance of web pages. This algorithm works by analyzing the number and quality of links pointing to a web page. The basic idea is a webpage's score is increased when it is linked by numerous other webpages compared to a webpage that has fewer incoming links.  The PageRank algorithm assigns a numerical value to each page, known as its PageRank score. The score is calculated using a complex formula that takes into account the number and quality of links pointing to the page, as well as the PageRank scores of the pages linking to it.   Source : https://co.delante.co/definitions/pagerank/ ","version":"Next","tagName":"h3"},{"title":"React JS","type":0,"sectionRef":"#","url":"/cs-notes/frontend-web-development/react-js","content":"","keywords":"","version":"Next"},{"title":"React State​","type":1,"pageTitle":"React JS","url":"/cs-notes/frontend-web-development/react-js#react-state","content":" In React, a state is an object that represents the current state of a component. A state can be anything such as data that a component holds or element that describe the UI.  For example in the image below, when the button is clicked, the text showing a value will be incremented.   Source : https://www.geeksforgeeks.org/reactjs-setstate/  ","version":"Next","tagName":"h3"},{"title":"Virtual DOM​","type":1,"pageTitle":"React JS","url":"/cs-notes/frontend-web-development/react-js#virtual-dom","content":" After writing HTML and Javascript code in a JSX file, React will render this and creates a virtual DOM tree. Virtual DOM contains information about the type of each element (such as div or button), its attributes (such as class or style), and its children (which can be other virtual DOM nodes or plain text).  When the state of a React component changes, React uses diffing algorithm to check which component has changed. This is an efficient way to only make the necessary changes, after this React generates a new virtual DOM tree that represents the updated state of the UI.   Source : https://programmingwithmosh.com/react/react-virtual-dom-explained/  ","version":"Next","tagName":"h3"},{"title":"Lifting State Up & React Props​","type":1,"pageTitle":"React JS","url":"/cs-notes/frontend-web-development/react-js#lifting-state-up--react-props","content":" When developing a web app we will try to breaks the UI down into smaller component, often times a child component has some data that other child component needs it. These child component can't communicate directly, a way to solve this is to lift the state up. This mean we will store the data to the closest common ancestor of the components that need it.   Source : https://dev.to/lauratoddcodes/a-really-simple-intro-to-lifting-state-in-react-1fli  For example, consider the following code :  function ClickCountDisplay() { return ( &lt;div&gt; &lt;p&gt;The count is: ???&lt;/p&gt; &lt;/div&gt; ); } function ClickCountButton() { const [count, setCount] = useState(0); function handleClick() { setCount(count + 1); } return ( &lt;div&gt; &lt;button onClick={handleClick}&gt;Click me&lt;/button&gt; &lt;/div&gt; ); }   In this example, we made a button and a text that should displays the click count. However, the ClickCountDisplay doesn't know how many clicks are now, because the click count is stored in the button.  The solution for this is to make a parent that holds the state and pass the required data.  function ParentHolder() { const [count, setCount] = useState(0); function handleClick() { setCount(count + 1); } return ( &lt;div&gt; &lt;ClickCountDisplay count={count} /&gt; &lt;ClickCountButton onClick={handleClick} /&gt; &lt;/div&gt; ); } function ClickCountDisplay({ count }) { return ( &lt;div&gt; &lt;p&gt;The count is: {count}&lt;/p&gt; &lt;/div&gt; ); } function ClickCountButton({ onClick }) { return ( &lt;div&gt; &lt;button onClick={onClick}&gt;Click me&lt;/button&gt; &lt;/div&gt; ); }   ParentHolder make it possible for ClickCountDisplay to know the data, when we pass in a data through ClickCountDisplay count={count} and accept the data by function ClickCountDisplay({ count }), this is called props short for properties, which refers to a way of passing data from a parent component to a child component.  ","version":"Next","tagName":"h3"},{"title":"React Use Effect​","type":1,"pageTitle":"React JS","url":"/cs-notes/frontend-web-development/react-js#react-use-effect","content":" When a state changes, React only updates the UI. Often times there's something in our web app that we also want to updates everytime a state changes. For example we want to always backup user data in local storage. This is called side effect, any code that is not related to rendering your component, but that needs to be executed as a result of some state or prop changes.  We can achieve this using react useEffect, this will allows you to run some code in your functional components after every render.   Source : https://www.memberstack.com/blog/uselayouteffect-vs-useeffect ","version":"Next","tagName":"h3"},{"title":"Static & Dynamic Site","type":0,"sectionRef":"#","url":"/cs-notes/frontend-web-development/static-and-dynamic-site","content":"","keywords":"","version":"Next"},{"title":"Static Website​","type":1,"pageTitle":"Static & Dynamic Site","url":"/cs-notes/frontend-web-development/static-and-dynamic-site#static-website","content":" Static Website are made up of pre-built HTML, CSS, and JavaScript files that are served to the user's browser as they are. Browser communicate to server to request such files and will render its content and never communicate with the server again. Static website doesn't need to connect to server to compute what to do next. This mean that the content is fixed and will never change, it will only change because of Javascript interactivity.   This site is an example of static site  ","version":"Next","tagName":"h3"},{"title":"Dynamic Website​","type":1,"pageTitle":"Static & Dynamic Site","url":"/cs-notes/frontend-web-development/static-and-dynamic-site#dynamic-website","content":" Dynamic Website on the other hand, are websites that generate content on the fly in response to user requests. Dynamic websites are built using server-side programming languages such as PHP, Ruby, Python, or JavaScript, and are typically backed by a database.  When a user requests a page on a dynamic website, the server processes the request, retrieves the necessary data from the database, and generates the HTML code for the page on the fly. This allows dynamic websites to provide more advanced functionality, such as user authentication, content management, and e-commerce capabilities.  Dynamic Website can be further classified into two :  Client-Side : Client-side is similar to static as it doesn't involves server, but in client-side website aren't pre-built, the content are generated on the fly.Server-Side : Server-side relies on server processing the data and generate the HTML before sending it to the user's browser.   Source : https://en.wikipedia.org/wiki/Dynamic_web_page  ","version":"Next","tagName":"h3"},{"title":"Static Site Generator​","type":1,"pageTitle":"Static & Dynamic Site","url":"/cs-notes/frontend-web-development/static-and-dynamic-site#static-site-generator","content":" Static site generators are engines that is able to generate static HTML, CSS, and JavaScript files from a collection of source files, typically written in a markup language such as Markdown or HTML.  Static site generators are often used for websites that do not require frequent content updates or complex functionality, such as blogs, documentation sites, or portfolios.  How does it works?​  Docusaurus is one of the popular static site generator used to make documentation website made using React JS and some other tools. Docusaurus is basically a template website that is customable by user, it has pre-made react component and other site functionality. Docusaurus only needs the document from user in markdown template.  The markdown will then be processed and HTML file containing the markdown content will be generated. Docusaurus also provides customization such as theme or other functionality like blog and sidebar. This mean that docusaurus will just change their default site depending on user desire.   Source : https://cloudcannon.com/blog/what-is-a-static-site-generator/ ","version":"Next","tagName":"h3"},{"title":"Web Hosting","type":0,"sectionRef":"#","url":"/cs-notes/frontend-web-development/web-hosting","content":"Web Hosting Main Source : What is web hosting? Web hosting is a service that allows you to store your website's files and data on a server that is connected to the internet. A web is typically hosted in a server, a web hosting service means that we pay someone to keep their computers on and process request and respond to any user who wants to visit our website. This make anyone who communicate with the server possible to access or visit your website. When someone access a server from their web browser, it sends a request to the server where your website is hosted, and the server responds by sending the necessary files and data back to the user's browser, which displays the website on their screen. There are several types of web hosting services available, these are some of the most common types : Shared Hosting : This is the most popular type of web hosting, where multiple websites share the same server resources. Shared hosting is typically the most affordable option, but it can also be slower and less secure than other types of hosting. Virtual Private Servers (VPS) : VPS hosting is similar to shared hosting, but each website is hosted on a virtual server that has its own dedicated resources. VPS hosting is faster and more secure than shared hosting, but it can also be more expensive. Dedicated Hosting : With dedicated hosting, you get an entire server to yourself. This option is the most expensive, but it also provides the highest level of performance, security, and flexibility. Cloud Hosting : Cloud hosting uses a network of servers to host your website, which provides better scalability and redundancy than traditional hosting options. Source : https://www.quora.com/How-does-web-hosting-work","keywords":"","version":"Next"},{"title":"Web Browser","type":0,"sectionRef":"#","url":"/cs-notes/frontend-web-development/web-browser","content":"","keywords":"","version":"Next"},{"title":"Page Rendering​","type":1,"pageTitle":"Web Browser","url":"/cs-notes/frontend-web-development/web-browser#page-rendering","content":" The rendering process involves 2 steps :  1. DOM Creation​  HTML code is parsed to create DOM (Document Object Model), this process is broken down into 3 steps :  Tokenization : The browser breaks the HTML code into tokens, which are small units of the code that represent different elements, attributes, and text content. Lexing : The browser converts the tokens into a series of nodes, which are the building blocks of the DOM. Each node corresponds to a different element, attribute, or piece of text content in the HTML code. Building the DOM Tree : The browser uses the nodes to build a hierarchical tree-like structure that represents the structure of the web page. The root node of the tree is the HTML element, and each child node corresponds to a different element or piece of content within the HTML code. Source : https://dev.to/arikaturika/how-web-browsers-work-parsing-the-html-part-3-with-illustrations-45fi  2. CSS Styling​  After creating the HTML DOM, each element is applied by the appropriate style on CSS file :  Creation of CSSOM : The browser breaks the CSS code into tokens which are again small units of the code that represent different elements, properties, and values. Tokens are used to create a CSS Object Model (CSSOM). Each node in the CSSOM corresponds to a different CSS selector, property, or value. Computing Styles : The browser matches the CSS selectors in the CSSOM with the HTML elements in the DOM tree to determine which styles should be applied to which elements. The browser then computes the final style for each element based on the matching rules in the CSSOM. Layout and Painting : Once the final styles for each element have been computed, the browser uses them to perform layout and painting, which involves determining the size, position, calculting the padding, border, and margin properties, and visual appearance of each element such as color and background color on the web page and painting the content onto the screen. Source : https://www.lambdatest.com/blog/css-object-model/  ","version":"Next","tagName":"h3"},{"title":"Browser Features​","type":1,"pageTitle":"Web Browser","url":"/cs-notes/frontend-web-development/web-browser#browser-features","content":" Browser also provide useful feature for user such as :  History : Web browsers maintain a history of the web pages that a user has visited. This history allows users to quickly revisit a web page that they've previously viewed, by clicking the back or forward button in the browser. The history also allows users to view a list of their recently visited web pages, which can be helpful for tracking their browsing history. History can also be manipulated by developer from Web History API Local Storage : Local storage is a feature that allows web developers to store data on a user's computer that persists even after the user closes the browser. This data can be used to store user preferences, application state, and other information that needs to be preserved between sessions. To access the local storage, we uses Web Storage API Cookies : Cookies are small text files that are stored on a user's computer by a web server. Web browsers use cookies to store information about a user's preferences and activity on a website. For example, a cookie might store a user's login credentials so that they don't have to enter them every time they visit the website. Cookies can also be used for tracking user activity across multiple websites, which has raised privacy concerns in some cases.   Source : https://classnotes.ng/lesson/the-internet-browser/ ","version":"Next","tagName":"h3"},{"title":"Web URL","type":0,"sectionRef":"#","url":"/cs-notes/frontend-web-development/web-url","content":"","keywords":"","version":"Next"},{"title":"URL Syntax​","type":1,"pageTitle":"Web URL","url":"/cs-notes/frontend-web-development/web-url#url-syntax","content":" The syntax of a URL consists of several parts, separated by special characters. Here is a breakdown of the main components of a typical URL :  Scheme : The scheme is the protocol or scheme used to access the resource. It specifies the rules and format for transferring data between the client and the server, and it determines the type of connection that will be established. The most common schemes are http:// and https://, but there are many others, such as ftp:// for file transfer protocol, and mailto: for email addresses. Subdomain : Subdomain separate different sections or areas of a website or to create separate websites that are related to the main website. For example mail.google.com and maps.google.com which are subdomain of google.com Domain : The domain name is the name of the website or server that hosts the resource. It is the main part of a website and typically uses a name that is easy to remember. The domain name is then translated into an IP address by the domain name system (DNS), which allows the client to locate the server on the internet. TLD (Top Level Domain) : TLD is the part of domain that is used to classify domain names by their purpose or geographic location. There are two types of TLD : Country Code TLDs : These are TLDs that are tied and commonly used for a specific country or region, such as .uk, .jp, and .ca.Generic TLDs : These are TLDs that are not tied to any specific country or region, such as .com, .org, .net, and .edu. Generic TLDs are commonly used for commercial, organizational, and educational websites. Ports : The port number is an optional parameter that can be used to specify the network port to use for the connection. It is typically used when the server is running multiple services or applications on different ports. The default port number for a scheme is usually implied if no port number is specified, such as port 80 for HTTP and port 443 for HTTPS. Path : The path is the location of the resource on the server, represented as a series of directories or files separated by forward slashes /. It specifies the location of the file or resource that the client wants to access, and it can include additional parameters and subdirectories as needed. For example, /path/to/resource.html might refer to an HTML file located in the resource.html file, which is located in the to directory, which is located in the path directory. Query String : The query string is an optional string of parameters that can be appended to the end of the URL, separated by a question mark. It is used to provide additional information or data to the server, such as search terms or user preferences. The query string consists of one or more key-value pairs, separated by an ampersand &amp;, and each key-value pair is separated by an equals sign =. For example, ?id=1234&amp;name=example might represent a query string with two parameters, id and name, with values of 1234 and example, respectively. Fragment : The fragment is an optional component of a URL that specifies a specific location or section within a web page. It is indicated by a hash symbol # followed by the name of the anchor or section. The fragment is used to provide a direct link to a specific section of a web page, such as a heading or paragraph, and it can be useful for navigating long or complex pages. For example, clicking this will take you the the beginning of this lists.  This is an example URL that brings them all together :https://www.example.com:8080/path/to/resource.html?id=1234&amp;name=example#section1   Source : https://www.semrush.com/blog/what-is-a-url/  ","version":"Next","tagName":"h3"},{"title":"Internationalized URL​","type":1,"pageTitle":"Web URL","url":"/cs-notes/frontend-web-development/web-url#internationalized-url","content":" An internationalized URL (also known as an IDN or Internationalized Domain Name) is a URL that includes non-ASCII characters, such as accented letters, non-Latin scripts, or other characters that are not part of the ASCII character set.  The use of internationalized URLs allows websites to use domain names that are more easily recognizable and memorable to users in different regions and languages. For example, a website in Japan might use a domain name that includes Japanese characters, such as &quot;日本語ドメイン.テスト&quot;, which translates to &quot;Japanese domain.test&quot; in English.  Internationalized URLs are encoded using the Punycode system, which allows non-ASCII characters to be converted into ASCII-compatible characters that can be used in a domain name. For example, the Japanese domain name &quot;日本語ドメイン.テスト&quot; would be encoded as &quot;xn--wgv71a119e.xn--zckzah&quot;. ","version":"Next","tagName":"h3"},{"title":"Machine Learning","type":0,"sectionRef":"#","url":"/cs-notes/machine-learning","content":"","keywords":"","version":"Next"},{"title":"All pages​","type":1,"pageTitle":"Machine Learning","url":"/cs-notes/machine-learning#all-pages","content":" Linear RegressionLogistic RegressionNaive BayesK-Nearest NeighborsDecision TreesRandom ForestGradient Boosting MachinesSupport Vector MachinePrincipal Component Analysisk-Means ClusteringCollaborative Filtering ","version":"Next","tagName":"h3"},{"title":"Collaborative Filtering","type":0,"sectionRef":"#","url":"/cs-notes/machine-learning/collaborative-filtering","content":"","keywords":"","version":"Next"},{"title":"How it works​","type":1,"pageTitle":"Collaborative Filtering","url":"/cs-notes/machine-learning/collaborative-filtering#how-it-works","content":" The first thing we do is to gather the user's data, suppose the recommendation system is to recommend specific movie. We can summarize all user rating to the corresponding movie with table like below.   Source : https://youtu.be/n3RKsY2H-NE?si=13c6_Ug3sDtUDXxb&amp;t=122  Some of the data will obviously be empty, indicating that the user hasn't rated or watched the movie yet. Suppose the system is going to recommend a movie to the user where the rating is empty, remember that the purpose of the recommendation system is to attract the user to watch the movie.  However, how can we know if a user will like a movie they haven't watched before? We will need an approximate measurement to indicate whether the user will like the movie or not. Thus, the problem becomes completing the data table.  Item-based recommendation​  In item based recommendation, recommendation is based on item similarities, in this case, it might be a movie genre. So, the movie genre can be the variables or the features that determine if a user likes a movie or not.  User rating to specific genre will be gathered, and will be represented in matrix. The numbers will be calculated using dot product. We can also scale it down to suit with the appropriate rating ranges (e.g. minimum 0 and maximum 5).  The idea of using the dot product is that if a user has a low rating for a particular genre, multiplying that rating with another number in another genre will result in an even lower value. Consequently, this lowers the overall rating. In other words, a low rating for a genre will decrease the rating for another movie with the same genre as well.  In the image below, user A likes comedy and dislikes action. Multiplying with the jurassic park movie, even though it has slight comedy, because the user rating toward action genre is 0, this makes the overall rating low.   Source : https://youtu.be/n3RKsY2H-NE?si=hxIzf4yWCppQHrc8&amp;t=229 (with modification)  However, this system might not work very well, considering not every user has rating to every genre and some of the user might not be great at describing their preferences.  User-based recommendation​  Another system is to recommend based on similarities between user's preferences. Instead of using user's rating as feature to generate unavailable rating toward specific movie, we use the available movie rating to generate user's feature.  Latent Factors​  In user-based recommendation, user features is called latent factors or latent features, these features are used to determine the pattern or relationship between user and the movie.  The numbers will first be random generated, and will also be represented in matrix. The prediction is same as before, user and movie latent factors will be calculated using dot product. The result is the rating prediction for the movie, higher rating means the user will more likely to like that movie.   Source : https://youtu.be/p4ZZq0736Po?si=OpsMAfAoQ0-6Aak8&amp;t=4307  But this won't be an accurate prediction, considering the latent factor used are random generated. This is where machine learning comes, these numbers will be adjusted. We will first calculate the loss function e.g. Mean Squared Error (MSE), where we sum all the difference between predicted rating and actual rating squared and divided it by the number of ratings. We can then use optimizer like gradient descent.  We can also add some bias terms or extra constant value to the user and movie ratings and use activation function like sigmoid, this is used to capture relationship between them that are not captured by latent factors.  We can even visualize this in graph, because latent factors or the features are in form of vector (matrix). Higher dimensional features can be reduced using technique like PCA. During the training process, the model will be trained to group similar items together.   Source : https://devopedia.org/word-embedding  tip This recommendation system is based on dot product model, where the model predict using dot product. Making recommendation system using deep learning technique like deep neural network is also possible.  Compression​  Now, after all the technique used, the latent factors is all we need to predict user rating. We can keep calculating the dot product and fill out the empty rating table. These latent factors are able to capture pattern of user's rating.  The latent factors technique is not only used in collaborative filtering, it can also be used as compression method. If we think the movie's rating as a color of image represented in matrix, taking the dot product between the latent factors will give us original image back. ","version":"Next","tagName":"h3"},{"title":"Index","type":0,"sectionRef":"#","url":"/cs-notes/index","content":"Index This page contains all the notes on this site. The structure is similar to the sidebar on the left. A total of 19 topics, and a total of 430 notes (may not be accurate for now, as it calculates the count by the number of lines it appear on topic list, in which some topic's list consist of placeholder). Digital Signal Processing (21 subtopics) SignalFourier Analysis Fourier SeriesFourier TransformConvolutionDiscrete Fourier TransformFast Fourier TransformDiscrete Cosine TransformWaveletsLaplace TransformZ Transform Signal Transmission SamplingQuantizationCodingMultiplexingSignal Transmission Medium Signal Processing FilteringCompressionDenoising Computer &amp; Programming Fundamentals (25 subtopics) Computer Representation Number SystemBinary RepresentationBitwise OperationFloating NumberData Representation Computer Fundamentals Operating SystemMemory Programming Fundamentals Programming ConceptsData Structures &amp; AlgorithmsProgramming Paradigm Imperative Imperative &amp; Procedural ProgrammingObject-Oriented Programming Declarative Declarative &amp; Functional ProgrammingQuery Language Concurrency &amp; Parallelism Code Execution CompilationInterpreterRuntime Environment Computer &amp; Programming Terminology Digital Media Processing (39 subtopics) Image Processing Image PropertiesImage EnhancementImage Acquisition &amp; SensingImage RestorationImage Editing Audio Processing Sound &amp; Audio PropertiesAudio Input &amp; OutputAudio EqualizationAudio EffectsAudio EditingSpeech Processing Video Processing Video RepresentationVideo RecordingVideo Effects &amp; EnhancementFlash Player SWF Digital Media Formats MIME TypeImage Bitmap (BMP)JPG / JPEGPNGWebPSVGGIF Audio WAVOGG VorbisMP3 Video AVIMP4 Document XMLMarkdown (MD)Text File (txt)PDF Computer Networking (45 subtopics) Network Standard OSI ModelTCP/IP Model Network Fundamental Network Addressing IP AddressSubnet MaskMAC AddressRoutingGatewayPorts Network Device HubsSwitchRouterDial-up Modem Computer Connection SocketBroadbandEthernetLAN &amp; WANServer Network Topology Network Services &amp; Protocol DNSDHCPNATProxyVPNNetwork Protocol TCP ProtocolUDPFTPEmail ProtocolRTPRTCHTTP &amp; HTTPS Wireless &amp; Mobile Networking Wi-FiBluetoothCellular NetworksSim Card Network SecurityNetwork EncryptionNetworking Command Data Structures &amp; Algorithms (33 subtopics) Analysis of AlgorithmsData Structures ArrayLinked-ListStackQueueHash TableSetTreeHeapGraphTrie Algorithms Common Types SortingSearchRecursionDivide And ConquerTraversalBacktrackingGreedyDynamic Programming Other Algorithms Two PointerSliding WindowPrefix SumGraph Algorithms Cycle DetectionShortest PathUnion FindTopological SortMinimum Spanning Tree Complexity Theory Computer Organization &amp; Architecture (15 subtopics) COA FundamentalsBoolean LogicCPU ALURegisters &amp; RAMControl UnitCPU Design Input/OutputAssembly LanguageComputer Architecture Von NeumannHarvardISA GPUVector Processors &amp; TPU Operating System (25 subtopics) OS FundamentalsKernelProcess ManagementMultithreadingProcess SynchronizationInterrupt HandlingSystem CallInter-process CommunicationMemory ManagementDisk ManagementFile SystemDevice ManagementBootingNetworkingUser InterfaceVirtualizationSecurityCase Studies Type of OSUnixBSDLinux KernelWindowsAndroidmacOS &amp; iOS Theory of Computation &amp; Automata (14 subtopics) TOC FundamentalsFinite AutomataRegular Languages (Part 1)Advanced AutomataRegular Languages (Part 2)Formal GrammarContext-Free GrammarPushdown AutomataTuring MachineChurch-Turing ThesisUndecidabilityChomsky HierarchyComputabilityComplexity-Theory Programming Language Theory (5 subtopics) abctype system (primitive, complex object, generics)implementation of modern language features, such as OOP, threading, handling null, handling exception. Compilers (11 subtopics) Compiliation Process preprocessing,lexical analysis,parsing,semantic analysis (syntax-directed translation),conversion of input programs to an intermediate representationcode optimizationmachine specific code generation. Compiler OptimizationDecompilationBuilding Process Database System (18 subtopics) Relational DataQuery LanguageDatabase Management Database Design Database ModelNormalizationTransactionsConcurrency ControlTrigger &amp; Constraints Logging &amp; RecoveryDatabase IndexDatabase Optimization NoSQLDatabase Implementation Storage ManagementIndex ImplementationQuery ProcessingQuery Compiler Computer Graphics (23 subtopics) Computer Images (Part 1)Computer Images (Part 2)2D Transformation3D TransformationGPU PipelineLow Level GraphicsCurvesSurfacesTextures (Part 1)Textures (Part 2)Shading (Part 1)Shading (Part 2)Rendering (Part 1)Rendering (Part 2)Ray TracingShadowsReflectionsSamplingSignal ProcessingComputer DisplayComputer AnimationPhysics-Based AnimationSimulation Frontend Web Development (13 subtopics) HTMLHTML DOMCSSJavascriptJSONHTML + CSS + JavascriptBrowserStatic &amp; Dynamic SiteWeb HostingNPM (Node Package Manager)Node JSReact JSDocker &amp; Kubernetes Backend Development (24 subtopics) Web ServerAPIs APIs &amp; Server LogicREST APISOAPGraphQLRPCWebhookWebSocket AuthenticationAuthentication TechniqueAuthorizationSearch EngineMessage BrokerArchitecture MonolithicMicroserviceSOAServerless ContainerizationDocker &amp; KubernetesBackend &amp; Server SecurityCachingBackend Optimization Computer Security (26 subtopics) Computer Security FundamentalsCryptography Math ConceptsHashing Hash FunctionMD5SHA EncryptionSymmetric Encryption DESAESBlowfish Asymmetric Encryption &amp; Key Exchange Diffie-HellmanRSAElliptic Curve CryptographyDSA Lattice-Based CryptographyBlockchain Antivirus &amp; AntimalwareReverse EngineeringNetwork SecurityWeb SecurityMobile SecurityBackend &amp; Server SecurityOther Attack &amp; Exploit Machine Learning (11 subtopics) Linear RegressionLogistic RegressionNaive BayesK-Nearest NeighborsDecision TreesRandom ForestGradient Boosting MachinesSupport Vector MachinePrincipal Component Analysisk-Means ClusteringCollaborative Filtering Deep Learning (35 subtopics) Deep Learning FoundationDeep Learning TasksNeural NetworkCNNResNetU-NetSiamese NetworkRNNLSTMGRUAutoencoderVariational AutoencoderGANTransformers Attention MechanismTransformers ArchitectureTransformers AudioBERTGPTBARTVision Transformers Diffusion ModelReinforcement Learning Reinforcement Learning FundamentalModel-Based Markov ModelsMarkov Decision Process Model-Free Monte Carlo MethodTemporal DifferenceSARSAQ-LearningPolicy GradientImitation Learning Multi-Agent Software Engineering (25 subtopics) Software ProcessSoftware Design Software PrinciplesDiagramsDesign Patterns Creational PatternsStructural PatternsBehavioral Patterns Software Architecture Client-ServerEvent-DrivenMaster-SlavePeer-to-PeerLayeredOther Architecture Patterns Software Management Software TestingSoftware DebuggingBuild &amp; Package ManagementVersion ControlOpen SourcingModularizationSoftware Deployment System DesignSystem Design Examples Cloud Computing &amp; Distributed Systems (22 subtopics) FundamentalsDistributed Systems Distributed Systems ModelDistributed Systems CommunicationDistributed File SystemDistributed Database Architecture &amp; Model Client-ServerEvent-DrivenMaster-SlavePeer-to-PeerMicroserviceMapReduceLambda Cloud-Native Technologies VirtualizationContainerizationDocker &amp; Kubernetes Cloud ModelsCloud DatabaseCloud SecurityCloud Services","keywords":"","version":"Next"},{"title":"Gradient Boosting Machine","type":0,"sectionRef":"#","url":"/cs-notes/machine-learning/gradient-boosting-machine","content":"","keywords":"","version":"Next"},{"title":"Gradient Boosting Process​","type":1,"pageTitle":"Gradient Boosting Machine","url":"/cs-notes/machine-learning/gradient-boosting-machine#gradient-boosting-process","content":" Initialize the ensemble : Start by initializing the ensemble model with a simple weak learner. A weak learner is a model or algorithm that performs only slightly better than random guessing on a given task. Weak learner is simply a simpler and smaller model, they are characterized by their limited predictive power and simplicity. This could be a decision tree with a small depth or a linear regression model. Compute initial predictions : Use the current ensemble model to make predictions on the training data. Compute residuals : Calculate the residuals, which are the differences between the predicted and actual values of the target variable. These residuals represent the errors made by the current ensemble model. These residuals are calculated by loss function, which is a measure of how well the models perform at one training. Example are MSE, MAE, and cross entropy. Fit a weak learner to the residuals : We will try to minimize error or correct mistake made by previous model by training new weak learner on the residuals. The goal is to find a model that can predict the residuals more accurately than the previous model. We can also use gradient descent for this. Update the ensemble : Add the new weak learner to the ensemble. The weak learner will have a paramater called shrinkage parameter or learning rate. The shrinkage parameter controls the contribution of each weak learner. The parameter also used for deciding the learning rate used in gradient descent. Update predictions : Update the predictions of the ensemble by combining the predictions of all the weak learners, including the newly added one. Iterate : Repeat steps 3-6 for a specified number of iterations or until a stopping criterion is met. In each iteration, the new weak learner focuses on the residuals left by the previous models, gradually improving the ensemble's predictive performance. Finalize the ensemble : Once the desired number of iterations is reached, the final ensemble model is obtained by combining the predictions of all the weak learners in the ensemble. Prediction : After training, we can then use the final ensemble model to make predictions on new unseen data.   Source : https://datascience.eu/machine-learning/gradient-boosting-what-you-need-to-know/  ","version":"Next","tagName":"h3"},{"title":"XGBoost​","type":1,"pageTitle":"Gradient Boosting Machine","url":"/cs-notes/machine-learning/gradient-boosting-machine#xgboost","content":" Extreme Gradient Boosting (XGBoost) is a library that implement of gradient boosting machine that is highly optimized with some extra features (not feature in data). Here are some example :  Regularized Learning : XGBoost includes regularization techniques such as L1 and L2 regularization. In simple term, regularization is a technique to make model prediction simpler by forcing some feature coefficients to be 0. In other word, we exclude some feature consideration for our prediction. The features we are excluding are the one that has low influence toward the overall prediction. This way we can focus on the more important and influential features. Computation Capabilites : XGBoost supports parallel processing, it optimizes the computation by parallelizing the construction of individual trees and parallelizing the evaluation of split candidates during tree building. Handling Missing Values : XGBoost has built-in capabilities to handle missing values in the dataset. It automatically learns how to handle missing values in a decision tree by assigning missing values based on available data. ","version":"Next","tagName":"h3"},{"title":"k-Means Clustering","type":0,"sectionRef":"#","url":"/cs-notes/machine-learning/k-means-clustering","content":"","keywords":"","version":"Next"},{"title":"Clustering​","type":1,"pageTitle":"k-Means Clustering","url":"/cs-notes/machine-learning/k-means-clustering#clustering","content":" The cluster itself is chosen by us, it is chosen by the machine learning practitioner. By choosing cluster, this mean we are choosing where how many cluster should be present. A cluster will have a central point called centroid, it is used as a reference point to measure the similarity or dissimilarity of data points within a cluster.   Source : https://youtu.be/R2e3Ls9H_fc?si=2dtBtEfRt0XrJ9Vf&amp;t=111  ","version":"Next","tagName":"h3"},{"title":"Assignment​","type":1,"pageTitle":"k-Means Clustering","url":"/cs-notes/machine-learning/k-means-clustering#assignment","content":" After setting up the centroid, each data point will be assigned to the cluster whose centroid is closest to it. The distance between a data point and a centroid is usually measured using Euclidean distance, although other distance metrics can be used as well.   Source : https://youtu.be/R2e3Ls9H_fc?si=iOL-ggvFbJoH32AG&amp;t=146  ","version":"Next","tagName":"h3"},{"title":"Adjustment​","type":1,"pageTitle":"k-Means Clustering","url":"/cs-notes/machine-learning/k-means-clustering#adjustment","content":" The centroids will be adjusted by taking the mean of all the data points values assigned to that cluster. This step moves the centroids closer to the center of their respective clusters. This will keep being repeated until the centroid no longer change or reach a predefined maximum number of iterations.  Once the limit is reached, the algorithm outputs the final clustering result, where each data point is assigned to a specific cluster based on the nearest centroid.   Source : https://youtu.be/R2e3Ls9H_fc?si=GQj0qVFf0Z_mRK2T&amp;t=176  ","version":"Next","tagName":"h3"},{"title":"k-Means Variation​","type":1,"pageTitle":"k-Means Clustering","url":"/cs-notes/machine-learning/k-means-clustering#k-means-variation","content":" In k-means, measuring of how well does a model performs to cluster, the within-cluster sum of squares (WSS) is often used. It measures the compactness or dispersion of data points within each cluster in k-means clustering. It quantifies how close the data points are to their respective cluster centroids. Measuring the WSS can be useful to decide how many cluster or &quot;k&quot; should we use.  In K-means clustering, the goal is to minimize the within-cluster sum of squares (to better cluster data). It is calculated by summing up the squared distances between each data point and its assigned centroid within the same cluster, and then summing these values across all clusters.  Mathematically, the within-cluster sum of squares (WSS) can be expressed as : WSS=ΣΣ∣∣x−c∣∣2WSS = \\Sigma \\Sigma ||x - c||^2WSS=ΣΣ∣∣x−c∣∣2   Source : https://datamahadev.com/understanding-k-means-clustering/  Elbow Method​  In k-means, the growth of WSS against to the number of cluster used or k, if graphed, looks like an elbow. The WSS generally decreases as K increases because adding more clusters allows for a better fit of the data points. However, beyond a certain point, the reduction in WSS diminishes, and adding more clusters may not significantly improve the clustering solution.  That certain point is typically 3 or 4, the &quot;elbow&quot; in the plot refers to the point of inflection, where the rate of decrease in WSS significantly slows down. The elbow point represents a trade-off between a low WSS more interpretable clustering solution.   Source : https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/ ","version":"Next","tagName":"h3"},{"title":"Decision Trees","type":0,"sectionRef":"#","url":"/cs-notes/machine-learning/decision-trees","content":"","keywords":"","version":"Next"},{"title":"Classification​","type":1,"pageTitle":"Decision Trees","url":"/cs-notes/machine-learning/decision-trees#classification","content":" Decision tree is used to classify data with no linear relationship between the features. To begin, we need to have a data with some features and their label (in case of classification, which group they belong to).  For example, the following image has 2 features which is x0x_0x0​ in x-axis and x1x_1x1​ in y-axis, the label or the group are red and green points.   Source : https://youtu.be/ZVR2Way4nwQ?si=KnrjjRrUxcUb423p&amp;t=126  We will then make a decision tree which will decide if a point should belong to red or green points. The goal of making the decision tree is to classify a new data with unknown label by asking several question until it can be distinguished by any group based on features of data observed before.   Source : https://youtu.be/ZVR2Way4nwQ?si=IL5ETPFD168sA8I5&amp;t=201  In this case, the question we are asking is whether the features are greater than or lower than some values (marked by straight line in the plot). By creating this decision tree, we can decided an unknown labeled data.   Source : https://youtu.be/ZVR2Way4nwQ?si=cDHANAN3Xw9o0nfr&amp;t=247  As the problem get more complex, this may be not enough, there are also many possible decision tree we can make to classify the data that result the same with before.  Entropy​  For example, we can choose to either split the data based on feature x1≤4x_1 \\le 4x1​≤4 or x0≤−12x_0 \\le -12x0​≤−12, using the first one, we obtained a fully distinguished red point.   Source : https://youtu.be/ZVR2Way4nwQ?si=HRwBlguUln_WyG-J&amp;t=387  We need to determine the best feature to split the data at each node of the tree. They help in selecting the most informative features that provide the most significant reduction to fully distinguish the data (remember we need to classify data).  We can measure the impurity of the data in something called Entropy. It quantifies the randomness or unpredictability of the target labels in a given set of examples. The entropy is calculated using the formula :   Source : https://youtu.be/ZVR2Way4nwQ?si=mtRqZ92xXOGmF63W&amp;t=414  The entropy formula is the sum of the probability of all label within the group of data. The first group (root node), has equal red and green point, the probability will be a 0.5. Subtituting 0.5 for the green probability to the entropy formula and summing it with the red probability, we will obtain 1. The result of entropy measure of how unpredictable our data is. The higher entropy means the more unpredictable.   Source : https://youtu.be/ZVR2Way4nwQ?si=JLeix7YIUmRlxqPE&amp;t=474  Information Gain​  And now we can measure how useful is the splitting the dataset based on specific feature by using information gain. A higher information gain indicates that the feature is more useful in separating the examples into different classes. Here is the formula :   Source : https://youtu.be/ZVR2Way4nwQ?si=SLLEhq-fNUcLhXuX&amp;t=508  Using the formula, we achieve a higher information gain for the second decision (the xo≤−12x_o \\le -12xo​≤−12), we will then choose to use this question to split at the root node. We will keep doing this until we reached the maximum depth of branch or until we can finally fully distinguish all the data if possible.   Source : https://youtu.be/ZVR2Way4nwQ?si=0Z2d9Lfye5MEXgQu&amp;t=602  ","version":"Next","tagName":"h3"},{"title":"Regression​","type":1,"pageTitle":"Decision Trees","url":"/cs-notes/machine-learning/decision-trees#regression","content":" Decision trees can also be used for regression tasks, the goal is to predict a continuous numerical value rather than a categorical label. When applying decision trees to regression, the basic structure and principles are similar to decision trees for classification, but with some differences in the way splits and predictions are made.  Instead of using measures like entropy or information gain, decision trees for regression uses variance as the measure of the impurity of the data. The higher variance means the higher impurity, the variance is calculated using the following formula :   Source : https://youtu.be/UhY5vPfQIrA?si=FXGyFY6aCQPYlYZy&amp;t=424  After calculating the variance, we can then calculate the variance reduction, it is similar to what information gain is in classification. The higher variance reduction means the better split we have. We will keep doing this again until certain conditions are satisfied.  The splitting process in a regression decision tree involves selecting a feature and a split point that minimizes the chosen error metric. The quality of the split will be measured in mean squared error (MSE) or mean absolute error (MAE).   Source : https://youtu.be/UhY5vPfQIrA?si=S5jc_TZ0jmgT3ceS&amp;t=496 ","version":"Next","tagName":"h3"},{"title":"K-Nearest Neighbors","type":0,"sectionRef":"#","url":"/cs-notes/machine-learning/k-nearest-neighbors","content":"","keywords":"","version":"Next"},{"title":"Classification​","type":1,"pageTitle":"K-Nearest Neighbors","url":"/cs-notes/machine-learning/k-nearest-neighbors#classification","content":" The first thing we do in k-NN is to plot the data point. The data point are plotted based on their features. We also need to know what are the points labeled as, in other word, what category do they belong to. The category is often referred as class.   Source : https://www.codespeedy.com/k-nearest-neighbor-algorithm-knn/  To classify which groups does a data belongs to, we will calculate distance to each nearest neighbors observed. The number of how many neighbors we want to calculate the distance is the &quot;k&quot;. It is called hyperparameter, a parameter that doesn't depend on the data itself, it is set by machine learning practitioner.   Source : https://youtu.be/0p0o5cmgLdE?si=sv97dptk2tx5TKIt&amp;t=30  We will sort the distance between each points and decide which group should the new data belongs to. Choosing k is very important as it directly influences how the algorithm makes predictions.   Source : https://youtu.be/0p0o5cmgLdE?si=aafjE06Goa3atf4B&amp;t=44  ","version":"Next","tagName":"h3"},{"title":"Regression​","type":1,"pageTitle":"K-Nearest Neighbors","url":"/cs-notes/machine-learning/k-nearest-neighbors#regression","content":" k-NN can also be used for regression tasks, where the goal is to predict the output of a new unseen data points based on the known output for similar feature values. To do that, we take the features of the k nearest neighbor and take the average of it.  Here is an example for 1-d k-NN where we only consider the feature on x-axis and the 2-d k-NN where we also consider the y-axis.   Source : https://youtu.be/3lp5CmSwrHI?si=eYKactnqqBtxBs-7&amp;t=125  If we draw a curve that approximate what should the next data point will be at based on the specific features :   Source : https://youtu.be/3lp5CmSwrHI?si=uuU8ht_VprtTEX4Y&amp;t=161 ","version":"Next","tagName":"h3"},{"title":"Naive Bayes","type":0,"sectionRef":"#","url":"/cs-notes/machine-learning/naive-bayes","content":"","keywords":"","version":"Next"},{"title":"Bayes Theorem​","type":1,"pageTitle":"Naive Bayes","url":"/cs-notes/machine-learning/naive-bayes#bayes-theorem","content":" Probability of a thing classified into some label can be mathematically writen as : P(Y=y∣X=(x1,x2,...,xn))P(Y = y | X = (x_1, x_2, ..., x_n))P(Y=y∣X=(x1​,x2​,...,xn​))  Where :  YYY : label, what we are going to classify a thing intoyyy : what the label actually isXXX : the features, the variable that affect why a thing is classified into some labelx1,...xnx_1,...x_nx1​,...xn​ : all the actual features  Let's say we have an event that has 2 features, x1 and x2, the outcome or can be either 0 or 1. We wanted to know in the future what is the outcomes for a specific features.   Source : https://youtu.be/lFJbZ6LVxN8?si=g3ks1QA5hPnq91xD&amp;t=304  We can use Bayes theorem here to calculate the outcome, we will need to calculate all the required number first, the evidence, prior outcomes, and the likelihood using the data known before. The calculation for them is simple, the number of outcomes of an event divided by total number event.   Source : https://youtu.be/lFJbZ6LVxN8?si=p3aB_jAKZMBnevEn&amp;t=353  This Bayes theorem looks working for a quite simple problem. However, as the problem getting more complex (more data and more features), we may never have an outcomes for particular features, and it's hard to predict if nothing particular happens before.  ","version":"Next","tagName":"h3"},{"title":"Naive Bayes Classifier​","type":1,"pageTitle":"Naive Bayes","url":"/cs-notes/machine-learning/naive-bayes#naive-bayes-classifier","content":" In naive bayes classifier, we assume in a naive way that all the features are independent, this mean the outcome of an event doesn't depend or influence each other. This is why we also need to make sure that the features we are studying are actually independent. By assumming they are independent, we multiply them instead.   Source : https://youtu.be/lFJbZ6LVxN8?si=St8QN9YRZJvRcjn_&amp;t=516 ","version":"Next","tagName":"h3"},{"title":"Logistic Regression","type":0,"sectionRef":"#","url":"/cs-notes/machine-learning/logistic-regression","content":"","keywords":"","version":"Next"},{"title":"Logistic vs Linear Regression​","type":1,"pageTitle":"Logistic Regression","url":"/cs-notes/machine-learning/logistic-regression#logistic-vs-linear-regression","content":" Logistic Regression is another form of regression, which relate to the study of the relationship between dependent variable (the variable we are trying to measure) and independent variable (the variable we believe affect the dependent variable).  Logistic regression is commonly used when the dependent variable is binary or categorical with two outcomes. The goal of logistic regression is to model the relationship between the independent variables and the probability of the binary outcome.  For example, logistic regression can be used to predict whether an email is spam or not based on the independent variable such as email's subject, sender, and content. The model would estimate the probability of the email being spam based on the values of the independent variables.  Logistic regression differs with linear regression, while linear regression model the linear relationship between dependent and independent variable using a line, logistic regression uses logistic function to model the non-linear relationship to obtain the probability of the binary outcome.  Using a best fit line to solve logistic regression problem is not suitable because the underlying relationship between the independent variables and the binary outcome is not linear. Logistic function instead model the relationship using an S shaped curve.   Source : https://youtu.be/Rt6beTKDtqY?si=_mR1DjREpVnYX014&amp;t=533  ","version":"Next","tagName":"h3"},{"title":"Logistic Function​","type":1,"pageTitle":"Logistic Regression","url":"/cs-notes/machine-learning/logistic-regression#logistic-function","content":" Logistic function, also known as sigmoid function takes any input value and produce 0 or 1 output. As the input approach negative value, the output will approaches 0 which indicate a low probability for the event to occur. Same as the opposite, as the input approach positive value, the output approaches 1, it indicates a high probability of the event occuring.  However, if input is 0, the output probability is 0.5 which means it has equal chance for the event to occur or not.   Source : https://medium.com/@shiny_jay/logistic-regression-a9a8749e1e68  The steepness or the slope of the logistic function at any given point represents the rate at which the probability changes with respect to the input. Steeper slopes indicate more significant changes in probability for small changes in the input, while flatter slopes indicate smaller changes in probability.  The z variable in logistic regression is used to adjust the sigmoid function to suit the relationship between dependent and independent variable. The z variable uses the same formula as linear regression :   Source : https://youtu.be/C5268D9t9Ak?si=axLEF6TXBSy572Cn&amp;t=243  Where :  x1...xkx_1 ... x_kx1​...xk​ : are the independent variable (often referred as weight), the variable that we believe affect the dependent variable. These variables are used to predict the binary outcomes.b1...bkb_1 ... b_kb1​...bk​ : bnb_nbn​ will be multiplied by corresponding xnx_nxn​, these are the independent variable coefficient that measures how &quot;strong&quot; does a particular variable affect.aaa : the a is same as y-intercept in a line, it capture the offset of the relationship.  Depending on the case, we could have just 1 independent variable.  ","version":"Next","tagName":"h3"},{"title":"Adjusting Logistic Function​","type":1,"pageTitle":"Logistic Regression","url":"/cs-notes/machine-learning/logistic-regression#adjusting-logistic-function","content":" Same as linear regression, the logistic function error will be calculated first and will be adjusted through an optimization process to minimize the error. The common optimization technique used in logistic regression is Maximum Likelihood Estimation (MLE). The point of optimization is to find the best coefficient for the logistic formula.  MLE​  In the first step, the probability of the event is expanded from 0 to 1 to negative infinity to positive infinity. We then plot each data, where the true value (1) is in positive infinity and the false value (0) is in negative infinity. We also draw a line which is constructed by the z variable in logisitc formula, it's based on the coefficient that we are looking to optimize.   Source : https://youtu.be/BfKanl1aSG0?si=BBCqyjSVBspkTKn_&amp;t=286  We will then project each data to the line, and we will observe the y value that correspond to it. The y value for it will be plugged in into a function called logit or log odds :   Source : https://www.google.com/search?q=what+is+log+odds  Where we input the y value as p here.  The result will be a number that becomes x-axis value in the logistic function :   Source : https://youtu.be/BfKanl1aSG0?si=OP0oSeGGvjfp2JH9&amp;t=317  Likelihood​  We will then project each point again to the logistic function and get the corresponding y-axis value. Except for the false value (red point), it will be calculated by 1 - the y-axis value. The result for each data point will be multiplied and it is called likelihood. We can also take the log of the probability and sum all of them, this is called log likelihood instead.   Source : https://youtu.be/BfKanl1aSG0?si=1hHmHRgzLxAGiBgs&amp;t=481  The result of this likelihood value measure how well does the logistic regression with the specific coefficient performs to predict the data.  We then try again several times using different coefficient to construct the line. We can even use optimmization algorithm such as gradient descent in linear regression.   Source : https://youtu.be/BfKanl1aSG0?si=EsbYzBJ8SSa2bYgC&amp;t=513  In linear regression, we try constructing a line with particular slope and calculate the error, we then use gradient descent to optimize the error function by obtaining the smallest error possible. In logistic regression, instead of error, we use the likelihood value. Also, the MLE which is the maximum likelihood estimation, we maximize the gradient descent instead of minimizing it.   Source : https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html (with modification)  Overall the loss function (function used to measure how well a model performs) used for logistic regression is the log likelihood. The log likelihood is also known as the binary cross entropy loss. ","version":"Next","tagName":"h3"},{"title":"Principal Component Analysis","type":0,"sectionRef":"#","url":"/cs-notes/machine-learning/principal-component-analysis","content":"","keywords":"","version":"Next"},{"title":"Principal Component​","type":1,"pageTitle":"Principal Component Analysis","url":"/cs-notes/machine-learning/principal-component-analysis#principal-component","content":" In simple term, PCA works by capturing the main variations in a dataset. It looks for patterns or trends that explain the differences between data points. PCA will keep a smaller set of features, called principal components, that retain most of the important information. The principal components are ordered by their significance. The first component explains the largest amount of variation in the data, the second component explains the next largest, and so on. By selecting a subset of the components, we can simplify the data while retaining most of its important characteristics.  We then project the original data onto the selected principal components. Each data point's values in the new reduced feature space are determined by their coordinates along the principal components.  We can take account all the feature and map it to 1-dimensional first, this is called the first principal component. Increasing the dimension again to 2-d, this is the second principal component.  ","version":"Next","tagName":"h3"},{"title":"Optimization​","type":1,"pageTitle":"Principal Component Analysis","url":"/cs-notes/machine-learning/principal-component-analysis#optimization","content":" The point of PCA is to project a data while also preserving info. PCA uses the orthogonal projection in mathematics, it is basically mapping a point or a vector to a lower dimensional space.  To make it simple, lets take an example in 2-d. A projection can range from 0 to 1, where 0 means the point we are projecting is perpendicular with the line below. Where 1 means we fully project it to the line.  In PCA, the data are considered as vector where the vector magnitude represent the features a data have. We want to capture all the information, we should make the projection result as high as possible. We want the vector projection to be 1 as the constraint.  Not only a single data, we want all data to also have a high projection. This becomes an optimization problem, the goal is to maximize the information variety or the variance of the projected data along each features or the principal component.    Eigenvalues​  Solving a constrained optimization problem involve using the lagrange multiplier. After going into the mathematical problem, it produced a matrix called covariance matrix. In simple term, covariance matrix summarizes the relationships between the different variables in the dataset. Specifically, it quantifies how the variables vary together.  Solving the problem further, it produced an eigenvectors and eigenvalues equation, where the matrix in the equation is the covariance matrix. The eigenvalues represent the amount of variance captured by each corresponding eigenvector (principal component) in the dataset.   Source : https://youtu.be/FD4DeN81ODY?si=wwoMIXF5Wwy6-0x4&amp;t=192  After obtaining the eigenvalue equation with the covariance matrix in PCA, the next step is to solve this equation to find the eigenvalues. The solution is the highest variation or the highest previously projection result in the covariance matrix.  Once the eigenvalues are determined, they can be used to order which data should be higher than other. The measure of how high is data than order is called power. The power are based on the magnitude of the eigenvalues.  ","version":"Next","tagName":"h3"},{"title":"Second Component​","type":1,"pageTitle":"Principal Component Analysis","url":"/cs-notes/machine-learning/principal-component-analysis#second-component","content":" After projecting the data for the first time, called as first principal component, we can also achieve the second component by doing the similar things as before. Both the first and second principal component is taken from the eigenvector with the highest and second highest eigenvalues.  PCA takes account all feature in high dimension and transform it into 2D where the axes of PCA (the component) shows how high a data is compared to other. Each data point is typically mapped onto the line and also sorted. Overall, PCA provides way to visualize high dimensional data by also considering all the feature.   Source : https://medium.com/@raghavan99o/principal-component-analysis-pca-explained-and-implemented-eeab7cb73b72 ","version":"Next","tagName":"h3"},{"title":"Linear Regression","type":0,"sectionRef":"#","url":"/cs-notes/machine-learning/linear-regression","content":"","keywords":"","version":"Next"},{"title":"Linear Regression Idea​","type":1,"pageTitle":"Linear Regression","url":"/cs-notes/machine-learning/linear-regression#linear-regression-idea","content":" Linear Regression is a statistical technique used to model and predict between dependent and independent variable. Dependent variable is a variable that we are going to measure or predict while independent variable is the one we thought is going to affect the dependent variable. In other word, we believe that it has impact or influence to variable we are going to measure.  In linear regression, we believe that there is a linear relationship between the dependent and independent variable, this means that as one variable change, the other will also change, the change will be consistent and propotional.  For example, in real scenario, as a house area getting larger, it make sense that the price will be more expensive. In this case, house area and house price has a positive relationship. If we know for sure the relationship between dependent and independent variable, we can predict what will happen next when a variable (typically the independent variable) change.  In linear regression, we predict the outcome of dependent variable by drawing a line. The line should be as fit as possible to the data we know before. This line act as a &quot;standard&quot; that capture the relationship between dependent and independent variable. With this standard, we can predict what is the outcome of dependent variable in some independent variable.  ","version":"Next","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"Linear Regression","url":"/cs-notes/machine-learning/linear-regression#example","content":" Before starting, we will need a dataset that consist of known dependent variable with its independent variable.  Consider the image below, after gathering enough data, we plot them all in a graph (denoted as blue point). For example, when the independent variable (xxx) is equal to approximately 3.5, the dependent variable (yyy) is approximately 2.1. After plotting all the data, we will try to construct a line. The line should be as representative as possible.   Source : https://saichandra1199.medium.com/linear-regression-1e279814e2bb (with modification)  Using the data, we know that typically a lower xxx value will also result in lower yyy value and the higher xxx will result in higher yyy. By plotting these data and drawing a line, we can predict easier. The line is a mathematical model, meaning that we use mathematical equation or notation to model a real life situation.  A straight line typically modeled by : y=mx+by = mx + by=mx+b, where mmm is slope of line, bbb is y-intercept. The yyy is the dependent variable we are going to find out and the xxx is the independent variable.  So if we want to know a yyy value for a particular xxx value, we can use the line as a standard to predict the outcomes. Using the line equation, we can just plug in an xxx value together with the equation of line we construct and we will get the yyy value.  This will not work if our data has no linear relationship, also the more data we have will result in better prediction as we will be able to capture more relationship about the variable.    ","version":"Next","tagName":"h3"},{"title":"More Detailed Explanation​","type":1,"pageTitle":"Linear Regression","url":"/cs-notes/machine-learning/linear-regression#more-detailed-explanation","content":" The goal of regression is to construct a line that fit the data as fit as possible to predict the next outcomes. Here is an example of data plotted on a graph.    By just seeing it, indeed there is a positive linear relationship between the x-axis and y-axis value. Next, we will draw a line that act as the standard for the prediction. We could make any line we want like the image below :    But this will probably result in bad prediction, the difference between the actual data (blue points) and the prediction data (anything that lies on the line) is kinda large. The difference between these are called error.  The error is calculated by calculating the difference between actual data and prediction data. These error will be summed up for each points. Error is used to measure how bad or good a regression line is, the more error means the worse line.  note There are many technique to measure the line error, an example is the Sum of Squared Errors (SSE), where the error is calculated by squaring the difference, the formula is SSE=∑(yi−y^i)2SSE = \\sum(y_i - \\hat{y}_i)^2SSE=∑(yi​−y^​i​)2 yiy_iyi​ = actual y value data y^i\\hat{y}_iy^​i​ = predicted y value data By the way, a mathematical function that measures the error (like this SSE) is called cost function. Other cost function we can use is the Mean Squared Error (MSE), which has the similar formula to SSE : SSEn\\frac{SSE}{n}nSSE​ (n is data size)  So we would want a line with less error like this :    By the way, when the predicted value is lower than the actual value, its called Underestimation. On the opposite, overestimation happens when the predicted or estimated value is higher than the actual value.  ","version":"Next","tagName":"h2"},{"title":"Minimizing Error​","type":1,"pageTitle":"Linear Regression","url":"/cs-notes/machine-learning/linear-regression#minimizing-error","content":" For simplicity, we will use a simpler data and we will ignore the y-intercept for now, we will just use the slope to make the line.   Source : https://youtu.be/Rt6beTKDtqY?si=heJuep-IpaK_V6ki&amp;t=108  The approach of fitting the best line is to try to draw a line with particular slope and calculate the error to see how it performs. We keep trying this for a several times, the result can be visualized where the x-axis is the particular slope of the line and the y-axis is the error. If we see the graph, the smallest possible error we can get is by drawing a line with a slope of 1.68.   Source : https://youtu.be/Rt6beTKDtqY?si=9_O7XygkvOvZRjiq&amp;t=202  This approach sounds good, but how do we know what slope to try next? Also, in real case, we can't visualize it as we need to try a bunch of times to be able to draw a perfect graph like above. This makes us not sure if increasing or decreasing slope will decrease the error or not.  The point of this is to find the corresponding x value for the minimum y value. Mathematically speaking, we are trying to find the minima of a function. The concept of minima is often used in optimization problems, where the goal is to find the input value(s) that minimize a given function. The commonly used technique to minimize the error in linear regression and other machine learning technique is the Gradient Descent algorithm.  ","version":"Next","tagName":"h3"},{"title":"Gradient Descent​","type":1,"pageTitle":"Linear Regression","url":"/cs-notes/machine-learning/linear-regression#gradient-descent","content":" Gradient descent is an iterative algorithm used to find optimal coefficient value such as the slope of the line in linear regression, it aims to minimize the error or the cost function.  The concept of gradient descent comes from calculus, where we find the critical points of a function that may be local minima, local maxima, or a saddle point when the derivatives of the function is equal to zero.  So we will need a function to do that, in this case the function may look like the image above, where we graph the slope in x axis and the corresponding error in the y axis. However, we can't guarantee it will be easy for us to capture the function as formula and to solve the function derivative equal at 0. Instead we will try to approximate the actual solution, gradient descent is such the method. Gradient descent can be thought as the numerical method for finding critical points of a function.  The derivative of a function shows the function behavior at that point, the positive derivative means that the function is increasing. To find the minima, we will go to the opposite direction where the function is decreasing.   Source : https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions/a/what-is-gradient-descent  To use gradient descent for optimizing slope of a line, we can use the following formula :    Where :  mnew\\text{m}_{\\text{new}}mnew​ : new slope we will try next mcurrent\\text{m}_{\\text{current}}mcurrent​ : current slope kkk : step size or learning rate, used to measure of how far we want to go to the negative direction of function. The learning rate is very important, it affects the learning significantly. A smaller rate may slow down computation but a higher rate may make us &quot;go to far&quot; in the minimum direction. Source : https://www.jeremyjordan.me/nn-learning-rate/ ddmE(m)\\frac{d}{dm} E(m)dmd​E(m) : The derivative of the function E(m)E(m)E(m), where E(m)E(m)E(m) itself is the error function we graphed before  The subtraction between the slope and the derivative indicates that we are moving in the opposite (negative) direction of the derivative. The slope is gradually adjusted through iterations. The speed and effectiveness of the learning process can vary depending on factors such as the function, the data, and the learning rate. This variability can result in slow or fast learning and lead to either favorable or unfavorable outcomes.  After using gradient descent to optimize the slope, we may achieve a better fitting line. However, we can still improve this by also taking account the y-intercept of the line.  Gradient Descent in 3D​  If we also use y-intercept to draw the line and we want to optimize it aswell, we can do the same gradient descent method. This mean we will need to extend gradient descent method to 3D, because now we have two input which is the slope of the line and the y-intercept.  To extend gradient descent to 3D or higher multi-dimensional function, we will use the concept of gradient, which is the generalization concept of derivative in higher dimension, this mean we will use partial derivative instead. The positive gradient at a point tells us the direction of the steepest accent of the function at a given point. To find the minima, we would do the same, which is to go to the opposite direction of positive gradient or the negative gradient.   Source : https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/  The formula now becomes :   Where : b is y-intercept  Using gradient descent in 3D, we can know have more than 1 independent variable, we could have 10 variable that affects the dependent variable. For example, a house price may not only depends on area, it may depends on location, how much beds it has, and etc.  ","version":"Next","tagName":"h3"},{"title":"Other Regression​","type":1,"pageTitle":"Linear Regression","url":"/cs-notes/machine-learning/linear-regression#other-regression","content":" There are various form of regression along with the optimization and the cost function used. An example is :  Polynomial Regression : Polynomial regression is the form of linear regression where the relationship between the variables are not linear, instead it is a form of polynomial function such as curves. Source : https://www.javatpoint.com/machine-learning-polynomial-regression ","version":"Next","tagName":"h2"},{"title":"Random Forest","type":0,"sectionRef":"#","url":"/cs-notes/machine-learning/random-forest","content":"","keywords":"","version":"Next"},{"title":"Random Forest​","type":1,"pageTitle":"Random Forest","url":"/cs-notes/machine-learning/random-forest#random-forest","content":" Aggregating​  Random forest solves decision tree problem by having several tree and each tree will handle their own features. This helps to capture different patterns and reduce the chances of all trees making the same mistakes or being overly influenced by a small subset of features. This process is called aggregating or feature selection.  Aggregating also introduces randomness, the trees become more independent and will offer a wider range of perspectives for each features.  Bootstraping​  After assigning feature to each tree, the tree will be trained on some sample data. Each tree will be trained on the same data size as the original training data. It may contain duplicate instances and lack some instances from the original data. This process is called bootstraping and the samples are called bootstrap samples.   Source : https://youtu.be/v6VJ2RO66Ag?si=JQ9By3pUHDSXJGq6&amp;t=293  Majority Voting​  To use random forest to predict a new unknown data, random forest uses the voting concept. This mean the new data with some features will be processed to corresponding tree that handles it. Each tree will decide what label or category does the new data belongs to. All the result will be combined and the most label will be the result of the prediction.   Source : https://youtu.be/v6VJ2RO66Ag?si=SeQePcebnrmx20fW&amp;t=346  ","version":"Next","tagName":"h3"},{"title":"Random Forest Regression​","type":1,"pageTitle":"Random Forest","url":"/cs-notes/machine-learning/random-forest#random-forest-regression","content":" The concept of using random forest as regression is the same as random forest for classifying. The difference is we use decision tree regressor instead. At the end of predictions of the individual decision trees, the result are typically averaged or combined to obtain the final regression prediction.  Each decision tree in the Random Forest predicts a continuous numerical value, and the final prediction is calculated as the average or median of the predictions from all the individual trees. ","version":"Next","tagName":"h3"},{"title":"Support Vector Machine","type":0,"sectionRef":"#","url":"/cs-notes/machine-learning/support-vector-machine","content":"","keywords":"","version":"Next"},{"title":"Hard vs Soft Margin​","type":1,"pageTitle":"Support Vector Machine","url":"/cs-notes/machine-learning/support-vector-machine#hard-vs-soft-margin","content":" In real scenario, sometimes the data can't be perfectly separable from different categories.  Hard Margin : In hard margin, the margin constraint is strictly enforced, meaning that the SVM aims to find a decision boundary that perfectly separates the data points of different classes with no misclassifications. This is applicable when the data is linearly separable and there are no sudden or unexpected data points that are significantly different from the rest. Soft Margin : In soft margin, a more relaxed approach is taken by allowing a certain degree of misclassification or violation of the margin constraint. This is applicable when the data is not perfectly separable or contains outliers. It introduces a slack variable (ξ\\xiξ) (pronounced as &quot;xi&quot;) for each data point, which quantifies the degree of misclassification or violation of the margin constraint. Source : https://www.quora.com/What-are-the-objective-functions-of-hard-margin-and-soft-margin-SVM  ","version":"Next","tagName":"h3"},{"title":"Kernel Trick​","type":1,"pageTitle":"Support Vector Machine","url":"/cs-notes/machine-learning/support-vector-machine#kernel-trick","content":" A data may not always be linearly separable, this mean they can't always be separated by a line or plane in higher dimension. Here is an image that show linearly and non-linearly separable data.   Source : http://www.tarekatwan.com/index.php/2017/12/methods-for-testing-linear-separability-in-python/  SVM has a technique called kernel trick to handle non-linear boundaries by implicitly mapping the data into a higher-dimensional feature space. By using this trick, we are able to separate the data better.  This works by multiplying each data point by some mathematical function (called kernel function) that measures the similarity between two data points. The result of multiplication will represents the data points in the higher-dimensional. There are many types of kernel function, different functions capture different types of relationships.  For example, a 2-dimensional data may be non-linearly separable, by using kernel trick, we added another dimension implicitly and it becomes 3-dimensional. Turns out the data is separable at 3-dimension.   Source : https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d ","version":"Next","tagName":"h3"},{"title":"Operating System","type":0,"sectionRef":"#","url":"/cs-notes/operating-system","content":"","keywords":"","version":"Next"},{"title":"All pages​","type":1,"pageTitle":"Operating System","url":"/cs-notes/operating-system#all-pages","content":" OS FundamentalsKernelProcess ManagementMultithreadingProcess SynchronizationInterrupt HandlingSystem CallInter-process CommunicationMemory ManagementDisk ManagementFile SystemDevice ManagementBootingNetworkingUser InterfaceVirtualizationSecurityCase Studies Type of OSUnixBSDLinux KernelWindowsAndroidmacOS &amp; iOS ","version":"Next","tagName":"h3"},{"title":"Android","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/android","content":"","keywords":"","version":"Next"},{"title":"Architecture​","type":1,"pageTitle":"Android","url":"/cs-notes/operating-system/android#architecture","content":"  Source : https://developer.android.com/guide/platform  Starting from the bottom :  Linux Kernel : The Android OS is based on the modified version of Linux kernel. The kernel is responsible for managing the system's core functions, such as memory management, process management, device drivers, and system security. Let's discuss some components related to the Android in this layer : Binder : Binder is the implementation of IPC mechanism in Android. The binder driver is part of the Linux kernel and manages the low-level details of the Binder IPC mechanism. The IPC is based on client-server model. Various type of data, including calls, parameters, and results are encapsulated in binder objects. The binder driver handles the creation, registration, and management of Binder objects, as well as the routing of transactions between clients and servers. Power Management : Power management involves managing and optimizing the power consumption of hardware devices and system components. This layer is different compared to traditional computing, due to Android being a mobile device. Android introduces a feature called wake locks to manage how the system goes to sleep. Wake locks allow the system to maintain the desired power state, either running and ready for user input or deeply asleep, based on the device's activity and user interaction. Camera : The camera handles the interaction between the hardware camera sensors and the software components that utilize them. It provides the necessary drivers and interfaces for controlling camera sensors, capturing images or videos, and handling camera-specific functionalities. Because Android can be customized, it is possible that different manufacturers offers different hardware, making it possible for specific Android device to have advanced feature such as dynamic range. Hardware Abstraction Layer (HAL) : The HAL layer provide a standardized interface for interacting with device-specific hardware components. It allows the upper layers of the Android stack to access hardware features, such as camera, sensors, audio, and display, without worrying about the underlying hardware details. Native Libraries : Android includes a set of native libraries written in programming languages like C and C++. These libraries provide essential functionality to the Android system, including graphics rendering, multimedia processing, SQLite database support, networking, and more. Android Runtime (ART) : Android Runtime is the runtime environment for executing Android applications. The majority of Android apps are developed using the Java programming language, this makes Android follow object-oriented design. In order to run a Java application, we will need a Java Virtual Machine (JVM) for its runtime environment. Android apps do not run directly on a standard JVM. Instead, Android introduced its own runtime environment called the Dalvik Virtual Machine (DVM). The DVM was designed for device with limited resources like smartphones and tablets. The DVM runs Java app using just-in-time (JIT) compilation, which dynamically translates Java bytecode into machine code at runtime to improve performance. However, since Android 5.0, DVM is replaced with Android Runtime (ART). The primary reason for this is that ART uses an ahead-of-time (AOT) compilation approach, where the application bytecode is compiled into native machine code during installation or upon first execution. This optimization improves performance and reduces runtime overhead. Java API Framework : This layer consist of a set of high-level APIs for developers to build Android applications. It includes various components, such as activity manager, content providers, broadcast receivers, and services. System Apps : At the topmost layer of the Android architecture are the applications themselves. These can be pre-installed system applications, such as the Phone app, Contacts, or Messaging, or third-party applications; they can be from specific manufacturers or downloaded from the Google Play Store or other sources.  ","version":"Next","tagName":"h3"},{"title":"Characteristics​","type":1,"pageTitle":"Android","url":"/cs-notes/operating-system/android#characteristics","content":" Activity : A single screen with a user interface is represented as an Activity. Activity has its own lifecycle, which indicates their status such as onCreate(), onStart(), onResume(), onPause(), onStop(), and onDestroy(). Compared to desktop, Android can only show a single activity to the user, activity shown to user is called foreground activity. Multiple activity can be run at the same time, but some will be put on the background. Activities in the background may retain their state and continue to run, but they have a lower priority compared to the foreground activity. Source : https://socs.binus.ac.id/2017/09/26/activity-dan-fragment-di-android-studio/ Memory Management : One of the memory management aspect of Android involves freeing up system resources by closing or &quot;killing&quot; the app's process on the background. By killing the app process, the app will need to go to the onCreate() lifecycle again. So, when the app is activated again from the background, the app will be started as a fresh start, losing all their state. Receiver : Receiver is a component that listens for and responds to broadcast messages sent by the system or other apps. The app can register to listen to specific broadcast intents. Receivers enable your app to respond to events happening outside its own context, such as receiving a text message, Bluetooth update, sensor information, or a low battery warning. Service : A service is a component that runs in the background, performing long-running operations without a user interface. Services can be categorized into foreground and background. Foreground service is shown to user, typically through notification, this can be an act of downloading files or playing music. On the other hand, background service is not shown to user, this can be an operation to sync local data to remote server. Intent : Intents are a messaging mechanism in Android that allow components (such as activities, services, and receivers) to communicate with each other. An intent can be used to start an activity, launch a service, or send a broadcast to a receiver. It carries information such as the action to be performed and the data to be used. Intents can be explicit, specifying the target component, or implicit, allowing the system to determine the appropriate component based on the intent's action and data (e.g., default gallery app to open photos). ","version":"Next","tagName":"h3"},{"title":"BSD","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/bsd","content":"","keywords":"","version":"Next"},{"title":"FreeBSD​","type":1,"pageTitle":"BSD","url":"/cs-notes/operating-system/bsd#freebsd","content":" The FreeBSD is an open source, portable, complete operating system that can run on x86, ARM, AArch64, RISC-V, and PowerPC computers.  FreeBSD features :  Kernel : FreeBSD has its own kernel, which is a monolithic one with modular design. The kernel initially uses the M:N threading model, where multiple user-level threads (M) were mapped onto a smaller number of kernel-level threads (N). In the version 7.0, FreeBSD switched to a 1:1 threading model. FreeBSD uses kqueue (Kernel event queue), which is an event notification mechanism. kqueue follows an event-driven architecture, where applications register interest in specific types of events and the kernel notifies them when those events occur. This way, the applications don't have to constantly check for changes or wait indefinitely. Instead, they can be notified promptly when something they're interested in happens. File Systems : FreeBSD supports various file systems, including UFS (Unix File System), ZFS (Zettabyte File System), and FAT (File Allocation Table). Networking : FreeBSD has a robust networking stack that includes support for TCP/IP, IPv6, routing, firewall, and Wi-Fi. Jails : FreeBSD introduced the concept of &quot;jails,&quot; which are a form of operating system-level virtualization. Jails allow the creation of isolated environments within a single FreeBSD installation, providing secure separation of processes, file systems, and network stacks. Ports Collection : FreeBSD has an extensive and well-maintained Ports Collection, which is a collection of software packages and their build scripts. The Ports Collection simplifies the installation and management of third-party software, allowing users to easily add and update applications from a vast selection of packages. ","version":"Next","tagName":"h3"},{"title":"Booting","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/booting","content":"","keywords":"","version":"Next"},{"title":"Boot Process​","type":1,"pageTitle":"Booting","url":"/cs-notes/operating-system/booting#boot-process","content":" When a computer is turned off, program can't run, this is because they are typically stored in RAM, which is a volatile memory that requires power. The goal of booting is to load the operating system into memory, including the kernel as its core component, which can execute user applications and provide various services.  When computer is turned off, it may be switched on via button. After that, series of process will occur :  Power-on self-test (POST) : Immediately after the computer is powered on, the computer's hardware components perform a Power-On Self-Test to check their functionality. This test ensures that essential hardware, such as the processor, memory, and storage devices, are working properly. Source : https://en.wikipedia.org/wiki/Power-on_self-test#/media/File:POST_P5KPL.jpg Bootstrap Program : A small piece of code stored in a read-only memory (ROM), known as the bootstrap program, bootstrap loader, or bootloader is executed. This program is responsible for locating the operating system's kernel and loading it into main memory. In a larger OS, the bootstrap loader retrieves a more complex boot program from the disk. This boot program is stored in a reserved block sector known as the boot sector. First-stage Boot Loader (Hardware Initialization) : This stage is responsible for initializing essential hardware components and preparing the system for the operating system to take control. The first-stage bootloader often resides in firmware, such as the system BIOS (Basic Input/Output System) or UEFI (Unified Extensible Firmware Interface). They are computer software stored in a non-volatile memory embedded within the hardware. They have low-level control with the system's hardware components. Source : BIOS, UEFI Second-stage Boot Loader (OS Initialization) : The second-stage bootloader is responsible for loading the actual operating system into memory and initiating its execution. The second-stage bootloader is often specific to the operating system and may include additional functionalities such as device driver loading, system configuration, OS choice, dual boot, or other boot options. GNU GRUB bootloader Source : https://en.wikipedia.org/wiki/Bootloader#/media/File:Debian_Unstable_GRUB2_(2015).png  After the operating system is loaded, the boot is considered done, as the operating system can now handle system or application programs.  ","version":"Next","tagName":"h3"},{"title":"Master Boot Record (MBR)​","type":1,"pageTitle":"Booting","url":"/cs-notes/operating-system/booting#master-boot-record-mbr","content":" MBR is a data structure located in the first sector (or boot sector) of a storage device. The MBR contains essential information for the system to start up.  Bootloader Code : The bootloader's code is stored in the MBR.Partition Table : The MBR stores a partition table, which is a data structure that describes the layout of partitions on the storage device. The partition table defines the starting and ending sectors of each partition, as well as the partition type.   Source : https://www.ionos.ca/digitalguide/server/configuration/what-is-mbr/ ","version":"Next","tagName":"h3"},{"title":"Device Management","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/device-management","content":"","keywords":"","version":"Next"},{"title":"Device Driver​","type":1,"pageTitle":"Device Management","url":"/cs-notes/operating-system/device-management#device-driver","content":" Device driver is a software component that acts as an intermediary between an operating system and a specific hardware device. Different type of hardware has different requirements, purpose of device driver is to create a standardized way to communicate between the higher-level applications and the hardware, abstracting away the complexities of the device.  Device drivers work by implementing the necessary software routines and algorithms to interact with the hardware device. The driver communicates with the device through the device's specific interface or protocol, which could be a bus interface (e.g., PCI, USB) or a network protocol (e.g., Ethernet). It uses device-specific registers, commands, and protocols to send and receive data to/from the device.   Source : https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/13_IOSystems.html  The driver exposes a standardized interface or API to the operating system and applications. This interface encapsulates the device's functionality and provides a set of functions or commands that the operating system and applications can use to interact with the device.  ","version":"Next","tagName":"h3"},{"title":"I/O Technique​","type":1,"pageTitle":"Device Management","url":"/cs-notes/operating-system/device-management#io-technique","content":" There are methods and approaches used to handle input and output operations between I/O devices and computer system.  Programmed I/O : In this technique, the CPU directly controls the data transfer between the I/O device and memory. The CPU actively polls or checks the status of the I/O device to initiate or complete data transfers. Programmed I/O is simple to implement but can be inefficient as it ties up the CPU.Interrupt-Driven I/O : I/O device initiates an interrupt signal to the CPU when it is ready to transfer data. The CPU then suspends its current execution, saves its state, and transfers control to an interrupt handler routine. The handler reads or writes data between the I/O device and memory. Interrupt-driven I/O allows the CPU to perform other tasks while waiting for I/O operations to complete, the CPU doesn't need to check periodically.Direct Memory Access (DMA) : This technique enable data transfers between memory and I/O devices without CPU intervention. Data between I/O devices and CPU are sent to a DMA controller. The controller is responsible for coordinating the data transfer directly between the device and memory.Memory-mapped I/O : Memory-mapped I/O treats I/O devices as if they were memory locations. The I/O device is assigned a range of memory addresses, and the CPU can read from or write to these addresses to communicate with the device. Memory-mapped I/O simplifies the programming interface, as the CPU can use standard load and store instructions to access the device.  ","version":"Next","tagName":"h3"},{"title":"I/O Devices Characteristics​","type":1,"pageTitle":"Device Management","url":"/cs-notes/operating-system/device-management#io-devices-characteristics","content":" To create a unified interface for I/O devices, each device is differentiated based on their characteristics, functionality, and purpose.  Data-transfer Mode : Specific data formats in which they send or receive data, it could be a character, or a block of bytes as a unit.Access Method : How the device access and transfer data, it could be sequential access or random access. Sequential access reads or writes data sequentially, while random access allows for direct access to specific locations.Transfer Schedule : Determines when and how data transfers occur between the I/O device and the computer system, such as whether transfers are synchronous or asynchronous, whether they occur in real-time or in bursts, and how data is buffered during the transfer process.Sharing : Some devices support sharing, allowing multiple processes to access them simultaneously, while others may have exclusive access, allowing only one process to use the device at a time.Device Speed : Rate at which an I/O device can process and handle data, such as the latency, seek time, transfer rate, and delays.I/O Direction : Whether an I/O device is primarily used for input (receiving data into the computer system) or output (sending data from the computer system). Some devices, such as storage devices, support both input and output operations.  ","version":"Next","tagName":"h3"},{"title":"I/O Scheduling​","type":1,"pageTitle":"Device Management","url":"/cs-notes/operating-system/device-management#io-scheduling","content":" When there are multiple I/O requests, the I/O scheduler decides the sequence in which these requests are processed by the storage devices to optimize performance and fairness. Some scheduling algorithm for CPU such as FIFO, priority-based, SJF, RR, can also be used in I/O scheduling.  For scheduling, the OS keep track device status, it maintains a device status table. It is a data structure that contains essential details about I/O devices, such as whether it is available, busy, or in an error state, a queue that holds pending I/O requests for the device.   Source : https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/13_IOSystems.html  ","version":"Next","tagName":"h3"},{"title":"Other Concepts​","type":1,"pageTitle":"Device Management","url":"/cs-notes/operating-system/device-management#other-concepts","content":" I/O requests from application can be made through system calls. System call allows user-level processes to request services from the kernel, which operates at a privileged level, can then perform the requested I/O operations on behalf of the user-level process. Not only for abstraction, this also introduce isolation to prevent user-level processes from directly manipulating or interfering with critical system resources and devices.  Buffer​  When transferring data between I/O devices, they potentially different speeds or data formats. When data is read from an I/O device, it is first stored in a buffer, which is a temporary storage area in memory, before being processed by the CPU or transferred to the application. Similarly, when data is written to an I/O device, it is first placed in a buffer before being transmitted.  Buffers help optimize I/O operations by allowing for asynchronous data transfer. They can reduce the impact of data rate mismatches between devices and provide a more efficient way to handle data transfers in chunks or blocks rather than individual bytes.  Cache​  Caches are typically used in conjunction with storage devices, such as hard disk drives or solid-state drives, to reduce the latency associated with accessing data from slower storage media.  In I/O operations, a cache can be used to temporarily store recently accessed data or frequently used instructions. By keeping this data closer to the CPU, the cache reduces the need to access the slower storage media, resulting in faster data retrieval.  Spool​  Spool is a temporary storage to hold output of a device, that cannot accept interleaved or concurrent data streams. Device such as printer can only handle one print job at a time. Spooling is a technique that incorporate spool by capturing the print jobs as they are submitted and stores them in the spool file. Each job is added to the queue in the order it was received.  The printer processes the print jobs one at a time, in the order they are stored in the spool file. It retrieves the next job from the spool file after completing the previous one. While the printer is working on one job, other jobs can continue to be spooled and added to the queue. The spooling system ensures that the printer receives the jobs sequentially, allowing it to print continuously without interruptions.   Source : https://www.javatpoint.com/what-is-a-print-spooler  ","version":"Next","tagName":"h3"},{"title":"I/O Requests​","type":1,"pageTitle":"Device Management","url":"/cs-notes/operating-system/device-management#io-requests","content":" There are several steps involve when I/O request is issued :  I/O request issue : The life cycle begins when a process initiates an I/O request by issuing a system call, such as a blocking read() or write(). The process specifies the file descriptor of the file to be accessed.Parameter Checking : The system-call code in the kernel verifies the parameters of the I/O request for correctness. If the requested data is already available in the cache, the data are returned to the process, and the I/O request is completed.Device-Driver Buffering : If the requested data is not available in the buffer cache, the kernel's device driver receives the I/O request. The device driver then stores the data in its buffer and schedules the I/O.I/O Processing : The device driver sends command to the specific device controller. The controller receives the command and perform the requested operation. In this step, the driver may monitor the device's status or progress.Interrupt Handling : When the I/O operation is completed, an interrupt is generated. The interrupt signals the completion of the I/O operation.Signal Completion : After processing the completed I/O request, the device driver signals the completion of the I/O operation to the process that initiated the request. This signal indicates that the requested data is now available for the process to use.   Source : https://techlarry.github.io/OS/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%A6%82%E5%BF%B5/12%20IO%20Systems/ ","version":"Next","tagName":"h3"},{"title":"Inter-process Communication","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/inter-process-communication","content":"","keywords":"","version":"Next"},{"title":"IPC Mechanism​","type":1,"pageTitle":"Inter-process Communication","url":"/cs-notes/operating-system/inter-process-communication#ipc-mechanism","content":" There are two main technique to communicate, by utilizing shared memory, or passing messages.  Shared Memory​  In shared memory, a portion of memory is allocated and made accessible to multiple processes for read and write. However, this communication mechanism may require additional synchronization technique to prevent concurrency issues.  Sharing memory is possible through virtual memory mapping. When a process is created, they are given some memory addresses space, which is the region of memory they have access to read/write. Behind the scene, the memory is not an actual physical memory, they are just some range of addresses that maps to the actual memory, which is called virtual memory address. When the process accesses the virtual memory space, the OS will translate the memory they are referring to the actual memory address located in the physical memory.  For example, when a process want to access some data in the address &quot;0x05&quot;, the actual data located on the physical memory may not be in that address. It could exist on address &quot;0x08&quot;, or &quot;0xFF&quot;, or any other address. This is why the memory address is called virtual, because the process treat the memory as if it's their own memory, but it is not.   Source : https://en.wikipedia.org/wiki/Virtual_memory  The same concept applies for shared memory, where there are several processes each with their own isolated virtual memory. When a process want to share memory, the OS will allocate some shared memory somewhere. After that, the process will attach to it, meaning the process will be given some range of address on their virtual address that it can use to access the shared data. Essentially, the OS maps the shared memory to the virtual memory owned by the process.   Source : https://www.linkedin.com/pulse/interprocess-communicationipc-using-shared-memory-pratik-parvati  The shared memory is located in a single place somewhere in the physical memory, but each process access it as if it's located on their memory space. For example, process A may own its shared memory in the address &quot;0x60000&quot; to &quot;0x80000&quot;, while the process B own its shared memory in address &quot;0x70000&quot; to &quot;0x90000&quot;.  After all the mapping, the process can now read/write to the shared memory as usual.  Message Passing​  Message Passing is an act of communication between processes or thread without using shared memory, it is inherently associated with &quot;message&quot;.  Message passing can be synchronous or asynchronous. In the synchronous model, the sender process blocks until the message is received by the recipient process. In the asynchronous model, the sender process continues execution immediately after sending the message, without waiting for a response from the recipient. The recipient process can receive the message at a later time.  Some example of message passing :  Pipes : Pipes are a form of inter-process communication (IPC) that allows the output of one process to be used as the input of another process. In a pipe, data flows in a unidirectional manner from the writer process to the reader process. Pipes can be either named or unnamed, with unnamed pipes typically used for communication between related processes (e.g., parent-child processes). Sockets : Sockets are a communication endpoint that enables bidirectional communication between processes over a network. They can be used for IPC within the same machine (domain sockets) or across different machines (network sockets). Message Queues : Message queues is where processes exchange messages through a shared queue in the operating system. Each message has a specific format and is placed into the queue by the sending process. The receiving process can then retrieve messages from the queue in a first-in-first-out (FIFO) order. Channels : Channels is a higher-level concept for message passing. Channels typically provide a set of operations, such as sending and receiving messages, and may incorporate synchronization mechanisms like blocking or non-blocking operations. Channels can be implemented using various underlying mechanisms, including shared memory, pipes, or sockets.   Source : https://beingintelligent.com/difference-between-shared-memory-and-message-passing-process-communication.html  ","version":"Next","tagName":"h3"},{"title":"RPC​","type":1,"pageTitle":"Inter-process Communication","url":"/cs-notes/operating-system/inter-process-communication#rpc","content":" Remote Procedure Call (RPC) is an IPC that specifically facilitates communication between processes or programs running on different computers or networked systems. RPC typically used for system that adopts the client-server architecture. It is an architecture where exist a client, someone who request something, and the server, the entity that respond to the request.  The primary purpose of RPC is to request a server to execute a particular instruction for you. In RPC, when you make a request, you would send a message containing a procedure or function including its parameter to the server. The server will receive the message and will execute the procedure. After that, the server will send back the result to us.  The server will know how to execute the procedure through a defined interface. The client specifies the available procedures or functions that the client can invoke on the server. The interface defines the method signatures, input parameters, and return types of these procedures.  The difference between RPC with HTTP requests is, HTTP is a general communication protocol, while RPC is a specific communication that lets you execute a procedure and get its result back from a server. In fact, RPC can use HTTP protocol under the hood. RPC is typically used in distributed computing, it enables applications to run something on remote systems as if it's running on user's machine.  tip See also RPC ","version":"Next","tagName":"h3"},{"title":"Disk Management","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/disk-management","content":"","keywords":"","version":"Next"},{"title":"Mass-Storage Structure​","type":1,"pageTitle":"Disk Management","url":"/cs-notes/operating-system/disk-management#mass-storage-structure","content":" Mass-storage structure is the organization and management of storage devices, which encompasses the physical and logical components that enable the disk to store, access, and retrieve large amounts of data.  There are 3 common secondary storage :  Magnetic Disk (Hard Disk Drive [HDD]) : Structure : Magnetic disk consist of platter disks stacked on top of each other on a spindle. Each plate has a special coating that can hold a magnetic field. The plates are enclosed within an assembly arm, where each plate has a read/write head above it. Working Principle : HDDs use magnetism to store and retrieve data. The head can rotate to move across the surface of the platter. The rotation speed of the head is measured in revolutions per minute (RPM). A higher RPM indicates faster rotation speed, which imply a faster data access times. The heads can read and write data by changing the magnetic orientation of tiny regions on the platter's surface, representing the 0s and 1s of digital data. Data Access : When data is written, the read/write heads apply a magnetic field to a specific location on the platter, aligning the magnetic particles in the desired orientation. When reading data, the heads detect the magnetic field on the platter and convert it into electrical signals that represent the stored data. The heads move across the platter surfaces to access different tracks and sectors, allowing random access to data. Source : https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/10_MassStorage.html Solid-State Drive (SSD) : Structure : An SSD consists of multiple flash memory chips organized into memory blocks. Each memory block contains multiple memory cells, which are further organized into pages. The SSD also contains a controller that manages the data flow between the flash memory chips and the computer system.Working Principle : SSDs use flash memory, a type of non-volatile memory, to store and retrieve data. Flash memory is made up of floating-gate transistors that can hold an electric charge, representing the 0s and 1s of digital data.Data Access : To write data, the controller sends electrical signals that charge the floating-gate transistors, altering their electrical state and storing the data. To read data, the controller measures the electrical charge in the transistors to determine the stored data. Unlike HDDs, SSDs have no moving parts, allowing for faster data access and improved reliability. Magnetic Tape : Structure : Magnetic tape is wound on reels or cartridges. The tape is passed over a read/write head, which magnetically writes data onto the tape or reads data from it. Working Principle : Magnetic tape is a sequential storage medium that uses magnetism to record and retrieve data. It consists of a long, narrow strip of plastic coated with a magnetic material. Data Access : Magnetic tape is designed for sequential access, which means that data is read or written linearly from one end of the tape to the other. To access a specific piece of data, the tape must be fast-forwarded or rewound until the desired location is reached. Magnetic tape is commonly used for backup and archival purposes due to its relatively low cost and high storage capacity, although it tends to have slower access times compared to disk-based storage. Source : https://cs.hofstra.edu/~cscvjc/Fall06/Slides/Sess09/img25.html  ","version":"Next","tagName":"h3"},{"title":"Disk Structure​","type":1,"pageTitle":"Disk Management","url":"/cs-notes/operating-system/disk-management#disk-structure","content":" Disk is divided into several logical components :  Track : A track is a concentric circular path on a disk's platter. It is defined as a complete circle around the disk's surface at a specific radius. Sector : A sector is the smallest addressable unit of storage on a disk. It represents a fixed-size section of a track. Sectors are typically formatted to hold a specific amount of data, such as 512 bytes or 4 kilobytes. Data is written to and read from sectors using the disk's read/write heads. These sectors are abstracted logically by file system and grouped together into a unit called block. The block size can vary depending on the file system and disk configuration. Cluster : A cluster, also known as an allocation unit, is a group of blocks that are treated as a single unit by the file system. It is the smallest addressable unit of disk space for file allocation. Source : https://www.kompasiana.com/image/stevenmlf/5e8ae37ed541df08e35c66b2/bagaimana-harddisk-membaca-data-dan-arsitektur-sektor?page=1  ","version":"Next","tagName":"h3"},{"title":"Disk Scheduling​","type":1,"pageTitle":"Disk Management","url":"/cs-notes/operating-system/disk-management#disk-scheduling","content":" To read/write data to disk, we will issue system call provided by the operating system. The I/O request require :  Input or Output : Specify whether it's an input (read) or output (write) operation.Disk Address : Specify the location on the disk including information about the track, sector, and cylinder.Memory Address : The address or the location in memory where the data to be read from or written to the disk is stored. It represents a buffer or a specific region of memory allocated for the I/O operation.Number of Sectors : Indicates the size or quantity of data to be transferred between the disk and memory.  Disk Scheduling is a technique used by operating systems to determine the order in which disk I/O requests are serviced. It is crucial to optimize disk access and minimize the seek time, which is the time taken for the disk's read/write heads to move to the desired location on the disk.  Some example of disk scheduling algorithms :  First-Come, First-Served (FCFS) : This is a simple disk scheduling algorithm that services requests in the order they arrive. However, it does not consider the location of the requests on the disk, which can lead to poor performance if requests are scattered across the disk. Source : https://www.geeksforgeeks.org/difference-between-fcfs-and-sstf-disk-scheduling-algorithm/ Shortest Seek Time First (SSTF) : This algorithm selects the request that requires the least movement of the read/write heads from the current position. It aims to minimize the seek time by prioritizing the closest request. Source : https://www.geeksforgeeks.org/difference-between-sstf-and-look-disk-scheduling-algorithm/ SCAN (elevator algorithm) : The SCAN algorithm moves the read/write heads in one direction (e.g., from the outermost track to the innermost track) and services all the requests in that direction. When it reaches the end, it reverses direction and services the remaining requests. There is also a variant of SCAN, C-SCAN (Circular SCAN), which jump immediately to the other end instead of reversing the direction. Source : https://stackoverflow.com/questions/58289534/scan-and-cscan-algorithm-in-operating-system  ","version":"Next","tagName":"h3"},{"title":"Disk Management​","type":1,"pageTitle":"Disk Management","url":"/cs-notes/operating-system/disk-management#disk-management","content":" Disk Formatting​  Disk formatting is the process of preparing a storage device to be used by an operating system. Disk formatting involves three process :  Physical Formatting : Initialize the physical tracks and sectors on the disk, as well as configuring parameters such as sector size. Physical formatting is typically performed by the manufacturer during the production of the disk and is not normally necessary or accessible to end-users.Partition : Disk is typically divided into logical regions that can be managed separately, called partition.Logical Formatting : Involves creating the file system structures such as creating an empty file or directory and the necessary metadata.  Boot Sector​  When the computer goes through the boot process, it needs an initial program to run. A portion of the program data is stored in a read-only memory (ROM), while another portion resides in the boot sector, which is a fixed position on the disk dedicated to storing the initial program.   Source : https://en.wikipedia.org/wiki/Boot_sector#/media/File:GNU_GRUB_components.svg  info See also master boot record (MBR).  Bad Sector​  Bad sector refers to a portion of a storage device that is physically damaged or unable to reliably store data. A bad sector can occur due to various reasons, including manufacturing defects, physical damage, aging, or other issues with the disk.  There are two types of bad sectors :  Hard Bad Sector : It is a permanent physical defect on the disk's surface. It cannot be repaired or recovered because it represents a physical flaw, such as a scratch, magnetic degradation, or a faulty read/write head.Soft Bad Sector : It is a logical or temporary issue that arises due to problems with data storage or retrieval. Soft bad sectors can occur due to data corruption, software errors, or issues with the disk's internal mechanisms. In some cases, soft bad sectors can be repaired by performing disk maintenance operations.  When bad sector is found, it is flagged to be unusable. If it is possible to recover the bad sector, we can remap the bad sector to a reserved spare sector. This process involves updating the disk's internal mapping table to redirect any read or write operations from the original bad sector to the spare sector.  Free-space management​  When we delete a file, it is not immediately removed from the disk. File is simply marked as free space instead of being deleted. When we want to create a new file, we will overwrite the disk which is marked as free. This effectively delay the physical deletion of files, which will reduce the performance overhead when deleting file.  The OS can choose various data structure to keep track the free space, the data structure is called free-space list.  Bit Vector : A block in the disk has a bit, each bit will either be 0 or 1, where 0 represent allocated and 1 represent free space. This data structure is simple, but when it needs a free space, it needs to check the bit one by one, potentially going through all the block. Source : https://www.scaler.com/topics/free-space-management-in-os/ Linked List : The system maintains a linked list of free blocks. This approach allow us to get free space efficiently, because all the node in the linked list is already a free space. However, because the pointer to the next block is located inside the block, traversing the list would be inefficient, because it requires I/O operation to read the block. Source : https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/12_FileSystemImplementation.html Counting : This approach involves keeping track of the count of contiguous free blocks. This approach takes advantage of the fact that data is often stored in contiguous blocks, so when a block is freed, it is likely that the adjacent blocks are also free. We only need to store the address of the first free block and the number of free contiguous block next to it.  Storage Allocation​  Storage allocation is the process of assigning and managing storage space for data or resources on a storage device, determining where and how they are stored, organizing them systematically, and tracking their locations for efficient retrieval.  There are three common method to allocate :  Contiguous Allocation : In this method, files are stored as continuous blocks of data on the storage device. Each file occupies a contiguous section of the storage space. We will then store the location of the block with its length. Contiguous allocation provides fast access to data, but it can suffer from fragmentation as files are created, modified, and deleted. Source : https://mocki.co/contiguous-allocation/ Linked Allocation : Linked allocation uses linked data structures, such as linked lists, to manage storage space. Each file is divided into blocks, and each block contains a pointer to the next block. The file system maintains a table or index to keep track of the blocks' locations scattered around. Linked allocation allows for dynamic storage allocation and flexibility, solving fragmentation issue, but can introduce overhead due to the need to traverse the linked structure. Source : https://www.geeksforgeeks.org/file-allocation-methods/ Indexed Allocation : Indexed allocation uses a table to store the location of a file's block. The file's block will contain pointers or addresses that directly reference the data blocks on the storage device. Indexed allocation allows efficient access to files by random access, but it requires additional storage space for the index structure. Source : https://www.scaler.com/topics/file-allocation-methods-in-os/  ","version":"Next","tagName":"h3"},{"title":"RAID​","type":1,"pageTitle":"Disk Management","url":"/cs-notes/operating-system/disk-management#raid","content":" Redundant Array of Independent Disks (RAID) is a technology that combines multiple physical disk drives into a single logical unit. RAID provides parallel data transfer across all the disks by distributing data across multiple disks, it utilizes all the disk transfer rate.  Another benefits of RAID are data redundancy. A technique called mirroring allow us to have a backup of data. The technique involves writing a copy of a data written into a specific disk into every disk. While it is expensive, it ensures that data is still accessible even if one or more disk drives fail.  RAID can be configured in many levels :  RAID 0 : Also known as striping, RAID 0 improves performance by splitting data across multiple disks without redundancy, offering no data protection.RAID 1 : RAID 1, or mirroring, duplicates data across two or more disks to provide redundancy, providing a backup data.RAID 0 + 1 and 1 + 0 : RAID 0 + 1 combines mirroring and striping. It provides both high performance and data redundancy, but it requires a larger number of drives compared to other RAID levels.  And many other levels…   Source : https://ttrdatarecovery.com/raid-1-vs-raid-10-comparison/ ","version":"Next","tagName":"h3"},{"title":"File System","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/file-system","content":"","keywords":"","version":"Next"},{"title":"File​","type":1,"pageTitle":"File System","url":"/cs-notes/operating-system/file-system#file","content":" File is a named collection of related data that is stored on a storage medium. File contains information, depending on the file format, it can either be binary data (machine-readable) or plain text data (human-readable), which is encoded in specific encoding format such as ASCII or UTF-8.  Internally, a file is typically represented as a sequence of bytes or characters. File have specific format that explain how the file is structured. For example, image file format such as PNG structure it's contents such that, computer know how to interpret it meaningfully. It may contain information about the file or the actual file information; in the case of images, it is the pixel data.   Source : PNG file, TXT file  The specific format or type of files is typically associated within the name as its extension. They are letters or symbols that appear after the dot in a file name (e.g., .txt for a text file, .jpg for an image file).   Source : https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/11_FileSystemInterface.html  tip More about the structure of specific file format  File Attributes​  The file system keep track of the data information, file has many attributes :  Name : Every file has a unique name, which is human-readable, that identifies it within the file system. The name is used to reference and locate the file.Size : The size of a file refers to the amount of storage space it occupies on the storage medium, usually measured in bytes, kilobytes, megabytes, or larger units.Type/Format : Examples include text files (e.g., .txt), image files (e.g., .jpg, .png), audio files (e.g., .mp3), video files (e.g., .mp4), and many more.Location : The location of a file refers to its physical position or address on the storage medium.Metadata : Other information about the file, such as creation and modification timestamps, file permissions, file owner, file attributes, and other relevant details.Protection : Security related information, such as, who is allowed to read or write, is it visible, is it read-only, etc.  File Operation​  Create : Create a new file entry along with information associated with the file, this requires allocating space for the file, and a directory to store the file.Write : The write operation is used to modify the contents of a file or append new data to it. Through system call, we will specify the target file, starting position, and the data to be written. The OS translates the logical file address to a physical disk location and writes the data to the appropriate sectors or blocks on the storage medium.Read : The read operation retrieves data from a file and transfers it into memory. It involves specifying the file to read, the starting position, and the number of bytes to read. The OS translates the logical file address to a physical disk location, retrieves the requested data, and transfers it to the requesting process or buffer.Reposition / Seek : The seek operation is used to reposition the current read/write position within a file. It involves specifying an offset from a reference point (e.g., the beginning, current position, or end of the file) and a direction (forward or backward). The OS updates the file's read/write pointer accordingly, allowing subsequent read or write operations to occur at the desired location.Delete : The delete operation removes a file from the file system. It involves locating the file's metadata, freeing the associated disk space, and updating directory entries or file system structures to reflect the deletion.Truncate : Truncating a file adjusts its size by removing or discarding a portion of its contents, based on the specified target size. The OS updates file metadata and frees disk space as needed. Truncation can both shrink and expand a file, making discarded data inaccessible.Open : Establishes a connection or handle to a file, enabling subsequent operations on it. It involves specifying the file's name or identifier, access mode (e.g., read-only, write-only, or read-write), and other optional parameters. The OS verifies access permissions, allocates necessary resources (e.g., file descriptors), and prepares the file for subsequent operations.Close : The close operation terminates the connection or handle to an open file. It involves releasing any resources associated with the file, such as file descriptors or locks.  tip Terminology : File pointer is reference or indicator that points to a specific location within a file, in which next read or write operation will occur. File pointer for read operation is called read-pointer and for write operation, it is called write-pointer.File descriptors are unique identifiers or numerical representations that represent access to a file, socket, or other I/O resources.File locks are mechanism to restrict access to a file or a portion of a file to a single process or thread, to prevent concurrent access. There are two types of locks, shared lock, which allows multiple processes to read concurrently, or exclusive lock, which grants exclusive access to a process for writing to the file.File-open count is a number of times a file has been opened by different processes or within the same process. It helps keeping track of how many processes currently have the file open.  File Security​  To ensure only authorized processes have access to files, the OS provide a mechanism called access control. The OS maintains metadata about each file, including ownership and permissions. These permissions specify which processes are allowed to perform certain operations on the file, such as read, write, execute, append, delete, or list. Read, write, execute, and list are descriptive, append means writing new information at the end of the file, and list, list the name and attributes of the file.  In a multi-user environment where multiple users operate the same computer, meaning they may share access to files, this is where file security becomes even more critical. The operating system provides features to handle file sharing and control how files may be accessed by different users, not just processes.  In Unix OS, each file is assigned a 9-bit binary protection code, which consists of three 3-bit fields representing the owner, the owner's group, and everyone else (also called universe). These fields determine the level of access granted to different users or groups for a particular file.  The three permission bits for each field are :  Read (r) : View the contents of the file.Write (w) : Modify or delete the file.Execute (x) : Grants the user or group the ability to execute the file, in the case of executable files or scripts, or to access the contents of a directory.  For example, a file with the permission code &quot;rwxr-x--x&quot; means that the owner has read, write, and execute permissions, the owner's group has read and execute permissions, and others have only execute permission.   Source : https://www.researchgate.net/figure/File-system-protection-algorithm_fig3_260525066  ","version":"Next","tagName":"h3"},{"title":"File Organization​","type":1,"pageTitle":"File System","url":"/cs-notes/operating-system/file-system#file-organization","content":" Directory​  Within the file system on a disk, files are organized in directories or folders. A directory is a way of grouping files together, it is a container that holds related files and provides a hierarchical structure for organizing and managing these files. A directory itself does not represent a physical entity on the disk but rather serves as a logical container or organizational unit for files and subdirectories.  Directories create a hierarchy within the file system, forming a tree-like structure. At the top of the hierarchy is the root directory, which serves as the starting point for navigating the file system. Subdirectories can be created within the root directory, and further subdirectories can be created within those, forming a nested structure.   Source : https://informationtechnologyja.wordpress.com/2020/10/19/information-technology-grade-9-lesson-2-tree-directory-structure/  Directory Levels​  Directory can be categorized based on their organizational structure and the depth of their hierarchy :  Single-Level Directory : All files are stored in a single directory without any subdirectories. While it's simple, it can be confusing to manage and locate specific files as the number of files increases.Two-Level Directory : Files are organized into multiple directories, with each directory having a unique name. Each file is associated with the name of the directory it belongs to, along with its own individual name.Tree-Structured Directory : This is the most commonly used directory. This directory structure its file in hierarchical tree-like structure. The top-level directory is the root directory, and from there, subdirectories can be created, each containing files or additional subdirectories.   Source : https://www.scaler.com/topics/directory-structure-in-os/  Pathnames​  Files within the file system are identified by their pathnames, which specify their location within the directory hierarchy. A pathname typically includes the names of directories traversed from the root directory to the specific file, separated by slashes (&quot;/&quot;) in Unix-like systems or backslashes (&quot;&quot;) in Windows systems.  For example, in Unix, a document file could be inside the Documents directory : /home/user/Documents/file.txt, where the root directory is just /. In Windows system, C:\\Users\\user\\Documents\\file.txt, where the root directory is C:\\, the C is a drive letter which identify different disk partition in the storage.  There are three types of pathnames :  Absolute : An absolute pathnames from the root to the file or directory. For example, in Unix-like systems, an absolute pathname could be /home/user/Documents/file.txt, where /home/user/ represents the path to the Documents directory, and file.txt is the file name. Relative : Location of a file or directory relative to the current working directory. For example, if the current working directory is /home/user/, a relative pathname could be Documents/file.txt to refer to the file within the &quot;Documents&quot; directory. In the context of file system, the single dot &quot;.&quot; and double dot &quot;..&quot; has a special meaning. The single dot represents the current directory, it is used to refer to the current working directory in relative pathnames. For example, if the current working directory is /home/user/, the pathname ./file.txt refers to the file file.txt in the current directory. The double dot represents the parent directory, it is used to refer to the directory one level up in the directory hierarchy. For example, if the current working directory is /home/user/Documents/, the pathname ../file.txt refers to the file file.txt in the parent directory of Documents, which is /home/user/. Canonical : Absolute pathname that has been simplified or normalized to its simplest form. It eliminates any unnecessary or redundant elements such as &quot;.&quot; (current directory) and &quot;..&quot; (parent directory) references. For example, /home/user/Documents/../file.txt accesses the file.txt in the parent of Documents directory, this can be simplified to /home/user/file.txt.  Representation​  Other than tree, there are other data structure can be used to represent directories. One of them is graph, using graph allows directories to have multiple parents, whereas trees enforce a strict hierarchical structure with a single parent for each directory. This can be useful when a directory needs to be logically linked or shared between multiple locations within the file system.  Multiple parents allow directory to have links, which are references or pointers to other files or directories.  Hard / Physical Links : A hard link is a direct reference to a file or directory. When a hard link is created, it points to the same underlying data as the original file or directory. Changes made to the file through any of its hard links are reflected in all other hard links. Hard links can only be created for files within the same file system since they reference the underlying data blocks directly. While hard links can be used to create multiple references to a file, they cannot create cyclic dependencies or loops within a graph structure. Soft / Symbolic Links : On the other hand, soft link acts as a pointer or reference to another file or directory. Unlike hard links, soft links are separate files that contain a path to the target file or directory. When accessing a soft link, the file system resolves the link and redirects the request to the target file or directory. Changes made to the target file or directory are reflected in the soft link, but modifying the soft link itself does not affect the target. Soft links make it possible to create loops or cycles within a graph-like structure. Source : https://www.scaler.com/topics/hard-link-and-soft-link-in-linux/  The two types of graph for directory representation :  Acyclic Graph : This directory structure uses a directed acyclic graphs (DAGs) to represent directories and their relationships. Directories can have multiple parents, and symbolic links can be used to create additional directory references. However, it is important to maintain acyclicity, meaning there should be no cyclic dependencies or loops in the graph. Source : https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/11_FileSystemInterface.html General Graph : In a general graph directory structure, directories are represented using a general graph data structure, which may have cycles and arbitrary relationships. Managing and navigating a general graph directory structure may require additional algorithms or mechanisms to handle cycles and resolve conflicts. Source : https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/11_FileSystemInterface.html  note Tree is a special case of graph where the properties are undirected and acyclic. It contains no cycles or loops and connected, meaning there is a path between any two vertices.  ","version":"Next","tagName":"h3"},{"title":"File System Implementation​","type":1,"pageTitle":"File System","url":"/cs-notes/operating-system/file-system#file-system-implementation","content":" File System Structure​  File system is designed with multiple levels, forming a layered structure.  At the lowest level, the physical properties of the storage devices, such as disks, are dealt with. This level accesses data directly from the disk in the level of blocks. It is possible to access to any block of information on the disk directly. The hardware is controlled by I/O control, through device drivers and interrupt handlers. It manages the flow of data between the application programs and the storage devices, also handles error detection and recovery in case of I/O failures. A basic file system is implemented above the hardware. It is a logical file system on the physical storage devices that handles the low-level operations required to read and write data from and to the physical block of storage devices. The file-organization module is responsible for managing how files are stored and organized. This includes determining allocated space (free-space management), location of file, and translation between physical blocks and logical blocks. The logical file system is the highest-level in the system. It is the level where a standardized and abstracted interface exist for applications to access and manipulate files without needing to know the details of the underlying physical storage devices. File organization is abstracted using directory, file is also included with a File Control Block (FCB), which is a data structure used to maintain information about individual files, such as ownership, permissions, and location of the file contents. Source : https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/12_FileSystemImplementation.html  More about disk in disk management.  File Control Block​  FCB holds important metadata about the file, such as its name, location, size, permissions, creation and modification timestamps, and other attributes. It acts as a reference point for the operating system to track and manipulate the file throughout its lifecycle.  When a file is created, the file system allocates a new FCB to represent it. The FCB is then updated with the necessary information, such as the file name and its associated data blocks. This FCB is then stored in the appropriate directory structure, allowing the operating system to locate and access the file.   Source : https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/12_FileSystemImplementation.html  The OS keep track a data structure called open-file table, to manage open files by processes or applications. Within a specific process, an open-file table per-process is used. On the other hand, the OS also keep track the system-wide open-file table, which maintains a global view of all open files and is accessible by all processes in the system.  When an application opens a file, the operating system scans the system-wide open-file table to determine if the file is already being used. If the file is indeed in use, the operating system creates a new entry in the per-process open-file table that references the existing entry in the system-wide open-file table. This approach helps minimize the overhead of opening a new file. After that, the process is assigned a file descriptor or file handle, which serves as a reference to the opened file.  If the file is not already open, the directory structure is searched, and the FCB associated with the file is copied into the system-wide open-file table. This table not only stores the FCB but also keeps track of the number of processes that have the file open, so that it know when to close the file.  After the process receive the file descriptor, either from the existing system-wide open-file or from per-process, the process can now perform file operation through it.  Partition &amp; Mounting​  Partition is the process of logically separating section of a physical disk drive. When a disk is partitioned, it is divided into multiple sections that can be treated as independent units. Each partition is typically formatted with a file system and can be used to store data and host a file system. Partition help to create boundaries so that the system doesn't overwrite important data when the disk is full.  Mounting is the process of making a file system accessible and associating it with a specific directory (called mount point). When a file system is mounted, the directory specified as the mount point becomes the entry point for accessing the files and directories within that file system.  For example, in Windows, mounting a drive is assigning a drive letter. Each storage device or partition is assigned a drive letter (such as C:, D:, E:) to represent it. If a file system is mounted at drive letter C:, users can access files within that file system by specifying the drive letter followed by the file path.   Source : https://www.minitool.com/partition-disk/volume-vs-partition.html  Directory Implementation​  Two common approaches are using a linear list and a hash table.  Linear List : The directory entries are stored in a sequential list. Each entry contains the file name and a pointer to the corresponding file data. This method is simple to program, but it may result in slower searching, especially for large directories. To find a specific file, the directory must be searched from the beginning until the desired entry is found. Similar to other operation where traversal is needed. This linear search can be time-consuming, especially if the directory is long. One way to improve is to use a sorted list, it can decrease the average search time by allowing a binary search. Additionally, a sorted list enables producing a sorted directory listing without a separate sorting step. Hash Table : In this approach, a linear list is still used to store the directory entries, but a hash data structure is employed as well. The hash table takes a value computed from the file name and returns a pointer to the corresponding entry in the linear list. This allows for faster directory search by greatly reducing the search time. When a file name needs to be looked up, it is hashed to generate a value within a given range. This value is then used to directly access the corresponding entry in the linear list, avoiding the need for sequential searching. However, collisions may occur when two file names hash to the same location, requiring collision resolution techniques, such as using a linked list within each hash entry. Source : https://www.javatpoint.com/os-directory-implementation  Allocation Methods​  Allocation Methods are the methods of allocating space on a storage device to store files in a file system. It involves managing the storage resources and organizing the physical locations where files are stored on the storage medium.  See storage allocation.  Free-Space Management​  Free-space Management is the process of tracking and managing available space on a storage device. It involves keeping a record of which areas or blocks of the storage medium are free or allocated to files.  Allocation : When a new file needs to be created, the system identifies and allocates a suitable portion of the storage medium to store the file. The system finds contiguous or non-contiguous set of free blocks that can accommodate the file's size.Deallocation : When a file is deleted or no longer needed, the system marks the previously allocated blocks as free, making them available for future allocations.  See free-space management.  ","version":"Next","tagName":"h3"},{"title":"File System Example​","type":1,"pageTitle":"File System","url":"/cs-notes/operating-system/file-system#file-system-example","content":" FAT​  File Allocation Table (FAT) is a file system used in the old Windows version. FAT has three versions, FAT12, FAT16, and FAT32, where each newer version accommodate larger storage capacities and file sizes.  Structure : The FAT file system organizes data on a storage device into clusters, which are contiguous fixed-size units of allocation on the disk.The file system consists of four regions : reserved sectors, FAT region, root directory region, and data region.Reserved sectors is a portion of the file system that is reserved for the system file, it includes the boot sector, which contains information for booting the system.The FAT is the main component, it is a table that keeps track of the allocation status of each disk block.Root directory stores information about the files and directories located in the root directory.Data region holds the actual data of files and directories, they are divided into cluster which are linked together through the entries in the FAT. File Allocation Table : FAT is a table that keeps track of which clusters are allocated to files and directories.Each entry in the table represents a cluster and contains information about the status of the cluster (such as whether it is free, allocated, or marked as bad). Entries are chained together forming a linked list.An entry of FAT depends on the version, FAT12 uses 12 bits, FAT16 uses 16 bits, and so on.For example, an entry in FAT32 can be 0x00000000, this represents a free cluster, a 0x0FFFFFFF indicates an end-of-file marker or a special purpose value. Directory Structure : The file system keep track of directory table, which is a centralized index that contains information about the files stored on the disk. It provides the necessary details to locate and access specific files. The directory table consists of a collection of directory entries, each representing a file or a subdirectory.Each directory entry contains metadata about a file or a subdirectory, such as the file name, extension, attributes, creation/modification dates, and the starting of the cluster of the file's data.The root directory is located in a fixed position on the storage device, and subsequent directories are stored as separate entries within their parent directories. Source : top, bottom In the image above, FILE1 starts from the cluster 0002, it continues to cluster 0003 until the cluster 0004, where end of file mark is reached. Limitations : The FAT file system has certain limitations, such as the maximum file size and the maximum number of files supported, which depend on the FAT version and cluster size being used. For example, the original FAT16 file system had limitations on file size (up to 2 GB) and the number of files in the root directory (up to 512 entries).In FAT12 and FAT16, the file name is limited to 8 characters for the base name and 3 characters for the extension, to adapt with the limited resource of early days of computing. For example, myfile.txt is a valid file name, but thisismyverylongfilename.txt would be truncated to thisismy.txt.  To summarize, the FAT file system divides the disk into clusters and uses the FAT table to track the status of each cluster. Clusters store file data and are organized in a chain, the length of which varies depending on the file size. Files are grouped into directories, and information about directories and their entries is stored in the directory table. Each directory entry contains metadata and the starting cluster of the associated data. When reading a file, the system traverses the cluster chain until it reaches a specific end-of-file marker.  NTFS​  New Technology File System (NTFS) is a file system for the newer Windows version. NTFS is a successor of FAT, it introduces support for long file names, advanced file and folder permissions, encryption, compression, and fault tolerance mechanisms.   Source : https://en.wikipedia.org/wiki/NTFS#/media/File:NTPermissions.png  The image above is the access control lists in Windows 7. Permissions can be assigned to file or directory. They are assigned to a parent folder are automatically inherited by its subfolders and files, reducing the need of assigning permission individually to each file or directory.  NTFS consists three important components partition boot sector (PBS), master file table (MFT), and metafiles. The fundamental data structure in NTFS is the MFT. MFT keeps track of the file's metadata and the address of file's blocks, eliminating the need for a separate table like in FAT. If a file is extremely large, it may require multiple MFT records to contain the list of its blocks.  PBS : Located at the first sector of a disk partition. It contains essential information for the system to boot, such as the boot code that initiates the system's startup process, magic number that identifies NTFS file system, and the partition table that identifies the partition structure on the disk. In NTFS, the PBS includes the bootstrap code that loads the operating system's bootloader, which is responsible for starting the operating system. Source : https://twitter.com/jaredcatkinson/status/590333209495244801 MFT : MFT is the centralized database that stores metadata about files and directories on an NTFS volume. MFT is a linear sequence of fixed-size 1-KB records, where each entry is file or directory on the volume. It contains information such as file names, timestamps, file attributes, security descriptors, and data allocation. The MFT is used file system operations, it enables quick access to file metadata and efficient management of file data. Source : https://andreafortuna.org/2017/07/18/how-to-extract-data-and-timeline-from-master-file-table-on-ntfs-filesystem/ Metafiles : NTFS has several special system files that help structure and organize the file system. Some example : $MFT : The Master File Table itself is a meta file that serves as a database of file and directory metadata, as mentioned earlier.$MFTMirr : The MFT Mirror is a backup copy of the first few critical records of the MFT.$LogFile : The Log File is used for transaction logging in NTFS. It records changes made to the file system before they are committed, allowing for system recovery when unexpected events such as power failures occurs.$Bitmap : The Bitmap file keeps track of the allocation status of clusters on the disk, indicating which clusters are free and which are in use.$Volume : The Volume file stores information about the NTFS volume itself, including its label, version, serial number, and other volume-specific details.  ext​  Extended file system (ext) is a file system for Linux kernel, it consists of four versions, ext1, ext2, ext3, and the newest ext4. All of them are designed to be backward compatible of each other.  ext2​  Starting from the ext2 :  Disk Layout : ext2 organizes data on the disk into fixed-size blocks. The default block size is 4 KB, although larger block sizes can be used. The file system divides the disk into block groups, each containing a fixed number of blocks. Each block group has its own metadata to track file system structures and data within that group. Inodes : Inode (index node) is a data structure in Unix-like system for describing file-system object, such as file or directory. An inode contains metadata about the file, including file permissions, ownership information, timestamps, and pointers to the data blocks that store the file's content. Inodes are arranged in a table structure. The inode can contain pointer to the file's content (direct data blocks), or a pointer to another block that contains pointer to the actual content (indirect data blocks). Source : https://en.wikipedia.org/wiki/Inode_pointer_structure Directory : Directories in ext2 are organized as special files. A directory file contains a list of entries, where each entry represents a file or a subdirectory within that directory. Each entry consists of a name and an inode number that points to the corresponding inode. Source : https://premaseem.wordpress.com/2016/02/14/what-is-inode-in-linux-unit/ Block Allocation : ext2 uses a block allocation bitmap to track the allocation status of data blocks. The bitmap keeps track of free blocks and allocated blocks within the file system. When a new file is created or an existing file is extended, ext2 searches for free blocks using the allocation bitmap and assigns them to the file. When a file is deleted in ext2, its inode and data blocks are marked as free in the allocation bitmap, making them available for reuse.  ext3​  ext3 brought several improvements over ext2, such as the journaling system. The journaling system logs changes to the file system before committing them to the main file system structures. This journaling feature ensures that the file system can recover after a crash or an unexpected system shutdown.  There are three levels of journaling :  Journal (lowest risk) : In data journaling, both metadata and file data modifications are logged in the journal before being committed to the file system. Can suffer from performance overhead due to the increased number of disk writes required.Ordered (medium risk) : Only metadata are logged in the journal, after it is logged, file content is modified after.Writeback (highest risk) : This level is similar to ordered, but logging metadata and writing the file content is done asynchronously. This means that there is a possibility that file content writing is completed first, and a system crash occurs before the metadata is written.   Source : https://foxutech.com/journaling-filesystem/  ext4​  ext4 supports larger file systems and file sizes compared to ext3. ext4 allows storage capacities up to 1 exabyte (1 billion gigabytes) and individual file sizes of up to 16 terabytes. ext4 does not limit the number of subdirectories in a single directory, in contrast, ext3 a directory can have at most 32,000 subdirectories.  ext4 enhances the allocation algorithms, it introduces multiblock allocation, which allows for allocation of multiple blocks at once. In contrast, ext3 calls block allocator, once for each block.  ext4 implements delayed allocation, also known as allocate-on-flush. This technique improves write performance by delaying the allocation and writing of data blocks until they are actually needed. This technique allow writing larger amount of data at once, reducing unnecessary disk I/O operations. ","version":"Next","tagName":"h3"},{"title":"Kernel","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/kernel","content":"","keywords":"","version":"Next"},{"title":"Kernel Functionality​","type":1,"pageTitle":"Kernel","url":"/cs-notes/operating-system/kernel#kernel-functionality","content":" Some function of kernel :  Process Management : Manages the execution of processes, schedules and allocates processor time to different processes, switches between them, handle communication between processes (IPC), and many mechanisms for process creation, termination, and synchronization.Memory Management : The kernel allocates memory to processes, tracks memory usage, and handles memory deallocation when a process is no longer in use.Device Management : Kernel enable communication between operating system and hardware devices such as disks, network interfaces, and input/output devices through device drivers. The kernel handles device initialization, manages device resources, and provides an interface for user applications to access and control devices.File System Management : Kernel manages the organization and storage of files on disk by providing file system that handle file creation, deletion, reading, and writing.System Call Interface : The kernel provides a set of system calls, which are functions that allow user applications to request services from the operating system. By service, those are system functionality which can be file operations, network communication, or etc. These system calls are requested via interface, typically from wrapper functions provided by the operating system to abstract the low-level details of the system call invocation process.Interrupt Handling : Handles interrupts generated by hardware devices or exceptional events. It receives and prioritizes interrupts, determines the appropriate response, and dispatches interrupt handler to handle the interrupts.  Kernel is a very important program in an OS, it is the first program loaded and will run at all times. At the start, the kernel is responsible for initializing hardware components, including the processor, setting up memory management, initializing devices and communication with peripherals.  Because the kernel interact directly with the hardware, the security of kernel is important. The memory of kernel is typically separated from application memory, this is to ensure that application can't directly access or modify sensitive kernel data or interfere with critical system operations. The protected kernel's memory is often called kernel space, and the application's memory is called user space. The kernel will also implement memory protection mechanisms to prevent processes from accessing unauthorized memory locations.   Source : https://www.quora.com/What-is-meant-by-kernel-memory-is-not-page-able  ","version":"Next","tagName":"h3"},{"title":"Kernel Design​","type":1,"pageTitle":"Kernel","url":"/cs-notes/operating-system/kernel#kernel-design","content":" Depending on the goals, the design of a kernel involves making decisions about various aspects of its architecture, organization, and functionality. For example, how will system calls interface looks like, which strategy is used to manage process, what technique is used for memory management, the organization of file system, etc.  Monolithic Kernels​  Monolithic kernel is a kernel architecture, where the entire operating system, including all essential services and functionalities, is implemented as a single, large kernel image.  note A kernel image is a binary file that contains the compiled and linked code of the kernel of an operating system. It represents the executable form of the kernel that can be loaded into memory and executed by the computer's hardware.  In this design, all kernel services, such as process management, memory management, file system support, and device drivers, reside in kernel space. The benefits of monolithic kernels is that they are in a single address space, we can directly access and share data structures and resources, making communication and coordination between kernel components efficient.  However, as the kernel grows, development process can become more complex. A change or update to one component may require modifying and recompiling the entire kernel. One bug in some part of kernel may affect other component of kernel.   Source : www.javatpoint.com/monolithic-structure-of-operating-system  Microkernels​  Microkernels architecture aims to keep the kernel minimal by implementing only the most essential services, such as memory management, multitasking, IPC in kernel space. Additional functionality, such as file systems, device drivers, and networking, is moved to user space as separate processes or modules, called server.  The communication between each server or between server and microkernel relies on message passing. While it can be easier to maintain compared to monolithic kernels, the exchange of messages to request services or share information can introduce some overhead compared to direct function calls in a monolithic kernel.   Source : https://www.scaler.com/topics/microkernel-operating-system/  Hybrid Kernels​  Hybrid Kernels combine both monolithic and microkernel architectures. It attempts to strike a balance between performance and modularity by incorporating features from both approaches. Some essential services and functionalities, such as process management and memory management, are implemented in kernel space, similar to a monolithic kernel. At the same time, other non-essential services, such as file systems, device drivers, and networking protocols, are implemented as separate modules or processes in user space, similar to a microkernel.   Source : https://en.wikipedia.org/wiki/Hybrid_kernel#/media/File:OS-structure2.svg ","version":"Next","tagName":"h3"},{"title":"Interrupt Handling","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/interrupt-handling","content":"","keywords":"","version":"Next"},{"title":"Type of Interrupt​","type":1,"pageTitle":"Interrupt Handling","url":"/cs-notes/operating-system/interrupt-handling#type-of-interrupt","content":" Interrupts can be categorized based on their source of origin, which can be either hardware or software.  Hardware Interrupt​  Hardware interrupt is triggered when a specific event from the hardware occurs, such as a key press from keyboard, mouse movement from mice, network packet arrival from network card, or timer reaching a specific value.  Hardware interrupt signals are associated with interrupt requests (IRQs). Each hardware device is assigned a unique IRQ number for identification. When the device generates an interrupt, the corresponding interrupt handler associated with that IRQ is invoked.  Masking​  Many interrupts can be generated, there could be scenario where the less urgent or lower-priority interrupts interfere other time-sensitive events. Masking is an act to disable or ignore specific interrupts temporarily. This can be done by setting certain bits in the interrupt controller or by modifying interrupt flags in the processor.  Missing Interrupts​  When issues such as faulty interrupt signals, incorrect interrupt wiring, improper interrupt configuration, or interrupt masking occurs, this can result in the system failing to respond to important events or data from hardware device.  Sporious Interrupts​  Spurious Interrupts are unexpected or false interrupts that occur without any corresponding hardware event or interrupt signal. They are typically caused by electrical noise, voltage fluctuations, or signal reflections.  Spurious interrupts can be challenging to diagnose and resolve since they do not correspond to any real hardware events. To handle spurious interrupts, system designers often implement many techniques for additional checks to validate the occurrence of a genuine interrupt event. Software can also be designed to ignore or suppress spurious interrupts to prevent unnecessary interrupt handling or error conditions.  Software Interrupt​  Software interrupt originate from software, typically through system call. System calls are triggered by executing special software instructions or by raising specific flags or conditions. For example, a program may make a system call to request a file operation, network communication, or access to hardware resources.  Other source of software interrupt is from exception that occurs during the execution of program. Exceptions are anomalous condition that require special handling from the exception handler, this may be caused by error such as division by zero, invalid memory access, page faults, or illegal instruction execution.  ","version":"Next","tagName":"h3"},{"title":"Interrupt Implementation​","type":1,"pageTitle":"Interrupt Handling","url":"/cs-notes/operating-system/interrupt-handling#interrupt-implementation","content":" In the hardware-level, interrupt exist as electrical signals that are generated by hardware devices. These electrical signals are typically in the form of voltage transitions or maintained voltage levels that are detected and processed by the hardware. The interrupt signal can be handled by the CPU using a programmable interrupt controller.  An interrupt controller is a hardware component responsible for managing and prioritizing interrupts. It acts as a central hub for receiving interrupt signals and determining the order in which they are handled.  The interrupt signal is typically connected to a dedicated interrupt line, known as an Interrupt Request (IRQ) line. Each hardware device that can generate interrupts is connected to a specific IRQ line. The IRQ lines are part of the system's physical wiring and are connected to the interrupt controller or processor.   Source : https://www.javatpoint.com/what-is-interrupt-in-os  The primary functions of an interrupt controller include receiving interrupt signals, assign priority levels to interrupts based on their significance, masking, route the interrupt to their corresponding handler.  ","version":"Next","tagName":"h3"},{"title":"Interrupt Handler​","type":1,"pageTitle":"Interrupt Handling","url":"/cs-notes/operating-system/interrupt-handling#interrupt-handler","content":" Also known as Interrupt Service Routine (ISR), it is a specific block of code that is executed in response to an interrupt. The interrupt handler is typically written in low-level programming languages or assembly language to ensure efficient and precise execution to minimize the impact on the interrupted program.  Before executing the interrupt handler, the hardware typically perform context switch to save the current execution context, including the values of registers, program counters, and other relevant processor state information to be resumed on later.  The specific handling depends on hardware architecture and specific devices that triggers the interrupt. For example, an interrupt handler for a network interface card may read incoming network packets, or an interrupt handler for a storage device may write data to disk.  After the interrupt handler completes its execution, control is returned to the interrupted program or task. The hardware restores the saved context and resume the program execution from the point where it was interrupted.   Source : https://www.scaler.com/topics/operating-system/interrupt-handling/ ","version":"Next","tagName":"h3"},{"title":"Linux Kernel","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/linux-kernel","content":"","keywords":"","version":"Next"},{"title":"Architecture & Structure​","type":1,"pageTitle":"Linux Kernel","url":"/cs-notes/operating-system/linux-kernel#architecture--structure","content":" Linux is a modular monolithic kernel, each kernel component is called kernel modules. Linux supports loadable kernel modules, which are dynamically loaded and unloaded at runtime. Modules allow for the addition or removal of kernel functionalities without the need to rebuild or reboot the entire kernel.   Source : https://linux-kernel-labs.github.io/refs/heads/master/lectures/intro.html  Linux is built upon the GNU project, which is a compilation of freely available and open-source software. Linux kernel is developed using the C programming language, along with its standard library known as the libc or GNU C library. The C programming language is compiled using one of the compilers provided by the GNU Compiler Collection (GCC), which encompasses a collection of compilers offered by the GNU project.  ","version":"Next","tagName":"h3"},{"title":"Features​","type":1,"pageTitle":"Linux Kernel","url":"/cs-notes/operating-system/linux-kernel#features","content":" The Linux kernel is based on Unix, they have similar features. For system calls, take a look at type of system calls &amp; example.  Process Management​  Creation of new processes through a system call called fork() and exec(). The exit() system call is used to terminate the execution of a process, while the wait() system call is used by a parent process to wait for its child process before terminate.Process has some information associated with it, such as process ID (PID), file descriptors, which is information about the files opened by the process, and virtual memory info, which contains the information about process' memory layout, including the code segment, data segment, heap, and stack.Linux supports preemptive multitasking, the scheduling of process is done using the Completely Fair Scheduler (CFS) algorithm. In short, it is a priority-based scheduling that uses a red-black tree data structures to order tasks based on the amount of time they spend running on the CPU, known as vruntime, to ensure fairness.  Memory Management​  Linux manages physical memory by classifying memory into several zones, where their boundaries depend on the system architecture. Normal Zone : This zone is the largest regions that contains regular application memory, which is typically used for user-space processes.DMA Zone : This zone is dedicated to direct memory access (DMA) operations, where devices can access memory directly without CPU intervention.High Memory Zone : This is a memory zone that contains pages with high-memory addresses. This zone is used to allocate and manage physical memory that is not permanently mapped into the kernel address space. The purpose of this zone is to address the limitations of older hardware that cannot fully map all physical memory into the kernel's virtual address space. Linux kernel manages physical memory in the form of pages. When a process requests memory, the kernel allocates pages from the available pool. Linux implements a virtual memory system that allows processes to have their own virtual address spaces. It provides memory protection, demand paging, and memory sharing mechanisms. Linux kernel uses the slab and the buddy allocation.  File Systems​  Linux supports various file systems, including ext4, XFS, Btrfs, FAT, NTFS, and more.ext file systems utilize journaling mechanism, which is a mechanism to logs changes to the file system before committing them to the main file system structures. This ensures file system can be recovered after system crashes or power failures.  Device Management​  Devices are categorized into three classes, depending on their access method : block devices, character devices, and network devices. Block devices include devices that allow random access to independent, fixed-sized blocks of data, such as hard disks, floppy disks, CD-ROMs, Blu-ray discs, and flash memory.Character devices include devices like mice and keyboards that are accessed serially, meaning data is read or written sequentially, one character at a time.Network devices represent devices used for networking purposes. Although technically considered character devices, they are handled differently from other character devices due to their unique nature. In addition to Linux that supports dynamically loadable kernel modules, this also allows device drivers to be loaded and unloaded at runtime without rebooting the system.  Inter-process Communication​  Linux primarily uses wait queues to coordinate processes and communicate about incoming asynchronous events, rather than signals. When a process wants to wait for a particular event to complete, it places itself on a wait queue associated with that event. Multiple processes will wait for a single event and they will be awakened once the event has completed.For alternative of wait queues in asynchronous events communication, Linux also implement semaphore.For passing message among processes, Linux provide shared memory and pipe message passing mechanism.  Networking​  Networking is implemented through three layers of software : the socket interface, protocol drivers, and network-device drivers. The socket interface is the entry point for user applications to perform networking requests. It is designed to resemble the 4.3 BSD socket layer, this make it compatible with programs that utilize Berkeley sockets.The protocol drivers process packet, such as modifying, splitting, or reassembling. When data arrives at this layer, either from an application's socket or a network-device driver, it is expected to be tagged with an identifier specifying the network protocol it contains.Network-device drivers are responsible for interacting with the physical network devices. These drivers manage the communication between the kernel and the network hardware, handling tasks such as sending and receiving data packets. Linux kernel supports a wide range of network protocols, including TCP/IP, UDP, ICMP, IPv4, and IPv6. It also provides support for network services like DNS, DHCP, and NAT.  Security​  To ensure security of user accounts, Linux implements authentication mechanism. The authentication process in Linux is primarily based on the use of login names and passwords. When a user attempts to log in, the login program prompts for a login name and a password. The password is then encrypted and compared against the encrypted password stored in the password file, usually located at /etc/passwd. If the encrypted passwords match, the login is allowed; otherwise, it is rejected.Linux implements a file and directory access permissions. The kernel supports Access Control Lists (ACLs) and Security-Enhanced Linux (SELinux), which provide additional layers of access control and security policies. ","version":"Next","tagName":"h3"},{"title":"macOS & iOS","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/macos-and-ios","content":"","keywords":"","version":"Next"},{"title":"macOS Architecture​","type":1,"pageTitle":"macOS & iOS","url":"/cs-notes/operating-system/macos-and-ios#macos-architecture","content":"  Source : https://en.wikipedia.org/wiki/Architecture_of_macOS  Kernel (XNU) : BSD component of the kernel provides support for file systems, networking, POSIX APIs, network kernel extensions (NKE), and other essential functionalities. On the other hand, the Mach microkernel handles memory management, interprocess communication, and thread scheduling. Additionally, it includes the I/O kit, which is responsible for device driver development and management. System utilities : System utilities that perform various low-level tasks related to system administration, configuration management, disk management, and diagnostics. Core OS (Darwin) : The kernel and other system utilities are all included within the core Darwin OS. Java Runtime Environment (JRE) : macOS supports the Java programming language through the JRE. The JRE provides the necessary runtime environment for executing Java applications on macOS. It includes the JVM, which interprets and executes Java bytecode, as well as other libraries and components required for Java application execution. Carbon &amp; Core services : Both are the APIs for making application for macOS. Carbon was initially introduced to assist in transitioning applications from the classic Mac OS to macOS. It offers support for legacy Mac OS APIs and allows developers to maintain compatibility with older applications. Core Services includes a range of OS functionality, including file management, networking, and interprocess communication. These frameworks enable developers to access system resources and build robust applications for macOS. Application Services &amp; API : There are many APIs provided, such as Quartz, PrintCore, QuickTime. Quartz is a graphics rendering and compositing engine in macOS. PrintCore is a printing subsystem in macOS for printing documents and managing printer devices. QuickTime is a multimedia framework and player for handling multimedia data, including audio, video, animation, and interactive media. GUI (Aqua) : The highest level is the GUI known as Aqua. It provides the visual elements, windowing system, and user interaction components that users interact with on their Mac computers. Aqua is responsible for rendering the desktop, windows, icons, menus, dialog boxes, and other graphical elements. Source : https://en.wikipedia.org/wiki/Aqua_%28user_interface%29  ","version":"Next","tagName":"h3"},{"title":"iOS Architecture​","type":1,"pageTitle":"macOS & iOS","url":"/cs-notes/operating-system/macos-and-ios#ios-architecture","content":"  Source : https://medium.com/@ganeshrajugalla/ios-ios-introduction-and-structure-fdd7ecf08c4c  The architecture of iOS can be divided into several key components:  Core OS : Core OS includes the XNU kernel, drivers, security frameworks, networking, and file system management. Core OS handles low-level tasks such as hardware interaction, memory management, and security.Core Services : Core Services and provides additional system-level functionalities. It includes services such as iCloud, Core Data (a persistence framework), Core Location (for geolocation services), and Core Bluetooth (for Bluetooth communication).Media : Media layer consist of multimedia-related functionalities. It includes frameworks for audio, video, and graphics. Core Audio provides audio playback and recording capabilities, Core Animation for animations and transitions, Core Text for text rendering, Core Image for image processing, and AVFoundation framework for working with multimedia features like video editing and capture.Cocoa : Cocoa is the framework specifically designed for building iOS applications. It provides a set of high-level APIs and tools for developing user interfaces, handling touch input, and managing application lifecycle. Cocoa Touch includes UIKit, which is responsible for building the UI, handling user interactions, and managing navigation between screens.Application Layer : Layer where developers build their iOS applications. It includes the apps themselves, which are developed using the iOS SDK and various frameworks mentioned above. ","version":"Next","tagName":"h3"},{"title":"Multithreading","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/multithreading","content":"","keywords":"","version":"Next"},{"title":"Thread​","type":1,"pageTitle":"Multithreading","url":"/cs-notes/operating-system/multithreading#thread","content":" Thread is a unit of execution in CPU, it can execute a set of instruction, basically it is a &quot;worker&quot; in a CPU. Thread exist within a process and has its own data including thread ID, program counter, a register set, and a stack.  In multithreading, instead of just one thread or one worker in a process, multiple thread is utilized. The benefit is we are not limited to complete a single task at a time, for example, a mobile app can fetch data from remote server while also loading data from local storage. If possible, we can also divide a computationally intensive task into smaller, parallelizable subtasks, and use multiple threads to speeds up the overall execution time.  A thread can also be blocked, which means the thread is unable to make progress or continue its execution because it is waiting for a certain event such as I/O results or condition to occur. Blocked thread can be inefficient, as it is unable to perform any useful work.   Source : https://towardsdatascience.com/multithreading-and-multiprocessing-in-10-minutes-20d9b3c6a867  The image above shows the illustration of multithreading. Each thread holds different data, but they share the same memory space and resources of the parent process. In contrast, multiprocessing is when we utilize a processor that has several cores. Each core would have their own data and thread that will execute simultaneously.  Utilizing multiple threads is typically more efficient than making multiple process that execute the same tasks. Threads have a smaller memory footprint, require less time for context switching, and have lower scheduling overhead. Also, separate process means IPC is required to communicate between processes, whereas thread shares the same memory within a process, thus communication will be easier.  ","version":"Next","tagName":"h3"},{"title":"Multithreading Model​","type":1,"pageTitle":"Multithreading","url":"/cs-notes/operating-system/multithreading#multithreading-model","content":" User &amp; Kernel Thread​  There are two types of thread, user thread and kernel thread.  User threads, also known as green threads, are implemented and managed by a thread library or runtime system at the user level, without direct involvement of the operating system kernel. The creation, scheduling, and synchronization of user threads are handled entirely in the user space (memory space where user applications run).  On the other hand, kernel threads, which is also known as native threads, are managed directly by the operating system kernel. Each kernel thread is represented as a separate entity within the operating system and has its own program counter, stack, and thread control block.  User threads are generally lightweight, however, they are not fully aware of the underlying thread management mechanisms. If a user thread blocks or performs an operation that blocks, it can potentially block the entire process, including all other user threads.  Kernel threads are heavyweight, they require system calls and interaction with the operating system kernel for creation and management. The benefit of kernel thread is that they are fully managed by the OS, this allows for better utilization of system resources and efficient scheduling across multiple processors (achieving true parallelism).  Relationship Model​  Many-to-One : This model involves mapping multiple user-level threads to a single kernel-level thread. The thread management and scheduling are performed by a thread library or runtime system at the user level, and the operating system sees only a single thread. This model has efficient management, but may not take full advantage of multiprocessor systems as the execution of multiple threads is handled by a single kernel-level thread. One-to-One : In this model, each user-level thread is mapped to a separate kernel-level thread by the operating system. This model provides more concurrency and true parallelism to the kernel-level. However, the overhead of creating and managing kernel-level threads can be higher compared to other models. Many-to-Many : This model combines the aspect of many-to-one and one-to-one. Many-to-many model consist of many kernel threads and smaller or equal number of user thread. The operating system can create multiple kernel-level threads, while the thread library manages and schedules the user-level threads across the available kernel-level threads. Source : https://www.researchgate.net/figure/Three-types-of-thread-models-Popular-operating-systems-5-22-24-adopt-the_fig1_346379550  ","version":"Next","tagName":"h3"},{"title":"Thread Management​","type":1,"pageTitle":"Multithreading","url":"/cs-notes/operating-system/multithreading#thread-management","content":" Multithreading implementation depends on the programming language used. Threading process involves thread creation, execution and scheduling, synchronization, and termination.  Thread Creation &amp; Termination​  The thread library provided by programming languages have specific function or method to create and manage threads. For example, in Java, we can create a thread by extending the Thread class or implementing the Runnable interface and then invoking the start() method. In C++, you can use the std::thread class or the threading utilities provided by libraries like POSIX threads (pthread_create() function).  When creating thread, we can specify thread attributes such as stack size, thread priority, CPU affinity. After a thread is created, it is assigned a unique identifier called the thread ID.   Source : http://java-latte.blogspot.com/2015/07/create-thread-using-method-reference-in-java-8.html  Thread can be stopped explicitly using function like stop() in Java. Sometimes, the thread may not stop immediately due to specific logic or condition that is required to execute before it can safely terminate. Thread can also terminate naturally when it finishes its execution, where it automatically exits, and its resources are released by the system (or saved to thread pool).  It is important to note that thread termination should be handled carefully. For example, a thread may have used some data structure, but when it is not freed before the termination, this can cause memory leak.  Thread Execution &amp; Scheduling​  In Java, we can start the execution of a thread, by calling the start() method on the Thread object. The start() method internally calls the thread's run() method, which contains the code that will be executed by the thread. The JVM manages the execution of threads and ensures that the run() method is executed concurrently with other threads.  Thread scheduling in Java is handled by the JVM and the operating system. The JVM uses a preemptive scheduling algorithm, where the operating system decides when to switch between threads. Similar to process scheduling, this involves context switch to save the thread state, determining based on some scheduling algorithm.  Java provides methods like yield() and sleep() to influence thread scheduling. The yield() method allows a thread to voluntarily give up its remaining time slice, allowing other threads to run. The sleep() method pauses the execution of a thread for a specified period of time.   Source : https://medium.com/spring-boot/multithreading-in-java-with-examples-25b0bc80831b  Thread Communication​  Multiple threads exist within the same process, threads communicate using the IPC mechanism. There are two method, the first method is shared memory, where each thread read and write data in the same region of memory. The other method is message passing, where they send messages or signals to each other. One thread can send a message to another thread, which then receives and processes the message.  Thread Synchronization​  Synchronizing threads is crucial to ensure proper coordination and consistency when multiple threads access shared resources while communicating, to prevent concurrency issues.  When a code or data structure can be safely accessed and modified by multiple threads concurrently without causing unexpected or incorrect behavior, this is called thread safe.  Synchronization Primitives​  These are fundamental tools used in multithreaded programming to synchronize.  Locks / Mutex​  Mutex (mutual exclusion) is a synchronization primitive that ensure only one thread to access a shared resource. It works by having a lock, a thread that wants to access the resource must acquire the lock first. If the lock is already held by another thread, the requesting thread will be blocked until the lock is released. When the thread that access the resource has finished, then the lock will be released.  The mutex technique can be implemented in the software-level by memory synchronization instructions provided by the hardware architecture.  There are three types of locks :  Shared lock : Multiple thread is able to read same data simultaneously.Exclusive lock (mutex) : When a thread acquire an exclusive lock, it has exclusive access to the data until the lock is released.Update lock : Combination of shared and exclusive locks, allowing multiple thread to read, but only one thread to update the data at a time.  Monitor &amp; Condition Variables​  Condition variables are synchronization primitives that allow threads to wait for a specific condition to become true before proceeding with their execution. Condition variables are typically used together with lock, forming another construct, monitors.  Monitors is a higher-level synchronization construct that combines mutex and condition variables. The mutex is used to ensure only one thread is accessing the resource, while the condition variables is used for additional coordination between the threads.  In a sense, condition variable is actually a queue. Threads will be kept in the queue until a condition is met. The condition variable has three operations :  Wait : The &quot;wait&quot; operation temporarily release the associated mutex and enter a wait state on a condition variable, effectively blocking its execution. It is typically called when the thread or process encounters a condition that prevents it from proceeding.Signal : The &quot;signal&quot; operation is used to awaken one waiting thread or process that is blocked on a particular condition variable, such as wait state. It notifies a single waiting thread or process that the condition it was waiting for may have changed. The awakened thread or process can then reacquire the associated lock or mutex and continue its execution.Broadcast : The &quot;broadcast&quot; operation is used to awaken all waiting threads or processes that are blocked on a particular condition variable.  Monitors work like following :  When a thread wants to access the shared data, it first needs to acquire the monitor's lock. If the lock is already held by another thread, the requesting thread will be blocked until the lock becomes available.Once a thread has acquired the lock, it enters the monitor and gains exclusive access to the shared data. The thread can then perform operations on the data inside a critical section.In the case of success operation, once a thread completes, it releases the monitor's lock, allowing other threads to acquire it. The thread exits the monitor, making it available for other threads to enter.In the case when the thread encounter whatever condition that prevent it from proceeding, such as a specific state of the shared resource, it may signal or broadcast the other threads.Another thread, which are signaled, can modify the shared data in a way that affects the other waiting threads' conditions, in which it can signal or broadcast again.   Source : https://dev.to/l04db4l4nc3r/process-synchronization-monitors-in-go-4g4k  Semaphores​  Semaphore allows specified number of thread to access a shared resource. A semaphore maintains a count, and when a thread wants to access resource, it attempts to acquire the semaphore. If the count is greater than zero, the thread is allowed to proceed, and the count is decremented. If the count is zero, indicating that the resource is currently in use, the thread will be blocked until another thread releases the semaphore, which increments the count.  Barriers​  Barriers synchronize a group of threads at a specific point in code. Threads reach the barrier and wait until all participating threads have arrived. Once all threads have reached the barrier, they are released simultaneously, allowing them to continue their execution.  Spinlocks​  Spinlock is when a thread &quot;spins&quot;, which means that it continuously executes a loop, frequently checking a condition or waiting for a certain state to be reached.  The basic idea of a spinlock is that a thread attempting to acquire the lock repeatedly checks if the lock is available in a tight loop, spinning until it becomes available. The thread keeps spinning until it successfully acquires the lock, at which point it can proceed with the critical section of code or the shared resource it wants to access.  This differs from traditional locks where the thread would be put to sleep if the lock is unavailable. Spinlocks are useful in situations where the expected wait time for acquiring a lock is very short, and the overhead of putting a thread to sleep and waking it up is considered too costly.   Source : mutex, semaphores, barriers, spinlocks  Atomic Operation​  Atomic operations are operations that are guaranteed to be executed atomically, without interruption. They provide a way to perform thread-safe operations on shared variables without the need for locks or synchronization primitives. Atomic operations are typically used for simple operations like incrementing or decrementing a variable. Atomic operations can be supported by the OS or hardware, with specific instructions.  Thread Pool​  Thread pool is a technique to improve performance and resource management in multithreaded applications.  When there are pre-allocated threads which are waiting and ready to be used to execute tasks, they are stored in thread pools. Thread pool isn't just a place where you store thread after their creation, it is a place where you maintain a set of reusable threads. These threads are created in advance and added to the pool. When a task arrives, a thread from the pool is assigned to execute it. Once the task is completed, the thread is returned to the pool for reuse. Thread pool allows for efficient resource management, it helps to reduce the resource exhaustion when creating new thread.   Source : https://dip-mazumder.medium.com/how-to-determine-java-thread-pool-size-a-comprehensive-guide-4f73a4758273  ","version":"Next","tagName":"h3"},{"title":"Multithreading Problems​","type":1,"pageTitle":"Multithreading","url":"/cs-notes/operating-system/multithreading#multithreading-problems","content":" Race conditions : Race conditions occur when multiple threads access shared data concurrently and try to modify it simultaneously. This can lead to unpredictable and incorrect results, because each thread may observe the data differently, some may access it after modification, and so may not. Starvation : Starvation occurs when a thread is blocked from gaining access to required resources. This can lead to a thread not making progress and adversely affect overall system performance. Deadlocks : Deadlocks occurs when two or more threads are blocked indefinitely waiting for each other. Source : https://en.wikipedia.org/wiki/Deadlock Livelock : Livelock is similar to a deadlock, but in a livelock, the processes or threads are not blocked or waiting for a resource explicitly. Instead, they are continuously reacting to each other's actions in a way that prevents any of them from making forward progress. Source : http://15418.courses.cs.cmu.edu/spring2014/lecture/snoopimpl1/slide_021  tip In relation to concurrency, see also concurrency and parallelism. ","version":"Next","tagName":"h3"},{"title":"Networking","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/networking","content":"","keywords":"","version":"Next"},{"title":"Communication Structure​","type":1,"pageTitle":"Networking","url":"/cs-notes/operating-system/networking#communication-structure","content":" There are four steps which the OS need to handle in order to communicate.  Naming &amp; Name Resolution​  To communicate in a network, two system must locate each other. A participant in a network communication that sends or receives data is called a host. Each of the two hosts must have a unique identifier, which helps in locating them within the network.  For simple processing, computer uses number as an identifier instead of human-readable names. The identifier is called IP address, an example of IP address is 128.29.12.13. When we browse on the internet, we typically enter some URL such as www.google.com, this URL is actually the host name. However, as said earlier, computer uses number. It doesn't understand how to connect to www.google.com, it requires the IP address of www.google.com. However, it wouldn't make sense for human user to use raw IP address to browse the internet.  To address this, a solution is to implement a service that can translate a human-readable host name into its corresponding raw IP address. The service that provide this is called domain name system (DNS). When we intend to connect to some host, such as browsing with specific URL in browser, the browser application delegate the task and ask the OS to resolve the IP for corresponding URL.   Source : https://devopedia.org/domain-name-system  Routing Strategies​  Given that we know the location of the receiver, how should we transmit our data? Network are connected in various ways, it can be wired connection of computer (e.g., LAN and WAN), or it could be wireless (e.g., Wi-Fi, cellular connection), the connection can be complex when the network become larger. Routing is the act of deciding on the paths or routes that messages should follow to reach their intended destinations efficiently.   Source : https://www.researchgate.net/figure/Contemporary-wireless-complex-communication-network-architecture-presenting-cumulatively_fig1_258401293  Routing process will be handled by a router. The router will determine the route using routing algorithms and protocol. One of the simple protocol is the Routing Information Protocol (RIP). This algorithm minimizes the distance of routing by having a hop count, which is a metric that represent the number of routers or network segments that a packet must traverse to reach its destination.   Source : https://www.researchgate.net/figure/Distinguishing-safe-routes-from-hop-count-values_fig4_221979955  Packet Strategies​  When transmitting data, the messages are divided into smaller, fixed-size unit called packets. A packet consists of several components that enable it to be successfully transmitted and delivered to its destination :  Header : Header section contains control information necessary for routing and handling the packet, such as the source and destination addresses.Payload : The payload is the actual data being transmitted. It can include any type of information, such as text, images, audio, or video.Trailer : The packet concludes with a trailer section, which contains additional information for error detection and correction. This may include checksums or cyclic redundancy checks (CRC) to ensure data integrity during transmission.   Source : https://tournasdimitrios1.wordpress.com/2011/01/19/the-basics-of-network-packets/  Connection Strategies​  Two processes can establish a sequence of communication. Connections can be either connection-oriented or connectionless.  In a connection-oriented approach, a dedicated communication channel is established between the sender and receiver before data transmission. Protocols like TCP follow a connection-oriented approach. In short, TCP guarantee that packet will not lost in transmission, this is participant of TCP must ensure they are connected reliably. Upon sending message, the sender must send an ACK (acknowledgment) message, which can be thought as an indicator of whether data transmission should continue.  On the other hand, connectionless communication does not establish a dedicated connection and each message is sent independently. Protocols like UDP follow a connectionless approach. UDP doesn't require sender to send the ACK message, it doesn't have built-in mechanisms for error recovery, retransmission, making UDP generally faster. ","version":"Next","tagName":"h3"},{"title":"OS Fundamentals","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/os-fundamentals","content":"","keywords":"","version":"Next"},{"title":"Component of OS​","type":1,"pageTitle":"OS Fundamentals","url":"/cs-notes/operating-system/os-fundamentals#component-of-os","content":" OS is a large software systems, it is built on many components that works together.  Kernel​  Kernel is the core component of OS, it is responsible for managing hardware resources (e.g., CPU, memory, I/O). Kernel typically has complete control over everything in the system, it has direct access to the computer's hardware. It provides a set of low-level services and interfaces that enable software applications to interact with the hardware.   Source : https://en.wikipedia.org/wiki/Operating_system#Kernel  Process &amp; Thread​  Running a program or an application require loading the program into the memory. A running instance of program that is loaded into the memory is called a process. The OS, specifically the kernel, will take care of managing and controlling the processes, this includes allocating some portion of memory for the process and assigning system resources. The program's code contains sequence of instruction and logic in the form of binary instructions which can be executed by CPU. The execution of program goes through the fetch-decode-execute cycle.  Process contains the program's instructions, these instructions can be divided into smaller set of instructions. The purpose of dividing them is to enable multiple unit of execution within the CPU, making it possible to execute the instruction concurrently, potentially faster execution. A single &quot;worker&quot; or single component that can execute instructions is called a thread. A process can be executed by one or more threads. These threads have different memory for their execution, but they operate within the same context.   Source : https://byjus.com/gate/what-is-thread-in-operating-system-notes/  Interrupt​  Interrupt is an event that occurs during the execution of program, it indicates a time-sensitive events that require attention or special handling from the operating system. The CPU will temporarily suspend its current activity and will transfer its control to an interrupt handler. Before transferring its control, the CPU typically store the state of the program execution, making it possible to continue the execution after the interruption is handled, this is called context switching.  Interrupt can be produced by :  Software : Interrupt can be produced by program or the OS itself, they can be software exceptions, such as division by zero, illegal instructions, or memory access violations.Hardware : These are produced by hardware, they are typically events that are triggered by external I/O devices, such as, keyboards, mice, disks. For example, user moving their mice expect their cursor on the screen to move immediately, this is an example of time-sensitive event.System Call : System call is an interrupt issued by program to request services from the operating system, such as file operations, network communication, or process management.  IPC​  Inter-process Communication (IPC) is the mechanism used by processes or threads to exchange information, synchronize their activities, and coordinate their execution in a computer system. Some programs running in distinct process may need to collaborate or share data to accomplish a common task. This requires a proper coordination between two process, depending on the tasks, one process may need to notify other process of specific events, and the other process will respond to that event.  Input/Output​  Input/Output (I/O) is communication and interaction between the computer system and external devices. External devices include mice, keyboards, monitor, printers, speakers, disks, etc. There are many mechanisms to handle I/O events, an example is an interrupt-driven I/O, which trigger an interruption whenever I/O events are produced. The OS will interact with the hardware, typically through a software that provides a layer of abstraction between the hardware, called device drivers.   Source : http://www.it.uu.se/education/course/homepage/os/vt18/module-1/definitions/ (with modification)  Scheduling​  Scheduling is the process of determining the order of which tasks or process should be executed next and how long, by considering factors such as fairness, efficiency, and availability of system resources. The goals of scheduling are to minimize idle and response time and maximizing the amount of work done in some unit of time. There are many policies and strategies to implement scheduling, one of simple strategy is doing whatever task that is ready to be executed.  Memory Management​  OS is responsible for the management and organization of computer memory resources. This includes tracking, allocating, and deallocating memory to processes. The OS need to ensure the efficient utilization of available memory and safe access to memory, memory used by a program can't be used by different program (unless required). There are many memory access violation that can happen during the execution of program, the OS is responsible for resolving this issue, to prevent it from causing further harm.   Source : https://medium.com/@TheAnshuman/memory-management-in-os-450655fbc338  Multitasking​  CPU executes tasks in a scheduled manner, different programs have different execution time, which mean they are not being executed simultaneously. Computer is able to run multiple program at the same time due to multitasking. Multitasking works by creating an illusion of parallel execution, under the hood, the CPU switches rapidly between tasks. When trying to switch to different tasks, current state of a task or process will be saved (called context switch), and will be restored later after doing the other tasks.  File System​  The file system is a component of OS that is responsible for managing and organizing files on storage devices. File system uses many techniques to organize file for easy and fast access, as well as efficient utilization of storage. Files are typically organized into hierarchical structure in the form of directories or folders. These files are also included with specific attributes such as name, size, date, etc. The OS also provide a set of API to interact with the file system, such as creating, opening, closing, reading, and writing files.   Source : https://www.scaler.com/topics/file-systems-in-os/  Storage Management​  If file system is concerned about managing files, storage management is concerned about the management of physical storage devices such as hard disk drives (HDDs), solid-state drives (SSDs), optical drives, and tape drives. These devices are accessed through device drivers. In disks, files are divided into smaller units called blocks, contiguous blocks will be allocated to store the contents of a file. The OS is responsible for managing the allocation of storage space on storage devices. It tracks available space, assigns storage blocks to files, and keeps track of allocated and free storage blocks.   Source : https://lemp.io/what-is-disk-space-management-in-operating-system/  Networking​  Typically, an OS will provide API (socket APIs) to allow applications to establish network connections to send and receive data from the network. The OS will handle the low-level networking capabilities including implementing various network protocols, providing specific network tools for configuring network settings, such as IP addresses, etc.  Security​  An OS consists of many components, from the highest-level that interact with application, to the lowest-level that interact with the hardware. OS need to ensure that only privileged component in the system is allowed to make change. Many techniques are implemented for securing a system, such as encrypting file and having permission mechanism to prevent unauthorized access.  User Interface​  User Interface is a visual and interactive elements that enable users to interact with a system. Computer system provide a way for user to input commands and receive feedback. In modern computer system, user interface is often provided with visual elements such as windows, icons, menus, buttons, etc. This is called Graphical User Interface (GUI), the user will interact with the GUI through input devices like a mouse or keyboard. Once the user provides input, the application processes it and then forwards it to the operating system for further handling.   Source : https://www.britannica.com/technology/graphical-user-interface  ","version":"Next","tagName":"h3"},{"title":"OS Architecture​","type":1,"pageTitle":"OS Fundamentals","url":"/cs-notes/operating-system/os-fundamentals#os-architecture","content":" OS Architecture is the overall design and structure of an operating system. It defines how different components and modules of the OS are organized and interact with each other. Some common approaches to OS architecture :  Monolithic Kernel : In a monolithic kernel architecture, the entire OS operates as a single, large program running with special privileges and has direct control over the computer's hardware. All OS services, such as process management, memory management, file systems, and device drivers, are tightly integrated into a single executable. This architecture provides fast performance but lacks modularity and can be challenging to maintain and extend. Layered Architecture : A layered OS architecture divides the OS into multiple level of layers, each providing a specific set of functionalities. Each layer communicates with the layer above or below it through defined interfaces. The interface is a contract between the components, it specifies how they can interact and what functionalities they can expect from each other. The lower layers handle hardware-specific operations, such as device drivers and memory management, while higher layers provide more abstract services, such as file systems and networking. Source : https://www.javatpoint.com/layered-structure-of-operating-system Client-Server Architecture : A client-server architecture is a design model in which a client and server processes exist. The component within an OS with client-server architecture treat each other just like a client and server. Client is the one that provide user interface and request specific services or resources from the OS. The server is responsible for providing those services or resources by executing the requested operations and returning the results to the client processes. For example, a file explorer can be thought as a client, which request file information to the file system, acting as a server. This architecture is typically used for distributed computing or peer-to-peer network, which may use communication protocol like RPC. ","version":"Next","tagName":"h3"},{"title":"Memory Management","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/memory-management","content":"","keywords":"","version":"Next"},{"title":"Memory Management​","type":1,"pageTitle":"Memory Management","url":"/cs-notes/operating-system/memory-management#memory-management","content":" When we create a program and store data, say assigning a number on a variable, the memory management is handled automatically by the language runtime or the underlying system libraries.  In the case of automatic memory management, here's what might happen :  When we assign data to a variable, the compiler or interpreter reads the statement and determines the memory size required to store the variable based on its data type and other specific programming language rules. It then requests memory from the operating system to allocate space for the variable.The operating system, through the compiler or interpreter, assigns a memory address to the variable. The memory address is a unique identifier that specifies the location in memory where the variable's value will be stored. The assigned value is stored at the memory address allocated to the variable.Once the value is assigned, we can access the variable's value in our code by referring to its name. The compiler or interpreter translates the variable name into the corresponding memory address, allowing you to read or modify the value stored at that location.The variable remains in memory until its scope ends. When the variable goes out of scope or is no longer needed, the memory allocated to it can be reclaimed for future use.  ","version":"Next","tagName":"h3"},{"title":"Memory Allocation​","type":1,"pageTitle":"Memory Management","url":"/cs-notes/operating-system/memory-management#memory-allocation","content":" Memory Allocation is the process of assigning a portion of a computer's memory to store data or instructions during the execution of a program. It involves reserving memory space to hold variables, data structures, objects, or program code.  Here is the overview of memory allocation :  Memory Request : When a program or process requests memory, it communicates with the operating system to obtain the required memory space.Allocating Memory : The operating system will allocate the requested memory and maps the requested memory space to a specific location in the physical memory. By mapping, it means that it sets aside a specific portion of the computer's physical memory for a program or process to use. The allocated memory becomes reserved and dedicated to the program or process that requested it.Memory Access : The program or process can now access the allocated memory space. It can read from and write to the memory within the allocated boundaries. The OS won't allow when the program try to access region outside the boundary. When the program tries to access a part of the computer's memory that it shouldn't be accessing, an error called segmentation fault may occur.Deallocation : When a program no longer needs the allocated memory, it should inform the operating system to deallocate the memory space. The operating system marks the previously allocated memory as available for reuse.   Source : https://www.embedded.com/dynamic-memory-and-heap-contiguity/  Memory Segmentation​  Depending on the strategy and purpose of allocation, the memory allocated can take place in various location. Memory is divided into variable-sized segments, where each segment is associated with a particular process or program.  Overall, there are four segments, code segment, data segment, stack segment, and heap segment. The code segment contains the executable code of a program, the data segment holds the static and global variables used by a program, the stack segment is used for storing local variables and function call information, and the heap segment for dynamic allocation.  Stack : The stack is a region of memory used for the execution of programs. It is a data structure that follows the LIFO principle. The stack is used for storing function call information, local variables, and other data associated with function execution. Each time a function is called, a stack frame is created, which contains the function's parameters, return address, and local variables. When the function completes, its stack frame is removed, and control returns to the calling function.Heap : Heap is a region of memory that is a larger and more flexible area of memory compared to the stack. The heap is used for allocating memory dynamically at runtime when the size or lifetime of data is unknown or needs to be managed explicitly. In manual memory management, memory allocated on the heap must be explicitly requested and released by the program.Static : Static memory is a region of memory that stores global variables, static variables, and constants. It is allocated and initialized before the program execution begins and remains throughout the entire lifespan of the program. Variables declared outside any function (global variables) and variables declared with the static keyword have static storage duration and are stored in the static memory.   Source : https://www.digikey.com/en/maker/projects/introduction-to-rtos-solution-to-part-4-memory-management/6d4dfcaa1ff84f57a2098da8e6401d9c  The other two regions :  Literals : Literals are fixed values that appear directly in the source code of a program. They represent specific data types and are used to provide explicit values for variables or expressions. For example, if you have the code snippet int x = 5;, the literal value 5 is stored as part of the initialization of the variable x.Instruction : They represent individual operations that the CPU can execute (executable). In memory, instructions are stored as part of the program's code segment. The code segment contains the machine code instructions that make up the program's executable code.   Source : https://stackoverflow.com/questions/32418750/stack-and-heap-locations-in-ram  note For correction on the image, the stack is not managed automatically by the compiler; rather, it is managed by the generated machine code based on the instructions provided by the compiler. The compiler generates machine code that includes instructions for manipulating the stack, such as pushing or popping values onto or off the stack. These instructions are responsible for managing the stack frame during function calls and local variable allocation.  Stack Allocation​  Stack allocation is the allocation and deallocation of memory on the stack during the execution of a program. The stack is a region of memory used for managing function call information and local variables.  Stack allocation is simple and efficient because it operates on LIFO principle. This mean that the most recently allocated memory block is the first one to be deallocated. The stack data structure is managed with a stack pointer. The stack pointer is a pointer that points to the memory address where the next value will be pushed or popped from the stack.  Stack allocation is commonly used for storing temporary variables and function call information. It is suitable for variables with a limited lifetime within a specific scope, as they are automatically deallocated when the scope is exited.  When a function is called, the stack pointer is adjusted to create a new stack frame, which contains the function's parameters, return address, and local variables. When a function exits, the memory allocated for its local variables is immediately available for reuse by subsequent function calls.   Source : https://en.wikipedia.org/wiki/Stack-based_memory_allocation  However, the stack has a finite size, and exceeding its capacity can lead to an error called stack overflow, which is when the available space in the stack is exceeded due to excessive recursion or the allocation of a large amount of local variables.   Source : https://www.simplilearn.com/tutorials/data-structure-tutorial/stacks-vs-heap  Dynamic Allocation​  Also known as heap allocation, which is done manual by the programmer. It is the process of allocating memory at runtime (when the program is running) on the heap region, rather than the stack. Dynamic allocation is typically used when the size or lifetime of the data structure or object cannot be determined at compile-time or when it needs to be allocated and deallocated dynamically during program execution.   Source : https://cdinuwan.medium.com/java-memory-management-garbage-collection-f2075f07e43a  Manual Memory Management​  In manual memory management, programmers take control of memory allocation by requesting memory directly instead of relying on the compiler to do so on their behalf. When programmers allocate memory manually, they explicitly request memory from the operating system through the compiler or interpreter. Similarly, deallocation is also done explicitly by the programmer, releasing memory when it is no longer needed. In this process, the compiler acts as an intermediary, making requests to the operating system based on the programmer's instructions.  In languages that require manual memory management, such as C and C++, the programmer has direct control over the allocation and deallocation of memory. For example, we can use the malloc() and free() function to dynamically allocate and free the memory, respectively.  Dynamic allocation is flexible when resizing memory. We can use the realloc() function to resize it, increasing or decreasing its size as needed. Resizing dynamic memory involves allocating a new memory with the desired size, copying the existing data to the new block if necessary, and deallocating the old block.  Automatic Memory Management​  The direct control of memory in manual management can either be an up or down.  One of the major advantages of manual memory management is the potential for improved performance. Since the programmer has direct control over memory, they can optimize memory usage based on the specific needs of the program. However, manual memory management also introduces challenges and potential risks. It requires careful attention to small detail to avoid issues like invalid memory access, or memory leaks. Memory leak is a problem that occurs when unused memory is not released. While it may not be a significant concern if the leaked memory is small, it can accumulate over time.  There are many techniques to automatically manage memory, two examples are :  Garbage Collection (GC) : Garbage collection automatically detect if objects or data are still in use and which are not. It identifies objects that are no longer reachable through any references from the program's root objects or variables, and marks them as eligible for garbage collection. After identifying the garbage objects, the GC reclaims the memory occupied by these objects, making it available for future allocations. When scanning for memory, the GC typically pauses the execution of program. This is to ensure consistent state of object, avoiding concurrent modifications (i.e., what if the object we just marked as garbage is actually assigned new reference right after?). While GC is very useful, it can introduce some overhead depending on the application.Reference Counting : Reference counting is a strategy to determine if an object is no longer used and should be freed from the memory. Reference counting may be used in GC. In reference counting, every object has a reference count associated with it. When an object is created or a reference to an object is assigned, its reference count is incremented. When a reference to an object is removed or goes out of scope, the reference count is decremented. When the reference count of an object reaches zero, it means there are no more references to the object, and it is considered garbage. At this point, the memory occupied by the object can be freed.   Garbage collection in Java language Source : https://www.startertutorials.com/corejava/garbage-collection.html  Contiguous Memory Allocation​  Contiguous memory allocation is a memory management technique where memory is divided into continuous blocks or regions. In this scheme, each process is allocated a block of memory that is contiguous next to each other, meaning it occupies a single continuous range of memory addresses.  The block partition can be fixed or variable.  Fixed (Static Partitioning) : Total memory is divided into fixed-sized partitions or blocks in advance. Each partition is assigned to a process at the time of process creation based on its size. Once a partition is allocated to a process, it remains fixed for the lifetime of the process, even if the process doesn't fully utilize the entire partition. This partitioning suffer from internal fragmentation.Variable (Dynamic Partitioning) : Memory is dynamically divided into variable-sized partitions based on the size requirements of processes. The OS keep track of free and occupied memory in a table. When a process requests memory, it will search for a suitable contiguous block of memory that is large enough to accommodate the process. This partitioning is efficiently utilized memory, but it suffers from external fragmentation.   fixed size partition Source : https://www.scaler.com/topics/contiguous-memory-allocation-in-os/  Contiguous memory allocation requires proper memory protection mechanisms to prevent processes from accessing the memory of other processes. These mechanisms include using base and limit registers or utilizing virtual memory techniques.  Slab Allocation​  Slab allocation is a memory allocation mechanism that involves allocating a fixed-size block of memory called slab, whose size is supposed to fit an object, thereby minimizing memory waste due to fragmentation.  Collection of slabs is stored in cache, it is used to track slab status. The cache maintains a list of free slabs, partially used slabs, and full slabs.  The slab allocator sets up a pool of memory, which is then divided into slabs of a fixed size. When an allocation request is made for an object of a specific size, the slab allocator looks for a cache dedicated to that size. If a cache is found, the allocator checks if there are free objects within the cache's slabs. If there are, it assigns one of the free objects to the requester. If not, it allocates a new slab and assigns an object from that slab.  When an object is deallocated, it is returned to the cache's list of free objects. If the slab becomes empty (all objects are deallocated), it is moved to the list of free slabs.   Source : https://cs.stackexchange.com/questions/45159/can-someone-explain-this-diagram-about-slab-allocation  Buddy Allocation​  Buddy allocation is a memory allocation algorithm that manages memory in power-of-two block sizes, which can be divided or merged.  When a memory request is made, the buddy allocator allocates memory from a buddy of a size that is a power of 2, such as 4 KB, 8 KB, or 16 KB. If the request asks for smaller memory than is available, then the block can be divided. If the request asks for larger memory, then the block can be merged.  The properties of divide and merge of buddy allocation make it reduces external fragmentation. However, it still suffers from internal fragmentation when the requested size is larger than available. For example, if requested memory is 66 KB, then we would have to allocate for 128 KB, because of the fixed power-of-two nature.   Source : https://www.researchgate.net/figure/Memory-management-in-Linux-via-the-buddy-allocator-algorithm-Memory-spaces-are-divided_fig1_360496423  Allocation Strategy​  During allocation, there are few strategies to determine the best location to allocate a block of memory from a free memory pool :  First-Fit : Allocator searches for the first available memory block that is large enough to accommodate the requested size.It starts at the beginning of the free memory pool and selects the first block that meets the size requirement.This strategy is simple and efficient in terms of time complexity, but it may lead to fragmentation as it tends to allocate memory from the beginning of the pool, leaving smaller free blocks scattered throughout the memory. Best-Fit : The best-fit strategy aims to find the smallest available memory block that can satisfy the requested size.It searches through the entire free memory pool to find the block that minimizes wastage, leaving the smallest unused space after allocation.This strategy can lead to better memory utilization, but it may require more time for searching the entire free memory pool to find the best-fit block. Worst-Fit : The worst-fit strategy allocates the largest available memory block to the requested size.It searches through the entire free memory pool to find the largest block and allocates from it, which results in larger unused space or external fragmentation.It can be useful because sometimes leaving larger memory hole is better than a small memory hole.   Source : https://prepinsta.com/operating-systems/page-replacement-algorithms/best-fit/  Fragmentation​  Fragmentation is a phenomenon where memory or storage space that is divided into blocks keep dividing until it becomes very small, non-contiguous blocks that are not efficiently utilized. It can occur in both RAM and disk storage.  Internal : Internal fragmentation occurs when allocated memory or storage space is larger than what is required by a process or file. The unused portion within an allocated block, which cannot be utilized by other processes or files, is wasted.External : External fragmentation occurs when free memory or storage space is divided into small, non-contiguous blocks, making it challenging to allocate large contiguous blocks to processes or files, even if the total free space is sufficient. They occur in variable partitioning, when there are memory or storage gaps between allocated blocks.   Source : https://stackoverflow.com/questions/1200694/internal-and-external-fragmentation  To eliminate fragmentation, a process called defragmentation is done. It involves reorganizing files and data on a disk is reorganized. This process rearranges fragmented file segments into contiguous blocks.  Compaction​  Compaction is a technique to rearrange memory contents, such as processes or data, to create larger contiguous blocks of free memory. It involves moving processes or files and updating memory references accordingly. Compaction can be time-consuming and may require additional memory or storage to facilitate the movement of data.  The difference between compaction and defragmentation is, compaction moves any data so that hole is minimized. On the other hand, defragmentation involves rearranging common data, such as data of the same file, so that the file is stored continuously.   Source : https://www.semanticscholar.org/paper/Memory-Compaction-Performance-Improvement-by-a-Page-Jang-Kwon/579ddc1c3fb50a554ac93a9d6d7095aa1f506a66  ","version":"Next","tagName":"h3"},{"title":"Virtual Memory​","type":1,"pageTitle":"Memory Management","url":"/cs-notes/operating-system/memory-management#virtual-memory","content":" Virtual Addressing​  The operating system assigns a portion of memory to a process by providing it with a range of memory addresses. When the process needs to read from or write to memory, it uses any of these addresses. However, the address given to the process are not directly associated with physical memory. In other words, the locations the process refers to are not actual physical locations. This concept is known as virtual addressing, where the process accesses a &quot;virtual&quot; location.  We call memory that use virtual addressing a virtual memory. Virtual memory make it possible to use secondary storage such as hard disk to store process data. Main memory or RAM, while they are fast, they are limited in space. Secondary storage is typically slow, but it has much larger space.   Source : https://www.tutorialspoint.com/operating_system/os_virtual_memory.htm  Process is given a range of address that it can access to. It is the OS responsibility to handle the mapping between the virtual address and the physical address. If the process want to access, say the address &quot;0x200&quot;, the OS must translate that virtual address into a real address, it needs to locate a physical location from a virtual address.  In summary, virtual memory is a technique to use secondary disk as an extension of memory. Each process uses a virtual address instead of a physical address. The memory management system decides whether to place the data on the main memory, or place it in the secondary disk. The location where it is placed doesn't matter, as the OS will the mapping between virtual and physical address.  Paging​  Paging is one of the mechanism to implement virtual memory. Paging divides the virtual address space of a process into contiguous fixed-size blocks called pages. Likewise, the physical memory is divided into frames of the same size. The pages of a process are mapped to the frames in physical memory through a page table.   Source : https://byjus.com/gate/paging-in-operating-system-notes/  Translation​  When a program references a memory address through virtual address, it will need to go through an address translation. Memory management unit (MMU), which is the hardware that handles memory related operations, will intercept the memory access and performs the necessary translation.  The MMU uses a page table, which is a data structure maintained by the OS that contains various mapping information between virtual pages and physical frames, such as an indicator that indicates whether the corresponding address that the process is accessing is present in the main memory.  A virtual address contains consists of multiple components or fields :  Page Number : Page or segment within the virtual address space that the address belongs to.Page Offset : Represents the offset or displacement within the page or segment. It specifies the specific location or byte within the page or segment that the address refers to.   Source : https://blogs.vmware.com/vsphere/2020/03/how-is-virtual-memory-translated-to-physical-memory.html  When a program references a memory page that is not currently present in main memory, an exception called page fault occurs. When a requested page is not available in main memory, it needs to be fetched from secondary storage. The program generates a memory access request for the page, this will trigger a page fault interrupt, causing the control to transfer to the operating system.  This entire process is called demand paging, which is a technique to load data into memory only when it is needed, rather than loading the entire program or data set into memory at once. Virtual memory make it possible to implement demand paging, it allows the system to allocate and manage memory resources dynamically, as needed, by utilizing secondary storage as an extension of the physical memory.  Page Replacement​  When page fault occurs, data need to be fetched from secondary storage to the main memory. This will involve evicting or replacing an existing page on the main memory, the process is called page replacement or page swapping. There are many algorithm and strategy to decide which page to replace, some examples are :  Least Recently Used (LRU) : This algorithm selects the page that has not been accessed for the longest period of time. It assumes that pages that have not been accessed recently are less likely to be accessed in the near future.First-In, First-Out (FIFO) : This algorithm evicts the page that has been in physical memory the longest. It maintains a queue of pages and removes the page that entered the memory first.Least Frequently Used (LFU) : This algorithm selects the page that has been accessed the least number of times. It assumes that pages with lower access frequencies are less likely to be used in the future.  Advantages &amp; Disadvantages​  Paging make it possible to swap memory, pages that are not currently needed can be temporarily swapped out to disk storage. By swapping out unnecessary pages to disk, the system can free up valuable space in the main memory for other processes or programs.  Each page in virtual memory can be assigned access permissions, such as read-only or read-write, this increase memory protection between processes.  However, the division of memory into pages lead to potential internal fragmentation, when the process does not utilize all memory within a page. Unused portion of the page is wasted and cannot be allocated to other processes.  tip In the mapping between virtual and physical memory, file can also be mapped into the virtual memory address space of a process. This technique is called memory-mapped files, it enables process to access the contents of a file as if it were a block of memory, providing an efficient way to read from and write to files. ","version":"Next","tagName":"h3"},{"title":"Process Management","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/process-management","content":"","keywords":"","version":"Next"},{"title":"Process & Thread​","type":1,"pageTitle":"Process Management","url":"/cs-notes/operating-system/process-management#process--thread","content":" Process is an instance of a program that is being executed by a computer system. To execute a program, the program's code and data will need to be loaded into the memory. This mean each process will have its own memory space and system resources allocated to it.  Processes are isolated from each other, meaning they cannot directly access each other's memory or resources. However, they can still communicate through an inter-process communication (IPC) if they need to share data or information.  PCB​  Process has many information associated with it, individual process is organized in a data structure called Process Control Block (PCB), which contains process state, process identifier (PID), various registers such as program counter, etc.  Those individual process are organized together with the linked list data structure, where each process has a pointer to the next process.   Source : https://byjus.com/gate/process-control-block-notes/  Thread​  A process consists of a sequence of instructions, the unit of processing that executes instruction is called a thread. A thread represents a single sequence of instructions that can be scheduled and executed independently by the operating system.  Modern OS supports multiple threads within a single process, sharing the same memory space and system resources. Having multiple thread meaning we can execute a set of instruction much faster.   Source : https://www.javatpoint.com/process-vs-thread  In contrast to multiprocess, each process has different memory space   Source : https://www.javamex.com/tutorials/threads/how_threads_work.shtml  Threads also have their own unique thread identifier (TID). Because threads are working together, they can communicate and share data with each other within the same process by accessing or modifying the same memory and resources.   Source : https://en.wikipedia.org/wiki/Process_(computing)#/media/File:Concepts-_Program_vs._Process_vs._Thread.jpg, The execution of process through various steps  ","version":"Next","tagName":"h3"},{"title":"Multitasking​","type":1,"pageTitle":"Process Management","url":"/cs-notes/operating-system/process-management#multitasking","content":" A single CPU core executes a single process at a time, meaning one program can only be run at a time. Multitasking is a technique where the CPU core switches rapidly between different processes, allocating a small-time slice to each process. This gives the impression that multiple processes are executing concurrently, even though only one process is actually being executed at any given moment.  This is made possible by scheduling, the scheduler will allocate CPU time to different processes based on various factors such as priorities, fairness, etc.  There are two types of multitasking :  Preemptive Multitasking : This technique allocates CPU time to multiple processes by forcibly interrupting and suspending the execution of one process to give time to another process.Cooperative Multitasking : This technique relies on processes voluntarily yielding control to other processes. In this approach, each process is responsible for explicitly relinquishing the CPU when it has completed its task or when it wants to allow other processes to run.  Preemptive multitasking introduces overhead as it relies on the OS to schedule and manage the execution between tasks, while cooperative multitasking manages the execution themselves. Cooperative multitasking can be an up or down, it's important to manage the execution fairly between thread, or an issue called starvation may occur. Thread is the construct that allows you to multitask preemptively, the construct that allows you to multitask cooperatively is called coroutine.  When trying to switch the execution between process, the current state of process being executed will be saved, so it can be restored and resumed later. This process is called context switch.  ","version":"Next","tagName":"h3"},{"title":"Process State​","type":1,"pageTitle":"Process Management","url":"/cs-notes/operating-system/process-management#process-state","content":" A process has several states which represents the current condition or status.  New or Created : The process is in the &quot;new&quot; state when it is being created or initialized by the operating system. It is being loaded from secondary storage such as hard drive into main memory. Ready or Waiting : In this state, the process is loaded into main memory, has all required resources allocated, and is awaiting execution. It is eligible to run, but it is waiting for the CPU scheduler to allocate CPU time to it. Running : When a process is in the &quot;running&quot; state, it is actively being executed by the CPU. Only one process can be in the running state on a single CPU core at any given time. The CPU scheduler allocates CPU time to the process, and the process executes its instructions. The process transitions back to the &quot;waiting&quot; state when it is not actively executing again. Blocked : The process enters the &quot;blocked&quot; state when it cannot proceed further until a certain event or condition occurs. This could be waiting for user input, waiting for a resource to become available (e.g., waiting for I/O completion), or waiting for a signal from another process. In this state, the process is suspended, and it is not eligible for execution until the event or condition it is waiting for is satisfied. Once the condition is met, the process moves back to the &quot;waiting&quot; state. Terminated : The &quot;terminated&quot; state represents the end of the process's execution. It occurs when a process completes its execution or is explicitly terminated by the operating system or another process because of exceptions. In this state, the process is no longer running or eligible for execution. The operating system releases the resources held by the terminated process, removes it from the system's process table, and cleans up associated resources in the main memory. Source : https://en.wikipedia.org/wiki/Process_(computing)#/media/File:Process_states.svg  ","version":"Next","tagName":"h3"},{"title":"Process Scheduling​","type":1,"pageTitle":"Process Management","url":"/cs-notes/operating-system/process-management#process-scheduling","content":" Scheduling is the process of determining the order in which tasks or processes are executed on the CPU. Since the CPU can only execute one task at a time, scheduling determines which task gets CPU time and for how long. Scheduling is important for multitasking, where multiple programs are executed at a time, and it is essential to ensure fair execution for all of them.  Efficient scheduling scenario :  Throughput is high : The scheduler maximizes the number of tasks completed within a given time frame.CPU utilization is high, or wait time is short : The scheduler keeps the CPU busy by promptly assigning tasks to it, minimizing idle time. When a program is waiting for an I/O operation to complete, instead of wasting time on waiting and doing nothing, it is more efficient to allow other programs to execute in the meantime.Response time is low : Interactive tasks or user interface interactions receive quick CPU time, ensuring a responsive system. In an efficient scheduling scenario, interactions from user such as clicking a button should be given high priority, thus will minimize response time and improve the user experience.Fairness is maintained : Each program or process is allocated a reasonable share of CPU time, preventing starvation, which is the case when a process or task is unable to make progress to complete its execution. In a priority-based scheduling, fairness is a classic problem. The idea of priority is that tasks with higher priority should be executed first. However, if there exist many high priority task, the lower-priority task may not have the opportunity to complete.  CPU-I/O Burst Cycle​  The CPU-I/O burst cycle is the alternating pattern of CPU computation and I/O operations that occur during the execution of a program or process. The CPU burst actively utilizes the CPU for computation or processing tasks. The I/O burst phase initiates input/output operations, this phase causes the CPU to wait.  This cycle can be inefficient, as it leads to long wait times and underutilization of resources.    Process Queue​  Running processes are stored in queue based on their current state. For example, processes that are in ready state are put in ready queue.   Source : https://byjus.com/gate/process-scheduling-in-operating-system-notes/  The job queue consists of all processes awaiting execution. It represents a queue of tasks or jobs that are submitted to the system for processing. Process in I/O queue is waiting for I/O completion.  Process Scheduler​  The process scheduler is responsible for determining the execution order and allocation of system resources to processes. Based on how often scheduling decision is made, there are 3 time frames :  Short-Term Scheduling (CPU Scheduling) : This scheduling focuses on selecting processes from the ready queue for execution on the CPU. Its primary goal is to allocate CPU time efficiently among competing processes. Short-term scheduling decisions are made frequently, typically on the order of milliseconds or microseconds, to quickly switch between processes and provide the illusion of concurrent execution. Medium-Term Scheduling (Process Swapping) : Medium-term scheduling swap processes in and out of main memory (RAM). When the system's memory becomes heavily utilized or overloaded, the medium-term scheduler may decide to move some processes from memory to secondary storage (such as a hard disk) to free up memory space. This process is called swapping out. Later, when more memory becomes available, the medium-term scheduler may select processes from the swap area and bring them back into main memory (swapping in) for execution. Long-Term Scheduling (Job Scheduler) : Long-term scheduling focuses on deciding which processes should be admitted into the ready queue, which occurs when a new process is created. Long-term scheduling determines if the system has enough resources (CPU, memory, I/O) to accommodate the new process. Long-term scheduling helps to control the degree of multitasking, it ensures that the system does not become overwhelmed with too many processes.  Context Switch​  During context switch, which is done by the dispatcher, the current state of the running process, including the contents of CPU registers, program counter, and other relevant information will be saved. This step ensures that the process can be resumed from the same point when it regains CPU execution time.  It is important for context switch to be fast as possible, because it is invoked very frequently, and during invocation the CPU does nothing.  Each process has its own PCB, which holds important information about the process. When context switch occurs, the corresponding PCB of current process is accessed and modified to reflect the current state. After that, the PCB associated with the new process, is retrieved, and its relevant data, including register values, is loaded into the CPU registers.   Source : https://byjus.com/gate/context-switching-in-os-notes/  Process Attributes​  Process has several attributes, in the context of process scheduling, they can be used to determine the scheduling :  Priority : In priority-based scheduling, priority is the main attributes that represents the relative importance or urgency of a process compared to other processes, where the higher value indicates a higher priority process. Priority can be assigned based on factors such as system requirements, process characteristics, or user-defined criteria. Burst Time : Burst time, also known as execution time or CPU time, is the amount of time required by a process to complete its execution on the CPU. It represents the duration of time during which a process actively utilizes the CPU for computation or processing tasks. The burst time can be obtained by estimation, observation based on historical data, or tasks that are influenced by user input. Arrival Time : Time in which process arrives at the system's ready queue or scheduler. The arrival time can be used as the measurement for the start of the scheduling algorithm. Waiting Time : Waiting time is the total amount of time a process spends waiting in the ready queue before it gets the CPU for execution. It is the difference between the arrival time of the process and the time it starts executing. Turnaround Time : Turnaround time is the total time taken by a process to complete its execution, including both waiting time and burst time. It is the difference between the completion time of the process and its arrival time. Response Time : Response time is the time taken from when a process enters the system until it starts its first execution. It is particularly relevant in interactive systems, where users expect quick responses. Source : https://www.shiksha.com/online-courses/articles/turnaround-time-in-cpu-scheduling/ (copy and modified)  Scheduling Algorithms​  There are many algorithms to schedule processes or tasks, different algorithm has different objectives and target specific aspects of process or task scheduling.  First in, first out (FIFO) or First come, first served (FCFS) : This is the simplest algorithm, the next process to be executed is the order they arrive in the ready queue. While it is simple and easy to implement, it may result in poor average response time, especially if long-running processes are ahead in the queue. Source : https://en.wikipedia.org/wiki/Scheduling_(computing)#/media/File:Thread_pool.svg Priority Scheduling : Processes are assigned priorities, and the process with the highest priority is selected for execution. The priority can be anything, for example it could be deadline. Source : https://www.embedded.com/tasks-and-scheduling/ Shortest Remaining Time First or Shortest Job First (SJF) : Strategy where the process with the smallest total execution time is selected for execution next. However, this requires knowledge of the total execution time of each process (or at least estimation), which is often not available or accurate in practice. Source : https://en.wikipedia.org/wiki/Shortest_remaining_time Round Robin (RR) : Each process is allocated a fixed time slice, and processes are executed circularly. This strategy provides fair sharing of CPU time among processes, however, shorter processes may still experience longer response times due to the fixed time slice. Source : https://en.wikipedia.org/wiki/Round-robin_scheduling  Scheduling Problems​  Priority Inversion​  Priority Inversion is a phenomenon that can occur in systems where different tasks or processes have different priorities. It refers to a situation where a lower-priority task or process holds a resource needed by a higher-priority task, thereby causing a delay in the execution of the higher-priority task.  Example scenario :  Three tasks/processes are involved : a high-priority task (H), a medium-priority task (M), and a low-priority task (L).H need to access some resource, but the resource is currently held by L.H couldn't preempt L, because L has much lower priority than H. The reason low-priority tasks can't be preempted is due to the avoidance of starvation, if high-priority task keep preempting, lower-priority task may never get a chance to execute.Suppose that M is ready, M can preempt L because it is not the highest priority. Now, M is currently executing and holds the resource.This phenomenon causes H takes longer to complete, because of waiting for M additionally.  One way to mitigate is the priority inheritance technique, which temporarily raise the priority of a lower-priority task to a higher-priority task when the lower-priority task holds a resource needed by the higher-priority task. So, L priority can be changed to H, this way we don't prevent M for preempting, because essentially it has higher priority. After the resource access is complete, the priority is back to original.   Source : https://www.digikey.com/en/maker/projects/introduction-to-rtos-solution-to-part-11-priority-inversion/abf4b8f7cd4a4c70bece35678d178321 ","version":"Next","tagName":"h3"},{"title":"Process Synchronization","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/process-synchronization","content":"","keywords":"","version":"Next"},{"title":"Synchronization Techniques​","type":1,"pageTitle":"Process Synchronization","url":"/cs-notes/operating-system/process-synchronization#synchronization-techniques","content":" Some concept of synchronization for process is similar to thread synchronization, that is using synchronization tools such as mutexes, condition variables, semaphores, etc.  Peterson’s Solution​  In a concurrent program, the specific section of code where shared resources are accessed or modified is called critical section. We could implement mutex to ensure only one process accesses the resource at a time.  Peterson's solution is an algorithm for mutex (mutual exclusion), it is a software-based solution which ensure that only one process or thread can enter its critical section at a time.  Peterson's solution requires the shared variables :  turn : A variable that indicates whose turn it is to enter the critical section.flag : An array of boolean flags, with each element representing the intention of a process to enter the critical section.  The algorithm works as follows :  Each process sets its flag to indicate its intention to enter the critical section.The process sets turn to indicate that it is the other process's turn.The process enters a loop and checks if the other process's flag is set and if it is the other process's turn. If both conditions are true, the process waits until the other process completes its critical section.If the conditions are false, the process enters its critical section and executes the desired code.After the process completes its critical section, it resets its flag to indicate that it is no longer interested in entering the critical section. Also, exist a remainder section where we can perform any necessary cleanup or non-critical tasks.   Source : https://www.geeksforgeeks.org/introduction-of-process-synchronization/  ","version":"Next","tagName":"h3"},{"title":"Synchronization Problems​","type":1,"pageTitle":"Process Synchronization","url":"/cs-notes/operating-system/process-synchronization#synchronization-problems","content":" Readers-Writers​  Readers-Writers problem is a scenario where there exist multiple processes, one is reader (process that access data), and another is writer (process that writes data), which are trying to access a shared resource simultaneously.  Having multiple process that reads at the same time is not a problem, because every reader will always read the same data. However, in a case where a reader and a writer access the data at a time, this will obviously be a problem, it may result in race condition.  There are two variants of readers-writers problem :  The first problem assumes that readers have priority over writers. Multiple reader is allowed to read simultaneously, reader shouldn't wait if the resource is currently opened for reading.Writers must wait until all active readers have finished accessing the shared resource. Only a single writer is granted exclusive access to the resource at a time.This problem may starve writers. The second problem assumes that writers have priority over readers. When a writer is ready to write, it needs to write as soon as possible.Reader shouldn't write when the writer is going to write.This problem may starve readers.  The criteria for solution :  Readers can access the shared resource simultaneously if no writers are currently accessing it.Writers should access the shared resource exclusively, meaning that no other readers or writers can access it while a writer is writing.The solution should avoid starvation and ensure fairness, neither readers nor writers should be indefinitely blocked from accessing the resource.  The general solution :  Shared variables: readers_count = 0 read_lock = Semaphore(1) write_lock = Semaphore(1) Reader process: while true: wait(read_lock) readers_count = readers_count + 1 if readers_count == 1: wait(write_lock) signal(write_lock) // Perform reading... wait(read_lock) readers_count = readers_count - 1 if (read count == 0): signal(write_lock) signal(read_lock) Writer process: while true: wait(write_lock) // Perform writing... signal(write_lock)   The read_lock and write_lock variable indicates the mutex for reader and writer, respectively.The wait() and signal() function is the implementation of semaphores, it will enable/disable the access. Semaphore keeps a count, it will only allow access if the count is greater than 0. Passing a lock to the wait() function will decrement the lock's count, which effectively block any other reader/writer for accessing. On the other hand, signal() is the opposite of wait(), which will increment the count, effectively notifies other reader/writer.The process of incrementing and decrementing reader count is synchronized with lock, when reading, we will not use the lock, we will allow multiple reader to read simultaneously.When readers_count reaches 0, we will allow the writer to write by signaling the lock.When writing, we will also use wait() and signal() before and after the writing is done, to ensure only one writer writes.  Dining Philosophers​  The dining philosophers problem illustrate the deadlock.   Source : https://www.scaler.com/topics/operating-system/dining-philosophers-problem-in-os/  The problem involves a group of philosophers sitting around a table, alternating between thinking and eating.There exist 5 single chopsticks on the left and right each philosopher.Each philosopher requires two chopsticks to eat but can only pick up adjacent chopsticks, which are in the left or right.The philosopher can't pick chopsticks which are used, they need to wait.Once they are done eating, they put the chopsticks back.The problem demonstrates the challenges of allocating resources to multiple processes, with the primary goal of enabling all philosophers to eat while avoiding deadlock and starvation.  The solution for this problem uses monitors :  monitor DiningPhilosophers { enum: THINKING, EATING, HUNGRY philosophers_state = [THINKING, THINKING, THINKING, THINKING, THINKING] condition = initialize 5 condition variable function pick_chopstick(i: int) { philosophers_state[i] = HUNGRY is_possible_to_eat(i) if philosophers_state[i] != EATING: condition[i].wait() } function is_possible_to_eat(i: int) { if left_and_right_chopstick_available(i): philosophers_state[i] = EATING condition[i].signal() } function left_and_right_chopstick_available(i: int) -&gt; boolean { return philosophers_state[(i + 4) % 5] != EATING &amp;&amp; // left is not eating philosophers_state[i] == HUNGRY &amp;&amp; // is hungry right now philosophers_state[(i + 1) % 5] != EATING // right is not eating } function done_eating(i: int) { philosophers_state[i] = THINKING is_possible_to_eat((i + 4) % 5) is_possible_to_eat((i + 1) % 5) } }   We represent the state of philosopher in enum, which are either THINKING, EATING, HUNGRY. Initially, all philosophers are set to the THINKING state. The specific order in which philosophers start their actions depends on the specific problem at hand.The condition is the condition variable associated with each philosopher, it will change whenever other philosopher uses wait() or signal(). Calling wait() on the condition effectively block the philosopher from eating. When signal() is called, it notifies other philosophers about the current state of the philosopher on which the signal is invoked.Starting from the pick_chopstick, when we call this function given some index i, the philosopher at index i will start eating.Before actually eating, it will first confirm if it's possible to eat by checking the left and right chopstick. If possible, it will eat and notify other philosopher, else it will wait.When eating is done, the done_eating function will be called. It will set the philosopher state back to THINKING, and also decide if the philosopher on the left and right can eat.  The dining philosophers problem is just a theoretical problem. By solving and examining this problem, we can gain insights into various solutions and synchronization techniques that can be applied to more complex real-world scenarios.  Deadlock​  Processes need resource, this mean the process has a dependency on a resource to perform certain operations or computations. For example, a process may need access to a printer resource to print a document, or it may need access to a database resource to retrieve or update data. Deadlock occurs when a set of processes is unable to proceed because each process is waiting for a resource that is held by another process in the set.  Deadlock are typically characterized by :  Mutex : It occurs when mutex is used, when at least one resource must be held in a non-sharable mode, meaning that only one process can use it at a time.Hold and Wait : When accessing resource, the processes will hold resources while waiting to acquire additional resources.No Preemption : Resources cannot be forcibly taken away from processes that hold them, only the process itself can release it.Circular Wait : There exists a circular chain of processes, each waiting for a resource held by the next process in the chain.  Resource Allocation Graph​  We can detect a deadlock by representing the circular chain of processes in a graph called resource allocation graph (RAG). It consists of nodes representing processes and resources, and directed edges representing resource requests and allocations. If the graph contains a cycle, it indicates the possibility of a deadlock.   Source : https://www.geeksforgeeks.org/resource-allocation-graph-rag-in-operating-system/ (box R is resource, circle P is process)  A single resource type means only one instance of a resource can be accessed at a time. However, this method is not applicable when dealing with multi-instance resource types, which is the case when single instance of resource can be accessed by some number of processes.  A deadlock happens when processes are stuck waiting for resources that are held by other processes, forming a cycle in the RAG. However, in multi-instance resource systems, processes can sometimes release some instances of a resource and continue running. So, even if there is a cycle in the RAG, it does not always mean there is a deadlock.  Banker's Algorithm​  The Banker's algorithm can be used to detect deadlock in multi-instance resource systems.  The algorithm operates based on the concept of a safe state, which is a state in which a system can allocate resources to processes in a way that avoids deadlock. In multi-instance resource systems, the resource can be accessed by some number of process, safe state occurs when there are enough resources available to satisfy the resource requirements of all processes in the system.  The opposite concept, unsafe state occurs when there is not enough available resources to satisfy the resource requirements of all processes, which can potentially lead to a deadlock.  The algorithm works by considering the maximum resource needs of each process, the currently allocated resources, and the available resources in the system.  Initialization : The algorithm starts by gathering information about the maximum resource needs of each process, the currently allocated resources, and the available resources in the system.   Source : https://www.geeksforgeeks.org/bankers-algorithm-in-operating-system-2/  The resource allocation can be represented in a table (called available matrix), for example, the resource A has a total of 10, where 2, 3, 2 of it is allocated by process 1, 2, 3, respectively. Resource A can only be accessed by max of 7, resulting in availability of 3.  Request Handling : When a process requests additional resources, the algorithm checks if granting the request would result in an unsafe state or potential deadlock. It evaluates whether there are enough available resources to satisfy the request without violating the safety conditions. Resource Allocation : If the requested resources can be allocated without causing an unsafe state, the algorithm grants the resources to the requesting process. It updates the allocation and available resource matrices accordingly. Safety Check : After each resource allocation or request, the algorithm performs a safety check to determine if the system is in a safe state. It simulates the allocation of resources to all processes and checks if it can reach an end state where all processes can complete their execution without deadlock. Deadlock Avoidance : If the safety check determines that the system is in a safe state, the requested resources are allocated to the process, and the system continues its execution. Otherwise, the requested resources are not immediately granted, and the process must wait until sufficient resources become available.  Recovery​  If a deadlock occurs after detection, there are some method to recover from it :  Process Termination : One approach is to terminate one or more processes involved in the deadlock. By terminating a process, the resources held by that process are released and become available for other processes. The terminated process may need to restart or reattempt its task after the deadlock is resolved. Resource Preemption : In some cases, it may be possible to preempt or forcibly reclaim resources from one or more processes to break the deadlock. The preemption can be achieved by rolling back the process to a checkpoint or by freeing resources that are less critical to the process. Preempted resources can then be allocated to other processes to allow them to proceed. ","version":"Next","tagName":"h3"},{"title":"Protection & Security","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/protection-and-security","content":"","keywords":"","version":"Next"},{"title":"Protection Ring​","type":1,"pageTitle":"Protection & Security","url":"/cs-notes/operating-system/protection-and-security#protection-ring","content":" The protection ring is a concept where the components of computer system, including the operating system, is represented as numbered rings or levels. The rings are numbered, with higher numbers indicating higher privilege levels. The most common implementation includes four rings, often referred to as Ring 0 (most privileged) to Ring 3 (least privileged), although other ring configurations are possible.   Source : https://en.wikipedia.org/wiki/Protection_ring  In the ring 0, often called kernel mode, this is where kernel exist. Kernel is the core of an operating system that has full access to all the computer resources, such as CPU and memory.  Some device drivers may operate at ring 1 or ring 2 to handle I/O operations or provide low-level access to specific hardware devices.  Ordinary user applications, such as word processors, web browsers, media players, or games, operate in ring 3. They have the lowest privilege level and run in a restricted environment, relying on the operating system to perform privileged operations on their behalf.  ","version":"Next","tagName":"h3"},{"title":"Access Control​","type":1,"pageTitle":"Protection & Security","url":"/cs-notes/operating-system/protection-and-security#access-control","content":" Access control is the practice of restricting access to resources in a computer system. It is a security mechanism that determines who is allowed to access specific resources, what actions they can perform on those resources, and under what conditions.  The list of who has access and what they can do is defined in an access control list (ACL). It is a list of permissions associated with each resource that determines who can perform specific actions, such as read, write, execute, or modify, on that resource.  On the other hand, access control list can be rearranged in a matrix structure, this is called access control matrix. It is a security model that represents the access rights and permissions between subjects (users, processes) and objects (resources, files).   Source : https://cybersecurityglossary.com/access-control-matrix/  In the matrix, each row represents a subject, each column represents an object, and the entries in the matrix indicate the access rights or permissions that subjects have on objects. The access rights can include read, write, execute, delete, create, or other specific operations.  One way to implement the access control matrix is to maintain a global table. The operating system maintains a data structure that represents the entire matrix, with subjects as rows and objects as columns. However, the table's size can increase substantially, thus can't be stored in main memory. Additionally, access times may be slower because the table is stored on disk rather than in main memory.  note In Unix, files have protection code associated with it, this includes permission of what operations can be done on it.  ","version":"Next","tagName":"h3"},{"title":"Common Threats​","type":1,"pageTitle":"Protection & Security","url":"/cs-notes/operating-system/protection-and-security#common-threats","content":" Some common security threats : Malicious software, such as viruses, worms, Trojans, ransomware, and spyware. Unauthorized access which happen through brute-force attacks, password cracking, or exploiting weak authentication mechanisms. Social engineering technique such as phishing, pretexting, or impersonation.  See computer security and other attack &amp; exploit for more.  Example : Stoned​  One example of a virus is stoned, it's a dangerous virus that attacks the boot sector and infects the Master Boot Record (MBR) of floppy disks and hard drives.  The virus starts from a disk, when a user inserts their disk to an infected one, the virus will spread. When the user boots from an infected disk, the virus code in the boot sector will be executed during the boot process.The virus can move the location of MBR, it is typically located in the first sector of the disk. The files that exist in the location that become the new location for MBR can be lost.The Stoned virus's primary purpose is to display a humorous or taunting message on the screen once the computer boots up. The specific message displayed varies between different variants of the virus but often includes phrases related to drug use or legalization.  Unlike some other viruses, the Stoned virus does not typically cause significant damage or data loss. Its main impact is the display of the message, which is more of a nuisance than a destructive action. ","version":"Next","tagName":"h3"},{"title":"System Call","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/system-call","content":"","keywords":"","version":"Next"},{"title":"System Call Interface​","type":1,"pageTitle":"System Call","url":"/cs-notes/operating-system/system-call#system-call-interface","content":" System calls are provided by the operating system kernel, this mean each OS may have different way to request access. They are achieved through calling a set of defined function or method provided by the operating system. For example, functions with descriptive names like open(), read(), write(), are used to manipulate file.   Source : https://stackoverflow.com/questions/60136893/what-is-system-call-interface  The application that calls doesn't need to know the low-level details of how the operating system implements that system call. The application only needs to know the interface provided by the operating system and how to use the system call correctly.  The use of interface allows for a simple development, application developers can write portable code that can run on different operating systems without modifications, as long as the system call interface is consistent.  In Unix-like systems, system calls are included in the C language, specifically the libc library. In other OS like Windows NT, system calls are commonly referred to as &quot;Windows API functions&quot;, and it has different interface with the libc.  ","version":"Next","tagName":"h3"},{"title":"Type of System Calls & Example​","type":1,"pageTitle":"System Call","url":"/cs-notes/operating-system/system-call#type-of-system-calls--example","content":" System calls can be categorized into several types based on the services they provide. In the example, the system calls are associated with Unix-like operating systems.   Source : https://www.scaler.com/topics/operating-system/system-calls-in-operating-system/  Process Control​  They are related to the management of processes and their execution.  Process Creation : System call like fork() duplicates the existing process, creating a new child process with a new ID, the child process also inherits some of the attributes and resource such as memory space from the parent process. The exec() system call replaces the current process's memory with a new program, loading a new executable file.Process Termination : The exit() system call terminates a process, returning an exit status. The wait() system call allows a parent process to wait for its child process to terminate and retrieve its exit status.Process Status : System calls like getpid(), getppid(), and getuid() provide process-related information such as process ID, parent process ID, and user ID.Process Scheduling : The yield() system call allows a process to voluntarily release the CPU and let other processes run. The nice() system call adjusts process scheduling priority, with lower values indicating higher priority. These system calls manage process scheduling and priorities.  File Manipulation​  System calls in this category are used for working with files and file systems.  File Operations : System calls with descriptive names such as open(), read() to, write(), and close() are used for manipulating files.File and Directory Management : System calls like mkdir() to create a new directory, rmdir() to removes an empty directory, and rename() for renaming a file or directory.File System Information : System calls such as stat() to retrieve file metadata, such as size and timestamps, while chmod() is used to change file permissions, such as read, write, and execute permissions.  Device Manipulation​  System calls related to devices and device drivers are used for interacting with hardware devices.  Device I/O : System calls such as ioctl(), read(), and write() are used for device input/output operations. ioctl() allows for performing device-specific control operations, while read() and write() are used to read data from and write data to devices.Device Control : System calls like open(), close(), and poll() are used to control device access and manage device connections. open() is used to open a device and obtain a file descriptor for it, close() is used to release the device and close the file descriptor, and poll() is used to monitor the status of multiple devices for events or data availability. These system calls provide mechanisms for managing device interactions and controlling device access.  Information Maintenance​  System calls in this category are used to retrieve and manipulate system information.  Time and Date : System calls such as time(), gettimeofday(), and clock_gettime() provide access to system time and date. They allow programs to retrieve the current time, measure time intervals, and receive a precise timestamp.System Configuration : System calls like uname() retrieve system config information such as the operating system name and version.System Resource Usage : System calls such as getrusage() provide information about resource usage by processes. They allow programs to retrieve statistics on CPU time, memory usage, and other resources consumed by a process.  Communications​  System calls in this category are used for inter-process communication (IPC) and network communication (socket operations).  Socket Operations: System calls such as socket(), bind(), listen(), accept(), connect(), send(), and recv() are used for network communication using sockets. Sockets provide an interface for processes to communicate over a network. These system calls allow processes to create network sockets, bind them to specific addresses and ports, listen for incoming connections, establish connections to remote hosts, and send/receive data over the network.Inter-Process Communication (IPC): System calls like pipe(), shmget(), semget(), and msgget() are used for various forms of IPC, including shared memory, semaphores, and message queues. These mechanisms facilitate communication and synchronization between different processes running on the same system. Processes can share data through shared memory, coordinate access to shared resources using semaphores, or exchange messages through message queues using these system calls.  tip See also message passing and IPC.  Protection​  System calls in this category are related to process and system security, including mechanisms for access control, user authentication, and cryptographic operations.  Access Control: System calls such as chmod(), chown(), and setuid() are used to modify file and process permissions and ownership. These system calls allow processes to change the access permissions and ownership of files, directories, and processes, providing control over who can read, write, or execute them.User Authentication: System calls like getpwuid() and getpwnam() retrieve user account information. They allow processes to retrieve user details, such as username, user ID, and other account information, which can be used for authentication and access control purposes.Cryptography: System calls such as encrypt(), decrypt(), and hash() provide cryptographic functionality for data security. These system calls allow processes to perform encryption and decryption operations on data, as well as generate cryptographic hashes for data integrity verification and password storage.  tip See also OS security.  ","version":"Next","tagName":"h3"},{"title":"System Call Implementation​","type":1,"pageTitle":"System Call","url":"/cs-notes/operating-system/system-call#system-call-implementation","content":" Implementation vary of the operating system kernel, the general steps are :  System Call Number &amp; Table : Each system call is assigned a unique number or identifier by the operating system. The number is used to identify the requested system call, they are maintained in a table that maps the system call numbers to their corresponding kernel functions or handlers.User-Space to Kernel-Space Transition : When a user program invokes a system call, it triggers a transition from user mode to kernel mode. This transition is typically done through a software interrupt or a special instruction, which transfers control from the user program to the kernel.System Call Handler : The kernel receives the system call request and identifies the requested system call using the system call number provided by the user program. The system call handler corresponding to the system call number is then invoked.Kernel Validation : The system call handler validate the system call, ensuring that the parameter is valid, the caller has necessary permission, memory address is valid, and checks through other potential security risk.Kernel Execution : If the system call is valid, the handler executes the requested operation on behalf of the user program, depending on the request.Result Return : After the system call handler completes its execution, the kernel returns the result of the operation back to the user program. This can include return values, error codes, or other relevant information.User-Space Resumption : The control is transferred back to the user program, and it continues execution from where it left off, now with the result of the system call available. ","version":"Next","tagName":"h3"},{"title":"Type of OS","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/type-of-os","content":"Type of OS Main Source : Operating system, Types of operating systems - WikipediaMostly other Wikipedia pages There are many types of OSes : Single-User or Multi-User : Single-user OS are designed to support and cater to the needs of a single user at a time. All resource such as storage are dedicated to the individual user. Examples are MS-DOS, Windows 3x, Windows 95, Macintosh OS. Multi-user OS allow multiple users to access the computer, it typically includes mechanism like permission and authentication for securing shared resources. Examples are Linux based OS, macOS, Windows. Single-Tasking or Multitasking : Single-tasking operating systems can only execute one task or process at a time. When a program is running, the entire system's resources are dedicated to that particular task until it completes or is interrupted. Examples are MS-DOS and earlier versions of Macintosh OS. Multitasking operating systems are designed to run multiple tasks or processes concurrently. CPU time and other resources is divided fairly among them. Examples are Windows, macOS, and Linux. Real-Time OS (RTOS) : RTOSs are designed to handle real-time applications and provide guaranteed response times for critical tasks. They are commonly used in embedded systems and industrial applications. Examples are FreeRTOS, QNX, and VxWorks. Network OS (NOS) : NOSs are specifically designed to manage and coordinate network resources. They enable sharing of files, printers, and other network devices among multiple connected computers. Examples are Novell NetWare and Windows Server. Mobile OS : These operating systems are designed for mobile devices such as smartphones and tablets. They focus on touch interfaces, power management, and mobile-specific features. Examples are Android, iOS, and Windows Phone. Distributed OS : Distributed operating systems run on multiple machines and coordinate their activities to appear as a single unified system. They are used in clusters, cloud computing, and distributed computing environments. Examples include Google's Chrome OS and Amoeba. Embedded OS : Embedded operating systems are designed to run on embedded systems with limited resources, such as microcontrollers or IoT devices. They are typically lightweight, compact, and tailored for specific hardware platforms. Examples are Embedded Linux, and ThreadX. Virtualization OS : These operating systems provide virtualization capabilities to enable multiple virtual machines to run on a single physical machine. Examples include VMware ESXi, Microsoft Hyper-V, and Xen.","keywords":"","version":"Next"},{"title":"Unix","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/unix","content":"","keywords":"","version":"Next"},{"title":"Architecture & Structure​","type":1,"pageTitle":"Unix","url":"/cs-notes/operating-system/unix#architecture--structure","content":" Below is the layered architecture of Unix OS, with the hardware being the innermost layer and applications in the outermost layer.   Source : https://www.tutorialspoint.com/unix/unix-getting-started.htm  The kernel layer wraps hardware layer, it handles low-level tasks such as process management, memory management, file system management, device driver interfaces, and scheduling of system resources.Beyond the kernel, there exist the interface layer. This layer contains command (shell) and set of system call interface that allows program to request services from the kernel, such as creating processes, accessing files, and managing memory.The last level contains application programs, which uses system APIs by requesting it through system call interface to the kernel.  The structure of Unix is as follows :   Source : https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/2_Structures.html  ","version":"Next","tagName":"h3"},{"title":"Features​","type":1,"pageTitle":"Unix","url":"/cs-notes/operating-system/unix#features","content":" Portability : Unix follows the Portable Operating System Interface (POSIX) standard. POSIX defines how should an OS provide both system and application APIs so that it can be portable between other platforms. Shell : Unix provides shell that serves as a command-line interface (CLI) for interacting with the system. The shell interprets user commands and executes them, allowing users to perform various operations, run programs, and manage files and processes. ls : Lists files and directories in the current directory.cd : Changes the current directory.mkdir : Creates a new directory.rm : Removes files and directories. Hierarchical File System : UNIX uses a hierarchical file system, where files and directories are organized in a tree-like structure, with one root directory (the &quot;/&quot;), and inode as the data structure of a file system object. usr : Stands for user, which contains user-related files and programs that are not essential for basic system functionality.home : The home directory is typically the default directory for each user. It stores personal files and user-specific configurations. Each user has their own home directory, typically represented as /home/username or ~.bin : The bin directory, short for binary, contains executable binary files (i.e., compiled programs or scripts) that are essential for basic system functionality. Common system utilities and commands are stored in this directory. Inter-process Communication : Uses of pipes as its IPC mechanism. For example, the command command1 | command2 creates a pipe between command1 and command2, where the output of command1 is piped directly as input to command2. System Calls : See type of system calls &amp; example. ","version":"Next","tagName":"h3"},{"title":"User Interface","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/user-interface","content":"","keywords":"","version":"Next"},{"title":"CLI​","type":1,"pageTitle":"User Interface","url":"/cs-notes/operating-system/user-interface#cli","content":" CLI is a text-based interface, which means you enter specific command as text, and then the computer will execute the task associated with it. The output generated by the command is then displayed as text.   Source : https://avc.com/2015/09/the-return-of-the-command-line-interface/  When a command is entered, the computer will need to interpret the command. The program that interpret the command is called a shell. Example of shell in Unix-like systems include Bash. The shell interpret the command by searching the corresponding executable program to execute that command. The command is typically the program's name. Once the program is found, the shell loads it into memory and executes it, passing any required arguments. The program performs its task and produces output, which is sent back to the terminal for display.  The programs for shell is typically stored in the directories specified in the system's PATH environment variable. Environment variables are dynamic values that can affect the behavior of the system or individual programs. Environment variables store information such as system paths, user preferences, or configuration settings.   Source : https://medium.com/chingu/an-introduction-to-environment-variables-and-how-to-use-them-f602f66d15fa  info See also command line vs shell  ","version":"Next","tagName":"h3"},{"title":"GUI​","type":1,"pageTitle":"User Interface","url":"/cs-notes/operating-system/user-interface#gui","content":" GUI presents computer system using graphical elements such as windows, icons, menus, buttons, and other visual controls. To interact with computer in a GUI system, those visual elements are directly manipulated by user using input devices like a mouse, keyboard, touchscreen, or other pointing devices.  For example, users can move the mouse cursor on the screen, click or double-click on icons, buttons, or other graphical elements to perform actions. Users can perform actions by pressing specific key combinations, such as Ctrl+C to copy selected text or Ctrl+Z to undo an action. GUI systems typically have menus that provide a list of options. Users can select menu items by clicking on them or by using keyboard shortcuts.   Source : https://www.britannica.com/technology/graphical-user-interface  GUI relies on a windowing system, which manages the display of graphical elements on the screen. The OS also interact with the graphics subsystem that handles the rendering and displaying graphical elements on the screen. It interacts with the graphics hardware to draw images, text, icons, and other visual components.  The GUI systems will monitor for user input such as mouse clicks, keyboard input, touch gestures, or stylus interactions. These events trigger actions within the GUI, it will be sent to the appropriate windows or applications.  MVC​  One of the popular pattern for designing a GUI system is the Model-View-Controller (MVC) pattern. This pattern design GUI system in three separate components.  The model represents the data and the logic within the application. It would handle tasks such as data retrieval, storage, manipulation, and any other operations specific to certain tasks.  The view represents the graphical interface that the user interacts with. It is responsible for presenting the data from the Model to the user and receiving user input.  The controller acts as an intermediary between the Model and the View. It receives user input from the View, processes it, and updates the Model accordingly. It also listens for changes in the Model and updates the View to reflect those changes.   Source : https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller ","version":"Next","tagName":"h3"},{"title":"Virtualization","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/virtualization","content":"Virtualization See Cloud Computing &gt; Virtualization","keywords":"","version":"Next"},{"title":"Programming Language Theory","type":0,"sectionRef":"#","url":"/cs-notes/programming-language-theory","content":"","keywords":"","version":"Next"},{"title":"All pages​","type":1,"pageTitle":"Programming Language Theory","url":"/cs-notes/programming-language-theory#all-pages","content":" abctype system (primitive, complex object, generics)implementation of modern language features, such as OOP, threading, handling null, handling exception. ","version":"Next","tagName":"h3"},{"title":"Windows","type":0,"sectionRef":"#","url":"/cs-notes/operating-system/windows","content":"","keywords":"","version":"Next"},{"title":"History​","type":1,"pageTitle":"Windows","url":"/cs-notes/operating-system/windows#history","content":" DOS (Disk Operating System) is an OS that runs from a disk drive, they are operating system with simple command-line interface used to manage computer's hard drive and hardware components. MS-DOS is a specific DOS developed by Microsoft, initially released in 1981.   Source : https://en.wikipedia.org/wiki/MS-DOS  MS-DOS was very simple, it was more of a graphical environment rather than a complete operating system. Microsoft introduces many versions of Windows that was based on MS-DOS, including Windows 1.x, 2.x, 3.x, 9x (e.g., Windows 95, Windows 98), and Windows Me (Millennium Edition). The MS-DOS kernel is a monolithic kernel that was specifically developed for the x86 architecture. Most versions of MS-DOS are 16-bit, but there are some versions that are hybrid, combining elements of both 16-bit and 32-bit functionality.  Upon several MS-DOS based OS, Microsoft developed the Windows NT operating system. Windows NT (New Technology) was a complete rewrite from scratch and was a full 32-bit system. It was targeted to be a general-purpose operating system for personal computers.  The NT-based Windows line started with Windows NT 3.1 in 1993. The kernel used in Windows NT was called the NTOS kernel, which is a more advanced kernel that supports more feature such as multiuser, POSIX compatibility, portable kernel with preemptive multitasking, and support for more architecture.   Source : https://betawiki.net/wiki/Windows_NT_3.1  Microsoft continued to develop the NT-based Windows line with versions such as Windows NT 3.1, 3.5, 3.51, 4.0, and Windows 2000.  The next major version, Windows XP, released in 2001, had a significant run and replaced previous versions of Windows. It was the successor to Windows 2000 and was designed to provide a more intuitive graphical interface, enhanced security, and many new or upgraded user programs.  More version keep being developed with more features, Windows Vista, Windows 7, Windows 8, Windows 8.1, Windows 10, and the latest up to now, Windows 11. Along these versions, Microsoft also developed for various platforms beyond the desktop. This includes Windows Server, Windows Phone, Windows Embedded, and many more.   Windows XP Source : https://www.liputan6.com/tekno/read/2019112/tak-mau-imove-oni-dari-windows-xp-ini-risikonya  ","version":"Next","tagName":"h3"},{"title":"Windows 7​","type":1,"pageTitle":"Windows","url":"/cs-notes/operating-system/windows#windows-7","content":" Windows 7, based on the Windows NT family of operating systems was released to manufacturing on July 22, 2009, and became generally available on October 22, 2009. Windows 7 succeeded Windows Vista and was succeeded by Windows 8.  The design principles of Windows 7 were aimed at achieving various goals, including security, reliability, compatibility, performance, extensibility, portability, international support, energy efficiency, and dynamic device support [1][2].  For security, Windows 7 implement discretionary access controls, using access-control lists (ACLs) to protect system objects such as files, registry settings, and kernel objects.  Extensive code review, testing, and automatic analysis tools were utilized to identify potential defects that could lead to security vulnerabilities and prevent errors in drivers and applications.  Compatibility with existing applications was also a key consideration in the design of Windows 7. The operating system aimed to ensure compatibility with both Windows and POSIX applications.  Architecture​  The architecture of Windows is a layered system of modules. Windows operate on client-server model, meaning there is a role of client and server. In this model, clients are the end-user devices or applications that make requests for services or resources, while servers are responsible for providing those services or resources.  The client-server model is typically used in networking, but it is also appliable in the context of operating system. To facilitate communication and interaction between clients and servers, Windows employs various mechanisms, including Remote Procedure Calls (RPCs). RPCs allow clients to send requests to servers, and the servers respond with the required information or perform the requested actions.   Source : https://en.wikipedia.org/wiki/Architecture_of_Windows_NT  Hardware Abstraction Layer (HAL) : The HAL is a layer that abstracts the hardware-specific details and provides a uniform interface for the rest of the operating system. It allows Windows to run on different hardware platforms with minimal modification. Kernel : The kernel does four main things, thread scheduling, low-level processor synchronization, interrupt and exception handling, and switching between user mode and kernel mode. Executive : The Executive is a layer above the kernel and provides a set of essential services and components that support the operation of the operating system. It includes components such as the memory manager, process manager, I/O manager, security reference monitor, and the object manager. In the Windows kernel, objects are fundamental data structures that are allocated and utilized by the kernel itself. They represent various system resources, such as processes, threads, files, semaphores, I/O devices, and more. Objects in the Windows kernel are not used in the same sense as in object-oriented programming, but rather as data structures managed by the kernel, specifically by the object manager. User Mode : User mode includes various user-facing components and services. This includes the Windows shell, which provides the graphical user interface and manages the desktop, taskbar, and start menu. Other user mode components include system services, runtime libraries, security, and application frameworks that enable the execution of user applications. Subsystems : Windows supports multiple subsystems, a subsystem refers to a component that provides a specific environment for running applications and executing their code. There are three subsystems, Win32, POSIX, and OS/2 subsystem. The Win32 subsystem provides compatibility for 32-bit Windows applications, while the POSIX subsystem allows running applications compatible with the POSIX standard, and OS/2 subsystem allows OS/2 application to be run on Windows NT.  Features​  Thread &amp; Scheduling : Thread is scheduled based on their priority. For example, thread waiting for keyboard I/O would get a large priority increase, whereas a thread waiting for a disk operation would get a moderate one. The Windows scheduler employs preemptive scheduling, which means that a running thread can be interrupted and replaced by another thread of higher priority. Synchronization : Windows provides various synchronization mechanisms through dispatcher objects, such as mutex locks, semaphores, events, and timers. Interrupts : Interrupts, represented by interrupt object, are handled by interrupt dispatcher in the kernel which calls the appropriate handler (ISR) from an interrupt-dispatch table. Memory Management : Virtual memory system is managed by virtual memory manager (VMM). The virtual memory manager uses a page-based management scheme, with a fixed 4KB page size. Virtual address space for each process is divided into 32-bit segments, allowing for a maximum of 4 GB of virtual address space. For x86 machines, this space is split between the user and kernel, with each receiving 2 GB. For x64 machines, both the user and kernel receive more virtual addresses than they can practically use. The virtual address space is demand-paged, meaning that only the necessary pages are loaded into physical memory when needed. Access of virtual address is handled in multilevel page table. This table divides the virtual address space into multiple levels, each with its own page table entries. Process has a page directory, which is the main mapping of the page tables. It contains pointer to a Page Directory Entry (PDE). Each entry points to another table, Page Table Entry (PTE). Page directory has 512 PDE, and each PDE has 512 PTEs, each of which points to a 4-KB page frame in physical memory. Source : OS Concepts book page 848 Process Management : When a process is created, the process manager is responsible for handling the necessary steps. For example, when a Win32 application calls the CreateProcess() function, a message is sent to the Win32 subsystem to notify it of the process creation. I/O Manager : I/O manager keep track of loaded device drivers, filter drivers, and file systems, it maintains a record of which drivers and file systems are currently loaded in the system. Device drivers follow the Windows Driver Model (WDM) specification, they are represented as driver object. Application will make I/O request through the I/O subsystem and the I/O services will generate a request in a form called I/O request packet (IRP), which is then passed down to the driver object. Source : https://www.researchgate.net/figure/Windows-NT-I-O-driver-stack-model-The-I-O-Manager-of-the-Windows-NT-operating-system_fig1_220718462 File System : Windows initially uses the FAT file system, but it has been replaced with the NTFS. NTFS provide access control lists, encryption, data recovery, fault tolerance, compression, support for very large files and file systems, multiple data streams, Unicode names, and sparse files. Networking : Windows supports both peer-to-peer and client-server networking. It provides various networking components that facilitate data transport, inter-process communication, file sharing, and the ability to send print jobs to remote printers. Networking is standardized in two interfaces : Network Device Interface Specification (NDIS) and the Transport Driver Interface (TDI). NDIS operates between data link (layer 2) and network layer (layer 3), it separates network adapters from transport protocols, allowing either component to be changed independently without affecting the other. TDI, on the other hand, operates between transport (layer 4) and session layer (layer 5). Windows provides several networking features. For example, Windows supports DNS, DHCP, and Windows Internet Name Service (WINS), which help in managing IP addresses and name resolution on networks. For protocol, Windows implement TCP/IP and HTTP. Windows also support the RPC mechanism for making procedure call on other machine across the network. All networking in Windows is provided via the Winsock API (Windows Sockets). ","version":"Next","tagName":"h3"},{"title":"Build & Package Management","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/build-and-package-management","content":"","keywords":"","version":"Next"},{"title":"Package Management​","type":1,"pageTitle":"Build & Package Management","url":"/cs-notes/software-engineering/build-and-package-management#package-management","content":" Libraries​  While developing software, developers often use libraries made by other developers. This is to reduce the time spent to develop basic functionalities, so they can focus on business requirement.  When we say libraries, they are not limited to code made by others. It is really any piece of code that used to build a complete software. They can be code provided by the language itself (often called standard library), code made by others, or even your own function. A code doesn't need to be complex in order to be called a library. Even an addTwo function can be considered as library. In summary, libraries are reusable code that provide specific functionality to solve common problems.  The simplest way of programming a software is, to include all the source code in a single file. Obviously this is not ideal, especially for larger and more complex projects. Another way is to separate them to multiple files, it is more realistic.  Linking​  When using library, functions, classes, or variables are scattered around the source code (i.e., in another file). In order to connect them together, a program called linker will link together all the libraries used. The linking process is done during compilation process and doesn't require developer intervention. The only think that the linker need is associated library, which must exist in our source code.  There are two types of library based on how they are linked :  Static or archive library : This is a type of libraries that are combined into a single file. It contains pre-compiled code, functions, classes, or variables that can be linked directly into an executable at compile time. When a static library is linked, the linker copies the pre-compiled code from the library into the final executable. This means that the compiled code from the library becomes a part of the executable. Static libraries are identified by .a format on Unix or .lib format on Windows.Dynamic or shared library : This library also contains pre-compiled code, but they are linked and loaded at runtime. Instead of copying the code into our executable, the linker creates references to the functions, classes, or variables in the dynamic library. This can reduce the size of our executable, but it requires the library to be present in the system. Dynamic libraries are identified by .so format on Unix or .dll format on Windows.  Pre-compiled library like static and dynamic is called binary library. On the other hand, if the library is still in the form of source code, then it is source library.  Dependency​  After using many libraries, managing them can be complex. It's worth noting than others library can depend on another, if we use library A, it may depend on library B and C, and each of them may depend on another. If one of the libraries need to be updated, we will need to download them again manually.  One of the scariest problem of managing library is, if one of them depend on the other. Library has specific version, some of them may use the semantic versioning. The problem arise when different libraries have conflicting version requirements for their dependencies. This situation, known as dependency version conflicts, can create a complex puzzle to solve.  For example, we are using library A and C. Library A requires the version 1.2.0 of library B, while library C requires the newer version of library B, which is version 2.0.0. So, which version should we use? The problem is, it is possible that library B in the version 2.0.0 changes the behavior of some function that exist in version 1.2.0, which is used by library A.  tip The ability of a newer version of a library to be used with a previous version, specifically when there are breaking changes that affect the behavior or API of the library, is called backward compatible.  Furthermore, in another scenario, what if library A depends on library B, but library B also depends on library A, creating a circular dependency. Circular dependencies can occur when integrating libraries that were not originally designed to depend on each other. We can call all this problem dependency hell, where software can't work because of its dependencies.  Package Manager​  All the problem can be handled (not solved) by package manager, a specialized software that automates the process of installing, upgrading, configuring, or removing libraries.  The functions of package manager :  Managing dependencies : Package manager typically works with a configuration file, containing what are the dependencies needed for the project. If we want to use others library, we would add the identifier for the library and specify its version we want to use. Dependency Installation : Libraries are typically put on centralized code repository, which package managers can retrieve from. Example of them are Maven for Java, PyPI for Python, and NuGet for .NET. To actually install library, we need to tell the package manager to retrieve the package. This is done by executing commands provided by the package manager in the command line. They will handle the necessary steps to download, extract, and install the package files, including any additional resources or configurations, similarly for deletion or removal. The package manager also keep tracks the version we are using, and we can use another command to update the package to the latest version. Dependency Resolution : Package manager analyze each dependency of different packages. They examine the version requirements and constraints specified by each package and attempt to find a compatible set of versions that satisfy all dependencies. It can construct a directed graph to represent the dependency structure. It then uses algorithm such as cycle detection and topological sort to identify circular dependencies and determine the correct order to install dependencies. Dependency Locking : To ensure consistency between dependencies, package managers often generate lock files. These lock files capture the exact versions of the installed packages, including their dependencies. Lock files ensure that dependency version doesn't change suddenly. Over time, new versions of packages may be released. Without locking the version, it is possible that the package manager accidentally update the package, which may introduce compatibility issue with the existing dependencies.  info An example of package manager is npm, which is the default package manager for Node.js projects.  warning Using package manager doesn't mean it will solve every dependency issues. Conflicting issue, such as one requiring newer version that is not compatible with older version may not be solvable by nature, and require human intervention. What package manager guarantee is, it helps us to automate and handle issue that are solvable.  Semantic Versioning​  Semantic Versioning (SemVer) is a versioning scheme used to assign meaningful version numbers to software packages. It is a set of rules and guidelines that helps developers and users understand the nature of changes in a release and determine compatibility with other versions.  The format of the version in SemVer is x.y.z, for example, 1.2.4. It consists of three parts:  MAJOR (e.g., 1.y.z) : Indicates significant changes that may introduce breaking changes or incompatible API modifications compared to previous versions.MINOR (e.g., x.2.z) : Represents added functionality or features in a backward-compatible manner. It indicates that new features have been introduced, but existing APIs remain compatible with previous versions. Typically, a dependency is kept at highest minor version, unless it is possible to update to the major version.PATCH (e.g., x.y.3) : The patch version number is incremented for backward-compatible bug fixes, patches, or updates that do not introduce any new features.  In addition, we can provide additional information to the version :  Pre-release Version : Denoted by appending a hyphen followed by a series of alphanumeric identifiers (e.g., 1.2.3-alpha.1). Pre-release versions are used for releases that are not yet considered stable or production-ready. They can include alpha, beta, RC (release candidate), or any custom identifiers.Build Metadata : Additional information about the version and is denoted by appending a plus sign to the pre-release or patch version followed by arbitrary identifiers (e.g., 1.0.0-alpha+001).  ","version":"Next","tagName":"h3"},{"title":"Software Build​","type":1,"pageTitle":"Build & Package Management","url":"/cs-notes/software-engineering/build-and-package-management#software-build","content":" In order to run a program, it needs to be compiled, or converted into machine code, which can be executed by computers. The general process of converting source code into executable is called build. The build process is typically automated by build tools.  A build process includes :  Compilation : Translating human-readable source code into machine-readable code, or intermediate language in the case of interpreted languages.Dependency Resolution: Identifies and resolves the dependencies required by the software project. Ensures that the necessary external libraries, frameworks, or modules are available and compatible with the project. Some build tools such as Gradle includes a package manager for this.Quality Assurance : During build process, code quality is tested. This includes doing automated unit tests or analyzing code with integrated static analyzer tools.Code Packaging : Packages the necessary files and resources into a distributable format. This may involve creating an executable file, a library, an archive, or a deployable package, depending on the nature of the project.Deployment : In some cases, the build process includes deploying the software to a specific environment, such as a production server, a testing environment, or a cloud platform.  More specifically, a build process produces a software artifact, which is a general concept that refers to any output or result of the software development process. An artifact can be executable, which contains all the necessary instruction, packaged with dependencies for end-user to execute.  A build process can be full or incremental. A full build involves building the entire software project from scratch, while incremental build only builds the portions of the software project that have changed since the last build. ","version":"Next","tagName":"h3"},{"title":"Software Engineering","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering","content":"","keywords":"","version":"Next"},{"title":"Intro​","type":1,"pageTitle":"Software Engineering","url":"/cs-notes/software-engineering#intro","content":" Software engineering is an engineering discipline that is concerned with software programs. A software system is developed by programmers, which use programming languages like Python, Java, C++, or JavaScript to write instructions that the computer can understand and execute.  The difficulty of making software can vary depending on the type of software being made and the complexity of the software. A simple software made by individual may not involve more than one program. A complex software made by team of individuals may need multiple programs or components to work together. Often they require extra configuration to ensure the software can be run on different types of devices.  As software become more complex, more people coming to develop and more people using it, various principles of software engineering may be needed. Software engineering principles are a set of guidelines that help guide the development of high-quality software products.  An individual programmer may not need to structure their code in a specific standard. They can even name their program's variable or function in their own language. However, as we are developing the software in a larger scope with more people on it, we may need to follow specific standard or even document our code.  Not only making collaboration possible, following a specific standard helps to improve software development process efficiency. The simplest example of software engineering standard that probably everyone has done before is the DRY (Don't Repeat Yourself). In short, the principle encourages us to avoid code duplication. Instead of coding the same instruction multiple times, we can move it to a function that we can reuse it anytime.  ","version":"Next","tagName":"h3"},{"title":"Software Characteristics​","type":1,"pageTitle":"Software Engineering","url":"/cs-notes/software-engineering#software-characteristics","content":" A good software is associated with several characteristics :  Functionality : A software is designed to solve problem, whether a general problem, example of them are databases, word processors, drawing apps, or notes app. It can also be a custom software, such as the one that operates on an embedded device like fridge.Reliability : A reliable software operates consistently and predictably under various conditions. It should be stable, robust, and resistant to failures. Users should be able to depend on the software to perform as expected without unexpected crashes, errors, or data loss.Usability : This refers to how easily and efficiently users can interact with the software. Good software is user-friendly, intuitive, easy-to-learn, and well-designed, taking into account the needs and expectations of its target users.Performance : Good software is efficient, responsive, fast, and utilize system resources optimally. Users should not experience excessive delays or slowdowns while using the software.Maintainability : Often times a software is updated to introduce new features or bug fixes. Maintainable software is easy to modify and maintain. It should be well-organized, have a comprehensive documentation, and follow good coding practices.Scalability : Scalable software can handle increasing workloads, user demands, and data volumes without significant degradation in performance or functionality. It should be designed to accommodate future growth and potential changes in requirements.Security : Good software prioritizes security and protects against security vulnerabilities to ensure the confidentiality, integrity, and availability of user data.Compatibility : Good software is compatible with diverse computing environments, operating systems, and devices.Testability : Software creation goes through many tests to ensure its functionality. A good software should be easy to test and thoroughly tested.  ","version":"Next","tagName":"h3"},{"title":"All pages​","type":1,"pageTitle":"Software Engineering","url":"/cs-notes/software-engineering#all-pages","content":" Software ProcessSoftware Design Software PrinciplesDiagramsDesign Patterns Creational PatternsStructural PatternsBehavioral Patterns Software Architecture Client-ServerEvent-DrivenMaster-SlavePeer-to-PeerLayeredOther Architecture Patterns Software Management Software TestingSoftware DebuggingBuild &amp; Package ManagementVersion ControlOpen SourcingModularizationSoftware Deployment System DesignSystem Design Examples ","version":"Next","tagName":"h3"},{"title":"Client-Server","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/client-server","content":"Client-Server See Cloud Computing &gt; Client-Server","keywords":"","version":"Next"},{"title":"Event-Driven","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/event-driven","content":"Event-Driven See Cloud Computing &gt; Event-Driven","keywords":"","version":"Next"},{"title":"Diagrams","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/diagrams","content":"","keywords":"","version":"Next"},{"title":"Wireframe​","type":1,"pageTitle":"Diagrams","url":"/cs-notes/software-engineering/diagrams#wireframe","content":" Wireframe is a visual representation or a low-detail outline of a user interface (UI). It is a basic visual guide that depicts the layout, structure, and functionality of an application or website without focusing on visual design elements such as colors, typography, or detailed graphics. Images and text are typically replaced with placeholder elements, such as lorem ipsum for text.  Wireframes are typically created early in the design process to outline the key elements and user interactions of a digital product. They serve as a blueprint or a visual plan that helps stakeholders, designers, and developers align their understanding of the project and make informed decisions about the user experience.   Source : https://www.productplan.com/glossary/wireframe/  ","version":"Next","tagName":"h3"},{"title":"UML​","type":1,"pageTitle":"Diagrams","url":"/cs-notes/software-engineering/diagrams#uml","content":" UML (Unified Modeling Language) is a standardized notation for modeling software systems. There are two types of UML diagram, behavior and structure.  Class Diagram​  A class diagram describes the structure of class, such as their properties, methods, and relationship.  The diagram consists of box with hierarchical structure, the box is divided into three sections :  The name of the class, printed in bold and centered, and the first letter is capitalized.Contains attributes of the class., they are left-aligned and the first letter is lowercase.Contains the methods the class can execute, they are also left-aligned and the first letter is lowercase.  The visibility of class members :  + : Public- : Private# : Protected~ : Package  Relationship between class is denoted by arrows :   Source : https://en.wikipedia.org/wiki/Class_diagram#/media/File:Uml_classes_en.svg  Association : Represents a general relationship between two classes, indicating they are connected.Navigable association : A specific type of association where one class has a reference to the other class. It indicates that the class can navigate, communicate, or access the objects of the associated class.Inheritance : Inheritance represents an &quot;is-a&quot; relationship between classes, where one class inherits the properties, methods, and relationships of another class.Realization / Implementation : Represents the relationship between a model and its implementation. It indicates that the class implements the operations and behaviors defined by the interface.Dependency : Signifies that one class depends on another class, typically when one class uses or relies on the functionality provided by another class.Aggregation : Aggregation represents a &quot;has-a&quot; relationship between two classes. It indicates that one class is composed of or contains other classes.Composition : Composition is similar to aggregation but with a stronger ownership relationship. It represents a relationship where the lifecycle of the contained class is dependent on the container class. In composition, the contained class cannot exist independently of the container class.  An example :   Source : https://medium.com/@smagid_allThings/uml-class-diagrams-tutorial-step-by-step-520fd83b300b  An Animal class with property age and gender; and method isMammal and mate, all of them are public. There are three subclasses that inherits animal, Duck, Fish, and Zebra.  Another example :   Source : https://online.visual-paradigm.com/id/diagrams/templates/class-diagram/uml-class-diagram-example-car/  Some class diagram have text like &quot;0..1&quot; or &quot;0..*&quot; near the arrow, they are called multiplicity. It specifies cardinality or the number of instances that can participate in a relationship between classes. It specifies the allowed number of associations between classes.  n : Unspecified number of instances, indicating that there can be any number of associations.1 : Denotes exactly one instance.0..1 : Specifies that the association is optional, and there can be zero or one instance.0..n or 0..* : Indicates that the association is optional, and there can be zero or more instances.1..n : Specifies that there must be at least one instance, and any number greater than or equal to one is allowed.m..n : Specifies a range of allowed instances, where m is the minimum number and n is the maximum number of instances.  Object Diagram​  Object diagram is similar to a class diagram. It represents actual instance of a class in a specific scenario or situation by showing their concrete attributes.   Source : https://www.lucidchart.com/pages/uml-object-diagram  Sequence Diagram​  Sequence diagram illustrates the interactions between objects or components in a system over a specific period of time. It visualizes the flow of messages or method calls between objects, showing the order in which they occur.  Sequence diagrams are used to capture the dynamic behavior of a system, particularly emphasizing the sequence of events and the collaboration between objects during a specific scenario or use case. They help in understanding how different components or objects interact with each other to achieve a particular functionality.  Representation of sequence diagram :  Lifelines : Lifelines are the vertical dashed lines, it represent the participating objects or components in the sequence diagram. They are labeled with the name of the object or component it represents. Lifelines indicate the lifespan of an object or component during the sequence of events.Messages : Messages represent the interactions or communication between objects or components. They are depicted as horizontal arrows or lines between lifelines, indicating the flow of messages or method calls. Messages can be synchronous (denoted by a solid arrow) or asynchronous (denoted by a dashed arrow).Activation Boxes : Activation boxes show the period of time during which an object or component is actively processing a message. They are represented as boxes or rectangles on the lifeline, indicating the duration of the method execution or processing.Return Messages : Return messages depict the response or return values from the called object or component back to the caller. They are represented as dashed lines with an arrowhead pointing back to the lifeline of the caller.   Source : https://medium.com/thousand-words-by-creately/the-ultimate-guide-to-sequence-diagrams-a78e0e516886  Use Case Diagram​  Use case diagram is a diagram showing possible interaction with a system. Actors, such as users or other systems, are denoted depicted by stick man. Ovals or rectangles are use cases, which are specific functionalities or behaviors provided by the system. All the figure inside the big rectangle is considered as the scope of the system being modeled.   Source : https://en.wikipedia.org/wiki/Use_case_diagram  In addition, there are four types of relationship :  Association : Represented by a solid arrow, indicates a communication or interaction between an actor and a use case.Generalization : Represented by a dashed arrow, indicates a specific use case or actor that inherits the behavior and characteristics of a more general use case or actor.&lt;&lt;extend&gt;&gt; : Represents optional or alternative functionality that can be added to a base use case under certain conditions.&lt;&lt;include&gt;&gt; : Represents a situation where one use case includes the functionality of another use case.  Activity Diagram​  Activity diagram illustrates the flow of activities and actions within a system or a process. The flow of actions is connected with arrows, and the action are constructed with shapes, which are :  Black circle represents the start (initial node) of the workflowCapsule represent actionsDiamonds represent decisionsBars represent the start (split) or end (join) of concurrent activities;Encircled black circle represents the end (final node).   Source : https://en.wikipedia.org/wiki/Activity_diagram#/media/File:Activity_conducting.svg  State Diagram​  State diagram is a type of UML diagram that represent the behavior of a system that is affected by events over time.  System has states, which represent the different conditions or modes that an object or system can be in during its lifecycle. Each state is depicted as a rounded rectangle with the name of the state inside.  An event represent occurrences or stimuli that trigger state transitions. Transitions represent the movement of an object or system from one state to another in response to an event or condition. Transitions are depicted with arrow, forming a directed graph in the overall state diagram. When state transition takes place, actions, which describe operations or tasks will be performed.   Source : https://nulab.com/learn/software-development/a-simple-guide-to-drawing-your-first-state-diagram-with-examples/  Similar to activity diagram, the start and end of the diagram is represented as black circle and encircled black circle, respectively.  ","version":"Next","tagName":"h3"},{"title":"ER Diagram​","type":1,"pageTitle":"Diagrams","url":"/cs-notes/software-engineering/diagrams#er-diagram","content":" Entity Relationship (ER) Diagram is a flowchart that illustrates entities (or objects), relationships, attributes, and constraints within a database.  See E/R model. ","version":"Next","tagName":"h3"},{"title":"Layered","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/layered","content":"","keywords":"","version":"Next"},{"title":"Architecture​","type":1,"pageTitle":"Layered","url":"/cs-notes/software-engineering/layered#architecture","content":" The architecture itself is flexible; it doesn't have to be two, three, or four layers. We can customize the architecture with as many layers as we want. Just note that having too few layers can result in a monolithic and tightly coupled system, where each component are too dependent on each other. On the other hand, having too many layers can introduce unnecessary complexity.  One of the common layered architecture is the three-tier architecture, which divide the layers by three :  Presentation / UI layer : The topmost layer is responsible for handling user interactions and presenting information to the users. It typically includes components like user interfaces such as button, dialog, or web pages. Domain layer : This layer contains the core logic and functionality of the application. It's the layer that is responsible for processing and manages data, implements business rules, make decision based on input, and coordinates the application's behavior. It is independent of the user interface and data storage layers. Data layer : This layer is responsible for managing the storage and retrieval of data. It interacts with databases, file systems, or external services to persist and retrieve data for the application. Source : https://en.wikipedia.org/wiki/Multitier_architecture#/media/File:Overview_of_a_three-tier_application_vectorVersion.svg  Unidirectional Data Flow​  Consider a music app, you are in the ranking page to see the topmost song this year. If the app uses a layered architecture, the typical data flow might look like :  User Interaction : The presentation layer presents UI such as a button, which might says &quot;Top 10 songs&quot;. The user then click the button, triggering an event. UI State : The presentation layer might keep track a UI state, which is the state of the UI. It may contain information, such as on what page is the user at right now, or what filtering option is currently applied. Initially, the UI state describe that the user is still on the ranking page. When the event is triggered from the previous step (clicking the &quot;Top 10 songs&quot; button), the presentation layer updates the UI state to reflect the change in the user interface.  The steps we just discussed is a form of unidirectional data flow (UDF). UDF is a pattern typically used in layered architecture, where data flows in a single direction through the layers of the architecture, typically from the user interface to the data layer and back.  UDF is typically associated with UI state, which as we talked about before, the state of the UI. The UI state will be updated upon the trigger of an event. Typically, the UI state is held by another entity in the presentation layer, which can be called as state holder or UI controller. They are entity that keeps track UI state, as well as manipulating or updating the state based on UI events.   Source : Inspired from https://developer.android.com/topic/architecture#ui-layer  Depending on the interaction, the presentation layer doesn't need to communicate with the lower layer all the time. To continue with our example, checking the top 10 songs does indeed require interaction with the data layer because we are essentially retrieving data.  Domain Layer : Based on the event and UI state kept by state holder, it will make a request to the domain layer. The domain layer interprets the request and decide what should it do to fulfill the request. For example, determining whether the request can be fulfilled solely by querying the local database or if making a request to a remote server is necessary. We typically call local database or remote server as data sources, as they produce data for us to present in the UI.Data Layer : Data layer receive the request from the domain layer. The data layer can be abstracted with a repository. A repository is a pattern where we abstract away the underlying storage mechanism to access data sources. It acts as an intermediary between the domain layer and the data layer, providing a set of methods that the domain layer can use to interact with the data.Data Retrieval : After data is retrieved from the data layer, it is sent back to domain layer. The domain layer then applies required business logic, such as sorting or filtering the data based on the request made.UI Updated : Domain sent the processed data to presentation layer. The state holder gets the data and update the UI state, reflecting a new UI where we can see the top 10 songs.    The UDF pattern allows for organized data flow. Data flow, actions, or any updates can only happen in one-way manner. UI State can only be updated by the state holder based on the event the UI triggered. The data layer is abstracted with repository, which can be accessed by the domain layer.  ","version":"Next","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"Layered","url":"/cs-notes/software-engineering/layered#example","content":" A theoretical Kotlin code that follows layered architecture and UDF might look like this :  fun main() { val repository = Repository() val domain = Domain(repository) val uiState = StateHolder() val screen = Screen(uiState) val button = Button() screen.render(button) button.setOnClickListener { val songs = domain.fetchData(uiState.sortSetting) screen.displaySongs(songs) } } class StateHolder { var sortSetting = &quot;ascending&quot; } class Domain(private val repository: Repository) { fun fetchData(sortSetting: String): List&lt;String&gt; { if (sortSetting == &quot;descending&quot;) { repository.query(&quot;SELECT * ORDER BY DESC&quot;) } else { repository.query(&quot;SELECT * ORDER BY ASC&quot;) } } } class Repository { private val baseURL = &quot;http://www.example.com&quot; private val remoteDataSource = HttpClient() private val localDatabase = SQLiteDB() fun query(q: String): List&lt;String&gt; { val localData = localDatabase.query(q) if (localData != null) { return localData } else { val remoteData = remoteDataSource.requestTo(baseURL, &quot;/data&quot;) localDatabase.saveData(remoteData) return remoteData } } }   The button listens for click and fetch for data from the domain layer, based on the current uiState.The domain layer holds a repository, it will create the appropriate request to the data layer based on the sortSetting parameter.The data layer contains two data sources, a local database and remote data source, which is an HTTP client.When a query request is received, it initially retrieves the data from the local database. If the data is not null, meaning its available, it is immediately returned. Otherwise, the data must be obtained from a remote source. An HTTP request is made to a specific URL and endpoint, and the resulting data is saved to the local database. Finally, the retrieved data is returned to the domain layer. ","version":"Next","tagName":"h3"},{"title":"Master-Slave","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/master-slave","content":"Master-Slave See Cloud Computing &gt; Master-Slave","keywords":"","version":"Next"},{"title":"Modularization","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/modularization","content":"","keywords":"","version":"Next"},{"title":"Benefits​","type":1,"pageTitle":"Modularization","url":"/cs-notes/software-engineering/modularization#benefits","content":" Reusability : Modules are made independent and self-contained as much as possible, so it can be utilized in multiple parts of an application or even across different projects.Scalability : Modularization follows the separation of concerns principle, making it loosely coupled and flexible to be extended or modified.Testability : Modules can be tested individually, ensuring that each component works correctly in isolation. Additionally, when issues arise, it is easier to isolate and debug problems within a specific module rather than the entire system.Encapsulation : Modules encapsulate their internal implementation details, exposing only the necessary interfaces or APIs to interact with other modules, making it easier to use.  ","version":"Next","tagName":"h3"},{"title":"Types of Modules​","type":1,"pageTitle":"Modularization","url":"/cs-notes/software-engineering/modularization#types-of-modules","content":" There are several types of modules :  Data modules : Modules which are responsible for handling data-related operations and business logic. They encapsulate the logic (e.g., repository pattern) for data retrieval, storage, manipulation, and communication with data sources such as databases, APIs, or files. Source : https://developer.android.com/topic/modularization/patterns#data-modules Feature modules : Groups together related functionality or features of an application. They encapsulate all the components, services, and resources required to implement a specific feature. Feature modules depend on data modules. Source : https://developer.android.com/topic/modularization/patterns#feature-modules App modules : Represent the core modules of the application that connects different modules and coordinates the overall behavior of the application. These modules include the entry point of the application, the main user interface, and the high-level application logic. Source : https://developer.android.com/topic/modularization/patterns#app-modules Common modules : Contains shared functionality or resources that are used across multiple parts of an application. They encapsulate reusable code, utilities, or resources that are not specific to any particular feature or module. Examples are common UI widget, analytics module, networking module, or utilities such as date time formatter. Test modules : Modules which are only needed for development. It contains the test code and resources used to test individual modules. Test modules include unit tests or integration tests.  ","version":"Next","tagName":"h3"},{"title":"Communication​","type":1,"pageTitle":"Modularization","url":"/cs-notes/software-engineering/modularization#communication","content":" Each module can communicate in various ways. One way is through service or API calls, which can be handled by intermediary component called mediator. In this approach, modules expose interfaces or APIs that other modules can use to request specific functionalities or retrieve data.  Instead of modules directly invoking each other's services or APIs, they communicate with the mediator, which then routes and manages the requests and responses. Requests are encapsulated in a standardized format understood by the mediator. The mediator receives these requests and determines the appropriate action based on the content and intent of the request.   Source : https://developer.android.com/topic/modularization/patterns#communication  Dependency Inversion​  Modules should communicate with interfaces and depend on abstraction, following the dependency inversion that states &quot;High-level modules should not depend on low-level modules.&quot;  See Dependency inversion principle (DIP). ","version":"Next","tagName":"h3"},{"title":"Open Sourcing","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/open-sourcing","content":"","keywords":"","version":"Next"},{"title":"License​","type":1,"pageTitle":"Open Sourcing","url":"/cs-notes/software-engineering/open-sourcing#license","content":" Software is considered as a form of intellectual property that is protected by copyright laws. Copyright grants exclusive rights to the creators or owners of original works, including software, and provides legal protection against unauthorized copying, distribution, modification, or use of the software.  An open sourced software is typically licensed with a free and open-source license. These are special licenses that allows anyone to access, use, modify, and distribute the software freely.  Different license have different freedom, some of these licenses are :  GNU General Public License (GPL) : GNU GPL is a copyleft license that requires any derivative works or modifications of the software to be licensed under the same GPL terms. The GPL permits users to use, modify, and distribute the software freely, as long as the resulting works are also made available under the GPL. MIT License : MIT License is a permissive license that allows users to use, modify, and distribute the software with minimal restrictions. It grants users the freedom to use the software for any purpose, including commercial purposes, and allows users to sublicense, modify, and distribute the software under different terms if desired. Apache License 2.0 : Another permissive license with similar freedom to MIT license. The Apache license is stricter, it includes patent grants and provides more explicit terms related to contributions and patents compared to some MIT license. Creative Commons License : A public copyright license for creative works, including software (although it's not recommended). CC provides various permission which can be combined, including : Attribution (BY) : Anyone may copy, distribute, modify, or use, as long as the original author is credited.ShareAlike (SA) : Modified or adapted work must be shared under the same or a compatible license.NonCommercial (NC) : Restricts the use of the work for commercial purposes.NoDerivatives (ND) : The work must be used as-is, without any modifications or adaptations. Unlicense : The license that dedicates a work to the public domain, granting users with maximum freedom to use, modify, and distribute the software or creative work without any restrictions.  ","version":"Next","tagName":"h3"},{"title":"Documentation​","type":1,"pageTitle":"Open Sourcing","url":"/cs-notes/software-engineering/open-sourcing#documentation","content":" It is important to provide documentation for public software to enable developers to contribute easily. A documentation provide instructions, explanations, and guidance on how to install, configure, use, and contribute to the software.  A documentation can be software requirements, architecture or design, technical, or manuals for end user. One of the example of technical documentation is an API reference. API reference focuses on documenting the APIs exposed by software libraries or services. API documentation explains how to use the APIs, including details on input parameters, return values, error handling, and usage examples.   Source : https://developer.blackberry.com/devzone/files/blackberry-workspaces/java-doc/  An API reference typically includes documentation for all the packages and modules within the API. For object-oriented API, it may contain all the class or interfaces available.  The API reference shown in the image above is an example of object-oriented API. The documentation for each class includes constructor and method summary on the top. This help user to know essential information about the class quickly.   Source : https://developer.blackberry.com/devzone/files/blackberry-workspaces/java-doc/  Under the summary, the detail of each method is provided. The detail contains more comprehensive information about the parameters, return values, exceptions, and any additional usage notes or examples.  Here's another example, a REST API documentation.   Source : https://cdn.emnify.net/api/doc/swagger.html  Embedded Documentation​  Many languages provide a way to document their code through comments. For example, in languages like C, C++, Java, and JavaScript, comments are denoted using double slashes // for single-line comments or enclosed within /* ... */ for multi-line comments.  Some languages such as Java (Javadoc) and Kotlin (KDoc) have special comments enclosed within /** ... */ (double asterisk on start), which indicates a special documentation format specifically designed for documenting code. These format supports Markdown, allowing us to refer between documentation, and supports to document different elements of code, classes properties, constructor, parameters, and more.    The image above demonstrate using standard library API in Kotlin. Hovering over a variable, function, method, class, interface, or other identifier (i.e., map) will reveal a tooltip or pop-up that displays contextual information about the identifier. Turns out that the information in the tooltip comes from the documentation in the map declaration.  ","version":"Next","tagName":"h3"},{"title":"Code of Conduct​","type":1,"pageTitle":"Open Sourcing","url":"/cs-notes/software-engineering/open-sourcing#code-of-conduct","content":" Some open source project includes code of conduct, which outlines the expected behavior and standards of conduct for participants within the open source community.  Code of conduct may contain :  Respect : Promotes respectful environment where all participants should be treated with dignity and respect, regardless of their background, identity, or beliefs.Harassment and Discrimination : It explicitly prohibits any form of harassment, discrimination, or offensive behavior.Consequences for Violations : Specifies the potential consequences for violations, ranging from warnings and temporary suspensions to permanent bans from the community.  ","version":"Next","tagName":"h3"},{"title":"Contributing Guidelines​","type":1,"pageTitle":"Open Sourcing","url":"/cs-notes/software-engineering/open-sourcing#contributing-guidelines","content":" Contributing guidelines are a set of instructions and recommendations that outline how individuals can contribute to an open source project.  Contributing guidelines, sometimes in CONTRIBUTING.md file, may contain :  Overview : The introduction or the summary of the project, includes the purpose, goals, and target audience.Getting Started : Instructions on how to get started with contributing to the project. This may include details on setting up the development environment, obtaining the project's source code, and installing any necessary dependencies.Types of Contributions : A description of the different types of contributions that are welcome, such as code contributions, documentation improvements, bug reports, feature requests, translations, or community engagement.Conventions : Specify the convention, such as coding standards, style conventions, and best practices the project are using.  Issues​  Some platform, such as GitHub provide an issue tracker. Issues refer to bugs, problem reports, feature requests, or tasks that need to be addressed or completed within the project. Issues can be created by project maintainers, contributors, or users of the software, along with additional information :  Title : A brief and descriptive title that summarizes the issue.Description : A detailed explanation of the issue, including the context, problem statement, or desired improvement. In the case of bugs report, it may include step to reproduce.Labels : Labels categorize issues, making it easier to filter and search. Common labels include bug, enhancement, documentation, help wanted, and more.Milestones : Group related issues together and track progress towards specific project goals or releases. For example, for certain version releases, a particular bug issue need to be addressed.Status and Progress : Issues have status which can be open, closed, in progress, resolved, or duplicate.  Pull Request​  Pull request (PR) is a way for contributors to propose changes to a codebase. Pull request are reviewed, discussed, and need to be approved by the project maintainer before it is applied to the codebase.  Pull request contains :  Title : A descriptive title summarizing the purpose of the pull request.Description : Detailed explanation of the changes, which may include context, reasoning, or any relevant information about the changes, such as bug references or feature requirements.Changes : The actual modifications made to the codebase. This can include new code, modified code, or deleted code. The changes are usually displayed in a &quot;diff&quot; format, highlighting the specific lines or sections that have been added, modified, or removed.Testing : Some project may require new changes to past the automated tests in order to validate the correctness.Documentation : If the changes impact the project's documentation, the pull request may include updates or additions to the relevant documentation files.Related Issues : If the pull request addresses a specific issue or feature request, it is common to reference the corresponding issue or feature request number. ","version":"Next","tagName":"h3"},{"title":"Behavioral Patterns","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/behavioral-patterns","content":"","keywords":"","version":"Next"},{"title":"Command​","type":1,"pageTitle":"Behavioral Patterns","url":"/cs-notes/software-engineering/behavioral-patterns#command","content":" Command pattern encapsulates a request as an object that contains information about the request. Each command object encapsulates a specific request along with any necessary parameters. We can provide useful information in the object, such as what it executes briefly, which allows us to support undo-redo operations.  Consider a simplified text editor :  class TextEditor { private var text = StringBuilder() fun write(start: Int, content: String) { text.append(content, start, content.length) } fun delete(start: Int, end: Int): String { val deleted = text.substring(start, end) text.deleteRange(start, end) return deleted } fun print() { println(text.toString()) } } fun main() { val t = TextEditor() t.write(0, &quot;asdf&quot;) t.print() // Output &quot;asdf&quot; t.delete(0, 2) t.print() // Output &quot;df&quot; }   It's a simple class that hold a text (in StringBuilder), which we can edit through the write and delete method.  If we were to implement undo functionality, one way to do that is storing a list of previously written text. So we would add current text to the list everytime we edit it. While this is easy to implement, it can consume so much space to store the string and a lot of time to copy the string.  Following command pattern, we would make another object that encapsulate the writing or deleting operation along with extra information, and another class that handles the undo. With the given information, we will reverse the text operation.  The text commands :  sealed interface TextCommand { val start: Int val end: Int val content: String // move content to first, so we can use its length for end data class Write( override val content: String, override val start: Int, override val end: Int = content.length ) : TextCommand // move content to last, so by default we don't have to specify it data class Delete( override val start: Int, override val end: Int, override val content: String = &quot;&quot; // only used for undoing ) : TextCommand }   In Kotlin, sealed interface can be thought as an interface that prevent anyone to implement it, unless they are inside the interface block. Sealed interface will form a hierarchy within its implementation, and allow us to access the member by dot notation, such as TextCommand.Write.  class TextCommandInvoker(private val textEditor: TextEditor) { private val history = mutableListOf&lt;TextCommand&gt;() fun execute(command: TextCommand) { when (command) { is Write -&gt; { textEditor.write(command.start, command.content) history.add(command) } is Delete -&gt; { val deleted = textEditor.delete(command.start, command.end) history.add(Delete(command.start, command.end, deleted)) } } } fun undo() { if (history.isNotEmpty()) { val lastCommand = history.removeAt(history.lastIndex) when (lastCommand) { is Write -&gt; { textEditor.delete(lastCommand.start, lastCommand.end) } is Delete -&gt; { textEditor.write(lastCommand.start, lastCommand.content) } } } } }   We move all the code to TextCommandInvoker, a class responsible for executing the text command. It will execute command based on the type of command it received. Any text command will be identified by TextCommand sealed interface. It will also keep track the text edit history by adding the command to a list for undoing operation. This will reduce much more space than if we were to store the copy of string each time we edit it.  Sample usage :  fun main() { val t = TextEditor() val invoker = TextCommandInvoker(t) invoker.execute(TextCommand.Write(content = &quot;asdf&quot;, start = 0)) invoker.execute(TextCommand.Delete(start = 0, end = 2)) t.print() // Output &quot;df&quot; invoker.undo() t.print() // Output &quot;asdf&quot; }   We will now do any text edit from TextCommandInvoker.  ","version":"Next","tagName":"h3"},{"title":"Iterator​","type":1,"pageTitle":"Behavioral Patterns","url":"/cs-notes/software-engineering/behavioral-patterns#iterator","content":" Iterator patterns provide a way to access elements of a collection (array, list, stack, tree, etc.) sequentially without exposing the underlying implementation details.  A traditional way of accessing a list would be :  fun main() { val collection = listOf(1, 2, 3, 4, 5) println(collection[0]) // Output 1 println(collection[1]) // Output 2 }   The downside of this approach is the potential for going out of bounds. To prevent that, we would first check if the index is in the collection range before accessing any element. The purpose of iterator pattern is to abstract away the index checking and provide a simpler API for user.  class SimpleIterator(private val collection: List&lt;Int&gt;) { private var currentIdx = 0 fun getNext(): Int { if (hasNext()) { val element = collection[currentIdx] currentIdx++ return element } return -1 } fun hasNext(): Boolean { return currentIdx &lt; collection.size } }   We created a simple iterator class, for simplicity, it can only take a list of integers. It still stores index under the hood, but it always checks if the index is out of bound using the hasNext method. User would use this iterator like :  fun main() { val collection = listOf(1, 2, 3, 4, 5) val iterator = SimpleIterator(collection) println(iterator.hasNext()) // Output true println(iterator.getNext()) // Output 1 while (iterator.hasNext()) { println(iterator.getNext()) // Output 2 to 5 } }   To make it possible to use this iterator in a loop, we can use hasNext as the condition for the while loop.  This iterator patterns seems useless for basic collection like list or array. This is because some language typically implements iterator under the hood for loop expression like for (num in collection). Still, iterator pattern for list allow for flexibility in getting element, we don't have to create a loop if we are just getting several elements. We can also customize the behavior of the iteration as we like. We may implement filtering to skip element, progress at different rates, or in different directions. Iterator pattern will be much more useful for complex data structure like tree or graph.  ","version":"Next","tagName":"h3"},{"title":"Observer​","type":1,"pageTitle":"Behavioral Patterns","url":"/cs-notes/software-engineering/behavioral-patterns#observer","content":" Observer pattern implements a subscription mechanism, which allow multiple observer, to be notified automatically when the state of a subject object (also known as the observable) changes.  The subject maintains a list of observers and provides methods to add, remove, and notify observers. The observers register themselves with the subject and receive updates when the subject's state changes.  Let's imagine a YouTube subscription system. There is a Youtuber, acting as the subject, and there is Subscriber, acting as the observer.  data class Subscriber(val name: String, val subscribeTo: List&lt;Youtuber&gt;) class Youtuber(val name: String) { fun hasUploaded(): Boolean { return true } }   One way for subscriber to check if a YouTuber has uploaded a new video is, checking themselves.  fun main() { val yt1 = Youtuber(&quot;yt1&quot;) val yt2 = Youtuber(&quot;yt2&quot;) val subs1 = Subscriber(&quot;subs1&quot;, listOf(yt1)) val subs2 = Subscriber(&quot;subs2&quot;, listOf(yt1)) val subs3 = Subscriber(&quot;subs3&quot;, listOf(yt2)) for (youtuber in subs1.subscribeTo) { youtuber.hasUploaded() } }   This is obviously not realistic whether in code or in real life. It won't be efficient to create a loop for every subscriber and check every single YouTuber it subscribed.  So, why can't the YouTuber be the one that notify their subscriber whenever they uploaded a video? We can alter the Youtuber class to make it stores a list of subscriber and method to add or remove them.  data class Subscriber(val name: String) { fun notify(from: String) { println(&quot;$name: Youtuber $from has uploaded a new video&quot;) } } class Youtuber(val name: String) { val subscribers = mutableListOf&lt;Subscriber&gt;() fun addSubscriber(subs: Subscriber) { subscribers.add(subs) } fun removeSubscriber(subs: Subscriber) { subscribers.remove(subs) } fun uploadVideoAndNotify() { for (subs in subscribers) { subs.notify(name) } } fun hasUploaded(): Boolean { return true } }   We also modified the Subscriber class for notify method. To use it :  fun main() { val yt1 = Youtuber(&quot;yt1&quot;) val yt2 = Youtuber(&quot;yt2&quot;) val subs1 = Subscriber(&quot;subs1&quot;) val subs2 = Subscriber(&quot;subs2&quot;) val subs3 = Subscriber(&quot;subs3&quot;) yt1.addSubscriber(subs1) yt1.addSubscriber(subs2) yt1.uploadVideoAndNotify() /* Output: subs1: Youtuber yt1 has uploaded a new video subs2: Youtuber yt1 has uploaded a new video */ }   While this code works, it may not make sense because the one adding or removing subscribers is the YouTuber itself. If you want a more realistic code where subscribers subscribe or unsubscribe themselves, you can edit the Subscriber code. Add a method, such as subscribeTo or unsubscribeFrom, which takes a YouTuber object and calls the addSubscriber or removeSubscriber method on it.  ","version":"Next","tagName":"h3"},{"title":"State​","type":1,"pageTitle":"Behavioral Patterns","url":"/cs-notes/software-engineering/behavioral-patterns#state","content":" State pattern allows an object to change its behavior dynamically as its internal state changes. It encapsulates each state as a separate class, and the object delegates its behavior to the current state class.  An example would be a vending machine that operates differently based on the condition if an item is selected or not. Each state would have access to the machine to change its internal state.  The vending machine would work this way :  User selects an item If item is not selected yet, select itIf item is already selected, don't do anything User dispense an item If item is not selected yet, don't do anythingIf item is already selected, dispense it  The vending machine state is modeled by an interface :  interface VendingMachineState { fun selectItem(item: String) fun dispenseItem() } class NoSelectionState(private val vm: VendingMachine) : VendingMachineState { override fun selectItem(item: String) { println(&quot;Selected item: $item&quot;) vm.changeState(ItemSelectedState(vm)) } override fun dispenseItem() { println(&quot;Please select an item first.&quot;) } } class ItemSelectedState(private val vm: VendingMachine) : VendingMachineState { override fun selectItem(item: String) { println(&quot;Item $item is already selected.&quot;) } override fun dispenseItem() { println(&quot;Dispensing item...&quot;) vm.changeState(NoSelectionState(vm)) } }   The vending machine :  class VendingMachine { private var currentState: VendingMachineState = NoSelectionState(this) fun changeState(newState: VendingMachineState) { currentState = newState } fun selectItem(item: String) { currentState.selectItem(item) } fun dispenseItem() { currentState.dispenseItem() } }   State changes go through changeState method, which overwrite the current state.  fun main() { val vm = VendingMachine() vm.selectItem(&quot;Coke&quot;) vm.dispenseItem() vm.dispenseItem() vm.selectItem(&quot;Chips&quot;) vm.selectItem(&quot;Chips&quot;) vm.dispenseItem() /* Output : Selected item: Coke Dispensing item... Please select an item first. Selected item: Chips Item Chips is already selected. Dispensing item... */ }   ","version":"Next","tagName":"h3"},{"title":"Chain of Responsibility​","type":1,"pageTitle":"Behavioral Patterns","url":"/cs-notes/software-engineering/behavioral-patterns#chain-of-responsibility","content":" Chain of Responsibility pattern allows an object to pass a request along a chain of potential handlers until the request is handled or reaches the end of the chain. Each handler in the chain has the ability to handle the request or pass it to the next handler in the chain.  Consider a math function that we can customize its behavior, maybe add the number first, then multiply, and finally subtract. Here's a simplified implementation of this :  abstract class MathOperationHandler { abstract val operand: Int abstract fun handle(prevResult: Int): Int var nextHandler: MathOperationHandler? = null fun m_setNextHandler(handler: MathOperationHandler) { nextHandler = handler } }   Each math operator has a handler, which has an operand provided for its calculation. They are able to set the nextHandler by the m_setNextHandler method (prefixed with m_ to avoid JVM conflict).  The handle method takes the previous result from the previous operand (or the initial value if they are the first operator). The handler will calculate the result with their given operand based on what type of math operation are they. The result is then passed again to the next handler.  We also made MathOperationHandler abstract instead of an interface, because the nextHandler and m_setNextHandler should be the same for any operator.  And here's all the operator implemented :  class AddHandler(override val operand: Int) : MathOperationHandler() { override fun handle(prevResult: Int): Int { val currentResult = prevResult + operand // Here lies the difference between operator return nextHandler?.handle(currentResult) ?: currentResult } } class SubHandler(override val operand: Int) : MathOperationHandler() { override fun handle(prevResult: Int): Int { val currentResult = prevResult - operand return nextHandler?.handle(currentResult) ?: currentResult } } class MulHandler(override val operand: Int) : MathOperationHandler() { override fun handle(prevResult: Int): Int { val currentResult = prevResult * operand return nextHandler?.handle(currentResult) ?: currentResult } } class DivHandler(override val operand: Int) : MathOperationHandler() { override fun handle(prevResult: Int): Int { val currentResult = prevResult / operand return nextHandler?.handle(currentResult) ?: currentResult } }   tip ? and ?: is a Kotlin operator related to Kotlin null safety. In short, using ? on a method call ensure that we are only invoking the method if the object we are invoking on is not null. On the other hand, the ?: replace a null value with an alternative value.  fun main() { val add = AddHandler(2) val mul = MulHandler(5) val sub = SubHandler(1) add.m_setNextHandler(mul) mul.m_setNextHandler(sub) val result = add.handle(0) println(result) // Output : 9 }   The code set addition, multiplication, and subtraction operation respectively. The number 0 we passed in the first handle method that we call on additionHandler act as the initial value. ","version":"Next","tagName":"h3"},{"title":"Other Architecture Patterns","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/other-architecture-patterns","content":"","keywords":"","version":"Next"},{"title":"MVC​","type":1,"pageTitle":"Other Architecture Patterns","url":"/cs-notes/software-engineering/other-architecture-patterns#mvc","content":" MVC (Model-View-Controller) patterns separate the concerns of the applications into three components, model, view, and controller.  Model : Model represents the data and the business logic of the application. It encapsulates the application's data, state, and behavior. The model is responsible for managing data access, data validation, processing, and implementing business rules and algorithms.View : View represents the user interface of the application. It is responsible for presenting the data from the model to the user and receiving user input. The view is typically passive and doesn't contain business logic. It observes the model for changes and updates its presentation accordingly.Controller : The controller sits between the model and the view. It handles user input and manipulates the model based on the user's actions. It contains the application's logic for handling user events, coordinating data flow, and updating the model.  The flow :  User interact with the view, such as clicking a button.The view notifies controller about the action.The controller receives the user input, performs necessary operations, and updates the model accordingly.The model updates its state and notifies the view of the changes.The view retrieves the updated data based on the model's state, it also updates its presentation to reflect the changes.   Source : https://stackoverflow.com/a/59336002/18335183  ","version":"Next","tagName":"h3"},{"title":"MVP​","type":1,"pageTitle":"Other Architecture Patterns","url":"/cs-notes/software-engineering/other-architecture-patterns#mvp","content":" MVP (Model-View-Presenter) patterns separate the application into model, view, presenter.  The model and the view are similar to MVC, but now it's the presenter instead of a controller. The difference between a controller and a presenter is that a presenter acts more as an intermediary. The model no longer directly interacts with the view; instead, it interacts with the presenter, which takes over the role of interacting with the view.  The flow :  The user interacts with the view.The view notifies the presenter about the user's action.The presenter receives the user input, performs necessary operations, and interacts with the model to retrieve or update data.The presenter updates the view with the retrieved data or triggers UI updates based on the model's state.   Source : https://stackoverflow.com/a/59336002/18335183  ","version":"Next","tagName":"h3"},{"title":"MVVM​","type":1,"pageTitle":"Other Architecture Patterns","url":"/cs-notes/software-engineering/other-architecture-patterns#mvvm","content":" MVVM (Model-View-ViewModel) separates the application into model, view, and view model.  MVVM is very similar to MVP, view and model is still the same, and the role of presenter is replaced with view model. The view model still acts as an intermediary between the model and the view. The difference is, more reactive programming is involved, as they now observe each other.  Typically, a binding mechanism is done between these three components. The view model holds the state of the model, and it is connected or bound to a view. Through this binding, the model's state becomes exposed, and the view should observe any changes in it.  The flow :  The view binds to properties exposed by the view model.The view model interacts with the model to retrieve or update data.The view model updates the properties that the view is bound to, triggering UI updates.The view displays the updated data and captures user input.User input is processed by the view model, which may update the model accordingly.   Source : https://stackoverflow.com/a/59336002/18335183  ","version":"Next","tagName":"h3"},{"title":"MVI​","type":1,"pageTitle":"Other Architecture Patterns","url":"/cs-notes/software-engineering/other-architecture-patterns#mvi","content":" MVI (Model-View-Intent) is an architectural pattern based on state machine, because a user interface can be thought as a predictable machine that has particular states. The state of UI can be updated through user interaction, which is modeled as intent.  In the actual implementation, we can still use a presenter or view model, as intent is just a form of coordination between the view and model.  The flow :  The user interacts with the UI, generating an intent.The intent is dispatched to the MVI system.The MVI system processes the intent and generates a new model state.The updated model state is emitted to the view.The view receives the new model state and updates the UI accordingly.   Source : https://blog.mindorks.com/mvi-architecture-android-tutorial-for-beginners-step-by-step-guide/  ","version":"Next","tagName":"h3"},{"title":"VIPER​","type":1,"pageTitle":"Other Architecture Patterns","url":"/cs-notes/software-engineering/other-architecture-patterns#viper","content":" VIPER (View-Interactor-Presenter-Entity-Router) separates the application into view, interactor, presenter, entity, and router.  View : Represents UI and handles user input to forward it to the Presenter for processing.Presenter : Acts as the intermediary between the View and the Interactor. It receives input from the View, processes it, and communicates with the Interactor to retrieve or update data.Interactor : Contains the business logic of the application. It handles data fetching, manipulation, and performs any necessary operations based on the user interactions received from the Presenter.Entity : Represents the data models or objects used within the application. Retrieved data is sent back to Interactor and Presenter, which will update the View.Router : Handles the navigation and flow between different screens or modules within the application. It is responsible for presenting new screens, passing data between them, and managing the overall application flow.   Source : https://twitter.com/sahnlam/status/1734090361105244533 ","version":"Next","tagName":"h3"},{"title":"Creational Patterns","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/creational-patterns","content":"","keywords":"","version":"Next"},{"title":"Creational Patterns​","type":1,"pageTitle":"Creational Patterns","url":"/cs-notes/software-engineering/creational-patterns#creational-patterns","content":" Creational patterns focus on object creation mechanisms, providing ways to create objects in a flexible and reusable manner.  Builder​  Builder design pattern aims to simplify the creation of complex objects. If we are constructing a class with many properties, we typically provide it via the constructor. An example of constructing a Pizza class would be :  class Pizza( val size: String, val crustType: String, val sauceType: String, val cheeseType: String, val toppings: List&lt;String&gt; ) {} fun main() { val pizza: Pizza = Pizza(&quot;Large&quot;, &quot;Thin&quot;, &quot;Tomato&quot;, &quot;Mozzarella&quot;, listOf(&quot;Onions&quot;, &quot;Pepperoni&quot;)) }   As the class and the constructor get larger, constructing the class can be confusing. We may need to see the class definition to know where is the size parameters or which position correspond to cheeseType.  The idea of builder patterns is to construct a complex object step by step using methods. This will increase the flexibility to create different variations of the same object, while keeping the construction clean with descriptive name. To do this, we will make another class called PizzaBuilder.  class PizzaBuilder { private var size: String = &quot;&quot; private var crustType: String = &quot;&quot; private var sauceType: String = &quot;&quot; private var cheeseType: String = &quot;&quot; private var toppings: MutableList&lt;String&gt; = mutableListOf() fun setSize(size: String): PizzaBuilder { this.size = size return this } fun setCrustType(crustType: String): PizzaBuilder { this.crustType = crustType return this } fun setSauceType(sauceType: String): PizzaBuilder { this.sauceType = sauceType return this } fun setCheeseType(cheeseType: String): PizzaBuilder { this.cheeseType = cheeseType return this } fun addTopping(topping: String): PizzaBuilder { toppings.add(topping) return this } fun build(): Pizza { return Pizza(size, crustType, sauceType, cheeseType, toppings) } }   These are the methods that will be used by the user to set the temporary stored in the class, which will be used to construct the final Pizza object using the build() method. An example constructing Pizza :  val pizza: Pizza = PizzaBuilder() .setSize(&quot;Large&quot;) .setCrustType(&quot;Thin Crust&quot;) .setSauceType(&quot;Tomato&quot;) .setCheeseType(&quot;Mozzarella&quot;) .addTopping(&quot;Pepperoni&quot;) .addTopping(&quot;Mushrooms&quot;) .addTopping(&quot;Onions&quot;) .build()   It is common to chain the method calls in each line to set the attributes of the Pizza object.  In some case, we may create an interface that represent a Builder. The specific class that use builder pattern will create ConcreteBuilder that implements the Builder interface. Each concrete builder represents a different variation or configuration of the object being built.  We can also introduce Director class, which provide a more high-level interface for object construction. For example, this class may take various type of builder, setting some default values, or constructing the builder in a specific order of steps.  info Although the code sample is in Kotlin, it is not commonly used in practice. This is because Kotlin class has default constructor parameters, and when constructing them, we can use named parameters. For example, constructing Pizza class by val pizza = Pizza(size = &quot;Large&quot;, toppings = listOf(&quot;Pepperoni&quot;, &quot;Mushrooms&quot;)) may not be that bad.  Singleton​  In OOP, if we have a class, we can create as many instance as we want. Singleton design pattern ensures that a class has only one instance. If we insist creating multiple instance, we will be prevented and redirected to that single instance. It is commonly used when we want to restrict the instantiation of a class to a single object throughout our application, such as a single instance of database.  It is very simple to implement in Kotlin :  class Database private constructor() { companion object { private var instance: Database? = null fun getInstance(): Database { if (instance == null) { instance = Database() } return instance as Database } } }   We will make the class constructor private, to prevent anyone for using it. We will instead divert any construct call to the static method getInstance(). In Kotlin, to make members of a class static, we put it inside the companion object block.  In the method call, we will first check if the instance of the class has been created before. If it is, the instance shouldn't be null, and we will simply return that instance. If the instance is null, assign it with newly created instance and return it.  tip In Kotlin, the ? symbol behind the type name indicates the type is nullable. In the end of getInstance method, we type cast it to Database, because we know that it's not null.  warning Its worth noting that the above implementation is not thread-safe, meaning it may not be safe to use it in multithreaded environment. This is because there could be multiple thread accessing the getInstance method at the same time, potentially creating multiple instances.  Prototype​  Prototype pattern allows us to create new objects by cloning or copying existing objects, rather than creating them from scratch.  The traditional way of copying an object is creating an object from scratch and copying and pasting the properties of the class we intend to copy from. There are three reason why this is not preferred :  Copying and pasting properties from large class can be exhausting.Some properties of the class may be private, which means the user that wants to copy it can't access it.Increase coupling, the user needs to know the member of the class. This is also not possible when the class it copies from is an interface, instead of concrete class.  Prototype pattern simplifies the process of copying objects by delegating the responsibility of copying to the class itself. Rather than having the user control the copy of an object, the class that needs to be copied takes charge. Users can simply call a method like clone() or copy() to obtain a copied object easily.  data class Rectangle(var width: Int, var height: Int) { fun clone(): Rectangle { return Rectangle(this.width, this.height) } } fun main() { val rectangle = Rectangle(2, 4) val rectangleCopy = rectangle.clone() }   The clone() method is defined from the Rectangle class itself, which has access to its members (although they are not private in this case).  tip In the case of using data class, the clone design pattern is typically not used. This is because Kotlin data class already has a built-in way to copy an object (although it's a shallow copy). We don't need to implement the copy mechanism, and simply call it like rectangle.copy(). We can also provide some new properties we intend to replace, such as rectangle.copy(width = 5), this will effectively create rectangle with width of 5 and height of 4 copied from the other rectangle.  Factory​  The primary purpose of factory pattern is to abstract away the complexity of constructing different type of objects. This is useful when we have a common interface or base class, and we have to choose which subclass or implementation we have to create.  interface Logistics { fun transport() } class RoadLogistics: Logistics { override fun transport() { println(&quot;Transporting via road&quot;) } } class SeaLogistics: Logistics { override fun transport() { println(&quot;Transporting via sea&quot;) } }   This example demonstrates the use of the factory pattern in the context of logistics. The logistics can be transported differently based on the specific type of transportation method chosen.  A traditional way to instantiate logistics would be :  fun main() { val delivery = Delivery(&quot;New York&quot;) // assume this class exist // and have the decideTransportation method if (delivery.decideTransportation() == &quot;Road&quot;) { val transportation = RoadLogistics() delivery.transport() } else if (delivery.decideTransportation() = &quot;Sea&quot;) { val transportation = SeaLogistics() delivery.transport() } }   Factory pattern allows us to simplify this creation by making a class dedicated to handle this.  class LogisticsFactory { fun createLogistics(delivery: Delivery): Logistics { if (delivery.decideTransportation() == &quot;Road&quot;) { return RoadLogistics() } else if (delivery.decideTransportation() = &quot;Sea&quot;) { return SeaLogistics() } } } fun main() { val delivery = Delivery(&quot;New York&quot;) val factory = LogisticsFactory() val transportation = factory.createLogistics(delivery) transportation.delivery() }   The LogisticsFactory class do the same thing as we did before. The createLogistics decide which class to be constructed based on Delivery, and returns it.  Abstract Factory​  Abstract factory pattern allows us to construct object with a combination of other related families of object without specifying their concrete classes. Abstract factory is achievable using the previous factory pattern implemented in each class.  class Furniture(val chair: Chair, val table: Table) {}   Let's say we are creating a furniture that consist of Chair and Table. There are different type of chair and table :  interface Chair {} class VictorianChair: Chair {} class ModernChair: Chair {} interface Table {} class VictorianTable: Table {} class ModernTable: Table {}   Suppose that we are going to create furniture with VictorianChair and ModernTable. The traditional way would look like this :  fun main() { val chair = VictorianChair() val table = ModernTable() val furniture = Furniture(chair, table) }   The user or the one that construct Furniture require the knowledge of concrete class. In other words, it tightly couples the user code to specific implementation. It requires user to manually constructing chair and table. It would make more sense to create a class that is dedicated to create a specific implementation, which user code would depend on instead.  info Hiding your implementation or code details from user can be useful when designing public API, to simplify the code user would use.  interface FurnitureFactory { fun createChair(): Chair fun createTable(): Table } class VictorianFurnitureFactory: FurnitureFactory { override fun createChair(): Chair { return VictorianChair() } override fun createTable(): Table { return VictorianTable() } } class ModernFurnitureFactory: FurnitureFactory { override fun createChair(): Chair { return ModernChair() } override fun createTable(): Table { return ModernTable() } }   We do this by creating a FurnitureFactory interface, which will be implemented by VictorianFurnitureFactory and ModernFurnitureFactory for victorian and modern furniture, respectively. To actually use it :  class Furniture(val chair: Chair, val table: Table) {} fun main() { val victorianFactory = VictorianFurnitureFactory() val modernFactory = ModernFurnitureFactory() val furniture = Furniture( chair = victorianFactory.createChair(), table = modernFactory.createTable() ) }   Having factory class become even more useful if the construction of object is complex, just like the previous Factory example. ","version":"Next","tagName":"h3"},{"title":"Peer-to-Peer","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/peer-to-peer","content":"Peer-to-Peer See Cloud Computing &gt; Peer-to-Peer","keywords":"","version":"Next"},{"title":"Software Debugging","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/software-debugging","content":"","keywords":"","version":"Next"},{"title":"Software Bugs​","type":1,"pageTitle":"Software Debugging","url":"/cs-notes/software-engineering/software-debugging#software-bugs","content":" Source of Bugs​  Bugs can be produced by :  Syntax errors : These bugs occur when the code violates the programming language's syntax rules, making the program can't be executed. Syntax error occurs before the program execution, typically catch by the compiler when compiling or static analyzer tools.Logic errors : Logic error are caused by programmer's fault. They occur when there is a mistake in the program's logic or algorithm, leading to incorrect results or unexpected behavior. These bugs can be subtle and may not always cause the program to crash or produce error messages.  tip See also Type of errors.  Types of Bugs​  Different programming languages may produce different type of bugs. The design and features of a programming language can impact the types of bugs that are more likely to occur.  Some types of bugs :  Resource bugs : Resource bugs are related to the incorrect management or usage of system resources, such as memory, file handles, network connections, or database connections. These bugs can include issues like memory leaks, file handle, or connection leaks. In language where the programmer manages the memory manually, such as C or C++, bugs like memory leaks, buffer overflow, use-after-free, dangling pointer, double free, etc., becomes more frequent. Type bugs : Errors or mismatches in the handling of data types. These bugs can arise from using incorrect or incompatible data types, performing operations on variables of the wrong type, or misinterpreting the expected behavior of a particular type. In statically-typed language, such as Java or C++, type bugs can be caught at compile-time, while dynamically-typed languages such as Python or Ruby, these bugs may be encountered during runtime. Concurrency bugs : Concurrency bugs occur in programs that have multiple threads or processes running concurrently. These bugs are related to the incorrect synchronization, coordination, or sharing of resources among different threads or processes. See multithreading problems.  Bugs Management​  The number of bugs increases as the software gets larger. Bugs are typically documented and tracked in special issue tracking software. These software helps to manage bugs that arise a software.  A bug issue is associated with relevant information, such as bug summary, description, steps to reproduce the bug, severity, and priority.  Severity : Severity refers to the impact or seriousness of a bug on the software system or its users. Severity is categorized into several levels, such as critical, high, medium, and low. Critical severity lead to unusable software, high may cause major issue but doesn't result in system failures, medium has moderate impact on functionality, while low are minor or cosmetic issues that have minimal impact.Priority : Indicates the importance or order in which bugs should be addressed based on various factors, such as business requirements, project goals, and user needs. Priority can be numerical, such as 1 to 5, or named, such as critical, high, medium, low, or deferred.  ","version":"Next","tagName":"h3"},{"title":"Debugging​","type":1,"pageTitle":"Software Debugging","url":"/cs-notes/software-engineering/software-debugging#debugging","content":" Debugging Process​  The first step to debug is to understand the bug and know how to reproduce it. Reproducing a bug require an understanding of bug's behavior and identifying the root cause.  We may try specific condition and various input to know which trigger the bug. After the root cause is identified, say it's function A, then we may isolate function A from others. By isolating it, this mean we are simplifying the code, removing unnecessary dependencies, and focusing on a particular line of code that causes the bug.  val userData = db.loadData() println(userData) val result = processData(userData) println(result)   Let's say we are processing some user data. In this case, the bug could originate from either incorrect retrieval of data from the database or an error in the processing itself.  db.loadData loads data from the database. After printing the data, we are sure that they are retrieved correctly. However, the printed result is incorrect, then it is possible that the bug originate from processData.  We can check the details of the function and see if we can identify it directly. If not, perhaps we should try different input to help us gain more insight. When trying for different input, we do not need to use the real data, we can simplify the input so that it takes a hard coded data instead.  Debug Tools​  The difficulty of debugging can vary depending on the complexity of the software and bugs. Advanced debugging is done with specialized tools that allows you to step into the program and stops it. The tool will also tell you the state of the program at the time, such as the value of variables and the call stack.  These tools work by attaching additional instructions that allow them to monitor the program's execution to gather information for us.  Debugging tools require a breakpoint, which indicate the specific lines of code we want to pause the program. When the program reaches a breakpoint, the debugger will take control.  The tool inspects the program state, we can see the value of variables and the call stack, which contains the sequence of function call. We can also :  Step in : Diving into a function or method call.Step over : Execute current line and move to the next line without stepping into any function calls in that line.Step out : Exit the current function or method and return to the calling context.Resume : Resume the program execution after a breakpoint or pause.   Source : https://blog.jetbrains.com/idea/2020/05/debugger-basics-in-intellij-idea/  Debugging Techniques​  Debugger tools : As mentioned previously, debugging tools are very useful because they allow us to stop and pause the program's execution, along with observing its state.Tracing : We had done this technique in debugging process, it involves using print statement to monitor the flow of execution and data progression.Activity tracing : Activity tracing focuses on tracking the overall time spent by the processor executing specific code segments. It helps identify areas of inefficient processor time allocation or faulty program logic that may require optimization.Divide and conquer : Systematically narrowing down the scope of investigation to locate the problematic section of code. It involves dividing the codebase or problem into smaller, manageable parts to isolate and identify the source of a bug or issue.Time travel debugging : Some tools allow to revert the execution of program, which mean we can try different inputs to change the result and see the response. ","version":"Next","tagName":"h3"},{"title":"Software Deployment","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/software-deployment","content":"","keywords":"","version":"Next"},{"title":"Release Lifecycle​","type":1,"pageTitle":"Software Deployment","url":"/cs-notes/software-engineering/software-deployment#release-lifecycle","content":" There are several stages of releasing software :  Pre-alpha : Early stage of development and the software is still being designed and built. It usually involves basic prototyping and proof of concept work, and the software may not have all planned features implemented.Alpha : The first phase of actual software development. Software is being tested, and it's in early testing phase. Some developers team or selected group of users are involved in the testing process. The testing approach is usually white-box testing technique.Beta : A more advanced testing phase, tested by a larger group users. The beta testing process can be closed, where the testers are chosen group, while the open beta is typically open for anyone interested. The software may still have some issues, but it is generally more stable and closer to the final release compared to the alpha stage.Release Candidate (RC) : Release candidate is a version that could potentially become the final release. It is keep being tested and refined with user feedback.Release to Manufacturing (RTM) : A version of the software that is considered ready for production deployment. The software is being prepared for mass distribution or installation on customer systems.General Availability (GA) : Stage at which the software is officially released and made available to the public or the target audience.Production : Final stage in which software is already deployed and used in a live production environment. The software is being actively used by end-users or customers for its intended purpose.  ","version":"Next","tagName":"h3"},{"title":"Feature Toggling​","type":1,"pageTitle":"Software Deployment","url":"/cs-notes/software-engineering/software-deployment#feature-toggling","content":" After the software is released to production, there is a technique, which is called feature toggling that allows developers to control the availability of specific features or functionality within the app.  One of the purpose of feature toggling is to gather feedback of new functionality from some subset of the user. This will help us gather feedback and potentially improve if there are issues.  Feature toggling can be implemented easily with conditional statement. We could have a variable, such as enableLogging, which will toggle the ability for the program to log. We will wrap the section of codes that setup for logging mechanism.  It could be like this :  fun main() { val enableLogging = false if (enableLogging) { val logger = Logger() val app = App(logger) } else { val app = App(null) // null object won't be used } // Use app, all the application logic here... }   Another technique is canary release, which involves making separate app version for the enabled feature and the original version. Canary version is typically released to a small subset of users. By doing this, any issues or bugs with the new release can be identified and addressed before rolling it out to a wider audience.  ","version":"Next","tagName":"h3"},{"title":"Deployment Environment​","type":1,"pageTitle":"Software Deployment","url":"/cs-notes/software-engineering/software-deployment#deployment-environment","content":" Deployment environment is a specific infrastructure or system where software are deployed and made available for use.  Some types of environment :  Development : Environment used by developers during the development phase of the software. It typically mirrors the production environment to allow developers to test and debug the application. Development environments often have additional tools and configurations, such as ability to debug or log.Testing/QA : Environment for testing and quality assurance purposes. It is used to perform various types of testing, including functional testing, integration testing, performance testing, and user acceptance testing.Staging or Model : Pre-production environment that exactly resembles the production environment. It is used to validate the application's behavior and performance in a real environment, such as using a real remote server than localhost.Production : Live environment where the application is made available to end-users or customers. It is the environment in which the application operates on a daily basis and serves actual user traffic.  ","version":"Next","tagName":"h3"},{"title":"Deployment Process​","type":1,"pageTitle":"Software Deployment","url":"/cs-notes/software-engineering/software-deployment#deployment-process","content":" Several steps involved in the deployment process :  Build : As mentioned before, it is necessary to build the necessary artifact and packaging them into a deployable format.Infrastructure Setup : Set up the target deployment environment, including servers, networking, databases, etc. This step may involve provisioning virtual machines, configuring containers, or setting up cloud services to create the necessary infrastructure.Configuration : Configure the deployment environment with the required settings and parameters. This includes configuring environment-specific variables, connection strings, or API keys.Deploy : Perform the deployment by transferring the artifacts to the target environment. This can be done using file transfer protocols (uploading it) or version control systems. Deployment is done in specific strategy explained below.  Deployment Strategy​  The process of deployment to production may require restart and downtime for users. An ideal strategy is to deploy it in zero downtime, which keep the application available and accessible to users throughout the deployment process. The deployment happens gradually, updating the application to a single instance (in the case of cloud server) or to a fraction of users first.  Other way to instantly update to all instance or all users is keeping two separate and identical environment. The technique is called blue/green deployment, where we maintain blue as the live environment, and the green as the new version or update environment. When updating the application, we will switch the traffic or update the DNS to redirect user traffic from the blue environment to the green environment.  ","version":"Next","tagName":"h3"},{"title":"DevOps​","type":1,"pageTitle":"Software Deployment","url":"/cs-notes/software-engineering/software-deployment#devops","content":" DevOps is the set of practices that aims to bridge the gap between software development (Dev) and IT operations (Ops). Dev is typically associated with development or the making of software, while IT operations are associated with the management of infrastructure, deployment, and maintaining the environment.  One practice of DevOps involves automating the deployment process, ensuring that the developer writing code is not concerned with deployment and can instead focus on improving the application.  CI/CD​  Continuous improvement and continuous deployment (CI/CD) is the process of continuously improving and deploying the software. It is the practice that is applied on DevOps to automate and streamline the software delivery pipeline. Many platform, such as GitHub Actions is used for CI/CD process.  Continuous Integration (CI) : CI is a development practice that involves frequently integrating code changes from multiple developers into a shared repository. Developer can use version control system, such as Git to manage and track code changes. Whenever code changes are committed to the repository, an automated build process can be triggered to compile the code, run unit tests, and generate build artifacts.Continuous Delivery (CD) : CD focuses on automating the entire software release process up to the point of deployment. For example, GitHub Actions, specifically the GitHub Workflow, allows us to define a deployment pipeline. It is a stages and actions that will be executed whenever a particular event happened. For example, a common pipeline would be defining a task to build, test, package, and deploy the application whenever code is pushed to the main branch. ","version":"Next","tagName":"h3"},{"title":"Software Process","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/software-process","content":"","keywords":"","version":"Next"},{"title":"Prototyping​","type":1,"pageTitle":"Software Process","url":"/cs-notes/software-engineering/software-process#prototyping","content":" A prototype is an early sample of a product, specifically designed to test or obtain feedback. Prototyping a software means developing a functioning model of a software system to demonstrate the system's potential functionality.   Source : https://smartlogic.io/blog/startup-advice-dont-waste-money-on-an-expensive-prototype/  For example, when making an app, the first priority might be on ensuring the primary functionality of the application to functions properly. In a social media app, we can develop basic user interface or use fake data and profile pictures rather than investing significant effort in creating a visually appealing user interface. The primary focus of such a prototype is to demonstrate the core features and interactions of the app rather than the aesthetics.  Prototyping can be beneficial in software development process, as it can save time and effort in gathering and curating real user data for the prototype.  ","version":"Next","tagName":"h3"},{"title":"Software Methodologies​","type":1,"pageTitle":"Software Process","url":"/cs-notes/software-engineering/software-process#software-methodologies","content":" Software Methodologies are specific approaches or frameworks that defines software process.  Agile​  Agile is a group or framework of iterative approach to software development that emphasizes collaboration, adaptability, and continuous improvement.   Source : https://medium.com/@temitopeolanipekun12/what-is-agile-methodology-713c9e29361a  Agile projects are divided into small, time-boxed iterations called sprints. Each sprint typically lasts from one to four weeks and results in a potentially shippable increment of the software. This iterative approach allows for frequent feedback and the ability to adapt and adjust based on changing requirements.  Planning : Define the overall project goals, scope, and initial requirements. In agile, we can list the functionality requirement of a product based on their priority. This is called product backlogs. Design : Design decisions are made during each sprint or iteration throughout development, rather than developing all design elements upfront. This allows for flexibility and responsiveness to changes and feedback. Development : The agile team select a set of user stories from the product backlog and develop the corresponding functionality. A user story is a way to describe how should a software function in the perspective of user. For example, a user story for an e-commerce website could be : &quot;As a customer, I want to be able to add items to my shopping cart, so that I can easily keep track of the products I want to purchase.&quot; This is not an actual feedback from user, but rather a simple and concise way to describe the desired functionality or behavior of the software system. Testing : Software testing is performed in each sprint or iteration, this is called continuous testing. User stories in agile are typically accompanied by acceptance criteria. Acceptance testing is a type of testing to determine whether a software system meets the specified acceptance criteria. Release : Agile promotes frequent and incremental releases. At the end of each sprint, a potentially shippable product increment is produced. Feedback : Feedback is collected from stakeholders, users, and customers during various stages of the process, including sprint reviews and demonstrations. Feedback is used to validate assumptions, refine requirements, and guide future iterations.  There are several examples of agile development :  Scrum : Scrum is the implementation of agile methodology that focuses on incremental changes. It involves previously talked elements like product backlog, sprint, and daily scrum. Daily scrum is a short daily meeting where the development team synchronizes and plans their work. It is an opportunity to discuss progress, identify obstacles or blockers, and plan for the day's activities. Extreme Programming (XP) : XP is an agile methodology that emphasizes close collaboration between the development team and the customer or product owner. It promotes frequent and small releases of software. One common practice in XP is pair programming, in which two developers working together on the same codebase, with one actively writing code and the other reviewing and providing immediate feedback. This promotes knowledge sharing and reduces defects. Source : https://en.wikipedia.org/wiki/Extreme_programming Lean : Lean is a principles and practices derived from Lean manufacturing, which focus on maximizing customer value while minimizing waste. Customer value is the perceived worth or benefit that a customer receives from a product. In software, maximizing customer value could be prioritizing features and functionalities that provide the most value to the end-users. On the other hand, waste refers to any activity or process that does not add value to the software product. These can be unnecessary documentation, waiting time, defects, and overproduction of features that occurs during software engineering process. Kanban : Kanban is an agile methodology that helps to manage the development of software by visualizing it in a Kanban board. Kanban board represent a workflow of progress of work items. The board consists of columns that represent different stages of the development process, such as &quot;To Do,&quot; &quot;In Progress,&quot; &quot;Testing,&quot; and &quot;Done.&quot; Work items, which represent the actual work represented by cards or sticky notes, are moved across the board as they progress through the workflow. Source : https://www.geeksforgeeks.org/kanban-agile-methodology/  Waterfall​  Waterfall model is a sequential development approach, where each phase of the project is completed before moving on to the next one. The methodology is called &quot;waterfall&quot; because progress flows steadily downwards, similar to a waterfall.   Source : https://business.adobe.com/blog/basics/waterfall  It starts from identifying what the requirements are, designing the system, implementing the system in code, verify and test the software, and deploy as well as maintain the software.  Spiral​  Spiral is an iterative software development methodology that combines elements of both waterfall and iterative approaches. A complete iteration of the software development process is represented as a cycle of the spiral.   Source : https://en.wikipedia.org/wiki/Software_development_process#/media/File:Spiral_model_(Boehm,_1988).svg  A cycle is divided into four phases :  Planning : This phase involves gathering requirements, defining the project objectives, and constraints.Risk Analysis : In this phase, risks are assessed and analyzed. The identified risks are evaluated based on their potential impact on the project and the likelihood of occurrence. Strategies and mitigation plans are developed to address the identified risks.Development : Focuses on the actual development and implementation of the software. It includes activities such as designing the system architecture, developing the software components, and thorough testing to ensure quality and functionality.Evaluation : The evaluation phase involves reviewing and assessing the progress made. Based on this evaluation, decisions are made regarding the next steps, such as making adjustments.  ","version":"Next","tagName":"h3"},{"title":"Software Planning​","type":1,"pageTitle":"Software Process","url":"/cs-notes/software-engineering/software-process#software-planning","content":" Making a software project can be overwhelming if we don't have the plan. It is easy to be distracted by the amount of work we have to do. Here are some tips to plan and build a program :  System Design : System design is the process of designing architecture and all related component to build a software system. It is a very good practice to always sketch the system first, at least get a high-level view on how the system will look like. A system design involves : Requirements Gathering : To know what should we build, we need to know what our requirements are. Requirement can be categorized into two, functional and non-functional. Functional requirements are the specific features and functionalities that the software should provide. Non-functional requirements, on the other hand, specify the qualities or characteristics of the software, such as performance, security, and usability. The functional requirements for a streaming platform include enabling users to broadcast videos to the server and allowing other users to receive the broadcasts to watch them. The non-functional requirement would be designing the app to be scalable with thousands of traffic around the world. The app should be able to handle such workload and keep functioning. In this step we may also choose our tech stack, including what programming languages, library, framework, or cloud services that suit our needs. High-level Design : After knowing what is required to build the software, we can start designing the system in high-level. This involves making a design that outlines the architecture and structure of the software. We could make various diagram that represent how each component interact or determining how should we store the data. More Detailed Design : Then, we can create a more detailed design that provides a deeper understanding of the internal components, modules, and implementation details of the software application. We can imagine how would the component works or what algorithm it would use. Core Design : Core design focuses on designing the central and critical components of the software system. In the case of creating a video streaming app, we can focus on designing how users can broadcast their videos, how the server receives them, and how it simultaneously sends the content to all viewers. We can ignore other non-essential components such as user authentication. Scale : After everything is evaluated, we can now focus on how to scale and maintain the system. We should identify which component would most likely be causing performance bottlenecks or limiting the system's scalability. Let's say our streaming app is targeting the international market, which mean anyone around the world would use it. We can choose to distribute server instances, so that users can have low-latency access to the streaming content, regardless of their geographical location. Start Development : Begin the actual development by following the design. Incremental &amp; MVP : It is a good practice to build the software incrementally, meaning that we develop the software in small increments, each providing a valuable and functional piece of the system. We start breaking down the system into smaller component to help us manage the development process. We should identify the MVP (Minimum Viable Product), which represents the smallest set of features that provides value. We don't need to be bothered by advanced authentication system, recommendation algorithms, or social features. Perhaps we don't need to integrate Google or Facebook authentication, we can use a simple username and password for the time being. In the case of video streaming : The initial step would be establishing a connection between the client app and the server by sending a basic text message.The next step would be to enhance it to transmit an actual video.Progress would continue gradually, allow multiple clients to connect to the server and send text messages concurrently. The server should be capable of broadcasting messages to all connected clients concurrently.The text message would be upgraded to a real-time video stream, ensure that all clients receive the stream correctly.Create a video player capable of running the stream. Testing : Write automated tests to verify the software and to prevent functionality from becoming incorrect when code is modified. Keep Going : After creating the minimum viable product and testing it, we should identify any problems or areas for improvement. We can create a list of TODOs that may include bug fixes, new features, corrections, etc. We can prioritize each task on the To-do list and determine the next steps accordingly. If the bug significantly affect the app, we can fix them. If working on a new feature, we can start again the process from step 1 by creating the high-level design of the new feature, developing it incrementally, and targeting the MVP, and so on going iteratively. ","version":"Next","tagName":"h3"},{"title":"System Design","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/system-design","content":"","keywords":"","version":"Next"},{"title":"Performance​","type":1,"pageTitle":"System Design","url":"/cs-notes/software-engineering/system-design#performance","content":" There are several performance measures :  Latency : Delay in transmission of request or response.Processing time : The time it takes for the request to be processed.Response time : The time it takes for the system to process a request from client and response.Throughput : Number of requests handled within a unit of time.  To put it simply, response time = latency + processing time + latency. The goal is to get low latency, low processing time, low response time, and high throughput.  Technique to improve performance includes :  Implement caching mechanism.Optimize database, such as using database index.Load balancing, incorporate multiple servers or instance to distribute the workload.Efficient algorithms and data structures.Asynchronous systems, use algorithm that are capable of doing concurrent operations.  ","version":"Next","tagName":"h3"},{"title":"Scalability​","type":1,"pageTitle":"System Design","url":"/cs-notes/software-engineering/system-design#scalability","content":" To handle more workload, a system can either be scaled horizontally or vertically.  Horizontal scale includes adding more instance, such as adding more server to handle the request. Vertical scale increase the individual resources of existing instances, such as upgrading the server CPU, memory, or disk.  Improving scalability is typically related to distributing the workload: :  Use load balancer with distributed architecture like SOA or microservice, allowing the system to be decomposed into smaller and independent server that can be developed, deployed, and scaled individually. For example, the authentication system may struggle with workload, but not the file uploader, which means we can scale up the authentication system. Make service or system that are stateless. Stateless refer to the ability of a system or service to operate without relying on stored state information. A stateless system treats each request as an independent transaction and does not maintain any session or context information between requests. With more users present, storing all their session data can be a burden. One approach is to share the state across all the server, to reduce the burden of a single server storing the data. However, this approach require additional synchronization mechanism, which can increase the complexity of the system. Database partitioning is a term for breaking down a large database into smaller, more manageable units called partitions. This is done within a single database instance. Partition can scale the system horizontally, this is done by distributing the partition across multiple servers, this process is called database sharding.  ","version":"Next","tagName":"h3"},{"title":"Availability & Reliability​","type":1,"pageTitle":"System Design","url":"/cs-notes/software-engineering/system-design#availability--reliability","content":" Availability measures the extent to which users can access the system without interruption or downtime. On the other hand, reliability measures the likelihood that a system will operate without failure.  Availability is often expressed as a percentage, such as 99.9% (three nines) or 99.99% (four nines). For example, a system with 99.9% availability would be expected to have less than 0.1% (or 8.76 hours) of downtime within a year. Higher availability percentages indicate better system performance and reliability.  A system can operate in parallel (at the same time) or in sequence (one after the other). A parallel system generally have higher availability, because when one fails, the other is still operating.  The formula of availability :  Parallel : availability (total)=availability (system 1)×availability (system 2)\\text{availability (total)} = \\text{availability (system 1)} \\times \\text{availability (system 2)}availability (total)=availability (system 1)×availability (system 2).Sequence : availability (total)=1−(1−availability (system 1))×(1−availability (system 2))\\text{availability (total)} = 1 - (1 - \\text{availability (system 1)}) \\times (1 - \\text{availability (system 2)})availability (total)=1−(1−availability (system 1))×(1−availability (system 2)).  In parallel, if both system availability is 99.9%, the total would be 99.8%. While in sequence, the total would be 99.9999%.  Two mechanisms to improve availability and reliability :  Failover : Quickly and automatically transfer workload from a failed component or system to a backup or standby component. When a failure is detected, the failover mechanism triggers the transition to the backup component, which assumes the workload and resumes normal operations. Failover can be done in active-passive or active-active. In active-passive, one system is running, and the other is on standby. In the latter one, two systems are running together, to distribute workload as well as to increase the availability. Replication (redundancy) : Create and maintain duplicate copies of data or system components across multiple locations or systems. Ensure that data or services are accessible even if one copy or component becomes unavailable. See also database replication for replicating database and database sharding.  ","version":"Next","tagName":"h3"},{"title":"Consistency​","type":1,"pageTitle":"System Design","url":"/cs-notes/software-engineering/system-design#consistency","content":" Consistency is concerned with concurrency of multiple component or system modifying the same data.  The level of consistency required for a system can be adjusted based on the trade-offs we are willing to make. These consistency models are categorized based on their degree of strength :  Weak : Low synchronization among replicas. There are no guarantees about the order in which updates are seen by different system. Different replicas may have inconsistent views of the data at any given time. Used in scenarios such as real-time systems, where immediate consistency is not critical, and the system prioritizes availability and performance over strict consistency. Eventual : Eventual consistency guarantees that data will eventually be consistent with just slight delay. Strong : All replicas observe the same order of updates, and any read operation will return the most recent write. Strong consistency ensures that replicas reflect a single, globally agreed-upon state at all times.  Ways to improve consistency :  Use transaction on database operation and adhere to ACID properties of database.Implement synchronization mechanisms to coordinate access and updates to shared resources or data. Various technique, such as locks and semaphores can help to guarantee concurrent operations.  CAP Theorem​  Distributed system is a collection of interconnected computer that are separated and only connected by one central computer. The computers in the connection are often called as node. Each node communicate and coordinate their activities through message passing, shared memory, or other forms of inter-process communication.  CAP theorem is a principle that states that it is impossible for a distributed system to simultaneously provide all three CAP properties in the face of network partitions. Network partition is a phenomenon that in which nodes are disconnected, causing them unable to communicate. It is a realistic situation that can occur even in large system due to various reasons, such as network failures, hardware failures, misconfigurations, or even during routine maintenance activities.  The three CAP properties are :  Consistency : Requirement that all nodes in a distributed system have the same view of data at the same time.Availability : System remains operational and responsive even in the presence of failures.Partition Tolerance : System's ability to continue functioning even when network partitions occur.  The reasoning behind CAP theorem is one conflicting another. Nodes in distributed system can be thought as disjoint set, which is a non-overlapping set. If A={1,2}A = \\{1, 2\\}A={1,2} and B={3}B = \\{3\\}B={3}, then A and B are disjoint sets.  When a node receives new data, it has the option to write it or reject it. If it chooses to write it, then we can say the system is available at the time (it serves request), but it will need to synchronize with others in order to maintain consistency.  When a partition occurs, nodes can't communicate with each other. If they keep choosing to write the new data, then it would lead to inconsistency, as the data is not synchronized across all nodes and they can't synchronize. If they choose to reject the write request, then we just sacrificed availability. This means guaranteeing CA together is not possible, resulting in the impossibility of achieving CAP as well.  This leaves us with two possible system :  CP : CP system sacrifice availability whenever a partition occurs. It will block or delay responses until the partition is resolved.AP : An AP system will continue to operate and serve requests during partition, even if it means allowing temporary inconsistencies of the data.   Source : https://en.wikipedia.org/wiki/CAP_theorem#/media/File:CAP_Theorem_Venn_Diagram.png  tip More about distributed systems.  ","version":"Next","tagName":"h3"},{"title":"Infrastructure​","type":1,"pageTitle":"System Design","url":"/cs-notes/software-engineering/system-design#infrastructure","content":" Infrastructure refers to the hardware, software, and network components that provide the foundation for a system to operate.  DNS​  The main purpose of Domain Name System (DNS) resolves a domain name (e.g., google.com) to the corresponding IP address (i.e., 216.239.38.120), to allow browser to establish a connection with the correct server.  Other than that, we can configure DNS for load balancing and traffic distribution using DNS service, such as Cloudflare. DNS can rotate and distribute requests among different servers based on workload, latency, or geolocation.   Source : https://blogs.manageengine.com/clouddns/2022/10/24/how-does-a-dns-work.html  CDN​  Content Delivery Network (CDN) is a distributed network of servers located in various geographical locations that work together to deliver content to end users with improved performance and availability.  The idea of CDN is to spread server around the world, so that when a user requests content, the CDN serves it from the nearest edge server instead of the origin server, reducing latency and improving delivery speed. It is commonly used to deliver static and dynamic content, such as web pages, images, videos, and other media files.  There are two approaches of distribution in CDN :  Push CDN : In a Push CDN, the content is uploaded or &quot;pushed&quot; to the CDN's edge servers in advance, either manually or automated. When a user requests the content, the CDN delivers it directly from the edge server without needing to fetch it from the origin server. It is typically used to deliver content that doesn't change so often.Pull CDN : Pull CDN serve content in cache-like mechanism. When a user requests a specific content, the CDN checks if it has a cached copy at the edge server. If the content is not available or has expired, the CDN pulls it from the origin server, caches it on the edge server, and delivers it to the user. Subsequent requests for the same content can be served directly from the edge server's cache.   Source : https://www.domainesia.com/tips/apa-itu-cdn-dan-fungsi-cdn/  Load Balancer​  Load balancer distribute incoming traffic across multiple servers to reduce a burden of a single. It serves a purpose beyond just workload balancing, as it can also be utilized to reroute connections away from systems that are unhealthy or experiencing failures.  Other benefits of load balancer :  Session Persistence : Now, because the server are distributed, it could be possible that they don't share data with each other. In the case of stateful application, in which the server need to store user's session data, load balancer can maintain session persistence by ensuring that requests from the same client are consistently routed to the same server.SSL Termination : Client through web browser connects to server with secure connection, encrypted with SSL/TLS protocol. Load balancer can decrypt the connection before it connects to the application servers. Furthermore, it can encrypt outgoing connection from the server. Using load balancer as SSL termination help separating the concern of server, so they do not need to be concerned by security-related issues.  There are two levels of where load balancer can operate :  Network Level : In this level, load balancers distribute traffic based on network-level protocols, typically at the transport layer (layer 4) of the OSI model. They forward incoming requests using information such as source IP address, destination IP address, and TCP/UDP ports. They do not examine the content of the requests, and they are generally faster and more efficient, but lack of application layer access.Application Level : Application level is the layer 7 of the OSI model, it has access from the application layer protocol, such as HTTP headers, URLs, cookies, and session information. They are generally slower, but they can perform content-based routing, SSL termination, caching, etc.   Source : https://azure.microsoft.com/en-us/blog/build-a-globally-resilient-architecture-with-azure-load-balancer-2/  Reverse Proxy​  Reverse proxy can function similar to load balancer, however, instead of evenly distributing the workload, a reverse proxy forwards request to the appropriate backend server.  Architecture​  Several choices of architecture are :  Monolithic : Simple to develop and deploy but can become harder to maintain and scale as the application grows.Microservice : Flexible, scalable, and isolate fault, but introduces additional complexity in terms of service coordination and communication.Service Oriented Architecture (SOA) : Similar to microservice, it can lead to complexity in terms of service coordination and communication.Serverless : Easy development, automatic scaling, and cost efficiency but may have limited control over low-level components.  Message Broker​  Message broker can be useful to handle the communication and coordination between components or services in a system.  Benefits :  Message broker promotes loose coupling, components can communicate with each other without direct knowledge or dependency on one another.Message won't be lost as they can be kept temporarily when component or service are temporarily unavailable or experiences a failure.  ","version":"Next","tagName":"h3"},{"title":"Security​","type":1,"pageTitle":"System Design","url":"/cs-notes/software-engineering/system-design#security","content":" Ways to improve security :  Implement authentication and authorization mechanism.Use secure protocols such as SSL/TLS.Avoid various security vulnerability and apply best practices.Maintain IP whitelists and blacklists to allow or block traffic from specific IP addresses or ranges.Implement rate limiting to control the amount of incoming traffic, preventing DoS.  tip See also : Backend OptimizationDatabase OptimizationDatabase IndexNetwork SecurityWeb SecurityMobile SecurityBackend &amp; Server SecurityOther Attack &amp; Exploit ","version":"Next","tagName":"h3"},{"title":"Software Testing","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/software-testing","content":"","keywords":"","version":"Next"},{"title":"Testing Approach​","type":1,"pageTitle":"Software Testing","url":"/cs-notes/software-engineering/software-testing#testing-approach","content":" There are many approaches of testing that can be categorized based on different ways.  Based on the execution time​  Static : We do not execute the code in static testing. Typically, static testing revolve around code reviews or inspections. We do static testing by relying on compiler to check syntax or using specific static analysis tool.Dynamic : In contrast to static, dynamic testing runs the program. Dynamic testing focuses on evaluating the behavior and performance of the software under various conditions and inputs.Passive : We also run the program in passive testing, but we do not interact with it directly. In passive testing, we gather information about the programs' behavior, usage patterns, or performance characteristics. Passive testing may involve techniques such as logging, tracing, profiling, or analyzing system logs and metrics.  Manual​  Manual testing involve human intervention in the testing process. Software testers will verify the software behavior against different conditions and inputs. There are many ways manual testing can be performed :  Entering inputs by hand : The tester run the program, through the user interface (e.g., terminal or GUI), the tester will test different scenario by inputting both valid and invalid data to the software. The tester then see the result and verify its correctness. An example of this would be checking if clicking the back button in the GUI actually navigate us back. Entering inputs by code (semi-automated) : Having to interact with the software to test its behavior can slow us down. It can be time-consuming to test the behavior against a bunch of different inputs and cases. Software tester may make a script or code snippets dedicated to test the programs' behavior. Consider a calculator app, there is a function that adds two number : fun addTwoNumber(num1: Int, num2: Int): Int { return num1 + num2 } The function is used in addition functionality. Instead of interacting with the calculator UI and test its behavior, we can make several functions that automate the testing of the addTwoNumber function. fun addTwoNumber_negative() { println(addTwoNumber(-2, -3)) } Here, we are testing its behavior against negative number input. addTwoNumber(-2, -3) is expected to return -5. By calling the addTwoNumber_negative function, we can print its result to the console. It is so much faster than interacting with the UI. We can make a bunch more function to test : fun addTwoNumber_positive() { println(addTwoNumber(2, 3)) } fun addTwoNumber_zero() { println(addTwoNumber(0, 0)) } fun addTwoNumber_veryBigNumber() { println(addTwoNumber(100000, 100000)) } fun addTwoNumber_positiveNegative() { println(addTwoNumber(-100, 20)) } And then effortlessly test all this behavior by : fun main() { addTwoNumber_positive() addTwoNumber_zero() addTwoNumber_veryBigNumber() addTwoNumber_positiveNegative() } By just running it, we already see the result of 4 different inputs. However, the tester still need to see manually the result of this function in the console. This approach can be improved by making it fully automated, which will be explained next. Exploratory : In exploratory, the tester doesn't have specific plan to follow. The test is focused on exploring the behavior of the application, they make on-the-spot decisions about what to test, how to test, and what areas of the software need more attention. One potential downside of exploratory testing is the possibility of missing certain test cases or areas that were not covered during the exploration.  Automated​  Automated testing involves the use of automation tools, scripts, and frameworks to execute tests without human intervention.  The previously semi-automated approach make use of simple script to automate the test on different condition, but it just prints the output to the console, therefore requiring human intervention to verify the result. This can increase the effort of testing as the number of tests increases.  An automated testing tool involves execution engine, which will call the tests function as well as verifying them. More advanced tool may include test reporting and logging, to provide more detailed information about the test execution, such as what is wrong, where the behavior deviated from expectations, how many are failing or passing, etc.  An example of automated testing tool in Kotlin is the JUnit framework. Basically, JUnit require us to identify which function it should execute when we run the test. It also let us define our expectation of the function by using assertions.  Assertions are statements that check whether a given condition is true during the execution of a test case. Let's get to an example :  import org.junit.jupiter.api.Test import kotlin.test.assertEquals class Test { @Test fun addTwoNumber_positive() { assertEquals(5, addTwoNumber(2, 3)) } @Test fun addTwoNumber_zero() { assertEquals(0, addTwoNumber(0, 0)) } @Test fun addTwoNumber_veryBigNumber() { assertEquals(200000, addTwoNumber(100000, 100000)) } @Test fun addTwoNumber_positiveNegative() { assertEquals(-80, addTwoNumber(-100, 20)) } }   The assertEquals function requires two parameters. The first parameter is our expected true value, and the second is the actual result. For instance, when checking addTwoNumber(0, 0), where the expected result is 0, we place 0 as the first parameter and the result of addTwoNumber(0, 0) as the second parameter. If the expected is not equal to the actual result, JUnit will throw an exception, indicating that the assertion has failed.  There are other assert function, such as assertTrue, which assert that the expression is true, assertFalse, and assertNotEquals.  We will tell JUnit which function to call during the test by placing the @Test annotation at the top of the function. After that, we may run the selected test function. We can either do it via command line by calling specific command, or do this easily by pressing the &quot;Run test&quot; button in some IDE, such as IntelliJ.    We want to see what JUnit will tell us if one of our test is wrong. So, let's modify the expected parameter of test_addTwoNumber_zero() to be 1, instead of 0. Let's run the test.    Through the IntelliJ GUI, we can see the result of running these 4 tests. The test_addTwoNumber_zero() is indeed wrong, but all other test passed (indicated by green check symbol). The report says that expected is 1, but the actual is 0. It even tells us the difference of the result (if we click the &quot;Click to see the difference&quot;).  JUnit also provide us the call stack, which shows the sequence of function calls that led to the error or failure. Here's the few top most log.  org.opentest4j.AssertionFailedError: expected: &lt;1&gt; but was: &lt;0&gt; at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55) at org.junit.jupiter.api.AssertionUtils.failNotEqual(AssertionUtils.java:62) at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:182) at org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:1135) at kotlin.test.junit5.JUnit5Asserter.assertEquals(JUnitSupport.kt:32) at kotlin.test.AssertionsKt__AssertionsKt.assertEquals(Assertions.kt:63) at kotlin.test.AssertionsKt.assertEquals(Unknown Source) at kotlin.test.AssertionsKt__AssertionsKt.assertEquals$default(Assertions.kt:62) at kotlin.test.AssertionsKt.assertEquals$default(Unknown Source) at AdditionTest.test_addTwoNumber_zero(Test.kt:16)   The worth noting line is the last one, at AdditionTest.test_addTwoNumber_zero(Test.kt:16). It indicates that has error specifically at line 16. In this case, it may be useless, because the function is literally a single line function.  The call stack trace will be more valuable in more complex codebases with multiple function calls and dependencies. It helps pinpoint the location of errors or failures.  All the report produced by JUnit, such as the logs and call stack, are called test artifacts.  Box approaches​  Box approaches describe the point of view that the testers are taking.  White-box : Tester focuses on testing the internal structure, logic, and implementation details of a system. Testers have access to the internal code, data structures, and algorithms of the system and design test cases based on this knowledge. In white-box testing, we can about how the system does the logic.Black-box : Black box testing focuses on testing the functionality of a system without considering its internal structure or implementation details. Testers treat the system as a &quot;black box&quot; and only interact with it through its inputs and outputs. They are unaware of how the system processes the inputs or produces the outputs. In contrast to white-box testing, we do not care how a system does the logic, we are concerned with the expected behavior of the system.Grey-box : The grey-box is the hybrid approach of white-box and black-box testing. The testers test the system's behavior, but they also have the implementation details.  ","version":"Next","tagName":"h3"},{"title":"Levels of Testing​","type":1,"pageTitle":"Software Testing","url":"/cs-notes/software-engineering/software-testing#levels-of-testing","content":" Unit Testing : Unit testing focuses on testing individual units or components of the software, such as functions, methods, or classes, in isolation. It aims to verify that each unit behaves as expected. The one with did in automated testing was a unit test. It is common in unit testing to have several functions that responsible for testing specific lines of code or behavior.Integration Testing : Integration testing verifies the interaction and integration between different components or modules of the software. It ensures that these components work together correctly. Integration tests are designed to identify issues that arise when different units are combined.System Testing : System testing focuses on testing the entire system as a whole. System tests cover various aspects, including functional requirements, performance, security, usability, and compatibility.Acceptance Testing : Acceptance testing is conducted to ensure that the software meets the customer's requirements or predetermined criteria and is ready for deployment.  ","version":"Next","tagName":"h3"},{"title":"Test Artifacts​","type":1,"pageTitle":"Software Testing","url":"/cs-notes/software-engineering/software-testing#test-artifacts","content":" Test artifacts refer to the documentation or deliverables produced during the testing process. These artifacts provide a record of the testing activities, test design, test execution, and test results.  Test Plan : A test plan outlines the overall testing strategy, objectives, scope, and approach for a specific testing effort.Test Cases : Test cases are detailed instructions or steps that testers follow to execute a specific test scenario. They include inputs, expected outputs, and any preconditions or postconditions.Test Suite : A collection of test cases that are intended to test a set of behaviors.Test Scripts : Test scripts are automated scripts or programs written to execute test cases automatically.Test Data : Test data is the input data used for executing test cases. It includes both valid and invalid data to cover different scenarios.Test Logs : Test logs capture the details of the testing activities, including test execution results, defects found, and any issues or observations during testing.  ","version":"Next","tagName":"h3"},{"title":"Types, Techniques, and Tactics of Testing​","type":1,"pageTitle":"Software Testing","url":"/cs-notes/software-engineering/software-testing#types-techniques-and-tactics-of-testing","content":" Smoke Testing : A testing done in early development to test primary or basic functionality of software.Stress Testing : Stress testing subject the software to high loads or unfavorable conditions beyond its normal operating capacity. It helps identify performance bottlenecks, stability issues, and failure points under stress.Regression Testing : Testing performed after a major change to the code. It ensures that changes or updates to the software do not introduce new defects or adversely impact existing functionality.Alpha &amp; Beta Testing : These tests occur before releasing the software to external users. They are typically done by a software development team or group of chosen testers.Functional &amp; Non-functional Testing : Functional testing verify the software functionality, while non-functional testing focuses on evaluating the software's non-functional aspects, such as performance, security, usability, reliability, and compatibility.Accessibility Testing : Accessibility testing ensures that the software can be used by people with disabilities. It focuses on verifying whether the software is accessible to individuals with visual, auditory, motor, or cognitive impairments.Internationalization &amp; Localization : Internationalization (i18n) ensures that software is adaptable and customizable for different languages, regions, and cultures. Localization (l10n) adapt software to specific locale or target market, such as adapting date and time formats.A/B Testing : A/B testing compares two or more versions of software feature to determine which one performs better. It involves dividing the user base into different groups and exposing each group to a different version. The results and user behavior are analyzed to determine the version that yields better outcomes, such as higher conversion rates, user engagement, or usability.  ","version":"Next","tagName":"h3"},{"title":"Terminologies​","type":1,"pageTitle":"Software Testing","url":"/cs-notes/software-engineering/software-testing#terminologies","content":" Code Coverage​  In unit testing, we can make several functions, with each testing a specific behavior of a function. Often times, we can't cover all the possible cases.  Code coverage is a metric that measures the extent to which the source code of a software application is exercised during testing. It determines the percentage of code lines, branches, or paths covered by the executed tests.  Consider this function :  fun isGreaterThanN(x: Int, n: Int): Boolean { if (x &gt; n) { return true } else { throw Exception() } }   If we only run this test :  fun test_isGreaterThanN_greater() { isGreaterThanN(100, 10) }   By entering greater x value, we are effectively going to the true branch of the if statement. Our test function didn't consider the if branch in the case false was evaluated. We could say that the code coverage is not 100%, because there is some line of code we didn't execute.  Now, what if some portion of the code that we don't test doesn't work, and what if it actually encountered in the real application? Achieving a 100% code coverage is indeed ideal, but the development cost to create tests and considering all possible case may not be worth it. Common percentage such as 75% or 80% code coverage is often used as metrics.  Test Double​  Code works together, often times they depend on each other. In testing, often times these dependencies are not possible to achieve. Test double is a general concept of objects that is used in place of real dependencies during testing.  Mock​  Mock is a simulated object that mimics the behavior of a real object or component. It is used to isolate the code being tested by replacing dependencies with controllable counterparts, typically in unit testing. Mocks are typically used in unit testing to verify interactions and behaviors between objects and to test code in isolation. By mocking an object, we can focus to test the behavior of specific component.  For example, we may want focus on testing our business logic. Some logic involve interacting with the database in a remote server. However, we may not want to set up the server just for the testing, as it can increase hosting costs.  class MockRemoteDB { fun getData(): String { return &quot;Hardcoded data&quot; } fun connect(): Boolean { return true } } fun test_processData() { val remoteDatabase = MockRemoteDB() if (remoteDatabase.connect()) { val data = fakeDatabse.getData() processData(data) } }   Remote database is imagined to have connect and getData method. Because the server is not available, we will simulate the connection behavior to meet our expectation. We may also avoid loading the real data by providing it directly, such as hard coding it or having it in the local environment, so we don't need to waste bandwidth downloading the data.  Fakes​  Fakes are simplified implementations of dependencies that replicate the behavior of real objects or components. They are typically used to replace real dependencies, such as databases, web services, or external systems, during testing.  Fakes provide a controlled and simplified alternative that allows for easier testing. Fakes are often used when the real dependencies are costly, complex, or not easily accessible in a testing environment.  However, we should be careful when making fakes, as they may be &quot;too good to be true&quot;, or oversimplified.  fun connectToServer() { Thread.sleep(5000) }   Let's say we are testing for loading animation. The loading animation is supposed to run whilst we wait to connect to the server. It may not be possible to run the server in testing environment, so we fake up the connection by stopping the thread.  Stub​  Stub is a simplified implementation of a component or module used in testing. It provides predetermined responses to method calls or functions and is used to replace real functionality temporarily. Stubs help simulate specific scenarios or inputs.  fun test_processData() { val usersData = getAllDataNormalUser() processData(usersData) } fun getAllDataNormalUser(): User { return listOf(User(0, &quot;stub&quot;), User(1, &quot;data&quot;)) } fun getAllDataWeirdUser(): User { return listOf(User(-1, &quot;00000&quot;), User(100000, &quot;&quot;)) }   Sometimes, stub keeps track information how they are all called, which might be useful for debugging in the case of test failures. These kinds of stubs are called spies.  Dummy​  A dummy object is a placeholder object used to satisfy the requirements of a particular test scenario or to fulfill the dependencies of the system under test, but it does not have any significant behavior or functionality.  For instance, some business logic require us to back up the data before processing it, we can call the method that saves the data, but the method actually does nothing meaningful.  ","version":"Next","tagName":"h3"},{"title":"Test-Driven Development (TDD)​","type":1,"pageTitle":"Software Testing","url":"/cs-notes/software-engineering/software-testing#test-driven-development-tdd","content":" TDD is a software development approach to make a software, where tests are written before writing the actual code.  The developers write test case that obviously will fail. The test case should contain test that the desired functionality. Then, they write the minimum amount of code necessary to pass the test. Finally, they refactor the code to improve its design and maintainability, but still running the test to ensure the correctness is preserved.  TDD treats test as specification of the program, they are assumed to be what define the program's target. Whenever a test fails, then only the code can change. Therefore, it's very important to make a test that truly reflect the desired behavior of the program. However, it is worth noting that as software requirements evolve or tests become obsolete, it is permissible and necessary to update or remove tests accordingly.  Benefits of TDD :  Reliable code : As long as the tests are correct, then code should behave as expected and reduces the likelihood of introducing bugs or regressions.Faster feedback loop : With TDD, developers receive immediate feedback on their code. Failing tests highlight issues in the code, enabling developers to quickly identify and fix problems.Incremental Development : The lifecycle of TDD promotes incremental development. Developers write tests for a specific functionality, implement the code to make the tests pass, and then refactor if necessary. The process reduces the risk of building unnecessary complex feature. ","version":"Next","tagName":"h3"},{"title":"System Design Examples","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/system-design-examples","content":"","keywords":"","version":"Next"},{"title":"Meeting Platform​","type":1,"pageTitle":"System Design Examples","url":"/cs-notes/software-engineering/system-design-examples#meeting-platform","content":" A video conferencing and collaboration platform similar to application like Google Meet or Zoom.  Requirements​  Functional :  Users register/login through the client.Users configure settings from the client, such as their profile photo and background.Users use the app meeting client which will connect them to the meeting. Meeting room is based on ID and passcode.In the meeting, they will see other participants video and audio.Participant broadcast their video/audio and all the other participant should be able to see it in real time.Participant can send chat, reaction, share screen, etc.The meeting will end whenever a select participant, acting as host end the meeting. Unless if individual participant chooses to leave themselves, then the client will disconnect them.  Non-functional :  High availability, it would be an AP system in the context of CAP theorem. Scalable horizontally, because meetings are typically participated by people around the worlds. To suit with business constraints, we can limit the meeting size or duration. Meeting should be maximum of 100 people with 60 minutes limit. Assuming a 2 Mbps video bit rate and 128 Kbps audio quality, we would need 2,128 Mbps (per second). Bandwidth of share screen is same as normal stream. If we consider 80 people per meeting and 60 minutes (3,600 seconds) in every meeting, we would need 2,128 × 80 × 3600 = 612,864 Mb or ≈ 612 GB per meeting. If we calculate again with the same number, but consider some people turning off their cameras and audio, say 20 people open camera and 4 people actively speaking, then it would be a significant reduce to 145,843.2 MB or ≈ 145 GB per meeting. This is a rough estimation, as the real meeting may very on the numbers and actual network condition, which can reduce the bit rate.  High Level Design​    Meeting client act as the codecs for video/audio streamed by participant. The codec can be something like AVC or SVC.When a meeting is started, a logical meeting room is created on the meeting server which the participant can connect to.Video are sent to meeting server that communicates and synchronizes with each other. The meeting server act as central hub for gathering and combining participant video, which is then sent back to each participant.The meeting server will have a connection to a cloud server (e.g., AWS) that stores data, such as user profiles, user settings, chat logs, and meeting recording.If the participant share their screen, the video stream will be their window instead of the webcam. This mean the client will also handle the recording.Simple data such as chat or reaction can be sent to the server using standard protocol such as HTTPS. The client will also keep track of these.Video stream is sent using RTP, specifically the secure RTP.  More Detail​    Use as many libraries as possible to reduce development time and cost.  Client : The client that will either connect to meeting or web server. Basically we can use any programming languages, let's say we are using Kotlin Compose Multiplatform.We can use this JCodec library for codec. The codec should be adaptive depending on network condition.We can use libjitsi for media library (RTP protocol implementation).Find a way to set up the streams. We can use JavaFX media API to handle the capture of video, audio, and share screen.We can use Ktor as HTTP client.Client record and upload to the meeting server on-demand. Meeting Server : A server dedicated to handle meeting. One choice of backend library is Ktor.Client connect to the meeting server through some endpoint with meetingId and passcode as the parameters, initially using HTTP.If there exist such ID with the correct password, then client and server will start exchanging streams, also upgrading the protocol to RTP.The server listens to participants that broadcast their video, and also shares the data with other meeting server. Received broadcast is gathered and sent to all nearby participants.Could use message broker technology like RabbitMQ to handle the distribution of messages and events among meeting server instances. Web Server : We will have another server for serving web-based client. This server will also be used for serving request that aren't meeting related, such as authentication. Provide REST API services. Similarly, we can use Ktor for this. Ktor provide a simple way to create authentication, serialization, and other server stuff.Also, deploy the Compose Multiplatform app for the web. Cloud Server : Used for database. There are many options, such as Amazon S3 for video, Amazon DynamoDB for user's data, and Amazon CloudFront for static data. API keys or any configuration to connect to the cloud can be saved on meeting and web server.As users upload their profile photos, they will be stored on the CDN server, and the user information will include the specific URL for the photo.  Object &amp; API Design​    That's all I currently have in mind. Additional properties, methods, and endpoints could be included in the implementation.  We store only the necessary information in these objects, additional information may exist in the database schema. User is a logical representation of a user. Participant is derivative of User, representing a participant in the meeting. ParticipantConnection represent a client connection, which can be established using connect method. It has a startStream method that takes a stream of bytes to be sent to the meeting server. The meeting is represented as a Meeting, and it has a broadcastStream method, which will send all the received streams from individual participants to every other participant. User related data could be retrieved via /user endpoint. Web server or client may provide a way to see ongoing / ended meeting history, so they will be stored as well, and accessed via /meeting endpoint.  Scale &amp; Other Measures​  Horizontal Scaling : We can indeed scale the system horizontally by adding more meeting server. By default, we should connect user to the nearest meeting server. In some cases, we may sacrifice proximity by using a load balancer to balance the workload.Availability : The important thing is the meeting server, they should have high availability. We can implement failover mechanism to redirect traffic to healthy instances in case of server failures. Use service like AWS backup to back up important data such as meeting recording.Caching : Cacheable data such as web page static assets, meeting information, and profile photo can be cached to CDN server. ","version":"Next","tagName":"h3"},{"title":"Version Control","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/version-control","content":"","keywords":"","version":"Next"},{"title":"Basic Functions​","type":1,"pageTitle":"Version Control","url":"/cs-notes/software-engineering/version-control#basic-functions","content":" Version control works by tracking and managing changes to files and directories over time. A typical usage version control system (VCS) is as follows.  Initialization​  Version control process starts by initializing a repository. The repository is a central storage location that will hold all the files and their revision history. This can be a local repository on a developer's machine or a remote repository hosted on a server, although, both can be used together.  A team collaboration typically has a central remote repository. Any changes are made locally before &quot;pushing&quot; it to the main code in the remote repository.  Adding and Committing​  Developers begin by adding files to the repository or selecting existing files that they want to track. They will add the file to the staging area, which is an intermediate step before actually saving the changes. After adding files to the staging area, developers can commit the changes. This creates a new version of the file in the repository. Each commit is accompanied by a commit message that describes the changes made.  Diffing​  The VCS knows if a file is changed by using diff algorithm. The algorithm compares two input (the last committed and current file) and see the changes. If there is a change to committed file, the new changes are only the change made after committing it.    The above is example of difference of the same file. The - indicates of deletion and + indicates addition in the new file. If you notice there aren't significant difference, because the changes made is just adding two spaces to the end of the line.  Branching​  Version control systems often support branching, which is a separate version of the main repository. Developers typically create branches to work on different features, bug fixes, or experiments independently of the main codebase. This is to avoid bugs or errors that may occur in the main code, ensuring it can always work smoothly.  Once the branch is completes, it can be merged to the main repository. Merge means we are combining the code from the branch to another branch (i.e., the main branch itself).  Similar to main repository, each branch has its own copy of the files and its own revision history. With some configuration, the branch history can also be combined to main.   Source : https://towardsdatascience.com/a-quick-primer-to-version-control-using-git-3fbdbb123262  Conflicts​  However, the merge process may not happen smoothly. Consider a case where two independent developers working on the same issue on a different branch. Days later, they have completed their work and they wanted to merge it with the main branch. Let's say developer A merged the code first. Now, what if developer B were to merge after? What if developer B's changes overlap with the changes made by Developer A. It is definitely possible that these two have different way to solve the problem.  This is a common issue in collaboration projects. Conflicts are general issue where the VCS can't resolve automatically the differences between two commits. Solving conflict typically require human intervention to solve (e.g., deciding themselves which code should be used).  Furthermore, consider a scenario becomes developer A fixing bugs and developer B making new feature. At the start of their work, they own the latest copy of main code repository. Let's say developer A finishes their work first and he/she merged it directly with the main code. If developer B merge their code, a conflict will occur and the merge will be rejected. This is because at the time, developer B still owns the old main code, before developer A merges his/her code.  This conflict is typically easier to solve. As long as they are working on different tasks, and their changes do not overlap, Developer B can simply pull or fetch the latest main code before merging it.   Source : https://www.datacamp.com/tutorial/how-to-resolve-merge-conflicts-in-git-tutorial  ","version":"Next","tagName":"h3"},{"title":"Terminologies​","type":1,"pageTitle":"Version Control","url":"/cs-notes/software-engineering/version-control#terminologies","content":" Baseline : Specific version of a file that serves as a reference point or a stable snapshot of the codebase.Master or Main : A common term for the name of the default branch (typically containing the main codebase) in VCS.Blame : Identify the author or contributor responsible for the contribution of a particular line or section of code.Checkout : Switching to a different branch or a specific commit in a version control system.Clone : Creating a copy of a repository, including all of its files, commit history, and branches.Fork : Creating a personal copy of a repository, including all its files, history, and branches.Push : Upload local commits and changes to a remote repository.Pull or Fetch : Retrieve the latest changes from a remote repository.Head : Reference to the currently checked-out commit or branch, representing the latest state of the code that we are currently working on.  info The most commonly used version control system is Git. Not to be confused it is different with GitHub, a web-based platform that provides hosting services for Git repositories.  In systems like Git &amp; GitHub, another common terminology are :  Pull request : A request made by developer that wants to merge their code into another branch. It's called pull request because we are requesting for the branch to pull our code. Pull request is typically reviewed and discussed before accepted.Issues : Issues are task, bug report, or feature request that is tracked and managed within a project. Issues serve as a way to document, discuss, and track work items or problems related to a software project.  tip See also open sourcing for more information about collaboration and contributing. ","version":"Next","tagName":"h3"},{"title":"Structural Patterns","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/structural-patterns","content":"","keywords":"","version":"Next"},{"title":"Facade​","type":1,"pageTitle":"Structural Patterns","url":"/cs-notes/software-engineering/structural-patterns#facade","content":" Facade pattern advises simplifying a complex system by providing a unified and straightforward interface to the client. The system should feature an interface functioning as a &quot;facade&quot; or entry point to hide complexity underneath.  The point is to simplify complex interaction within the system. Complex interaction can occur in single or multiple class. For example, a computer consist of CPU, memory, and hard drive. A non-facade way of doing this would be :  class CPU { fun processData() {} } class Memory { fun load() {} } class HardDrive { fun read() {} } fun main() { val cpu = CPU() val memory = Memory() val hardDrive = HardDrive() cpu.processData() memory.load() hardDrive.read() }   The problem of this code is, the user require understanding about the three classes. They need to know that they must construct the specific CPU, Memory, and HardDrive class. Furthermore, they must know which method from which class should be called first.  Following facade pattern, instead of constructing class and calling these methods individually, we could make another class (e.g., Computer class) dedicated to handle it.  class Computer { val cpu = CPU() val memory = Memory() val hardDrive = HardDrive() fun run() { cpu.processData() memory.load() hardDrive.read() } } fun main() { val computer = Computer() computer.run() }   ","version":"Next","tagName":"h3"},{"title":"Adapter​","type":1,"pageTitle":"Structural Patterns","url":"/cs-notes/software-engineering/structural-patterns#adapter","content":" Adapter pattern allows objects with incompatible interfaces to work together. It acts as a bridge between two incompatible interfaces, converting required code in order for them to collaborate.  interface Charger { fun charge() } class EuropeanCharger { fun plugIn() { println(&quot;Plugged in European charger&quot;) } } class AmericanCharger { fun plugIn() { println(&quot;Plugged in American charger&quot;) } } class Phone(val charger: EuropeanCharger) { fun chargePhone() { charger.plugIn() } } fun main() { val europeanCharger = EuropeanCharger() val phone = Phone(europeanCharger) phone.chargePhone() }   This code illustrates phone that we can charge using one of the two chargers, EuropeanCharger and AmericanCharger. This code assumes that both charger are incompatible with each other, and they can't implement Charger interface directly.  To make phone compatible with any type of charger, we will introduce two adapter class for AmericanCharger and EuropeanCharger.  class EuropeanChargerAdapter(private val charger: EuropeanCharger) : Charger { override fun charge() { // do complex convertion here... charger.plugIn() println(&quot;Charging using European charger (adapted)&quot;) } } class AmericanChargerAdapter(private val charger: AmericanCharger) : Charger { override fun charge() { // do complex convertion here... charger.plugIn() println(&quot;Charging using American charger (adapted)&quot;) } } class Phone(var charger: Charger) { fun chargePhone() { charger.charge() } } fun main() { val europeanCharger = EuropeanCharger() val europeanChargerAdapter = EuropeanChargerAdapter(europeanCharger) val phone = Phone(europeanChargerAdapter) phone.chargePhone() val americanCharger = AmericanCharger() val americanChargerAdapter = AmericanChargerAdapter(americanCharger) phone.charger = americanChargerAdapter phone.chargePhone() }   The phone now takes charger that implements Charger interface. The adapter class is dedicated for the conversion between specific type of charger to a general type of charger.  ","version":"Next","tagName":"h3"},{"title":"Bridge​","type":1,"pageTitle":"Structural Patterns","url":"/cs-notes/software-engineering/structural-patterns#bridge","content":" Bridge pattern decouples an abstraction from its implementation, allowing them to vary independently.  Consider a Shape interface that can be drawn on the screen. A shape can be Square or Circle, which are abstract classes.  interface class Shape() { fun draw() } abstract class Square: Shape() { abstract fun draw() } abstract class Circle: Shape() { abstract fun draw() }   Now, assume there are two method to render the shape on the screen, vector and raster. If we were to render Square using vector method, we would need to create SquareVector that implements Square abstract class. The same for the remaining, SquareRaster, CircleVector, and CircleRaster.  class SquareRaster(): Square { override fun draw() {} } class SquareVector(): Square { override fun draw() {} } class CircleRaster(): Circle { override fun draw() {} } class CircleVector(): Circle { override fun draw() {} }   As many shape and type of rendering available, the class hierarchy becomes exponentially large. If we have triangle shape, we would need two additional classes, TriangleRaster and TriangleVector.  Bridge pattern solve the problem by switching inheritance to composition. Instead of making different renderer class for each type of shape, we associate a shape with a type of renderer. This effectively split the class hierarchy, which we can develop independently.  The Renderer hierarchy :  interface Renderer { fun render() } class VectorRenderer : Renderer { override fun render() {} } class RasterRenderer : Renderer { override fun render() {} }   The Shape hierarchy :  abstract class Shape(val renderer: Renderer) { abstract fun draw() } class Circle(val renderer: Renderer) : Shape(renderer) { override fun draw() { renderer.renderShape() } } class Square(private val renderer: Renderer) : Shape(renderer) { override fun draw() { renderer.renderShape() } }   Usage example :  fun main() { val vectorRenderer = VectorRenderer() val rasterRenderer = RasterRenderer() val circle = Circle(vectorRenderer) circle.draw() val square = Square(rasterRenderer) square.draw() }   If we add Triangle class, then we would only be adding one additional class in the Shape hierarchy.  ","version":"Next","tagName":"h3"},{"title":"Decorator​","type":1,"pageTitle":"Structural Patterns","url":"/cs-notes/software-engineering/structural-patterns#decorator","content":" Decorator pattern allows us to add new behavior to an object dynamically. It is useful when you want to modify the functionality of an object without changing its underlying structure.  Consider a Pizza class that lets us order pizza with different type of toppings.  class Pizza { fun pepperoni() { println(&quot;Ordered pizza with pepperoni&quot;) } fun cheese() { println(&quot;Ordered pizza with cheese&quot;) } fun pepperoniWithCheese() { println(&quot;Ordered pizza with pepperoni, cheese&quot;) } }   There are three possible topping, pepperoni, cheese, or pepperoni with cheese. As many toppings available, the combination of different toppings causes the method to grows up exponentially.  Decorator pattern solve this problem by making class that can extend the plain pizza class with new toppings.  The Pizza class become an interface, and the default implementation would be PlainPizza.  interface Pizza { fun getDescription(): String } class PlainPizza : Pizza { override fun getDescription(): String { return &quot;Plain Pizza&quot; } }   The topping will become the decorator for Pizza :  class PepperoniDecorator(val pizza: Pizza) : Pizza { override fun getDescription(): String { return pizza.getDescription() + &quot;, Pepperoni&quot; } } class CheeseDecorator(val pizza: Pizza) : Pizza { override fun getDescription(): String { return pizza.getDescription() + &quot;, Cheese&quot; } }   Each decorator wraps around a Pizza object and adds its specific topping to the description.  fun main() { val plainPizza: Pizza = SimplePizza() val pizzaWithPepperoni: Pizza = PepperoniDecorator(plainPizza) val pizzaWithPepperoniAndCheese: Pizza = CheeseDecorator(pizzaWithPepperoni) println(plainPizza.getDescription()) // printed Plain Pizza println(pizzaWithPepperoni.getDescription()) // printed Plain Pizza, Pepperoni println(pizzaWithPepperoniAndCheese.getDescription()) // printed Plain Pizza, Pepperoni, Cheese }   ","version":"Next","tagName":"h3"},{"title":"Composite​","type":1,"pageTitle":"Structural Patterns","url":"/cs-notes/software-engineering/structural-patterns#composite","content":" Composite pattern allows us to treat individual objects and groups of objects uniformly.  Let's say we are grouping objects together. However, these objects have their own way of doing things, they do not behave similarly. The composite pattern solves the problem by enforcing individual objects to implement a common interface or base class. This is useful particularly where we want to group objects together, such as a tree-like structure.  class Computer { var totalCosts = 0 } class Processor(val core: Int) { fun getCost(): Int { return 1000 * core } } class Memory(val sizeMB: Int) { fun getCost(): Int { return 2 * sizeMB } } class HardDrive(val sizeGB: Int) { fun getCost(): Int { return 50 * sizeGB } } fun main() { val processor = Processor(4) // A processor with 4 core val memory = Memory(1024) // A 1024 MB RAM val hardDrive = HardDrive(20) // A 20 GB hard disk val computer = Computer() computer.totalCosts += processor.getCost() computer.totalCosts += memory.getCost() computer.totalCosts += hardDrive.getCost() }   The processor, memory, and hard drive doesn't have clear abstraction. These objects are not supposed to be scattered, the computer is supposed to own them. To group these objects together, they could implement a unified interface like Hardware, and the computer will own a list of hardware.  interface Hardware { fun getCost(): Int } class Processor(val core: Int): Hardware { override fun getCost(): Int { return 1000 * core } } class Memory(val sizeMB: Int): Hardware { override fun getCost(): Int { return 2 * sizeMB } } class HardDrive(val sizeGB: Int): Hardware { override fun getCost(): Int { return 50 * sizeGB } } class Computer { val hardwares = mutableListOf&lt;Hardware&gt;() fun addHardware(hardware: Hardware) { hardwares.add(hardware) } fun getTotalCost(): Int { total = 0 for (hardware in hardwares) { total += hardware.getCost() } return total } }   Now that these pieces of hardware implement a common interface, we can treat them as a single unit. We can calculate the total cost by calling getTotalCost(), which will calculate each cost of individual hardware with getCost(). ","version":"Next","tagName":"h3"},{"title":"Software Principles","type":0,"sectionRef":"#","url":"/cs-notes/software-engineering/software-principles","content":"","keywords":"","version":"Next"},{"title":"OOP​","type":1,"pageTitle":"Software Principles","url":"/cs-notes/software-engineering/software-principles#oop","content":" Object-Oriented Programming (OOP) is a common paradigm used in the world of software engineering. The elements of OOP such as classes, objects, inheritance, polymorphism, encapsulation, and many more promotes code organization, reusability, and modularity in a software system. These elements serve as the foundation of other software principle that we will see later.  We will see an example that demonstrate the benefits OOP about code reusing (in Kotlin programming language).  Non-OOP approach :  val employee1Name = &quot;John Doe&quot; val employee1Age = 30 val employee1Department = &quot;Sales&quot; val employee2Name = &quot;Jane Smith&quot; val employee2Age = 35 val employee2Department = &quot;Marketing&quot; fun introduceEmployee(name: String, age: Int, department: String) { println(&quot;Hello, my name is $name. I am $age years old and work in the $department department.&quot;) } introduceEmployee(employee1Name, employee1Age, employee1Department) introduceEmployee(employee2Name, employee2Age, employee2Department)   OOP approach :  class Employee(val name: String, val age: Int, val department: String) { fun introduce() { println(&quot;Hello, my name is $name. I am $age years old and work in the $department department.&quot;) } } val employee1 = Employee(&quot;John Doe&quot;, 30, &quot;Sales&quot;) val employee2 = Employee(&quot;Jane Smith&quot;, 35, &quot;Marketing&quot;) employee1.introduce() employee2.introduce()   The code is about storing employee's data and have a function that can introduce them. The Non-OOP approach require us to make a bunch of variable just to store the data, while in OOP approach, we can make a class and represent the employee as an Employee object. Class and objects provides a structured and organized way to encapsulate related data and behavior, leading to more maintainable code.  ","version":"Next","tagName":"h3"},{"title":"DRY​","type":1,"pageTitle":"Software Principles","url":"/cs-notes/software-engineering/software-principles#dry","content":" DRY (Don't Repeat Yourself) is a software development principle that promotes code reuse and avoids duplication. The DRY principle is stated as &quot;Every piece of knowledge must have a single, unambiguous, authoritative representation within a system&quot;.  When code is repeated in multiple places, it becomes harder to update or modify because changes need to be made in multiple locations, increasing the chances of introducing bugs and inconsistencies. When a change is required, it only needs to be made in one place, making maintenance and updates more efficient and less error-prone.  Non-DRY approach :  fun main() { println(&quot;Hello, Alice!&quot;) println(&quot;Hello, Bob!&quot;) println(&quot;Hello, Charlie!&quot;) }   DRY approach :  fun greet(name: String) { println(&quot;Hello, $name!&quot;) } fun main() { val names = listOf(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;) for (name in names) { greet(name) } }   The code is about storing names and having a function to greet them. Non-DRY approach manually call println thrice with the name data. If we somehow wanted to change the greet message, we will need to modify all the three line of code. The DRY approach helps us to avoid printing the greeting message by putting it all in one function, and then integrate it with a for loop.  The previous OOP example also follows this DRY principle.  ","version":"Next","tagName":"h3"},{"title":"KISS​","type":1,"pageTitle":"Software Principles","url":"/cs-notes/software-engineering/software-principles#kiss","content":" KISS (Keep It Simple, Stupid) is a software development principle that promotes emphasizes the importance of simplicity. The principle suggests that systems and code should be kept as simple as possible, avoiding unnecessary complexity.  An example of unnecessary abstracted code :  class Parity(val num: Int) { var isEven = false fun updateIsEven() { if (decideIsEven(num)) { numIsEven() } else { numIsNotEven() } } fun numIsEven() { isEven = true } fun numIsNotEven() { isEven = false } fun decideIsEven(n: Int): Boolean { if (n % 2 == 0) { return true } else { return false } } fun getIsEven(): Boolean { return isEven } } fun main() { val parity = Parity(3) parity.updateIsEven() println(&quot;Is 3 even?: ${parity.getIsEven()}&quot;) }   This code creates a class that is supposed to handle the parity of a number. The purpose of introducing a class in this code is to create an abstraction, this way the function caller doesn't need to handle the parity logic. In reality, the handling of parity is actually very simple and doesn't need to be overly abstracted like above. The checks for num % 2 == 0 to determine if a number is even is a relatively well-known concept.  We don't need to make function for every single expression we are executing. Making a function just to set a field variable is unnecessary. Making a getter to get the isEven variable is also not needed, because in Kotlin properties can have implicit getters generated automatically.  ","version":"Next","tagName":"h3"},{"title":"YAGNI​","type":1,"pageTitle":"Software Principles","url":"/cs-notes/software-engineering/software-principles#yagni","content":" YAGNI (You Ain't Gonna Need It) principle suggests developers should avoid adding functionality until it is necessary. It is typically associated with agile development methodology, where the development style revolve around quick incremental software development.  Its main idea is to focus on delivering the minimum viable product (MVP) or the necessary features to meet the immediate requirements, rather than adding functionality that may or may not be needed in the future.  There are some factors to consider when developing functionality :  Development Costs : Resources required to build the software, such as developers' salaries, project management expenses, software licenses, development tools, and infrastructure costs.Maintenance and Repair Costs : There is a potential that the functionality may need bug fixes, updates, security patches, or technical support.  Unnecessary development sacrifice time and effort for future development and maintenance, possibly introducing technical debt. Technical debt is a consequence of software development bad decisions that keep accumulating costs and challenges as the software evolves, similar to financial debt that accumulates interest over time.  ","version":"Next","tagName":"h3"},{"title":"Coupling & Cohesion​","type":1,"pageTitle":"Software Principles","url":"/cs-notes/software-engineering/software-principles#coupling--cohesion","content":" Both describe the relationship and dependency between software components. Coupling refers to the inter-component interaction, while cohesion refers to the intra-component interaction.  Cohesion is the degree to which elements within a component or module are related and work together to accomplish a single, well-defined purpose.  High Cohesion : High cohesion indicates that the elements within a component are closely related and focused on a specific functionality or responsibility. A highly cohesive component performs a single task or represents a cohesive set of related tasks, making it easier to understand, reuse, and maintain.Low Cohesion : Low cohesion occurs when a component has multiple unrelated responsibilities or tasks. This can lead to a component that is difficult to understand, modify, and test. Low cohesion may also result in code duplication and reduced reusability.  On the other hand, coupling describe the interaction between modules.  Loose Coupling : In a loosely coupled system, components are independent and interact through well-defined interfaces. Loose coupling principle encourages components to have minimal dependencies on each other. Changes to one component have minimal impact on other components, making the system more flexible, modular, and easier to maintain.Tight Coupling : Tight coupling occurs when components have strong dependencies and rely heavily on each other's internal details or implementation. In a tightly coupled system, changes in one component may require corresponding changes in multiple other components. This can lead to code that is harder to modify, test, and maintain.   Source : top, down  Below are code that demonstrate cohesion and coupling.  Low Cohesion :  class Customer { fun calculateOrderTotal(order: Order) {} fun sendOrderConfirmationEmail(order: Order) {} fun updateCustomerInformation(customerData: CustomerData) {} }   High Cohesion :  class OrderProcessor { fun processOrder(order: Order) { validateOrder(order) calculateOrderTotal(order) updateInventory(order) notifyCustomer(order) } private fun validateOrder(order: Order) {} private fun calculateOrderTotal(order: Order) {} private fun updateInventory(order: Order) {} private fun notifyCustomer(order: Order) {} }   The class in low cohesion code have their own responsibility within the class. On the other hand, the high cohesion code focuses on the processOrder method, where it calls the other private method.  The high cohesion code may be preferable as it makes the caller easier to interact with the class. The caller can just call the method processOrder passing in an Order to process it. In contrast, in the low cohesion code, caller requires an understanding of how Customer class process order. (i.e., should we call sendOrderConfirmationEmail or updateCustomerInformation first?)  info We typically call component that invokes or calls a function or method caller, while the one that is being invoked or called is called callee.  Tight Coupling :  class UserService { private val userRepository = UserRepository() fun getUserById(userId: String): User { return userRepository.getUserById(userId) } fun saveUser(user: User) { userRepository.saveUser(user) } } class UserRepository { fun getUserById(userId: String): User {} fun saveUser(user: User) {} }   Loose Coupling :  interface UserRepository { fun getUserById(userId: String): User fun saveUser(user: User) } class UserService(private val userRepository: UserRepository) { fun getUserById(userId: String): User { return userRepository.getUserById(userId) } fun saveUser(user: User) { userRepository.saveUser(user) } }   The UserService class in tight coupling code depends a lot on UserRepository class. By depend, it means that the class really need to suit their method and the way of how it operates based on the class it depends on. If somehow we want to change the behavior of UserRepository, we will need to change the UserService as well. Tight coupling decrease flexibility and maintainability.  On the other hand, the loose coupling code introduce an interface, which allows for different implementation of UserRepository to be provided for UserService class. The UserService depend on the interface, and the implementor adhere to it. A modification to UserRepository behavior doesn't require us to modify many things, this is because they are based on the interface contract.  The contract is nothing but a specification that the method getUserById must take a userId of type String and return an object User, similar to the saveUser method. This allows us to create a variety of class that may have different way of processing the data internally.  ","version":"Next","tagName":"h3"},{"title":"Law of Demeter (LoD)​","type":1,"pageTitle":"Software Principles","url":"/cs-notes/software-engineering/software-principles#law-of-demeter-lod","content":" LoD (Law of Demeter) is a design principle that promotes loose coupling and encapsulation in object-oriented programming. The principle states that an object should have limited knowledge about other objects and should only interact with its immediate neighbors.  There are three principles of LoD, based on Wikipedia :  Each unit should have only limited knowledge about other units: only units &quot;closely&quot; related to the current unit.Each unit should only talk to its friends; don't talk to strangers.Only talk to your immediate friends.  An object could have dependency to other object, meaning it relies on another object to fulfill certain tasks or provide specific functionality. We call the object that it depends on a friend or neighbor.  The principle states that an object should have limited knowledge about its friend. One of the benefits of an object with limited knowledge about its companion can be seen in the case of designing a public API. A random person using your API shouldn't need to understand the inner working of your API too much, because that is the point of API, to provide a high level of abstraction.   Source : https://blog.knoldus.com/the-law-of-demeter/  Let's say we are making an API that lets user (developer) to store book name data.  val dm = DataManager() dm.saveData(&quot;Software Engineering 9th Edition by Ian Sommerville&quot;)   We can save data by just calling saveData method. Now under the hood, the API may use SQLite database to actually store the data.  class DataManager(private val databaseManager: DatabaseManager) { fun saveData(data: String) { // preprocess the data... databaseManager.saveToDB(data) } } class DatabaseManager(private val db: SQLite) { fun saveToDB(data: String) { db.initialize() db.syncWithRemoteServer() // other database setup here... db.save(data) } }   The point of this API is to abstract away the process of saving data to database. DataManager is responsible for handling data, including preprocessing it and saving it to the database. The process of saving to database is abstracted again by DatabaseManager, it doesn't even know which database it is interacting to. Finally, the DatabaseManager choose SQLite as its database, and it contains the actual code that interact with database.  The application of LoD principle in this example can be seen in the DataManager class. It considers SQLite as a stranger, and they don't interact directly. They don't even know each other detail, DataManager is not responsible for instantiating the database.  ","version":"Next","tagName":"h3"},{"title":"Separation of Concerns (SoC)​","type":1,"pageTitle":"Software Principles","url":"/cs-notes/software-engineering/software-principles#separation-of-concerns-soc","content":" Separation of Concerns (SoC) is a principle for dividing a complex system into distinct and independent parts, where each part addresses a specific concern or responsibility.  Concern is a specific aspect or responsibility of a software system. It represents a distinct functionality or set of related behaviors that can be identified and separated from other parts of the system. For example, in a web application, concerns could include user authentication, database access, business logic, user interface rendering, logging, and error handling. These are general concern, concern can be as specific as &quot;the name of which class to instantiate&quot;.  In SoC, a software system should be divided into modules or components, and each module should be responsible for a single concern or functionality. In the example of Law of Demeter, DataManager chose to not handle database connection. It instead communicates with an intermediary, which is DatabaseManager. The example demonstrates separation of concerns, where we delegate the database related task to class that is supposed to handle these.  SoC allows for code to be modular. If DataManager were to communicate directly with SQLite database, then if there exist another class that interact with database, we would need to repeat the similar code (i.e., setting up the database). Modularity allows for code reuse, as individual modules can be used in different contexts or projects.  ","version":"Next","tagName":"h3"},{"title":"Dependency Injection​","type":1,"pageTitle":"Software Principles","url":"/cs-notes/software-engineering/software-principles#dependency-injection","content":" Dependency Injection (DI) is a technique used to achieve loose coupling and modularity in software engineering. If an object depends on another object, the object must be &quot;injected&quot; into the class rather than the class creating them internally.  Dependency injection is typically done through interface and provided via constructor. To demonstrate, consider this example.  Without DI :  // Service interface interface MessageService { fun getMessage(): String fun isMessageAvailable(): Boolean } // Implementation of MessageService class EmailService : MessageService { override fun getMessage(): String { return &quot;Email message&quot; } override fun isMessageAvailable(): String { return true // for simplicity, let's say message is always available } } // Class that directly creates an instance of EmailService class MessageBroadcaster { private val messageService: MessageService = EmailService() fun broadcast() { while (messageService.isMessageAvailable()) { val message = messageService.getMessage() println(&quot;Broadcast: $message&quot;) } } } fun main() { // Using MessageProcessorWithoutDI directly val broadcaster = MessageBroadcaster() broadcaster.broadcast() }   A message service is a service that should provide message, we should be able to retrieve the message using getMessage() and check its availability using isMessageAvailable(). Let's say we are making an email service, so we will make a class called EmailService that implements MessageService.  A message broadcaster is supposed to broadcast the latest message from the service. A message broadcaster without DI would instantiate its own service inside the class (look at private val messageService: MessageService = EmailService()).  The problem without DI arise when we wanted to have different instance of service, maybe EmailService and UserService. We can either make an entirely new broadcaster class or just instantiate an instance of UserService inside MessageBroadcaster. However, the latter is probably not an ideal solution. It introduces tight coupling and makes it difficult to replace or switch to a different implementation without modifying the class itself.  With DI :  // Service interface interface MessageService { fun getMessage(): String fun isMessageAvailable(): Boolean } // Implementation of MessageService class EmailService : MessageService { override fun getMessage(): String { return &quot;Email message&quot; } override fun isMessageAvailable(): String { return true // for simplicity, let's say message is always available } } // MessageService in constructor now class MessageBroadcaster(private val messageService: MessageService) { fun broadcast() { while (messageService.isMessageAvailable()) { val message = messageService.getMessage() println(&quot;Broadcast: $message&quot;) } } } class UserService: MessageService { /* ... */ } fun main() { // Creating an instance of EmailService (dependency) val emailService = EmailService() // Injecting the dependency into MessageBroadcaster val emailBroadcaster = MessageBroadcaster(emailService) // Easily create another type of MessageBroadcaster val userService = UserService() val userBroadcaster = MessageBroadcaster(userService) // Using MessageBroadcaster with injected dependency emailBroadcaster.processMessage() userBroadcaster.processMessage() }   The MessageBroadcaster no longer instantiates an instance of MessageService itself; instead, it accepts whatever is provided to it through the constructor. This increases flexibility and allows us to create different types of MessageBroadcaster. We can make implementation of MessageService as we want, and provide it to MessageBroadcaster easily.  ","version":"Next","tagName":"h3"},{"title":"Composition Over Inheritance​","type":1,"pageTitle":"Software Principles","url":"/cs-notes/software-engineering/software-principles#composition-over-inheritance","content":" In OOP, there is a concept called inheritance. Inheritance allows a class to inherit all the properties and behaviors from a parent class. While inheritance can be useful in some cases, it can also lead to a rigid and inflexible class hierarchy. Inheritance creates a tight coupling between classes, making it difficult to modify or extend the behavior of a class without affecting other classes in the hierarchy.  Inheritance is often associated with an &quot;is-a&quot; relationship. When a class inherits from another class, it is stating that the derived class is a specialized version of the base class. The derived class shares the characteristics and behaviors of the base class and adds additional features or overrides existing ones.  Composition, on the other hand, involves building complex objects by combining simpler objects or components. Instead of inheriting behavior from a parent class, an object is composed of other objects that provide the desired functionality. We often refer composition as a &quot;has-a&quot; relationship, signifying that one class has another class as part of its structure. This approach promotes loose coupling and flexibility in the design.  An example of code with inheritance :  open class Manufacturer(open val name: String) class Car( override val name: String, val model: String, val color: String ) : Manufacturer(name)   Inheritance is not suitable for this code. While a car is associated with a manufacturer, but a car doesn't have an &quot;is-a&quot; relationship. A car is not a manufacturer, but rather associated with particular manufacturer.  A scenario that would be suitable with inheritance is Person and Employee class. A person is a general representation of person, while an employee is a specialized person. An employee can definitely inherit a person and extend its properties and behavior.  Here's how Manufacturer and Car class would look like with composition :  class Manufacturer(open val name: String) class Car( val manufacturer: Manufacturer, val model: String, val color: String )   Instead of inheriting the manufacturer, we chose to include it inside car.  In summary, the principle of composition over inheritance doesn't mean we have use composition all the time. It suggests that, in certain cases, it is preferable to favor composition over inheritance when the class is actually composed of the other class. Inheritance is not the only choice when a class has particular characteristics of another class.  ","version":"Next","tagName":"h3"},{"title":"Clean Code​","type":1,"pageTitle":"Software Principles","url":"/cs-notes/software-engineering/software-principles#clean-code","content":" The definition of clean code typically refers to well-structured, readable, and maintainable code that follows best practices and conventions to make it is easy to understand and modify.  Some principles of clean code :  Following standard conventions (e.g., language or team standard).Following code principles such as KISS, DRY, etc.Following design principles such as dependency injection, Law of Demeter, etc.Pronounceable, searchable, descriptive, and meaningful naming of identifier.Function can be made small and focus on one logic at a time.Adding comment that explains &quot;why&quot; rather than &quot;what&quot;.  ","version":"Next","tagName":"h3"},{"title":"SOLID​","type":1,"pageTitle":"Software Principles","url":"/cs-notes/software-engineering/software-principles#solid","content":" SOLID principles is a set of five design principles aimed to make OOP code more understandable, flexible, and maintainable.  Single Responsibility Principle (SRP)​  A class should have only one reason to change, meaning it should have a single responsibility.  An Order class may have a calculateDiscount() method. Upon a successful order, we may want to send an email the customer. The email contains order information, which Order class have. It will be much easier to have sendEmail() method in the Order class, but this violates SRP principle. Violation of SRP introduces tight coupling and the mixing of unrelated code together. An alternative would be making another class, such as EmailSender, which is responsible for sending emails.  Open/Closed Principle (OCP)​  Software entities (classes, modules, functions, etc.) should be open for extension but closed for modification. This principle encourages designing systems that can be easily extended with new functionality without modifying existing code.  Some choices to implement OCP includes using interfaces, abstract classes, composition, dependency injection, and generics.  Liskov Substitution Principle (LSP)​  Objects of a superclass should be replaceable with objects of its subclasses without affecting the correctness of the program. This principle ensures that subtypes adhere to the behavior expected from their base types.  A common example illustrating LSP principle is Rectangle and Square class that inherits Shape.  open class Rectangle(val width: Int, val height: Int) { open fun setWidth(width: Int) { this.width = width } open fun setHeight(height: Int) { this.height = height } } class Square(val sideLength: Int) : Rectangle(sideLength, sideLength) { override fun setWidth(width: Int) { sideLength = width } override fun setHeight(height: Int) { sideLength = height } }   The LSP principle states that we should be able to treat subclass interchangeably with the superclass without having unexpected behavior. This mean we can treat Square as Rectangle. A rectangle have method setWidth and setHeight, treating Square as Rectangle would mean we can set its width and height independently. However, this violates the nature properties of square, where its side should be equal, and yet we change them independently.  A solution for this would be making Square to not inherit Rectangle, maybe inherit a more general class like Shape. Another way to solve this is simply updating the width in setHeight and updating the height in the setWidth method, although this sounds counter-intuitive.  Interface Segregation Principle (ISP)​  Clients should not be forced to depend on interfaces they do not use. This principle advises breaking down large interfaces into smaller, more focused ones, tailored to the specific needs of clients.  Consider a base class Animal, it could have method eat() and sleep(). We may have Dog, Cat, etc., that inherits the base class Animal. As our class hierarchy get larger, we may need more method such as fly(). However, some of animal may not be able to fly, such as penguin. Because penguin inherits Animal, we are required to implement it. A way to avoid this is to implement the method with meaningless body, but this violates the principle.  A way to adhere ISP principle is splitting the class or interface hierarchy into a smaller and specific ones. So, we could make a subclass FlyingAnimal that inherits Animal, and move the behavior related to flying inside the FlyingAnimal.  Dependency Inversion Principle (DIP)​  High-level modules should not depend on low-level modules. Both should depend on abstractions. This principle promotes loose coupling and allows for easier substitution of dependencies.  class EmailSender { fun sendEmail(message: String) { println(&quot;Sending email: $message&quot;) } } class NotificationService { private val emailSender = EmailSender() fun sendEmailNotification(message: String) { emailSender.sendEmail(message) } }   In this example, the NotificationService is the higher level module that we use that depends on EmailSender. The behavior or NotificationService is dictated by EmailSender, such as the requirement of passing message and calling the sendEmail method. Think of EmailSender as a public API, if we change its behavior (e.g., changing parameters), all the user using the NotificationService would need to change its behavior as well, which is not favorable. Furthermore, the cost of this become larger as we have a long chain of dependency within the system.  A code following dependency inversion would look like this :  interface MessageSender { fun sendMessage(message: String) } class EmailSender : MessageSender { override fun sendMessage(message: String) { println(&quot;Sending email: $message&quot;) } } class NotificationService(private val sender: MessageSender) { fun sendNotification(message: String) { sender.sendMessage(message) } }   The NotificationService now depend on the MessageSender interface, which can be implemented by various type of sender, including EmailSender. If somehow EmailSender need to change, all it needs is to follow the interface contract. It becomes decoupled from the specific implementation details of the message sending logic, allowing for greater flexibility and modularity.   Source : https://levelup.gitconnected.com/solid-programming-for-arduino-the-dependency-inversion-principle-4ce3bdb787d1 ","version":"Next","tagName":"h3"},{"title":"Theory of Computation & Automata","type":0,"sectionRef":"#","url":"/cs-notes/theory-of-computation-and-automata","content":"","keywords":"","version":"Next"},{"title":"All pages​","type":1,"pageTitle":"Theory of Computation & Automata","url":"/cs-notes/theory-of-computation-and-automata#all-pages","content":" TOC FundamentalsFinite AutomataRegular Languages (Part 1)Advanced AutomataRegular Languages (Part 2)Formal GrammarContext-Free GrammarPushdown AutomataTuring MachineChurch-Turing ThesisUndecidabilityChomsky HierarchyComputabilityComplexity-Theory ","version":"Next","tagName":"h3"},{"title":"Advanced Automata","type":0,"sectionRef":"#","url":"/cs-notes/theory-of-computation-and-automata/advanced-automata","content":"","keywords":"","version":"Next"},{"title":"Two-Way Finite Automata​","type":1,"pageTitle":"Advanced Automata","url":"/cs-notes/theory-of-computation-and-automata/advanced-automata#two-way-finite-automata","content":" Two-way finite automata (2DFA) have additional ability to move its read head in both directions on the input tape. It can read symbols from left to right and also move to the left to read symbols. Given an input string, it can advance forward to the end or back to the start.   Source : https://www.researchgate.net/figure/Representation-of-two-way-deterministic-finite-automata_fig1_345669547  A 2DFA is defined as octuple : M=(Q,Σ,∣−,−∣,δ,s,t,r)M = (Q, \\Sigma, |-, -|, \\delta, s, t, r)M=(Q,Σ,∣−,−∣,δ,s,t,r)  QQQ : finite set of statesΣ\\SigmaΣ : finite set of input alphabet∣−|-∣− and −∣-|−∣ : left and right endmarker, respectively, These symbols are not included in the input alphabet Σ\\SigmaΣ.δ:Q×(Σ∪{∣−,−∣})→(Q×{L,R})\\delta: Q \\times (\\Sigma \\cup \\{|-, -|\\}) \\rightarrow (Q \\times \\{L, R\\})δ:Q×(Σ∪{∣−,−∣})→(Q×{L,R}) :s∈Qs \\in Qs∈Q : start statet∈Qt \\in Qt∈Q : accept stater∈Qr \\in Qr∈Q : reject state, r≠tr \\ne tr=t  The transition function takes a state and a symbol it reads on the tape as argument, returning the new state as well as the direction to move the head, either LLL or RRR. If the machine reads a left or right endmarker, it will move to the opposite direction.  ","version":"Next","tagName":"h3"},{"title":"Finite Automata with Output​","type":1,"pageTitle":"Advanced Automata","url":"/cs-notes/theory-of-computation-and-automata/advanced-automata#finite-automata-with-output","content":" Some finite automata are able to produce output, it is defined as :  M=(Q,Σ,O,δ,λ,q0)M = (Q, \\Sigma, O, \\delta, \\lambda, q_0)M=(Q,Σ,O,δ,λ,q0​)  OOO or △\\triangle△ : finite output alphabetδ:Q×Σ→Q\\delta: Q \\times \\Sigma \\rightarrow Qδ:Q×Σ→Q : Similar to normal DFA, takes a state and alphabet, returns a new stateλ\\lambdaλ : output functionq0q_0q0​ : initial statesThe rest is same  There are two types of finite automata with output, Mealy machines and Moore machines. Both have the same definition, but they differ in output function. Mealy's output function is defined as λ:Σ×Q→△\\lambda: \\Sigma \\times Q \\rightarrow \\triangleλ:Σ×Q→△, while Moore's output function is defined as λ:Q→△\\lambda: Q \\rightarrow \\triangleλ:Q→△.  We can also define it with function notation, output function F(t)F(t)F(t) for Mealy is F(t)=λ(q(t),x(t))F(t) = \\lambda(q(t), x(t))F(t)=λ(q(t),x(t)). For Moore, F(t)=λ(q(t))F(t) = \\lambda(q(t))F(t)=λ(q(t)).  In other word, Mealy takes account current state and input symbol to produce an output, while Moore's output is only associated with states.   Source : https://youtu.be/EzMWYmRkHFQ?si=y1tFCwsE9c3BAyys&amp;t=599  warning Mealy and Moore machine doesn't have final state.  tip The 1/b1/b1/b on transition of Mealy machine means that on getting input &quot;1&quot;, produce output &quot;b&quot;. The A/aA/aA/a on state of Moore machine means that whenever we are at that particular state, produces output &quot;a&quot;.  The Moore machine that associates output with states implies the length of output string is always the length of input string + 1, as it takes account the initial state.  Mealy Machine example​   Source : https://youtu.be/LioitkXDfmA?si=EwGx0NP7AQsDp7bC&amp;t=534  The first example wants us to produces complement of the input string. We can do it easily by self-looping the input and make it produces whatever the complement is.Because Mealy machine has to produce an output for every transition function, we can produce &quot;b&quot; as the unwanted output, and only produce &quot;a&quot; if we got the sequence of input right.  Moore Machine example​   Source : https://youtu.be/MFnRF07SoFo?si=0l5W2_c2w2BGXoBE&amp;t=511  Similar to the Mealy machine, we produce &quot;b&quot; as the unwanted output.  Conversion of Moore to Mealy Machine​   Source : https://youtu.be/HEVWx4irOx4?si=OKG2BVRIcQowB9uD&amp;t=386  This example is the one we have seen on the Mealy and Moore machine example, it is fairly to convert between them. Just move the output to transition rather than the states.  Conversion of Mealy to Moore Machine​  For this particular example, we have to add new states BbB_bBb​ and CbC_bCb​, indicating it is state BBB that produces output &quot;b&quot;. The rest is just changing the association of output to states rather than to transition.   Source : https://youtu.be/-etILQcfgTg?si=DKocof81DknSFHna&amp;t=490  In summary, conversion of Moore to Mealy does not increase the number of state, while the opposite does.  ","version":"Next","tagName":"h3"},{"title":"Epsilon-NFA​","type":1,"pageTitle":"Advanced Automata","url":"/cs-notes/theory-of-computation-and-automata/advanced-automata#epsilon-nfa","content":" Epsilon-NFA (ε-NFA) is the extension of NFA that allows for ϵ\\epsilonϵ (epsilon) or empty string transition. The rest of definition is same as explained in NFA. However, the transition function for ε-NFA is now defined as δ:Q×Σ∪ϵ→2Q\\delta: Q \\times \\Sigma \\cup \\epsilon \\rightarrow 2^Qδ:Q×Σ∪ϵ→2Q.   Source : https://youtu.be/84oNUttWlN4?si=B2LxuA_aOE-XCKu8&amp;t=321  Every state is able to take epsilon as input, however it won't transition to anything if it's not defined.  Conversion of Epsilon-NFA to NFA​  To convert between ε-NFA to NFA, we have to follow the ε-closure, that is, listing the possible state that can be reached with the sequence of ε, input symbol, and ε again.   Source : https://youtu.be/WSGcmaHNBFM?si=RAMRSa3emKfvW77c&amp;t=271  For example, from state AAA, by inputting zero or more ϵ\\epsilonϵ, we can either remain at AAA itself, transition to BBB, or reach the final state CCC. Next, following the second sequence, input a symbol. If we input &quot;0&quot; in state AAA, we will remain at AAA, inputting &quot;0&quot; in state BBB would lead us nowhere, signified by the empty set, and with input &quot;0&quot; in state CCC, it will result in a self-loop.  This will be done for every state and defined transition on each.   Source : https://youtu.be/WSGcmaHNBFM?si=c9oiBEsxpPCYLwhk&amp;t=445  After filling up the table, determining which one will be the final state in the converted NFA is done by looking which states can reach the final state. In the example, state AAA, BBB, and CCC can reach the final state, so they are all going to be the final state in the converted NFA.   Source : https://youtu.be/WSGcmaHNBFM?si=pqyzMwwDEl0nLr3w&amp;t=567 ","version":"Next","tagName":"h3"},{"title":"Chomsky Hierarchy","type":0,"sectionRef":"#","url":"/cs-notes/theory-of-computation-and-automata/chomsky-hierarchy","content":"Chomsky Hierarchy Main Source : [] Chomsky Hierarchy page","keywords":"","version":"Next"},{"title":"Complexity-Theory","type":0,"sectionRef":"#","url":"/cs-notes/theory-of-computation-and-automata/complexity-theory","content":"Complexity-Theory Main Source : [] Complexity-Theory page https://en.wikipedia.org/wiki/Computational_complexity_theory","keywords":"","version":"Next"},{"title":"Church-Turing Thesis","type":0,"sectionRef":"#","url":"/cs-notes/theory-of-computation-and-automata/church-turing-thesis","content":"Church-Turing Thesis Main Source : [] Church-Turing Thesis page","keywords":"","version":"Next"},{"title":"Computability","type":0,"sectionRef":"#","url":"/cs-notes/theory-of-computation-and-automata/computability","content":"Computability Main Source : [] Computability page Reductionhttps://en.wikipedia.org/wiki/Computability_theory","keywords":"","version":"Next"},{"title":"Formal Grammar","type":0,"sectionRef":"#","url":"/cs-notes/theory-of-computation-and-automata/formal-grammar","content":"","keywords":"","version":"Next"},{"title":"Grammar​","type":1,"pageTitle":"Formal Grammar","url":"/cs-notes/theory-of-computation-and-automata/formal-grammar#grammar","content":" As explained in grammar, in general, a grammar is defined as quadruple : G=(V,T,S,P)G = (V, T, S, P)G=(V,T,S,P).  With grammar G1=({S,A,B},{a,b},S,{S→AB,A→a,B→b})G_1 = (\\{S, A, B\\}, \\{a, b\\}, S, \\{S \\rightarrow AB, A \\rightarrow a, B \\rightarrow b\\})G1​=({S,A,B},{a,b},S,{S→AB,A→a,B→b}), we have three categories of symbol, SSS (also known as the start symbol), AAA, and BBB. The category itself doesn't mean anything in language, the actual character that appears in the language are aaa and bbb. The production rules describe that the start symbol SSS can be replaced with ABABAB. The AAA and BBB itself can be replaced by aaa and bbb, respectively.  For example, starting from SSS S→ABS \\rightarrow ABS→AB (by S→ABS \\rightarrow ABS→AB) →aB\\rightarrow aB→aB (by A→aA \\rightarrow aA→a) →ab\\rightarrow ab→ab (by B→bB \\rightarrow bB→b)  The production rule of a general grammar is in the form of A→αA \\rightarrow \\alphaA→α, where AAA is a non-terminal symbol, and α\\alphaα is a string of terminal or non-terminal symbol (formally α={V∪T}\\alpha = \\{V \\cup T\\}α={V∪T}). The production rule allows the transformation from non-terminal symbols to another non-terminal or terminal symbols, making it possible to expand the language recursively.  tip The book also notate the production rule in the form of (V∪T)+→(V∪T)∗(V \\cup T)^+ \\rightarrow (V \\cup T)^*(V∪T)+→(V∪T)∗. In other word, one or more terminal or non-terminal symbol can be transformed to zero or more (so includes empty string) terminal or non-terminal symbol.  For example, when the production rule generates another start symbol, such as S→SABS \\rightarrow SABS→SAB, then, starting from SSS, we can produce SABSABSAB, then further apply the same production rule to obtain SABABSABABSABAB, and continue this process until we replace all non-terminal symbol to terminal symbol.  Example​  The process of generating language from grammar is also known as derivation. The set of all language generated by some grammar is denoted as L(G)L(G)L(G). The language generated by the previous grammar example, L(G1)L(G_1)L(G1​), consists of only a single string: {ab}\\{ab\\}{ab}.  Another example from the video :   Source : https://youtu.be/ejXgLRSIxsA?si=K8kIBdPOzhFgqsQW&amp;t=573  This grammar has a recursive production rule. We can replace AAA with aAaAaA, which mean we can potentially generate infinite amount of &quot;a&quot;'s. The language generated by this grammar creates a pattern ambna^m b^nambn, therefore the L(G)L(G)L(G) is a set of string with that patterns.  ","version":"Next","tagName":"h3"},{"title":"Regular Grammar​","type":1,"pageTitle":"Formal Grammar","url":"/cs-notes/theory-of-computation-and-automata/formal-grammar#regular-grammar","content":" Regular grammar is the grammar that describes regular language, which is the languages recognized by finite automaton like DFAs and NFAs.  Left &amp; Right Grammar​  The formal description of regular grammar is same as the general grammar, except the production rule follows either left or right types of grammar. With AAA, B∈VB \\in VB∈V and x∈Tx \\in Tx∈T, right and left grammar is in the form :  Right linear grammar : A→xB∣xA \\rightarrow xB|xA→xB∣x, example S→abS∣bS \\rightarrow abS|bS→abS∣b.Left linear grammar : A→Bx∣xA \\rightarrow Bx|xA→Bx∣x, example S→Sbb∣bS \\rightarrow Sbb|bS→Sbb∣b.  In right grammar, the production rule places non-terminal symbols on the right, while in a left grammar, it is the opposite.  Additionally,  info The &quot;|&quot; in the notation S→A∣aS \\rightarrow A|aS→A∣a is a shorthand to describe two S→AS \\rightarrow AS→A and S→aS \\rightarrow aS→a.  ","version":"Next","tagName":"h3"},{"title":"Context-Free Grammar​","type":1,"pageTitle":"Formal Grammar","url":"/cs-notes/theory-of-computation-and-automata/formal-grammar#context-free-grammar","content":" See the next topic. ","version":"Next","tagName":"h3"},{"title":"Finite Automata","type":0,"sectionRef":"#","url":"/cs-notes/theory-of-computation-and-automata/finite-automata","content":"","keywords":"","version":"Next"},{"title":"Finite State Machine​","type":1,"pageTitle":"Finite Automata","url":"/cs-notes/theory-of-computation-and-automata/finite-automata#finite-state-machine","content":" Finite state machine (FSM) or Finite automata is the simplest model of computation. It is an abstract machine and a type of automaton that consist of finite number of states and transitions between those states. The transitions between states is the response of the machine when taking certain input.  Components of FSM :  States : Represents the possible configurations or conditions of the system being modeled.Alphabet : A finite set of symbols or inputs (possibly a formal language) that the automaton can read or process.Transitions : A set of rules that specify how the automaton transitions from one state to another based on the input symbol it reads. Each transition is associated with a specific input symbol and defines the state change that occurs when that symbol is encountered.Start state : A designated initial state from which the automaton begins its operation.Accepting states : A subset of the states that are designated as accepting or final states. When the automaton reaches an accepting state after reading the entire input, it indicates that the input is accepted or recognized by the automaton. Not all finite automata have accepting states.   Source : https://brilliant.org/wiki/finite-state-machines/  An example of state machine is a vending machine. A vending machine is initially locked (start state). If it is being pushed, it kept locked (transition at locked state with &quot;push&quot; as input). If a coin is inserted, then it will be unlocked, and so on.  Acceptors​  An automaton that recognizes or accepts certain input strings is called acceptors. Acceptors don't take action as input, but instead an input string belonging to a specific language or set of strings.  The purpose of an acceptor is to determine whether the input string belong to certain language. If the input string satisfies the criteria defined by the acceptor, it is accepted. If it does not meet the criteria, it is rejected.  A complex example of acceptors is a compiler. It is a program that determine whether a code from certain programming language follows the grammar rules and syntax of that language.  Finite state machine can be categorized into to, DFA and NFA.  ","version":"Next","tagName":"h3"},{"title":"DFA​","type":1,"pageTitle":"Finite Automata","url":"/cs-notes/theory-of-computation-and-automata/finite-automata#dfa","content":" Deterministic Finite Automata (DFA) is an FSM in which each transition is deterministic, or simply predictable based on current state and input.  A DFA is defined with a 5-tuple : M=(Q,Σ,δ,q0,F)M = (Q, \\Sigma, \\delta, q_0, F)M=(Q,Σ,δ,q0​,F)  QQQ = Finite state of &quot;internal states&quot;Σ\\SigmaΣ = Finite set of symbols called &quot;input alphabet&quot;δ\\deltaδ = Transition function, and it's defined as δ:Q×Σ→Q\\delta: Q \\times \\Sigma \\rightarrow Qδ:Q×Σ→Q, or simply it maps a pair consisting of a state and an input symbol to a new state. For example a δ(q0,a)=q1\\delta(q_0, a) = q_1δ(q0​,a)=q1​ means with state q0q_0q0​ and input aaa, it will transition to state q1q_1q1​.q0∈Qq_0 \\in Qq0​∈Q = Initial stateF⊆QF \\subseteq QF⊆Q = Set of final states.  DFA first example​  A visualization of DFA would look like this :   Source : https://youtu.be/Qa6csfkK7_I?si=iZ5Nn8DVGVJoGGs4&amp;t=648  It has four states, namely AAA, BBB, CCC, and DDDThe language it takes is just 000 and 111 symbolsThe initial state is AAA, it is depicted by arrow without sourceThe final state is DDD, depicted by double circleThe transition function is described in the table. For example, if we are at the initial state AAA, inputting 000 would bring us to state CCC, conversely, inputting 111 will transit to state BBB.  DFA with self-loop and dead state​  Let's say we are designing a DFA that accepts certain type of strings from a language. The DFA should accept any strings that start with &quot;0&quot;, such as &quot;0&quot;, &quot;00&quot;, &quot;01&quot;, and so on. If a DFA has reached the final state (even if there is more input), then we can say it accepts the string, otherwise it rejects it.   Source : https://youtu.be/40i4PKpM0cI?si=q-EDJZKbKjcsmYAK&amp;t=569  To do that, we can make a DFA consisting three states, AAA as the initial state, BBB as the final state, and CCC as a dead state.  A dead state refers non-accepting or non-final state from which there are no outgoing transitions for any input symbol in the input alphabet. Once the DFA enters a dead state, it remains in that state indefinitely, regardless of the input symbols it receives. Dead states are often used to represent invalid or rejected inputs.  So, this DFA will immediately transition to final state if the first input is &quot;0&quot;. If the first input is &quot;1&quot;, it will be brought to dead state. We will also create a self-loop on state CCC and BBB, this is to make sure that the DFA needs to stay in a particular state while processing a specific input symbol.  info FSM, DFA, and NFA (covered later) are the simplest automata that doesn't have memory, so it requires a careful consideration to design the structure and transition of DFA so that it accepts or rejects a certain string.  DFA with the same transition function across different inputs​  This DFA is designed to accept sets of all strings over {0,1}\\{0, 1\\}{0,1} of length 2, including &quot;00&quot;, &quot;01&quot;, &quot;10&quot;, and &quot;11&quot;.   Source : https://youtu.be/2KindKcLjos?si=Um-ERDdwC_PGrjON&amp;t=592  The transition function is defined the same for input 0 and 1, this is because the constraint is the string being length 2. The DFA won't reach final state with input empty string and string of length 1. An input string with length greater than 2 will be in a dead state.  DFA with complex example​   Source : https://youtu.be/_2cKtLkdwnc?si=aGpdnvn6TGJhN0q4&amp;t=779  This DFA is more complex, it must be able to accept string over {a,b}\\{a, b\\}{a,b} that contains the string &quot;aabb&quot;. Because it says &quot;contains&quot;, then the placement of &quot;aabb&quot; can be anywhere. So, the DFA will not be included with a dead state, because who knows if there is &quot;aabb&quot; somewhere in the input.  After the first occurrence of &quot;a&quot;, if &quot;b&quot; comes after it, then the state will be reverted to initial state, making sure that two &quot;a&quot; is consecutive. Similarly, inputting &quot;b&quot; after &quot;aab&quot; reverts it back to the state BBB. In the case of &quot;a&quot; is keep being inputted after two &quot;a&quot;, it will be self-looped.  If the problem is worded differently, such that the DFA accepts all string that does not contain the string &quot;aabb&quot;, then we can do few changes.  Flip the statesSwitch final state to non-final stateSwitch non-final state to final state.  This will effectively negate the logic, rejected string becomes accepted, and vice versa.   Source : https://youtu.be/_2cKtLkdwnc?si=A1LaV3Hv92fqQBHm&amp;t=1011  These are four examples of DFA. More complex variants, such as having multiple states transitioning to the same dead state, more complex languages, or anything else, are also possible.  ","version":"Next","tagName":"h3"},{"title":"NFA​","type":1,"pageTitle":"Finite Automata","url":"/cs-notes/theory-of-computation-and-automata/finite-automata#nfa","content":" Nondeterministic Finite Automata (NFA) is the nondeterministic version of DFA, in which having multiple possible choices or transitions at a given state for a particular input symbol. It means that the behavior of the automaton is not uniquely determined by the current state and the input symbol, and decision as well as randomness is involved.  The nondeterministic property of NFA introduces additional complexity during simulation and analysis. If there are multiple possible transitions at a given state for an input symbol, the automaton can theoretically explore all those paths simultaneously. This exploration of multiple paths can be visualized as a branching tree of possible computations.   Source : https://youtu.be/ehy0jGIYRtE?si=pHDZ82c2vvwXu1Jr&amp;t=349  When taking input 0 on state AAA, this NFA can transition to AAA itself or CCC in parallel. It is also possible to accept empty string ϵ\\epsilonϵ.  An NFA is defined with a 5-tuple : M=(Q,Σ,δ,q0,F)M = (Q, \\Sigma, \\delta, q_0, F)M=(Q,Σ,δ,q0​,F)  QQQ = Finite state of &quot;internal states&quot;Σ\\SigmaΣ = Finite set of symbols called &quot;input alphabet&quot;δ\\deltaδ = DFA and NFA differs here, δ=Q×Σ→2Q\\delta = Q \\times \\Sigma \\rightarrow 2^Qδ=Q×Σ→2Q. In other words, for a given state and input symbol (or epsilon transition, can be denoted by λ\\lambdaλ or ϵ\\epsilonϵ), the transition function can have multiple possible outcomes, and will take any subset from the powerset 2Q2^Q2Q. In contrast, a DFA transition state only returns a set containing one state.q0∈Qq_0 \\in Qq0​∈Q = Initial stateF⊆QF \\subseteq QF⊆Q = Set of final states.  Because there are many possibilities in NFA, we can say input is accepted by NFA if there exists at least one path that leads to a final state.  NFA first example​   Source : https://youtu.be/egXhe55dAIk?si=bt0-l4Y6EsF1Prgj&amp;t=443  For example, there are 22=42^2 = 422=4 (Q=2Q = 2Q=2) number of possibilities for this particular NFA.  The possible transitions from the initial state are: A→A,B,AB,ϕA \\rightarrow A, B, AB, \\phiA→A,B,AB,ϕ. With any input, starting from state AAA, we can :  Transition to state AAA itself (by taking input 0).Transition to state BBB (another possibility by taking input 0).Transition to state AAA first and then to the second possibility, BBB (by taking input two zeros).If a particular state doesn't have a transition function for the given input, we denote it as going nowhere, represented by ϕ\\phiϕ.   https://youtu.be/4bjqVsoy6bA?si=e-Ee-hsc97lniQqE&amp;t=321  It turns out that this NFA is capable of taking all set of strings over language {0,1}\\{0, 1\\}{0,1} that ends with 0. With input &quot;100&quot;, there is one possibility that it reaches final state BBB. With input &quot;01&quot;, it reaches final state, but it hasn't processed the entire input string.  NFA that simplify certain DFA versions​   Source : https://youtu.be/Bcen1W_uFEU?si=eNKSxWqfA_Adi0CD&amp;t=313  This NFA is similar to this DFA version.  DFA has to be deterministic, it requires us to include all possible input and transition. In contrast, in this NFA, we don't need to include the transition from state AAA to CCC with input 1. This is because the nondeterministic nature of NFA that allows for taking input symbol for which there is no defined transition from a particular state. It simply does not take that transition and continues its computation.   Source : https://youtu.be/Bcen1W_uFEU?si=tPOEujmw6MGqWYoh&amp;t=518  Another example, related to this, we don't need to include the dead state beyond state CCC.  ","version":"Next","tagName":"h3"},{"title":"Equivalence of DFA & NFA​","type":1,"pageTitle":"Finite Automata","url":"/cs-notes/theory-of-computation-and-automata/finite-automata#equivalence-of-dfa--nfa","content":" Two finite acceptors are equivalent if they accept the same language. Both DFA and NFA are able to recognize the same language. DFA can be considered as special case of NFA, where there is no non-determinism, or there is exactly one transition defined for every state and input symbol.  We can say that every DFA is an NFA, but not vice versa. Also, there is an equivalent DFA for every NFA.   Source : https://youtu.be/--CSVsFIDng?si=hfykBdMisFntJn5e&amp;t=546  This DFA and NFA are the same examples as before. Although they have different formal definitions (distinct transition functions and states), we can say they are equivalent because the definition of equivalence arises when they accept the same language.  ","version":"Next","tagName":"h3"},{"title":"Conversion of NFA to DFA​","type":1,"pageTitle":"Finite Automata","url":"/cs-notes/theory-of-computation-and-automata/finite-automata#conversion-of-nfa-to-dfa","content":" It is possible to convert an NFA to DFA because every DFA is an NFA (through a process called subset construction or powerset construction). Converting NFA to DFA require us to keep the same transition function and fill out the empty transition by possibly adding new states and transitions.   Source : https://youtu.be/pnyXgIXpKnc?si=QfnTh8kttGDWrVsp&amp;t=529  In the NFA, taking input 1 on state AAA can result in transition to AAA itself or BBB. However, DFA doesn't allow this multiple transition, and we have to alter the NFA structure.  The author of the video decided to combine the state AAA and BBB, creating a new state called ABABAB. Regardless of the two letter state, it is considered as a single state (it can also be named as BBB).  Complex Conversion​   Source : https://youtu.be/i-fk9o46oVY?si=Afuzhcyp8GXaT6XW&amp;t=697  By combining two states, all possible transitions are merged to effectively represent the behavior of the original NFA. In the original NFA, upon receiving input &quot;a&quot; in state AAA, it can transition to either AAA or BBB. Thus, a new state called ABABAB is created to represent both states AAA and BBB. The new state ABABAB will transition to itself with input &quot;a&quot;, reflecting the possibility of transitioning from both AAA and BBB to either AAA or BBB (it is the same as doing union operation to both state).  ","version":"Next","tagName":"h3"},{"title":"Minimization​","type":1,"pageTitle":"Finite Automata","url":"/cs-notes/theory-of-computation-and-automata/finite-automata#minimization","content":" Minimization is the process of reducing the number of states in a DFA while preserving the language it recognizes. There are two main steps of minimizing DFA :  Try inputting various combination of input and determine if there are states unreachable from the initial state. Typically, they are depicted if no arrow are directed to it. Unreachable states can be safely removed without affecting the DFA. Source : https://youtu.be/kYMqDgB2GbU?si=5Os46YAwi6tYSP_j&amp;t=101 Combining two states into one state if they are equivalent. Two states are considered equivalent if, for every input symbol, they transition to states in the same equivalence class.  Specifically, two states are equivalent if :  δ(A,X)→F\\delta(A, X) \\rightarrow Fδ(A,X)→F and δ(B,X)→F\\delta(B, X) \\rightarrow Fδ(B,X)→F, orδ(A,X)↛F\\delta(A, X) \\nrightarrow Fδ(A,X)↛F and δ(B,X)↛F\\delta(B, X) \\nrightarrow Fδ(B,X)↛F  If both are able to transition to the final state with the same input string, or if both are unable to do so.  If the length of string ∣X∣=n|X| = n∣X∣=n, then AAA and BBB are said to be nnn equivalent. For example, given an input string of length 4, and AAA and BBB exhibit the same behavior with the input, then they are said to be 4 equivalent.  First example​  An example of DFA and its transition table :   Source : https://youtu.be/0XaGAkY09Wc?si=aYXQ8rokA9oRLbEY&amp;t=471  Next, we list the states in a set belonging to some equivalence group. If states belong to the same set within the equivalence group, it means they are equivalent.   Source : https://youtu.be/0XaGAkY09Wc?si=rPwa6mZpKVfUiPWW&amp;t=718  The purpose of representing states in sets based on their number of equivalence is to group together states with similar behavior, meaning states that transition to the same state with the same input. The intuition is that if they exhibit similar behavior, then we can group them together into a single state.  At 0 equivalence, which signifies when receiving an input string of length 0, obviously AAA, BBB, CCC, and DDD will be equivalent because they haven't received any input yet. EEE is listed in a different set because it is the final state and does not meet the definition of two states being equivalent.  At 1 equivalence, AAA, BBB, and CCC exhibit the same behavior upon receiving any input 0 or 1. Although we see in the transition table, BBB and CCC is different on input 1, CCC is still equivalent with AAA, so it's fine. While DDD starts to be different with any of AAA, BBB, or CCC, so it is listed in different set.  When two rows of different equivalence class are giving the same result (i.e., the 2 equivalence and 3 equivalence have the same set of states), then we can stop listing the equivalence.   Source : https://youtu.be/0XaGAkY09Wc?si=JJbtZFijynhE7DeK&amp;t=935  And the above is the result, AAA and CCC are combined into new state called ACACAC.  Minimize by combining final states​   Source : https://youtu.be/DV8cZp-2VmM?si=AElvyhBxDWbI4YHM&amp;t=341  The above is a DFA with multiple final states. After listing out the equivalence class, it turns out that the three final states are all equivalent, therefore, we can combine them all together.   Source : https://youtu.be/DV8cZp-2VmM?si=f2Kx9Li0LJ7Dkh-F&amp;t=530 ","version":"Next","tagName":"h3"},{"title":"Pushdown Automata","type":0,"sectionRef":"#","url":"/cs-notes/theory-of-computation-and-automata/pushdown-automata","content":"","keywords":"","version":"Next"},{"title":"PDA​","type":1,"pageTitle":"Pushdown Automata","url":"/cs-notes/theory-of-computation-and-automata/pushdown-automata#pda","content":" A finite automaton has a very limited memory to keep track its computation. If we were to design an automaton that determine the length of string, a finite automaton wouldn't work. Pushdown Automata (PDA) is the extension of finite automaton that has more memory, specifically a stack memory with the LIFO principle. A PDA recognizes context-free languages governed by context-free grammar. Similar to finite automaton, it can either accept or reject the language.  Component of PDA :  Input file/tape : Contains the input symbols that the PDA reads from left to right.Finite state control : The control unit of the PDA that determines its state.Pushdown store : It's the stack that is used to store symbols from the input tape. It has infinite size, and it allows for two stack operations : push (adding a symbol to the stack) and pop (removing the top symbol from the stack).   Source : Book page 160  Formal Definition​  Formally, a PDA is defined as 7 tuples : P=(Q,Σ,Γ,δ,q0,z0,F)P = (Q, \\Sigma, \\Gamma, \\delta, q_0, z_0, F)P=(Q,Σ,Γ,δ,q0​,z0​,F)  QQQ is the finite set of states.Σ\\SigmaΣ is the finite set of input alphabet/symbols.Γ\\GammaΓ (uppercase gamma) is the finite stack alphabet.δ:Q×(Σ∪ϵ)×Γ→Q×Γ∗\\delta: Q \\times (\\Sigma \\cup {\\epsilon}) \\times \\Gamma \\rightarrow Q \\times \\Gamma*δ:Q×(Σ∪ϵ)×Γ→Q×Γ∗ is the transition function, where Q×Γ∗Q \\times \\Gamma^*Q×Γ∗ represent a power set. The power set contains all the possible combination of the pair of states and stack contents.q0q_0q0​ is the initial or start state.ZZZ is the initial or start stack symbol.FFF is the set of accepting or final states.  The thing worth to note is the transition function, it takes the current state QQQ, input symbols Σ\\SigmaΣ, including the empty string ϵ\\epsilonϵ, and the current topmost symbol of the stack Γ\\GammaΓ. When transitioning, the topmost symbol may or not be popped, a new state will be produced, and a new symbol may be pushed onto the stack.  For example, δ(q1,a,X)\\delta(q_1, a, X)δ(q1​,a,X) could yield a set of pairs {(q2,XY),(q3,YZ),(q4,ϵ)}\\{(q_2, XY), (q_3, YZ), (q_4, \\epsilon)\\}{(q2​,XY),(q3​,YZ),(q4​,ϵ)}, indicating that from state q1q_1q1​ reading input symbol &quot;a&quot; while having &quot;X&quot; on topmost of the stack, the PDA could :  Transition to state q2q_2q2​ with &quot;XY&quot; on the stack, meaning it pushes &quot;Y&quot; onto the stack.Transition to state q3q_3q3​ with &quot;YZ&quot; on the stack, meaning it pop &quot;X&quot;, then pushes &quot;YZ&quot;.Transition to state q4q_4q4​ with ϵ\\epsilonϵ on the stack, meaning it will pop &quot;X&quot; from the stack, leaving the stack to be empty.  We see that the primary difference between finite automaton and PDA is, a PDA takes extra argument (the stack symbol) and produce an output (modifying the stack).  In the case when the topmost stack is ϵ\\epsilonϵ or empty, then the stack is simply not being read or popped.  Example​   Source : https://youtu.be/eY7fwj5jvC4?si=eRqTnkjH7ODIc1kN&amp;t=627  The PDA should accept language with equal length of &quot;0&quot;'s and &quot;1&quot;'s.  In the video, the notation ϵ,ϵ→z0\\epsilon, \\epsilon \\rightarrow z_0ϵ,ϵ→z0​ denotes that the transition takes input ϵ\\epsilonϵ, pop symbol on the left-hand side, and push the symbol on right-hand side. A symbol ϵ\\epsilonϵ means the transition does nothing to the stack. An ϵ\\epsilonϵ on the left-hand side means the transition doesn't pop, while on right-hand side, it doesn't push.  In state q2q_2q2​, upon receiving input &quot;0&quot;, it checks if the topmost symbol is ϵ\\epsilonϵ. Because it is just ϵ\\epsilonϵ, then it does nothing. Then, on the right-hand side, there is a symbol &quot;0&quot;, signifying that it should be pushed onto the stack. If the input is &quot;1&quot;, it checks if &quot;0&quot; is the topmost symbol; if it is, then it should be popped. After that, on the right-hand side, this ϵ\\epsilonϵ denotes that nothing is pushed onto the stack.  The concept behind this PDA is that, in state q2q_2q2​, we push as many &quot;0&quot;s as the input remains &quot;0&quot;. Then, upon encountering &quot;1&quot; in the input, transition to state q3q_3q3​ and start popping all the &quot;0&quot;s every time &quot;1&quot; is inputted. Upon reaching the final state q4q_4q4​, the PDA checks if the stack is empty. If it is, we can conclude that the number of &quot;0&quot;s matches the number of &quot;1&quot;s. If not, it indicates that either too many &quot;0&quot;s or &quot;1&quot;s were pushed (due to inputting them in the input string), preventing them from being popped altogether, thus leaving the stack not empty.  tip Sometimes, the symbol z0z_0z0​ is pushed at the beginning of the PDA and popped at the end of PDA. This symbol is used to indicate the last element on the stack.  PDA Even Palindrome​   Source : https://youtu.be/TEQcJybMMFU?si=NcawjrirsnjL8ybx&amp;t=803  This PDA should accept even-length palindrome, meaning the string length should be even, and it is palindrome, or can be read the same from the front or backward (e.g., try reversing the string &quot;abba&quot;, we will obtain the same thing, but not with &quot;abab&quot;).  The idea of this PDA is, in q2q_2q2​, we push any symbol that appears on the input. After an empty input ϵ\\epsilonϵ is encountered in the middle, it means that we can transition to state q3q_3q3​ and start popping the stack to check if the input is palindrome. In the q3q_3q3​, anything we read as input will be popped from the stack. If the topmost symbol on the stack is not equal to the input, then the PDA will not transition to anywhere from the state q3q_3q3​, indicating the string is not accepted.  As always, when the input is ϵ\\epsilonϵ again, we transition to state q4q_4q4​ and check if the stack is empty (denoted by z0z_0z0​ on top of the stack). If it is, then the string is accepted, as we have reached the final state.  For example, with the input &quot;abab&quot;, we will have z0z_0z0​, aaa, and bbb, respectively from the bottom of the stack.  However, the problem with this analysis is, how can we assume that an input is ϵ\\epsilonϵ or empty? Typically, input is ϵ\\epsilonϵ when we have reached at the end of the input, but in the PDA, we keep expecting for input even after the ϵ\\epsilonϵ transition from the q2q_2q2​ to q3q_3q3​.   Source : https://youtu.be/BxA-aI2dyRo?si=mmMY52_ormtTd3bB&amp;t=152  We can make assumption that ϵ\\epsilonϵ symbol appears before and after every input. After that, we can list all possible transition when input has or not ϵ\\epsilonϵ symbol. We will also include the current state and stack content for each of the path. If at least a single path exist in the list that leads from the start state to the final state, we can say that the PDA accepts the string.  This is similar to NFA, in which it is possible to transition to every path with any input and theoretically, each transition is done in parallel.   Source : https://youtu.be/BxA-aI2dyRo?si=K0ysvgZgszM3dMsH&amp;t=555 (combined images)  ","version":"Next","tagName":"h3"},{"title":"PDA & CFG​","type":1,"pageTitle":"Pushdown Automata","url":"/cs-notes/theory-of-computation-and-automata/pushdown-automata#pda--cfg","content":" Equivalence​  Conversion​ ","version":"Next","tagName":"h3"},{"title":"Regular Languages (Part 1)","type":0,"sectionRef":"#","url":"/cs-notes/theory-of-computation-and-automata/regular-languages-part-1","content":"","keywords":"","version":"Next"},{"title":"Regular Language​","type":1,"pageTitle":"Regular Languages (Part 1)","url":"/cs-notes/theory-of-computation-and-automata/regular-languages-part-1#regular-language","content":" Regular language is a language that can be recognized by some finite state machine. It is constructed by three set operations, union, concatenation, and Kleene star.  With alphabet Σ\\SigmaΣ, the class of regular languages over Σ\\SigmaΣ is defined as :  ∅\\varnothing∅ is a regular languageFor each σ∈Σ,{σ}\\sigma \\in \\Sigma, \\{\\sigma\\}σ∈Σ,{σ} is a regular languageFor any natural number n≥2n \\ge 2n≥2 if L1,L2,...,LnL_1, L_2, ..., L_nL1​,L2​,...,Ln​ are regular languages, then so is L1∪L2∪......∪LnL_1 \\cup L_2 \\cup ... ... \\cup L_nL1​∪L2​∪......∪Ln​.For any natural number n≥2n \\ge 2n≥2, if L1,L2,...,LnL_1, L_2, ..., L_nL1​,L2​,...,Ln​ are regular languages, then so is L1⋅L2⋅......⋅LnL_1 \\cdot L_2 \\cdot ... ... \\cdot L_nL1​⋅L2​⋅......⋅Ln​.If LLL is a regular language, then so is L∗L^*L∗.Nothing else is a regular language unless its construction follows from rules (1) to (5).  So, a regular language can be constructed using the three mentioned operations, including any symbol, and the empty set is also considered a regular language.  Closure Properties​  If an operation is said to have closure property, it means that performing the operation on members of the set should yield a result that is also a member of the set. For example, addition in the set of integers own the closure property, because adding two integers will always result in another integer. This is not the case for division, as it might result in fraction.  In regular language, operation that owns closure property are union, concatenation, intersection, set difference, complementation, and Kleene star.  ","version":"Next","tagName":"h3"},{"title":"Regular Expression​","type":1,"pageTitle":"Regular Languages (Part 1)","url":"/cs-notes/theory-of-computation-and-automata/regular-languages-part-1#regular-expression","content":" Regular expression is the formal notation of describing regular languages. It consists of a sequence of characters that represents a pattern or rule used to match and manipulate strings within a given alphabet.  Character and operators :  Terminal symbols : Terminal symbol including symbols that are contained within the Σ\\SigmaΣ, including ϵ\\epsilonϵ (empty string) and ∅\\varnothing∅ (empty set) are regular expression.Union (+++) : Union operation is like a logical OR, if we have two regular expression, R1,R2R_1, R_2R1​,R2​, union between them would be represented as R1+R2R_1 + R_2R1​+R2​. This mean we are making a pattern that match either R1R_1R1​ or R2R_2R2​.Concatenation (⋅\\cdot⋅) : With R1,R2R_1, R_2R1​,R2​, concatenation is R1⋅R2R_1 \\cdot R_2R1​⋅R2​, and it indicates that the two expressions must occur consecutively in the input string.Kleene star (∗*∗) : It acts like an iteration that specifies that the preceding expression can occur zero or more times. It matches any number of occurrences of the preceding expression, including no occurrence at all. For example, the regular expression a∗a^*a∗ matches strings like ϵ\\epsilonϵ or &quot;&quot;, &quot;a&quot;, &quot;aa&quot;, &quot;aaa&quot;, and so on.  All the operations behave recursively, meaning we can apply it not only to individual characters or subexpressions but also to larger expressions or groups of expressions. For example, with a⋅b⋅ca \\cdot b \\cdot ca⋅b⋅c, we can make it as (a⋅b)⋅c=(ab⋅c)=abc(a \\cdot b) \\cdot c = (ab \\cdot c) = abc(a⋅b)⋅c=(ab⋅c)=abc.  Common Expression​  Assume that Σ={a,b,c}\\Sigma = \\{a, b, c\\}Σ={a,b,c}  Zero or more : We can use Kleene star for this, a∗a^*a∗ means one or more a's.One or more : Use one 'a' before the zero or more expression : aa∗aa*aa∗ (a∗aa*aa∗a is also right).Zero or one : (a+λ)(a + \\lambda)(a+λ) describe optional 'a'.Any string : Union all alphabet with a Kleene star : (a+b+c)∗(a + b + c)^*(a+b+c)∗.Any nonempty string : Because Kleene star permits zero or more, then we would need extra any string in front of it, so it will be (a+b+c)(a+b+c)∗(a + b + c)(a + b + c)^*(a+b+c)(a+b+c)∗.Any string not containing : We should exclude the string that we don't want it to contain from the any string expression, (b+c)∗(b + c)^*(b+c)∗ exclude any 'a'.Any string containing exactly one : Just add the desired string before or after the any string not containing expression, (b+c)∗a(b+c)∗(b + c)^* a(b + c)^*(b+c)∗a(b+c)∗.  tip We can also use 0+0^+0+ to describe zero or more string excluding the empty string.  Regular Expression Identities​  Regular expression identities are regular expression laws or properties, or a set of rules or relationships that hold true for regular expressions. These identities are useful for simplifying or transforming regular expressions while preserving their matching behavior.   Source : https://youtu.be/yp4pYgXfYD8?si=rwI7XntRncW2qGoK&amp;t=26  For example, applying Kleene star multiple times (rule 8) doesn't affect anything. This property is similar to adding infinity to infinity, it remains infinity.  In the rule 9, RR∗RR^*RR∗ represents one or more string, meaning empty string is not possible to matched. If we add an empty string with that expression, in other words, we include ϵ\\epsilonϵ in the set of all possible string matched, then it will be equal back to R∗R^*R∗ again.  Arden's Theorem​  One application of regular expression identities is the Arden's theorem. Arden's theorem states that :  If PPP and QQQ are two regular expressions over Σ\\SigmaΣ, and if PPP does not contain ϵ\\epsilonϵ, then the following equation in RRR given by R=Q+RPR = Q + RPR=Q+RP has a unique solution, that is R=QP∗R = QP^*R=QP∗.  Arden's theorem can be proved by using several regular expression identity :   Source : https://youtu.be/Idl_0mPzZjE?si=OyVyX6JPE4VhDaz2&amp;t=430  The proof consist of two part, the first part proves that RRR is equal to QP∗QP^*QP∗, and the second part proves that QP∗QP^*QP∗ is the unique solution for the equation R=Q+RPR = Q + RPR=Q+RP, by substituting and expanding the RRR with the QP∗QP^*QP∗. It mostly uses the distributive property of regular expressions. ","version":"Next","tagName":"h3"},{"title":"Turing Machine","type":0,"sectionRef":"#","url":"/cs-notes/theory-of-computation-and-automata/turing-machine","content":"Turing Machine Main Source : [] Turing Machine page turing test, turing complete for additionnondeterministic turing machinemodification","keywords":"","version":"Next"},{"title":"Undecidability","type":0,"sectionRef":"#","url":"/cs-notes/theory-of-computation-and-automata/undecidability","content":"Undecidability Main Source : [] Undecidability page halting problemPost’s Correspondence Problem","keywords":"","version":"Next"},{"title":"Regular Languages (Part 2)","type":0,"sectionRef":"#","url":"/cs-notes/theory-of-computation-and-automata/regular-languages-part-2","content":"","keywords":"","version":"Next"},{"title":"NFA to Regular Expression​","type":1,"pageTitle":"Regular Languages (Part 2)","url":"/cs-notes/theory-of-computation-and-automata/regular-languages-part-2#nfa-to-regular-expression","content":" The conversion of an NFA (or DFA) to a regular expression involves designing a regular expression that represents the paths leading to a specific state.  The first thing to do is to list regular expression equation for each state in the NFA. To do that, we have to list the state and symbol that are used to reach particular state.   Source : https://youtu.be/OKFrju0HB7k?si=urpjdmtbkJPJ8wm2&amp;t=171  q3=q2aq_3 = q_2aq3​=q2​a means q3q_3q3​ is reachable from state q2q_2q2​ and symbol &quot;a&quot;.  The second step is to simplify each state equation.   Source : https://youtu.be/OKFrju0HB7k?si=s5PWuHMUz19Q9HlF&amp;t=609  Some step substitute other terms, some also use regular expression identities, and Arden's theorem.  The last is to substitute other non-final states to the final state, so that the regular expression is produced.   Source : https://youtu.be/OKFrju0HB7k?si=T8uy19dz2X1lE5Yh&amp;t=755  tip The method of converting an NFA or DFA to a regular expression is the same. In cases where multiple final states exist in the automaton, we simply need to union the multiple regular expressions produced by the final state equations.  ","version":"Next","tagName":"h3"},{"title":"Regular Expression to Finite Automata​","type":1,"pageTitle":"Regular Languages (Part 2)","url":"/cs-notes/theory-of-computation-and-automata/regular-languages-part-2#regular-expression-to-finite-automata","content":" This means we have to design an automaton in such a way that it accepts the set of all strings that match the specified pattern.   Source : https://youtu.be/RxfXyvfTsgQ?si=_oeWTxvkSevhpHgL&amp;t=253  A regular expression (a+b)(a + b)(a+b) matches any string that contains &quot;a&quot; or &quot;b&quot;. Therefore, a finite automaton for this would be a state with transition function that accepts &quot;a&quot; or &quot;b&quot;.   Source : https://youtu.be/62JAy4oH6lU?si=w_zPMKLPSgzBjRoN&amp;t=512  The above is another example, a Kleene star can be converted into state that self-loops.  ","version":"Next","tagName":"h3"},{"title":"Equivalence of Two Finite Automata​","type":1,"pageTitle":"Regular Languages (Part 2)","url":"/cs-notes/theory-of-computation-and-automata/regular-languages-part-2#equivalence-of-two-finite-automata","content":" Two automata are said to be equivalence if :  For any pair of states {qi,qj}\\{q_i, q_j\\}{qi​,qj​} the transition for input a∈Σa \\in \\Sigmaa∈Σ is defined by {qa,qb}\\{q_a, q_b\\}{qa​,qb​} where qa=δ{qi,a}q_a = \\delta\\{q_i, a\\}qa​=δ{qi​,a} and qb=δ{qj,a}q_b = \\delta\\{q_j, a\\}qb​=δ{qj​,a}. The two automata are not equivalent if for a pair {qa,qb}\\{q_a, q_b\\}{qa​,qb​} one is intermediate state and the other is final state. In simpler term, two finite automata are equivalent if with the same input string, they both either accept or reject the string. If initial state is the final state of one automaton, then in the second automaton, its initial state must be a final state as well for them to be equivalent.  To actually check the equivalence, we will check each pair of the state from both automaton inputting the same symbol.   Source : https://youtu.be/nX4JrcHgpZY?si=W35ijxnJD-cGCo7l&amp;t=632  It turns out that for each pair of states from both automata, they either reach a final state or an intermediate state, indicating they are equivalent.  ","version":"Next","tagName":"h3"},{"title":"Myhill-Nerode Theorem​","type":1,"pageTitle":"Regular Languages (Part 2)","url":"/cs-notes/theory-of-computation-and-automata/regular-languages-part-2#myhill-nerode-theorem","content":" Myhill-Nerode theorem provides a way to determine whether a language is regular by examining the behavior of its strings.  It works by grouping together string into same equivalence classes. Equivalence classes are formed based on the behavior of strings with respect to acceptance or rejection by the language.  The conditions for language to be regular :  The number of equivalence classes is finite.That number is equal to the number of states in the minimal DFA accepting the language.Any minimal DFA acceptor for the language is isomorphic to the following one: Let each equivalence class [x][x][x] correspond to a state, and let state transitions be a:[x]→[xa]a : [x] \\rightarrow [xa]a:[x]→[xa] for each a∈Σa \\in \\Sigmaa∈Σ. Let the starting state be [ϵ][\\epsilon][ϵ], and the accepting states be [x][x][x] where x∈Lx \\in Lx∈L.  tip Two structures are said to be isomorphic if they have the same underlying structure, although they may have different names or representations. Isomorphism can be determined if there exists a mapping (or transformation) between one and another that can convert between them while preserving their behavior and language recognition capabilities.  This theorem is also related to the DFA minimization. In DFA minimization, we used the method of listing equivalence class to minimize a DFA. In fact, the similar method can be used to prove if the number of equivalence class is finite (but thorough an exhaustive case).  The third condition simply mean that the transition in the minimal DFA should correspond to the set in the equivalence classes. This is why when we minimize DFA, we are required to combine state that belong to the same set within the equivalence classes.  In summary, Myhill-Nerode theorem bridge the relationship between regularity of a language and the behavior of strings within that language. By grouping strings into equivalence classes based on their behavior with respect to acceptance or rejection by the language, the theorem allows us to identify indistinguishable states in a DFA, which in turns allow us to minimize the DFA by combining the states with similar behavior.  Table Filling Method​  Another way to minimize DFA is the table filling method, it is based on the Myhill-Nerode theorem. The minimization is done by following the 4 steps in the image below.   Source : https://youtu.be/UiXkJUTkp44?si=xEqD2nTn-bH8fxES&amp;t=436  Draw a table for all pairs of states, but there's no need to permute the pairs. Therefore, AB = BA, and we only need one of them. For each cell, we will mark it if only one of the state pair is final state and the other is intermediate state. Source : https://youtu.be/UiXkJUTkp44?si=RXdmpBdcyZ5Q3kSD&amp;t=916 Next, for the unmarked pairs, we will mark them only if the transition between them, given the same input, arrives at a pair of states that is already marked. For example, with pair BBB and AAA, by inputting &quot;0&quot;, BBB will transition to AAA and AAA will transition to BBB. The pair AAA and BBB is not marked, so we have to try another input. This will be repeated for each pair and input until no more markings can be made. Lastly, do the step 4. Source : https://youtu.be/UiXkJUTkp44?si=gcsPHCj7ApbyvdWe&amp;t=1159  ","version":"Next","tagName":"h3"},{"title":"Pumping Lemma for Regular Languages​","type":1,"pageTitle":"Regular Languages (Part 2)","url":"/cs-notes/theory-of-computation-and-automata/regular-languages-part-2#pumping-lemma-for-regular-languages","content":" In mathematics, a lemma is a proven statement or proposition used as a &quot;stepping stone&quot; in the proof of a larger theorem. The pumping lemma for regular languages is used to show that certain languages are not regular. It is a tool to prove a language is not regular, rather than proving its regular like Myhill-Nerode theorem.  Pumping lemma for regular languages states that :  If AAA is a regular language, then AAA has a pumping length 'p', such that any string S∈LS \\in LS∈L where ∣S∣≥P|S| \\ge P∣S∣≥P may be divided into three parts, S=xyzS = xyzS=xyz, satisfying the following conditions : xyiz∈Axy^iz \\in Axyiz∈A for every i≥0i \\ge 0i≥0∣y∣&gt;0|y| &gt; 0∣y∣&gt;0∣xy∣≤P|xy| \\le P∣xy∣≤P  In simpler terms, we can prove that a language is not regular by contradicting the statement. To check if a string is not regular, we should be able to divide it into three parts and &quot;pump&quot; the yyy part any number of times such that it still belongs to the language. Additionally, we have to conform with the second and third condition. The length of yyy should be greater than 0, and the combined length of xxx and yyy should be less than or equal to PPP.   Source : https://youtu.be/dikEDuepOtI?si=kCQM7RFKoldzU5je&amp;t=361  Example​  The proof follows the step above.   Source : https://youtu.be/Ty9tpikilAo?si=UyAWA_7gOwX5Fi7s&amp;t=261  Choosing a PPP and the string SSS.   Source : https://youtu.be/Ty9tpikilAo?si=TfK_BtM6FaSXlD9-&amp;t=794  Considering all the case to divide SSS into xxx, yyy, and zzz, as well as checking if any of the cases satisfy the first pumping lemma condition, that is, the formed xyizxy^izxyiz string needs to be in the language. The string needs to be aPbPa^P b^PaPbP, but the xyizxy^izxyiz in the case 1 does not satisfy it. Lastly, we need to show that none of the cases can satisfy all the 3 pumping conditions at the same time. The second condition is satisfied but not the third condition. ","version":"Next","tagName":"h3"},{"title":"Context-Free Grammar","type":0,"sectionRef":"#","url":"/cs-notes/theory-of-computation-and-automata/context-free-grammar","content":"","keywords":"","version":"Next"},{"title":"Derivation Trees​","type":1,"pageTitle":"Context-Free Grammar","url":"/cs-notes/theory-of-computation-and-automata/context-free-grammar#derivation-trees","content":" Also known as parse tree, it is an ordered rooted tree that graphically represent the derivation process of a context-free grammar. The five properties of derivation tree :  The root of a derivation tree is SSS.Every leaf node is a terminal symbol T∪{λ}T \\cup \\{\\lambda\\}T∪{λ}.Every vertex node is a non-terminal symbol in VVV.When a vertex A∈VA \\in VA∈V has a set of children labeled from a1,a2,...,ana_1, a_2, ..., a_na1​,a2​,...,an​, then vertex AAA will have production rule in the form of A→a1,a2,...,anA \\rightarrow a_1, a_2, ..., a_nA→a1​,a2​,...,an​.A leaf labeled with λ\\lambdaλ has no siblings, that is, a vertex with a child labeled λ\\lambdaλ can have no other children.  In other word, each level of the tree represents a step in the derivation process. Starting from the root, each level corresponds to the application of a production rule. The nodes at a particular level are derived from the nodes at the previous level.   Source : https://youtu.be/u4-rpIlV9NI?si=mAYV4hyYXS9j7sfh&amp;t=244  We see in the example that starting from SSS, it produces terminal 000 and non-terminal BBB. We will make it as the child of SSS, and continue the derivation process of BBB. This is done for all production rule. If there exist multiple production rule for one non-terminal symbol, then we will have to use both of them in distinct node. When the production rule is recursive (i.e., A→1AAA \\rightarrow 1AAA→1AA), we may stop the derivation and fill the leaf nodes with ϵ\\epsilonϵ.  We also call any step of the derivation as a sentential form. A derivation tree in which the leaves contain a label from V∪T∪{λ}V \\cup T \\cup \\{\\lambda\\}V∪T∪{λ}, or in other words, if any of the leaf still contains a non-terminal symbol, then the tree is said to be a partial derivation tree.  info See also tree data structure. ϵ\\epsilonϵ is same as λ\\lambdaλ.  Left &amp; Right Derivation​  When deriving a string with a grammar, there are two approaches, leftmost derivation and rightmost derivation. In leftmost derivation, the leftmost non-terminal in the current sentential form is always selected for expansion. While in the rightmost derivation, we select the rightmost non-terminal symbols. There is also mixed derivation, in which the two approaches is combined.  For example, a simple grammar with production rule S→ABS \\rightarrow ABS→AB, A→aA \\rightarrow aA→a, B→aB \\rightarrow aB→a.  Using this grammar, the string &quot;ab&quot; can be derived in the following ways :  Leftmost Derivation : S⇒AB⇒aB⇒abS \\Rightarrow AB \\Rightarrow aB \\Rightarrow abS⇒AB⇒aB⇒abRightmost Derivation : S⇒AB⇒Ab⇒abS \\Rightarrow AB \\Rightarrow Ab \\Rightarrow abS⇒AB⇒Ab⇒ab  Another example with the graphical derivation tree :   Source : https://youtu.be/u4-rpIlV9NI?si=mWlwEeGp_w5Ky123&amp;t=740  In the left derivation process, we chose to apply the production rule S→aSSS \\rightarrow aSSS→aSS, because applying the other rule S→aASS \\rightarrow aASS→aAS would require us to replace AAA. The production rule of AAA involve occurrence of bbb, if we use the A→baA \\rightarrow baA→ba production rule, we will have bbb as the second character of the string (which is invalid). The process of derivation continues like this until we successfully create a parse tree with the correct characters combined from all the leaf nodes.  ","version":"Next","tagName":"h3"},{"title":"Parsing​","type":1,"pageTitle":"Context-Free Grammar","url":"/cs-notes/theory-of-computation-and-automata/context-free-grammar#parsing","content":" Parsing is the process of finding a derivation for a string from a given grammar. It is a way to recognize a string and to determine if a string belong to the grammar.  The basic idea of parsing is, finding (or making) a parse tree. The manual way is to analyze a given string and the grammar, then carefully construct a parse tree (like what we did in the previous example above).  A brute force way of parsing, also called exhaustive search parsing involve generating all possible strings of the same length as the input and checking if any of them matches the input. This approach is obviously inefficient. A practical parsing algorithms would be designed to avoid exhaustive search by employing various optimization techniques.  Ambiguity​  In parsing, ambiguity refers to a situation where a given grammar can produce more than one valid parse tree for a particular input string. Depending on which production rule is applied on each step, a different but valid parse tree could be generated.  The grammar G=({S},{a,b},S,S→aSb∣bSa∣SS∣λ)G = (\\{S\\}, \\{a, b\\}, S, S \\rightarrow aSb | bSa | SS | \\lambda)G=({S},{a,b},S,S→aSb∣bSa∣SS∣λ) generates strings having an equal number of &quot;a&quot;'s and &quot;b&quot;'s. The string &quot;abab&quot; can be generated in two distinct ways as shown in the following.   Source : Book page 129  Furthermore, the string &quot;abab&quot; has two distinct leftmost derivations :  S⇒aSb⇒abSab⇒ababS \\Rightarrow aSb \\Rightarrow abSab \\Rightarrow ababS⇒aSb⇒abSab⇒ababS⇒SS⇒aSbS⇒abS⇒abaSb⇒ababS \\Rightarrow SS \\Rightarrow aSbS \\Rightarrow abS \\Rightarrow abaSb \\Rightarrow ababS⇒SS⇒aSbS⇒abS⇒abaSb⇒abab  And two distinct rightmost derivations :  S⇒aSb⇒abSab⇒ababS \\Rightarrow aSb \\Rightarrow abSab \\Rightarrow ababS⇒aSb⇒abSab⇒ababS⇒SS⇒SaSb⇒Sab⇒aSbab⇒ababS \\Rightarrow SS \\Rightarrow SaSb \\Rightarrow Sab \\Rightarrow aSbab \\Rightarrow ababS⇒SS⇒SaSb⇒Sab⇒aSbab⇒abab  Ambiguity can happen even in our daily lives, such as when encountering this ambiguous math expression &quot;2 + 3 × 4&quot;. Without parenthesis, it wouldn't be clear in which order do we process this expression. We can interpret &quot;2 + 3 × 4&quot; as either &quot;(2 + 3) × 4&quot; or &quot;2 + (3 × 4)&quot;.  We will say a string that leads to this situation &quot;ambiguously derivable&quot;. A grammar GGG is said to be ambiguous if there exists at least one string in L(G)L(G)L(G) which is ambiguously derivable. Ambiguity is a property of grammar, it is not always possible to find equivalent unambiguous grammar.  ","version":"Next","tagName":"h3"},{"title":"Simplification​","type":1,"pageTitle":"Context-Free Grammar","url":"/cs-notes/theory-of-computation-and-automata/context-free-grammar#simplification","content":" The simplification of context-free grammar involves transforming or modifying the grammar to make it simpler or more manageable while preserving its language. This includes removing useless symbols, unreachable symbols, and simplifying the production rules.  For example, if we have production rule A→aBaA \\rightarrow aBaA→aBa and B→yB \\rightarrow yB→y, we can simplify this by directly substituting BBB in the AAA production rule. So, the simplified production rule is A→ayaA \\rightarrow ayaA→aya. This technique is also called substitution rule.  There are three steps in CFG simplification, reduction of CFG, removal of unit production, and removal of null production.  Reduction of CFG​  The reduction involve eliminating useless symbols in the grammar, including non-terminals and terminals that do not contribute to generating any valid string in the language. It is divided into two phases.  Removal of useless symbols, non-terminals and terminals that do not contribute to generating any valid string in the language.Removal of unreachable symbols, symbols that do not participate in the derivation of any sentential form.  A helpful procedure from the video :   Source : https://youtu.be/EF09zxzpVbk?si=qtn-ff6Wb-TJ2Zxv&amp;t=323  With example grammar GGG with production rules PPP: S→AC∣BS \\rightarrow AC|BS→AC∣B, A→aA \\rightarrow aA→a, C→c∣BCC \\rightarrow c|BCC→c∣BC, E→aA∣eE \\rightarrow aA|eE→aA∣e.   Source : https://youtu.be/EF09zxzpVbk?si=gc_WXN5N0yI5d6et&amp;t=604  Create a set w1w_1w1​ that includes non-terminals that derives to some terminals.Create another set w2w_2w2​ that includes non-terminals that derives to all symbol in the previous step.It is repeated until we obtained the same set in two consecutive steps (end at w3w_3w3​).The new grammar G′G'G′ will contain non-terminal symbols from the last set w3w_3w3​, and terminal symbols that are derived by those non-terminal symbols.A new production rule is created in which the symbol that doesn't appear in the non-terminal of G′G'G′ won't be included. In the example, we see that we removed production rule S→BS \\rightarrow BS→B, because BBB itself doesn't generate any terminal symbol (i.e., BBB is a useless symbol).   Source : https://youtu.be/EF09zxzpVbk?si=ra95MHcYogTYxDC9&amp;t=779  Create a set y1y_1y1​ starting from the start symbol SSS.Create another set y2y_2y2​ that includes all the symbol (including non-terminals and terminals) that can be derived from the previous set.It is repeated until we obtained the same set in two consecutive steps (end in y4y_4y4​).A new grammar G′′G''G′′ we include the non-terminals and terminals from the last set y4y_4y4​.We will alter the production rule by only including those that are in the non-terminals of G′′G''G′′. In the example, we removed symbol EEE entirely, because in fact no other symbol can reach it (i.e., EEE is an unreachable symbol).  Removal of Unit Productions​  Unit production refers to any production rule in the form A→BA \\rightarrow BA→B, or a non-terminal that transform to another non-terminal. Unit production is sometimes redundant and can be simplified using substitution rule.  If BBB production rule is in the form B→y1∣y2∣...∣ynB \\rightarrow y_1|y_2|...|y_nB→y1​∣y2​∣...∣yn​ and there is a A⇒∗BA \\xRightarrow{*} BA∗​B or a sequence of derivation that leads from AAA to BBB, then we can delete A→BA \\rightarrow BA→B and replace it with A→y1∣y2∣...∣ynA \\rightarrow y_1|y_2|...|y_nA→y1​∣y2​∣...∣yn​.   Source : https://youtu.be/B2o75KpzfU4?si=NUhYWR4P7fznC6Lp&amp;t=219   Source : https://youtu.be/B2o75KpzfU4?si=PYaESnF-Flc8mljR&amp;t=493  The production rule involves a sequence of derivation from non-terminals to another non-terminals. We can substitute the final derivation step that derive to terminals to each sequence of derivation. Finally, we can remove all unreachable symbols.  Removal of Null Productions​  A null production is a production rule in the form of A→ϵA \\rightarrow \\epsilonA→ϵ. If the variable doesn't directly map to ϵ\\epsilonϵ, this mean it needs a sequence of derivation A⇒∗ϵA \\xRightarrow{*} \\epsilonA∗​ϵ, then we call the variable nullable.   Source : https://youtu.be/mlXYQ8ug2v4?si=i7_75uZdYmBzYOCL&amp;t=131   Source : https://youtu.be/mlXYQ8ug2v4?si=sSOBcDT_ll4KWpl6&amp;t=396  The first step is to remove all null productions from a variable and replace the variable occurrences in other production rules with epsilon. For example, we are going to remove A→ϵA \\rightarrow \\epsilonA→ϵ first. S→ABACS \\rightarrow ABACS→ABAC contains 2 AAA, and there is three possibility of removal. Changing the first AAA to ϵ\\epsilonϵ, so it becomes S→ABCS \\rightarrow ABCS→ABC, changing the second AAA to become S→BACS \\rightarrow BACS→BAC, and changing both AAA to be just S→BCS \\rightarrow BCS→BC. AAA also appear in production rule of AAA itself, and we can replace the AAA in A→aAA \\rightarrow aAA→aA to ϵ\\epsilonϵ to become just A→aA \\rightarrow aA→a.  Next step is to remove all null production of BBB, and obtain the simplified CFG.   Source : https://youtu.be/mlXYQ8ug2v4?si=7OuiYOepKinpa4z7&amp;t=492  ","version":"Next","tagName":"h3"},{"title":"Normal Forms​","type":1,"pageTitle":"Context-Free Grammar","url":"/cs-notes/theory-of-computation-and-automata/context-free-grammar#normal-forms","content":" A context-free grammar is said to be in normal forms when they follow specific structure or standards.  Chomsky Normal Form​  Chomsky Normal Form (CNF) is when the context-free grammar follow the production rule in form :  A→BCA \\rightarrow BCA→BC (where AAA, BBB, and CCC are non-terminals)A→aA \\rightarrow aA→a (where AAA is a non-terminal and aaa is a terminal)S→ϵS \\rightarrow \\epsilonS→ϵ (where SSS is the start symbol and ϵ\\epsilonϵ represents the empty string)  In CNF, all production rules on the right-hand side should either be two non-terminals or a terminal, except for the rule allowing the start symbol to derive the empty string. Other standard are the elimination of unit productions and null productions, as well as the conversion of longer productions.   Source : https://youtu.be/Mh-UQVmAxnw?si=EZD7ndYRKnodk7mJ&amp;t=143  The step 4 is a way to enforce maximum of two non-terminals on the right-hand side. It is done by adding another non-terminals that has the same production rule. For example, instead of A→B∣C∣DA \\rightarrow B|C|DA→B∣C∣D, we can add EEE that E→C∣DE \\rightarrow C|DE→C∣D, and change AAA to A→B∣EA \\rightarrow B|EA→B∣E.  Additionally, when a production rule contains both non-terminal and terminal symbols simultaneously, we can follow the step 5. The step 5 shows that we can separate non-terminal and a terminal by making another non-terminal that produce the terminal.  Example​   Source : https://youtu.be/FNPSlnj3Vt0?si=Ao7POmbn_eLJCk1O&amp;t=725  Start symbol can't appear on the right-hand side, so we can make a new start symbol S′S'S′ that produce SSS.Removing null production by following this.Removing unit production by following this.A new non-terminal XXX is added here.YYY is added to replace aaa that is combined with aBaBaB.  Greibach Normal Form​  In Greibach Normal Form (GNF), every production rule in the grammar is in the form :  A→aγA \\rightarrow a\\gammaA→aγ (where AAA is a non-terminal, aaa is a terminal, and γ\\gammaγ is a possibly empty sequence of non-terminals)  The right-hand side of the production rule starts with a terminal symbol, followed by a sequence of non-terminals.   Source : https://youtu.be/ZCbJan6CGNM?si=pS0u9kuYGdlGbHC8&amp;t=758  In order to convert from CFG to GNF, we have to convert it to CNF first. If CNF restrict the length of right-hand side of a production, GNF allows for longer right-hand sides compared to CNF.  Example​   Source : https://youtu.be/ZCbJan6CGNM?si=DiBqkV6DAv0zHJwb&amp;t=767  Keep in mind that we have to confirm it is in CNF first. The step 3 told us to change the name of non-terminals to numbered AAA, such as A1,A2A_1, A_2A1​,A2​.The production rule will be altered so that the AAA numbering in left-hand side is always lower than the right-hand side.   https://youtu.be/rauqqM0nfuI?si=FplhJA1avFCa6QsS&amp;t=294  A left recursion is when non-terminal on the left-hand side of a production rule appears as the first symbol on the right-hand side. Removing left recursion involve adding new variable to remove it. Upon adding the new variable, we will rewrite the production rule A4A_4A4​ twice. The first time will be same, following the old A4A_4A4​, the second time will be rewriting A4A_4A4​ but adding the new variable to each production rule.   Source : https://youtu.be/rauqqM0nfuI?si=cSCk3rJx4KJAg9KT&amp;t=633  After altering A4A_4A4​, there is still a problem, a non-terminal symbol appears as the first element on the right-hand side of the production rule for A1A_1A1​. The A1→A2A3A_1 \\rightarrow A_2A_3A1​→A2​A3​ can be replaced to A1→bA3A_1 \\rightarrow bA_3A1​→bA3​. Then, the first A4A_4A4​ in A1→A4A4A_1 \\rightarrow A_4A_4A1​→A4​A4​ is substituted with the whole A4A_4A4​ production rule. For example, the first A4A_4A4​ is changed to bbb and then added with the second. Next, A4A_4A4​ is replaced again with bA3A4bA_3A_4bA3​A4​ and added with the second A4A_4A4​, and so on for each production rule in A4A_4A4​.There is also a problem in ZZZ, we also address this by doing the similar thing.  ","version":"Next","tagName":"h3"},{"title":"Pumping Lemma for Context-Free Languages​","type":1,"pageTitle":"Context-Free Grammar","url":"/cs-notes/theory-of-computation-and-automata/context-free-grammar#pumping-lemma-for-context-free-languages","content":" Pumping lemma for context-free languages is to prove that a language is not context-free. In contrast, pumping lemma for regular language proves that a language is not regular.  The lemma states :  Any context-free language AAA has a pumping length OOO, such that any string SSS in AAA, where ∣S∣≥P|S| \\ge P∣S∣≥P can be divided into five parts : S=uvxyzS = uvxyzS=uvxyz, satisfying the following conditions : uvixyizuv^{i}xy^{i}zuvixyiz is in AAA for every i≥0i \\ge 0i≥0∣vy∣&gt;0|vy| &gt; 0∣vy∣&gt;0∣vxy∣≤P|vxy| \\le P∣vxy∣≤P  Similar to the previous pumping lemma, it is a proof based on contradiction. A helpful procedure from the video :   Source : https://youtu.be/jRhqx1_KcCk?si=PQWy2IpLX7wc_wfp&amp;t=259  Example​   Source : https://youtu.be/eQ0XkUk3qGk?si=2DO7eT_GCWy0jLFQ&amp;t=429  Pick a pumping length PPP and a string SSS. PPP is chosen to be 4 and S=apbpcpS = a^pb^pc^pS=apbpcp, so S=a4b4c4S = a^4b^4c^4S=a4b4c4.   Source : https://youtu.be/eQ0XkUk3qGk?si=RyCf4AbLvHGW-yxr&amp;t=700  Consider some case of language such that uvixyiz∉Auv^ixy^iz \\notin Auvixyiz∈/A for some iii. Remember that the language must be in the form of anbncna^nb^nc^nanbncn according to the original language.It is shown that in either two case, both fail to satisfy the three pumping lemma condition at the same time, so the language is proven to not be a context-free. ","version":"Next","tagName":"h3"},{"title":"TOC Fundamentals","type":0,"sectionRef":"#","url":"/cs-notes/theory-of-computation-and-automata/toc-fundamentals","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"TOC Fundamentals","url":"/cs-notes/theory-of-computation-and-automata/toc-fundamentals#introduction","content":" Theory of Computation (TOC) is the study of abstract models of computation and the fundamental limits of what can be computed. It seeks to understand the nature of computation, the power, and limitations of different computational models, and the relationships between different classes of problems.  This field is further divided into three :  Automata Theory &amp; Formal Language : Automata theory is the study of abstract models of computation or machines used to solve computational problems. Formal language is concerned about formalizing a natural language by specifying with symbols or character, following a set of rules called the formal grammar. Automata are designed to recognize formal language. In other word, the formal language is used as the input and output for the abstract model of computation. Different types of automata recognize different types of languages.Computability Theory : Computability theory focuses on the study of what can be computed and the limits of computation. It deals with the question of which problems can be solved algorithmically and which problems are unsolvable. Problems are analyzed with abstract model of computation along with other formal systems.Computational Complexity Theory : Computational complexity theory focuses on understanding the resources required to solve computational problems. It focuses on classifying problems into different complexity classes based on the amount of time, space, or other resources needed to solve them.  ","version":"Next","tagName":"h3"},{"title":"Foundational Concepts​","type":1,"pageTitle":"TOC Fundamentals","url":"/cs-notes/theory-of-computation-and-automata/toc-fundamentals#foundational-concepts","content":" TOC uses much math.  Notation​  TOC uses mathematical logics as the tool to formalize and analyze computational concepts. Therefore, there are various mathematical notation used here. These notations will be covered later.    Source : Book notation part  Sets​  In mathematical logic, set is a collection of objects. The objects within a set can be anything from numbers, letters, other sets, or even more abstract mathematical objects.  Sets are typically denoted using curly braces {}\\{\\}{}. For example, the set of natural numbers less than 5 can be written as {0,1,2,3,4}\\{0, 1, 2, 3, 4\\}{0,1,2,3,4}, where each number is an element of the set. If an object xxx is an element of a set AAA, it is represented as x∈Ax \\in Ax∈A, and if xxx is not an element of AAA, it is represented as x∉Ax \\notin Ax∈/A.  Sets can be finite or infinite, depending on the number of elements they contain. For example, the set of all even numbers is an infinite set, while the set of colors in a rainbow is a finite set.  Set operations and terminologies :  Empty set : A set is empty, denoted as ∅\\varnothing∅ if it contains no elements. Union : The union of two sets AAA and BBB, denoted by A∪BA \\cup BA∪B, is the set that contains all the elements that are in either set A or set B, or in both. For example, if #A={1,2,3}A = \\{1, 2, 3\\}A={1,2,3} and B={3,4,5}B = \\{3, 4, 5\\}B={3,4,5}, then A∪B={1,2,3,4,5}A \\cup B = \\{1, 2, 3, 4, 5\\}A∪B={1,2,3,4,5}. Intersection : The intersection of two sets AAA and BBB, denoted by A∩BA \\cap BA∩B, is the set that contains all the elements that are common to both AAA and BBB. For example, if A={1,2,3}A = \\{1, 2, 3\\}A={1,2,3} and B={3,4,5}B = \\{3, 4, 5\\}B={3,4,5}, then A∩B={3}A \\cap B = \\{3\\}A∩B={3}. Set difference : A−BA - BA−B denotes the difference between set AAA and BBB. It is a set that contains everything in AAA, but not in BBB. For example, if A={1,3,9}A = \\{1, 3, 9\\}A={1,3,9} and B={3,5}B = \\{3, 5\\}B={3,5}, then A−B={1,9}A - B = \\{1, 9\\}A−B={1,9}. Complement : Denoted by ACA^{C}AC, is a set that contains everything that is not in AAA. Properties : Set has some properties as follows. Source : Book page 2-3 Subset : A set AAA is said to be a subset of a set BBB if every element of AAA is also an element of BBB. If A={1,2}A = \\{1, 2\\}A={1,2} and B={1,2,3}B = \\{1, 2, 3\\}B={1,2,3}, then AAA is a subset of BBB (A⊆BA \\subseteq BA⊆B). It is possible for a set to be a subset of another set without being equal to it. If AAA is a subset of BBB and there exists at least one element in BBB that is not in AAA, then AAA is called a proper subset of BBB, denoted as A⊂BA \\subset BA⊂B. Superset : Conversely, a set BBB is said to be a superset of a set AAA if every element of AAA is also an element of BBB. In other words, if every element xxx in AAA satisfies the condition x∈Bx \\in Bx∈B, then BBB is a superset of AAA, denoted as B⊇AB \\supseteq AB⊇A. Similar to subsets, if BBB is a superset of AAA and there exists at least one element in BBB that is not in AAA, then BBB is a proper superset of AAA, denoted as B⊃AB \\supset AB⊃A. Disjoint sets : Disjoint sets are sets that have no elements in common. In other words, two sets AAA and BBB are disjoint if their intersection is the empty set ∅\\varnothing∅. For example, if A={1,2,3}A = \\{1, 2, 3\\}A={1,2,3} and B={4,5,6}B = \\{4, 5, 6\\}B={4,5,6}, then AAA and BBB are disjoint sets because their intersection is ∅\\varnothing∅. Cardinality : The number of element in some set. The cardinality of a set AAA is denoted as ∣A∣|A|∣A∣. Powerset : The powerset of a set AAA, denoted as P(A)P(A)P(A), is the set that contains all possible subsets of AAA, including the empty set ∅\\varnothing∅ and the set AAA itself. For example, if A={1,2}A = \\{1, 2\\}A={1,2}, then P(A)={∅,{1},{2},{1,2}}P(A) = \\{\\varnothing, \\{1\\}, \\{2\\}, \\{1, 2\\}\\}P(A)={∅,{1},{2},{1,2}}. Cartesian Product : The Cartesian product of two sets AAA and BBB, denoted as A×BA \\times BA×B, is the set of all possible ordered pairs where the first element comes from AAA and the second element comes from BBB. In other words, it combines every element of AAA with every element of BBB. For example, if A={1,2}A = \\{1, 2\\}A={1,2} and B={a,b}B = \\{a, b\\}B={a,b}, then A×B={(1,a),(1,b),(2,a),(2,b)}A \\times B = \\{(1, a), (1, b), (2, a), (2, b)\\}A×B={(1,a),(1,b),(2,a),(2,b)}.  Relations​  Relation is a set of ordered pairs that relates elements from one set, called the domain, to elements of another set, called the range or codomain. More specifically, a relation on sets SSS and TTT is a set of ordered pairs (s,t)(s, t)(s,t), where :  s∈Ss \\in Ss∈St∈Tt \\in Tt∈TSSS and TTT need not be differentThe set of all first elements in the “domain” of the relation, andThe set of all second elements is the “range” of the relation  Two sets A={1,2,3}A = \\{1, 2, 3\\}A={1,2,3} and B={a,b,c}B = \\{a, b, c\\}B={a,b,c}. A relation RRR from AAA to BBB could be {(1,a),(2,b),(3,c)}\\{(1, a), (2, b), (3, c)\\}{(1,a),(2,b),(3,c)}. This relation relates each element from set AAA with a corresponding element from set BBB.   Source : Book page 8-9  Equivalence Relations : An equivalence relation is a relation that satisfies three properties : Reflexivity : Every element is related to itself. For all elements aaa in the set, (a,a)(a, a)(a,a) is in the relation.Symmetry : If one element is related to another, then the other is related to the first. For all elements aaa and bbb in the set, if (a,b)(a, b)(a,b) is in the relation, then (b,a)(b, a)(b,a) is also in the relation.Transitivity : If one element is related to a second, and the second is related to a third, then the first is related to the third. For all elements aaa, bbb, and ccc in the set, if (a,b)(a, b)(a,b) is in the relation and (b,c)(b, c)(b,c) is in the relation, then (a,c)(a, c)(a,c) is also in the relation. Partial Ordering Relation : A relation that satisfies three properties : reflexivity, antisymmetry, and transitivity. A set with a relation being partial order is called partially ordered set. Antisymmetry : The opposite of symmetry, for all elements aaa and bbb in the set, if (a,b)(a, b)(a,b) is in the relation, then (b,a)(b, a)(b,a) must not be in the relation, unless a=ba = ba=b. Partition : A partition of a set is a collection of non-empty, disjoint subsets (called blocks) that cover the entire set. Each element of the set belongs to exactly one block of the partition. A set {1,2,3,4,5}\\{1, 2, 3, 4, 5\\}{1,2,3,4,5} can be partitioned into {{1,4},{2,5},{3}}\\{\\{1, 4\\}, \\{2, 5\\}, \\{3\\}\\}{{1,4},{2,5},{3}}.  Functions​  A function is a special type of relation that assigns a unique element from the range to each element in the domain. In other words, it relates each element in the domain to exactly one element in the range. Functions are often represented using mappings or formulas.  For example, the function f:A→Bf: A \\rightarrow Bf:A→B is defined by f(1)=af(1) = af(1)=a, f(2)=bf(2) = bf(2)=b, and f(3)=cf(3) = cf(3)=c. This function assigns a unique element from set BBB to each element in set AAA.   Source : Book page 12  Kind of functions :  Injection (one-to-one) : A function in which each element in the domain is mapped to a unique element in the range. In other words, no two distinct elements in the domain are mapped to the same element in the range. Surjection (onto) : A function in which every element in the range is mapped to by at least one element in the domain. In other words, the range of the function covers the entire range. Bijection (one-to-one and onto) : A function that is both injective and surjective. In other words, it is a function where each element in the domain is mapped to a unique element in the codomain, and every element in the codomain is mapped to by exactly one element in the domain. Invertible function : A function f:A→Bf: A \\rightarrow Bf:A→B is invertible if there exist an inverse relation f−1f^{-1}f−1 that maps from BBB to AAA. Source : Book page 12-13  Graphs​  Formally, a graph GGG is defined as a triple of (V,E,γ)(V, E, \\gamma)(V,E,γ), where VVV is a finite set of objects called vertices, EEE is another finite set of objects called edges, and γ\\gammaγ is a function that assigns to each edge in a graph a subset {v,w}\\{v, w\\}{v,w}, where vvv and www are vertices of the graph.   Source : Book page 15  See graph for terminologies and types of graph. In addition, we can say a graph is a tree if it is connected and has no simple cycles.  Strings &amp; Languages​  A string is a finite sequence of symbols chosen from a given alphabet. An alphabet aaa, denoted by Σa\\Sigma_aΣa​ is a finite set of symbols. The symbols can be characters, numbers, or any other discrete elements. For example, if we have Σa\\Sigma_aΣa​ consisting of the symbols {0,1}\\{0, 1\\}{0,1}, then &quot;0101&quot; and &quot;111&quot; are the example of strings over Σa\\Sigma_aΣa​.  Length : The length of a string is the length of its sequence. The length of string www is denoted as ∣w∣|w|∣w∣. Empty string : A string with zero length, denoted as ϵ\\epsilonϵ. Reverse string : The reverse of a string. If w=w1w2...wnw = w_1 w_2 ...w_nw=w1​w2​...wn​, where each wi∈Σw_i \\in \\Sigmawi​∈Σ, then the reverse is wnwn−1...w1w_n w_{n-1} ... w_1wn​wn−1​...w1​. Substring : A substring is a contiguous sequence of characters within a string. As an example, &quot;deck&quot; is a substring of &quot;abcdeckabcjkl&quot;. Concatenation : An operation of combining them together to form a new string. Suppose string xxx with length of mmm and string yyy with length of nnn. The concatenation between them is written as xyxyxy, which is obtained by appending yyy to the end of xxx, that is x1x2...xm y1y2...ynx_1 x_2...x_m \\space y_1 y_2...y_nx1​x2​...xm​ y1​y2​...yn​. In the case of concatenation of a string with itself, it is denoted as xkx^kxk, where kkk is the number of repetition. A concatenation of w=abcw = abcw=abc with empty string ϵ\\epsilonϵ will be wϵ=abcw\\epsilon = abcwϵ=abc. Prefix : Substring that appears at the beginning of the string. If w=vyw = vyw=vy for some yyy, then vvv is a suffix of www. Suffix : Substring that appears at the end of the string. If w=xvw = xvw=xv for some xxx, then vvv is a prefix of www. Lexicographical ordering : Lexicographical ordering is a way to compare strings based on their alphabetical order. For example, &quot;apple&quot; comes before &quot;banana&quot; and &quot;01&quot; comes before &quot;10&quot; in lexicographical ordering. The lexicographic ordering of all strings over the alphabet {0,1}\\{0, 1\\}{0,1} is (ϵ,0,1,00,01,10,11,000,...)(\\epsilon, 0, 1, 00, 01, 10, 11, 000, ...)(ϵ,0,1,00,01,10,11,000,...). Language : Formally, it is any set of strings over an alphabet. The set of all strings, including the empty string over an alphabet Σ\\SigmaΣ is denoted as Σ∗\\Sigma^*Σ∗. A language can consist of string that follows some property. Examples : L1={w∈{0,1}∗:w has an equal number of 0’s and 1’s}L_1 = \\{w \\in \\{0, 1\\}^* : w \\text{ has an equal number of 0's and 1's}\\}L1​={w∈{0,1}∗:w has an equal number of 0’s and 1’s}. Language L1L_1L1​ consist set of string that has the equal number of zeros and ones. For example, some strings that would belong to L1L_1L1​ are &quot;01&quot;, &quot;0011&quot;, &quot;1100&quot;, &quot;000111&quot;, and so on.L2={w∈Σ∗:w=wR}L_2 = \\{w \\in \\Sigma^*: w = w^R\\}L2​={w∈Σ∗:w=wR}, where wRw^RwR is the reverse string of www. This language consist of string where its original is equal to its reverse (i.e., a palindrome, such as racecar, poop.). Language with power : A language with a power Σn\\Sigma^nΣn, where Σ={0,1}\\Sigma = \\{0, 1\\}Σ={0,1} describe the set of all strings of length nnn under language Σ\\SigmaΣ. Σ0={ϵ}\\Sigma^0 = \\{\\epsilon \\}Σ0={ϵ} (consist of just empty string).Σ1={0,1}\\Sigma^1 = \\{0, 1 \\}Σ1={0,1}.and so on... We can also write Σ∗\\Sigma^*Σ∗, a set of all strings over an alphabet Σ\\SigmaΣ as Σ∗=Σ0∪Σ1∪Σ2...\\Sigma^* = \\Sigma^0 \\cup \\Sigma^1 \\cup \\Sigma^2 ...Σ∗=Σ0∪Σ1∪Σ2... (infinite). Language concatenation : If L1L_1L1​ and L2L_2L2​ are languages over alphabet Σ\\SigmaΣ, their concatenation is written as L=L1⋅L2L = L_1 \\cdot L_2L=L1​⋅L2​, or simply L=L1L2L = L_1 L_2L=L1​L2​, where L={w∈Σ∗:w=x⋅y for some x∈L1, and y∈L2}L = \\{w \\in \\Sigma^*: w = x \\cdot y \\text{ for some } x \\in L_1 \\text{, and } y \\in L_2\\}L={w∈Σ∗:w=x⋅y for some x∈L1​, and y∈L2​}. For example, given Σ={0,1}\\Sigma = \\{0, 1\\}Σ={0,1}, L1={w∈Σ∗:w has an even number of 0’s}L_1 = \\{w \\in \\Sigma^*: w \\text{ has an even number of 0's} \\}L1​={w∈Σ∗:w has an even number of 0’s} L2={w:w starts with a 0 and the rest of the symbols are 1’s}L_2 = \\{w: w \\text{ starts with a 0 and the rest of the symbols are 1's} \\}L2​={w:w starts with a 0 and the rest of the symbols are 1’s} then L1L2={w:w has an odd number of 0’s}L_1 L_2 = \\{w: w \\text{ has an odd number of 0's} \\}L1​L2​={w:w has an odd number of 0’s}. This is true because L1L_1L1​ itself consist of even number of 0's, concatenating it with L2L_2L2​, which mean adding 1 zero will make any language there have the odd number of zeros. Kleene Star : A language operation denoted as L∗L^*L∗, which is the set of all strings obtained by concatenating zero or more strings from LLL. More specifically, L∗={w∈Σ∗:w=w1⋅...⋅wk for some k≥0 and some w1,w2,...,wk∈L}L^* = \\{w \\in \\Sigma^* : w = w_1 \\cdot ... \\cdot w_k \\text{ for some } k \\ge 0 \\text{ and some } w_1, w_2, ..., w_k \\in L \\}L∗={w∈Σ∗:w=w1​⋅...⋅wk​ for some k≥0 and some w1​,w2​,...,wk​∈L}. For example, from a language L={01,1,100}L = \\{01, 1, 100\\}L={01,1,100}, we can form &quot;110001110011&quot;, since it can be formed by the concatenation of 1⋅100⋅01⋅1⋅100⋅1⋅11 \\cdot 100 \\cdot 01 \\cdot 1 \\cdot 100 \\cdot 1 \\cdot 11⋅100⋅01⋅1⋅100⋅1⋅1.  Boolean Logic​  See boolean logic.  Grammar​  A grammar is simply a mechanism to describe a language, or specifically a set of production rules that describe the structure of a formal language. A grammar consists of a quadruple (V,T,S,P)(V, T, S, P)(V,T,S,P), where :  VVV : Finite set of objects called variables or non-terminal symbols. These symbols represent different syntactic categories or components that can be combined to form valid strings in the language. Examples of non-terminal symbols can include syntactic categories like noun, verb, sentence, or expression.TTT or Σ\\SigmaΣ : Finite set of objects called terminal symbols. These symbols represent the basic building blocks or atomic units of the language. Terminals are the actual characters or tokens that appear in the strings of the language. Examples of terminal symbols include individual letters, digits, or punctuation marks.PPP, in the form of x→yx \\rightarrow yx→y : Finite set of productions rule. Each production rule specifies how a non-terminal symbol can be replaced by a sequence of terminals and non-terminals. It defines the valid ways to construct strings in the language.SSS, where S∈VS \\in VS∈V : A special non-terminal symbol that represents the starting point, called start variables. The goal is to generate valid strings in the language by repeatedly applying production rules, starting from the start symbol.  If we have a string www in the form w=uxvw = uxvw=uxv, and we apply the production rule x→yx \\rightarrow yx→y, it means that we replace the substring xxx in www with the string yyy to obtain a new string z=uyvz = uyvz=uyv. We call a sequence of production rule applications that transforms the start symbol SSS into a specific string www a derivation.  tip The set of all strings obtained by using production rules is the languages generated by the grammar.  Consider a grammar G=(V,T,S,P)G = (V, T, S, P)G=(V,T,S,P) and L(G)={w∈T∗:S⇒∗w}L(G) = \\{w \\in T^* : S \\xRightarrow{*} w \\}L(G)={w∈T∗:S∗​w}. This expression represents the language generated by the grammar GGG, comprising all strings that can be derived from the start symbol SSS according to the production rules defined in PPP.  T∗T^*T∗ represents the set of all possible strings that can be formed using the terminal symbols TTT.S⇒∗wS \\xRightarrow{*} wS∗​w denotes that the start symbol SSS can be derived or rewritten to produce the string www using a sequence of production rule applications. The S⇒∗wS \\xRightarrow{*} wS∗​w symbol indicates zero or more rewriting steps.  If a string WWW belong to the grammar, or W∈L(G)W \\in L(G)W∈L(G), then there exists a sequence S⇒w1⇒w2⇒w3...⇒wn⇒wS \\Rightarrow w_1 \\Rightarrow w_2 \\Rightarrow w_3 ... \\Rightarrow w_n \\Rightarrow wS⇒w1​⇒w2​⇒w3​...⇒wn​⇒w, which is a sequence of production rule applications (or a derivation) that transforms the start symbol SSS into the string www.  During the derivation process, we call intermediate strings produced a sentential form. Therefore, with the string SSS, we call w1,w2,...,wnw_1, w_2, ..., w_nw1​,w2​,...,wn​ a sentential form of the derivation.  tip See also Formal grammar. ","version":"Next","tagName":"h3"}],"options":{"id":"default"}}